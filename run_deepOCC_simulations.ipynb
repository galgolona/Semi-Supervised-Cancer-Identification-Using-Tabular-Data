{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f8de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 march 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daae2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "62e76a81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imblearn in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (0.0)\r\n",
      "Requirement already satisfied: imbalanced-learn in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from imblearn) (0.10.1)\r\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.23.5)\r\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.10.0)\r\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.3.2)\r\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (1.1.1)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from imbalanced-learn->imblearn) (2.2.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450e1e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy.linalg import lstsq\n",
    "#np.set_printoptions(suppress=True)\n",
    "#from matplotlib import colors\n",
    "#from scipy.optimize import nnls\n",
    "#import plotly.graph_objects as go\n",
    "#import tensorflow as tf\n",
    "#import seaborn as sns\n",
    "#from matplotlib import colors as clrs\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb2335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%ls\n",
    "#%cd /content/drive\n",
    "#%cd MyDrive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93696409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/glrz/Desktop/Thesis/src\n"
     ]
    }
   ],
   "source": [
    "cd /Users/glrz/Desktop/Thesis/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcb795b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DeepSAD.py              baseline_kde.py         \u001b[34mnetworks\u001b[m\u001b[m/\r\n",
      "Files introduction.rtf  baseline_ocsvm.py       \u001b[34moptim\u001b[m\u001b[m/\r\n",
      "__init__.py             baseline_ssad.py        requirements.txt\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m/            \u001b[34mbaselines\u001b[m\u001b[m/              test_main.py\r\n",
      "\u001b[34mbase\u001b[m\u001b[m/                   \u001b[34mdatasets\u001b[m\u001b[m/               \u001b[34mutils\u001b[m\u001b[m/\r\n",
      "baseline_SemiDGM.py     \u001b[34mlog\u001b[m\u001b[m/\r\n",
      "baseline_isoforest.py   main.py\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afd36508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pipreqs in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (0.5.0)\n",
      "Requirement already satisfied: docopt==0.6.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from pipreqs) (0.6.2)\n",
      "Requirement already satisfied: ipython==8.12.3 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from pipreqs) (8.12.3)\n",
      "Requirement already satisfied: nbconvert<8.0.0,>=7.11.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from pipreqs) (7.16.4)\n",
      "Requirement already satisfied: yarg==0.1.9 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from pipreqs) (0.1.9)\n",
      "Requirement already satisfied: backcall in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
      "Requirement already satisfied: decorator in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (3.0.36)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (2.11.2)\n",
      "Requirement already satisfied: stack-data in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (0.2.0)\n",
      "Requirement already satisfied: traitlets>=5 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (5.7.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (4.4.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (4.8.0)\n",
      "Requirement already satisfied: appnope in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from ipython==8.12.3->pipreqs) (0.1.2)\n",
      "Requirement already satisfied: requests in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from yarg==0.1.9->pipreqs) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (6.1.0)\n",
      "Requirement already satisfied: defusedxml in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.7.1)\n",
      "Requirement already satisfied: importlib-metadata>=3.6 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (8.5.0)\n",
      "Requirement already satisfied: jinja2>=3.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.1.4)\n",
      "Requirement already satisfied: jupyter-core>=4.7 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.2.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.3.0)\n",
      "Requirement already satisfied: markupsafe>=2.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (2.1.5)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (3.0.2)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (0.10.0)\n",
      "Requirement already satisfied: nbformat>=5.7 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (5.10.4)\n",
      "Requirement already satisfied: packaging in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (23.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.1)\n",
      "Requirement already satisfied: tinycss2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbconvert<8.0.0,>=7.11.0->pipreqs) (1.4.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from bleach!=5.0.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.5.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from importlib-metadata>=3.6->nbconvert<8.0.0,>=7.11.0->pipreqs) (3.20.2)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jedi>=0.16->ipython==8.12.3->pipreqs) (0.8.3)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jupyter-core>=4.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.5.2)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (7.4.9)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.20.0)\n",
      "Requirement already satisfied: jsonschema>=2.6 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (4.23.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from pexpect>4.3->ipython==8.12.3->pipreqs) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython==8.12.3->pipreqs) (0.2.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from beautifulsoup4->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from requests->yarg==0.1.9->pipreqs) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from requests->yarg==0.1.9->pipreqs) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from requests->yarg==0.1.9->pipreqs) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from requests->yarg==0.1.9->pipreqs) (2022.12.7)\n",
      "Requirement already satisfied: executing in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from stack-data->ipython==8.12.3->pipreqs) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from stack-data->ipython==8.12.3->pipreqs) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from stack-data->ipython==8.12.3->pipreqs) (0.2.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (24.2.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (5.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (2023.12.1)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.3.10)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.20.1)\n",
      "Requirement already satisfied: entrypoints in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (0.4)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.4 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (1.5.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=23.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (23.2.0)\n",
      "Requirement already satisfied: tornado>=6.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert<8.0.0,>=7.11.0->pipreqs) (6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pipreqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3950d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: appdirs==1.4.4 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from -r requirements.txt (line 4)) (1.4.4)\n",
      "Requirement already satisfied: appnope==0.1.2 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from -r requirements.txt (line 5)) (0.1.2)\n",
      "Requirement already satisfied: asttokens==2.0.5 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from -r requirements.txt (line 6)) (2.0.5)\n",
      "Requirement already satisfied: backcall==0.2.0 in /Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages (from -r requirements.txt (line 7)) (0.2.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement blas==1.0 (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for blas==1.0\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "35a0452b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "Collecting package metadata (repodata.json): | WARNING conda.models.version:get_matcher(546): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.9.0.*, but conda is ignoring the .* and treating it as 1.9.0\n",
      "WARNING conda.models.version:get_matcher(546): Using .* with relational operator is superfluous and deprecated and will be removed in a future version of conda. Your spec was 1.8.0.*, but conda is ignoring the .* and treating it as 1.8.0\n",
      "done\n",
      "Solving environment: failed with initial frozen solve. Retrying with flexible solve.\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - click==7.1.2=pypi_0\n",
      "  - imblearn==0.0=pypi_0\n",
      "  - imbalanced-learn==0.10.1=pypi_0\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://repo.anaconda.com/pkgs/main/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/osx-arm64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://conda.anaconda.org/conda-forge/osx-arm64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install --file requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c7728c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0342a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updating Homebrew...\u001b[0m\n",
      "Adjust how often this is run with HOMEBREW_AUTO_UPDATE_SECS or disable with\n",
      "HOMEBREW_NO_AUTO_UPDATE. Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://ghcr.io/v2/homebrew/portable-ruby/portable-ruby/blobs/sha256:303bed4c7fc431a685db3c3c151d873740114adbdccd23762ea2d1e39ea78f47\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mPouring portable-ruby-3.3.6.arm64_big_sur.bottle.tar.gz\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 2 taps (homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "action-docs                graphviz2drawio            packetry\n",
      "afl++                      grizzly                    paperjam\n",
      "age-plugin-se              h26forge                   passt\n",
      "aicommit                   hawkeye                    pcaudiolib\n",
      "aider                      http-server-rs             pgcopydb\n",
      "ansible-builder            icu4c@74                   pipet\n",
      "ansible@9                  icu4c@75                   polkit\n",
      "argtable3                  icu4c@76                   porter\n",
      "asak                       immich-cli                 postgresql@17\n",
      "asm6809                    inchi                      poutine\n",
      "awsdac                     iowow                      probe-rs-tools\n",
      "azqr                       ipsw                       progressline\n",
      "basedpyright               jikken                     pug\n",
      "bc-gh                      js-beautify                pulsarctl\n",
      "beautysh                   jsbeautifier               pytest\n",
      "bed                        jxl-oxide                  python-freethreading\n",
      "bibtex-tidy                kamal-proxy                python-gdbm@3.13\n",
      "binsider                   kanata                     python-tk@3.13\n",
      "blisp                      kaskade                    python@3.13\n",
      "boost@1.85                 kea                        pyupgrade\n",
      "boring                     keep-sorted                qrtool\n",
      "bump-my-version            ktor                       recc\n",
      "carapace                   kty                        rip2\n",
      "chkbit                     kubehound                  roxctl\n",
      "clang-uml                  kubelogin                  rpcsvc-proto\n",
      "clangql                    kubernetes-cli@1.30        rsgain\n",
      "clickhouse-sql-parser      kubetail                   rust-script\n",
      "cmrc                       kubevpn                    ryelang\n",
      "codecov-cli                kuzco                      safety\n",
      "comtrya                    lando-cli                  sequoia-sq\n",
      "coordgen                   langgraph-cli              serpl\n",
      "cortexso                   lbfgspp                    setconf\n",
      "cotila                     libassuan@2                sf\n",
      "crossplane                 libblastrampoline          sidekick\n",
      "crow                       libcss                     sigsum-go\n",
      "cyme                       libdex                     slackdump\n",
      "cyphernetes                libdom                     sleek\n",
      "decasify                   libgedit-gfls              soapyhackrf\n",
      "dependabot                 libgit2@1.7                spidermonkey@115\n",
      "dipc                       libhubbub                  spoofdpi\n",
      "distill-cli                libmps                     sq\n",
      "djlint                     libmsquic                  stellar-cli\n",
      "dnsgen                     libnice-gstreamer          subliminal\n",
      "dotnet@8                   libparserutils             surfer\n",
      "dra                        libspelling@0.2            sysprof\n",
      "dug                        libtatsu                   tabiew\n",
      "dwarfs                     libxpresent                tcl-tk@8\n",
      "ecs-deploy                 litmusctl                  tdb\n",
      "egctl                      lld                        termscp\n",
      "envelope                   llgo                       terrahash\n",
      "epoll-shim                 llvm@18                    terramaid\n",
      "erlang@26                  localai                    testscript\n",
      "facad                      m68k-elf-binutils          tevent\n",
      "fast_float                 m68k-elf-gcc               tex-fmt\n",
      "fatal                      maeparser                  tinymist\n",
      "fcft                       mako                       tllist\n",
      "fierce                     mariadb@11.4               tmpmail\n",
      "fileql                     markdown-oxide             tmux-sessionizer\n",
      "firefly                    mbpoll                     toml2json\n",
      "flang                      minijinja-cli              topfew\n",
      "flexiblas                  mkdocs-material            typos-lsp\n",
      "foot                       mysql-client@8.4           ufbt\n",
      "frizbee                    mysql@8.4                  usage\n",
      "ftnchek                    nanobind                   uvw\n",
      "gabo                       nerdfetch                  veryfasttree\n",
      "geni                       node@22                    vimtutor-sequel\n",
      "gfxutil                    nsync                      vipsdisp\n",
      "ghc@9.8                    nuspell                    wasi-libc\n",
      "git-spice                  oasdiff                    wasi-runtimes\n",
      "go@1.22                    onion-location             wasm-component-ld\n",
      "goku                       onnx                       wcurl\n",
      "golangci-lint-langserver   openapi-diff               wthrr\n",
      "gollama                    openbao                    wush\n",
      "goyacc                     otree                      xroar\n",
      "gplugin                    oxker                      zizmor\n",
      "gptme                      oxlint                     zsh-system-clipboard\n",
      "gql                        packcc\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "ableset                                  font-sour-gummy\n",
      "acronis-true-image-cleanup-tool          font-suse\n",
      "adlock                                   font-zpix\n",
      "aide-app                                 fujifilm-tether-app\n",
      "airdash                                  furtherance\n",
      "anytype@alpha                            gauntlet\n",
      "anytype@beta                             geoda\n",
      "approf                                   getoutline\n",
      "archivewebpage                           github-copilot-for-xcode\n",
      "avbeam                                   gitkraken-on-premise-serverless\n",
      "ba-connected                             gitlight\n",
      "backyard-ai                              homerow\n",
      "ball                                     huggingchat\n",
      "bbackupp                                 huly\n",
      "bentobox                                 hyperconnect\n",
      "blip                                     imaging-edge\n",
      "blood-on-the-clocktower-online           impel\n",
      "bobhelper                                inkdown\n",
      "boosteroid                               istat-menus@6\n",
      "brightvpn                                itermai\n",
      "cables                                   ivacy\n",
      "caido                                    jagex\n",
      "cap                                      jet-pilot\n",
      "ccstudio                                 k8studio\n",
      "charmstone                               kando\n",
      "choice-financial-terminal                keyguard\n",
      "clash-verge-rev                          kimis\n",
      "claude                                   kindle-create\n",
      "cocoapacketanalyzer                      label-live\n",
      "colemak-dh                               labplot\n",
      "colemak-dhk                              langgraph-studio\n",
      "cork                                     lazycat\n",
      "crashplan                                legcord\n",
      "crosspaste                               lets\n",
      "dataflare                                localcan\n",
      "default-handler                          localsend\n",
      "dfcf                                     longplay\n",
      "displaybuddy                             loop\n",
      "djuced                                   lunatask\n",
      "dockdoor                                 magicquit\n",
      "dockside                                 mailbird\n",
      "duplicateaudiofinder                     mailsteward\n",
      "ea                                       meta-quest-developer-hub\n",
      "elgato-capture-device-utility            microsoft-edge@canary\n",
      "emclient@beta                            microsoft-openjdk@21\n",
      "excalidrawz                              minstaller\n",
      "fathom                                   monarch\n",
      "feedflow                                 monokle\n",
      "find-my-ports                            morisawa-desktop-manager\n",
      "flutterflow                              mythic\n",
      "follow                                   navicat-premium-lite\n",
      "follow@alpha                             neo-network-utility\n",
      "follow@nightly                           neohtop\n",
      "font-afacad-flux                         nextcloud-vfs\n",
      "font-agave                               notchnook\n",
      "font-alumni-sans-sc                      nrf-connect\n",
      "font-batang                              orka-desktop\n",
      "font-batangche                           oxygen-xml-developer\n",
      "font-beiruti                             parallels@19\n",
      "font-big-shoulders-display-sc            pearcleaner\n",
      "font-big-shoulders-inline-display-sc     photostickies\n",
      "font-big-shoulders-inline-text-sc        pia\n",
      "font-big-shoulders-stencil-display-sc    pixel-shift-combiner\n",
      "font-big-shoulders-stencil-text-sc       plugdata@nightly\n",
      "font-big-shoulders-text-sc               positron\n",
      "font-bungee-tint                         processspy\n",
      "font-departure-mono                      productive\n",
      "font-doto                                pronotes\n",
      "font-dotum                               quba\n",
      "font-dotumche                            quickwhisper\n",
      "font-edu-au-vic-wa-nt-arrows             replaywebpage\n",
      "font-edu-au-vic-wa-nt-dots               replit\n",
      "font-edu-au-vic-wa-nt-guides             retcon\n",
      "font-edu-au-vic-wa-nt-pre                retroarch-metal@nightly\n",
      "font-eldur                               rize\n",
      "font-faculty-glyphic                     roblox\n",
      "font-fragment-mono-sc                    robloxstudio\n",
      "font-funnel-display                      rode-unify\n",
      "font-funnel-sans                         rouvy\n",
      "font-genkigothic                         sanctum\n",
      "font-goorm-sans                          silhouette-studio\n",
      "font-goorm-sans-code                     singlebox\n",
      "font-greybeard                           sketchup\n",
      "font-gulim                               sq-mixpad\n",
      "font-gulimche                            synology-image-assistant\n",
      "font-gungsuh                             teamspeak-client@beta\n",
      "font-gungsuhche                          tella\n",
      "font-host-grotesk                        thunderbird@esr\n",
      "font-ibm-plex-math                       treeviewer\n",
      "font-ibm-plex-sans-tc                    truetree\n",
      "font-lxgw-simxihei                       twingate\n",
      "font-lxgw-simzhisong                     typefully\n",
      "font-maname                              unraid-usb-creator-next\n",
      "font-matemasie                           vienna-assistant\n",
      "font-miracode                            viz\n",
      "font-moderustic                          wd-security\n",
      "font-mynaui-icons                        wealthfolio\n",
      "font-new-amsterdam                       webkinz\n",
      "font-new-computer-modern                 whimsical\n",
      "font-noto-serif-todhri                   winbox\n",
      "font-palemonasmufi-bold                  windows-app\n",
      "font-palemonasmufi-bolditalic            x32-edit\n",
      "font-palemonasmufi-italic                xmenu\n",
      "font-palemonasmufi-regular               xnapper\n",
      "font-parkinsans                          yaak\n",
      "font-recursive-desktop                   yellowdot\n",
      "font-satoshi                             zen-browser\n",
      "font-scientifica                         zipic\n",
      "font-server-mono                         zoom-m3-edit-and-play\n",
      "font-sixtyfour-convergence\n",
      "\n",
      "You have \u001b[1m1\u001b[0m outdated formula installed.\n",
      "\n",
      "libomp  is already installed but outdated (so it will be upgraded).\n",
      "\u001b[31mError:\u001b[0m Cannot install under Rosetta 2 in ARM default prefix (/opt/homebrew)!\n",
      "To rerun under ARM use:\n",
      "    arch -arm64 brew install ...\n",
      "To install under x86_64, install Homebrew into /usr/local.\n"
     ]
    }
   ],
   "source": [
    "!brew install libomp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ad65278",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'i' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mi\u001b[49m\u001b[38;5;241m.\u001b[39mvalue()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'i' is not defined"
     ]
    }
   ],
   "source": [
    "i.value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9050791",
   "metadata": {},
   "source": [
    "# Optimal construction of a dataset - proportion of labeled and unlabeled samples and dataset size (6.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ff9859",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# first simulation - test\n",
    "!python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"25_per_labeled_0_unlabeled\" --iteration_num 1 --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.25 --ratio_unknown_normal 0 --ratio_known_outlier 0.25 --ratio_unknown_outlier 0.00 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2be735a",
   "metadata": {},
   "source": [
    "# 10% labeled 0 unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917eaa88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10% labeled 0 unlabeled training set\n",
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "   echo \"Itration $i\"\n",
    "   python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"10_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.1 --ratio_unknown_normal 0 --ratio_known_outlier 0.1 --ratio_unknown_outlier 0.00 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1;\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1cd54",
   "metadata": {},
   "source": [
    "# 25% labeled 0 unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f6a956",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"25_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.25 --ratio_unknown_normal 0 --ratio_known_outlier 0.25 --ratio_unknown_outlier 0.00 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ff4043",
   "metadata": {},
   "source": [
    "# 5% labeled 0 unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0f1691",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.00 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8440264c",
   "metadata": {},
   "source": [
    "# 25% labeled 75% unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ba315",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"25_per_labeled_75_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.25 --ratio_unknown_normal 0.75 --ratio_known_outlier 0.25 --ratio_unknown_outlier 0.75 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d42a9",
   "metadata": {},
   "source": [
    "# 100% labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733e0597",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"100_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.0 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ff774a",
   "metadata": {},
   "source": [
    "# 75% labeled 25% unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0299a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"75_per_labeled_25_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.75 --ratio_unknown_normal 0.25 --ratio_known_outlier 0.75 --ratio_unknown_outlier 0.25 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed6050d",
   "metadata": {},
   "source": [
    "# 1% labeled 0% unlabeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "df1ce814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.064s | Train Loss: 0.221160 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.045s | Train Loss: 0.215344 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.042s | Train Loss: 0.209550 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.046s | Train Loss: 0.204216 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.198566 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.045s | Train Loss: 0.193373 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.044s | Train Loss: 0.187907 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.044s | Train Loss: 0.182855 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.042s | Train Loss: 0.178016 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.044s | Train Loss: 0.173331 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.043s | Train Loss: 0.168470 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.044s | Train Loss: 0.163345 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.159074 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.041s | Train Loss: 0.154519 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.045s | Train Loss: 0.150131 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.042s | Train Loss: 0.145642 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.043s | Train Loss: 0.141862 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.044s | Train Loss: 0.137579 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.042s | Train Loss: 0.134243 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.045s | Train Loss: 0.130668 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.043s | Train Loss: 0.127277 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.045s | Train Loss: 0.123622 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.046s | Train Loss: 0.120346 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.045s | Train Loss: 0.117275 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.047s | Train Loss: 0.114261 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.044s | Train Loss: 0.111567 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.046s | Train Loss: 0.108847 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.046s | Train Loss: 0.106229 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.044s | Train Loss: 0.103961 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.047s | Train Loss: 0.101275 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.048s | Train Loss: 0.098721 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.048s | Train Loss: 0.096524 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.046s | Train Loss: 0.094809 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.046s | Train Loss: 0.092243 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.048s | Train Loss: 0.090621 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.048s | Train Loss: 0.088581 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.048s | Train Loss: 0.086130 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.047s | Train Loss: 0.084136 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.047s | Train Loss: 0.082954 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.047s | Train Loss: 0.081149 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.046s | Train Loss: 0.079064 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.048s | Train Loss: 0.077972 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.046s | Train Loss: 0.076816 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.046s | Train Loss: 0.075315 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.046s | Train Loss: 0.074377 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.044s | Train Loss: 0.072151 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.046s | Train Loss: 0.071695 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.047s | Train Loss: 0.070531 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.044s | Train Loss: 0.069609 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.049s | Train Loss: 0.068132 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.044s | Train Loss: 0.067050 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.046s | Train Loss: 0.065918 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.048s | Train Loss: 0.065020 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.045s | Train Loss: 0.064749 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.047s | Train Loss: 0.063033 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.047s | Train Loss: 0.063204 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.042s | Train Loss: 0.061601 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.044s | Train Loss: 0.060354 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.045s | Train Loss: 0.059411 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.050s | Train Loss: 0.058541 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.047s | Train Loss: 0.057974 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.045s | Train Loss: 0.057396 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.042s | Train Loss: 0.055786 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.044s | Train Loss: 0.056104 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.045s | Train Loss: 0.055143 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.043s | Train Loss: 0.054669 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.047s | Train Loss: 0.054296 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.048s | Train Loss: 0.053214 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.049s | Train Loss: 0.051814 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.045s | Train Loss: 0.051837 |\n",
      "INFO:root:Pretraining Time: 3.211s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.057018\n",
      "INFO:root:Test AUC: 53.63%\n",
      "INFO:root:Test AUC: 53.63%\n",
      "INFO:root:Test Time: 0.235s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.443s | Train Loss: 1.599052 || Validation Loss: 0.017106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.418s | Train Loss: 1.457321 || Validation Loss: 0.013632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.447s | Train Loss: 1.326204 || Validation Loss: 0.012689 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.489s | Train Loss: 1.251050 || Validation Loss: 0.013763 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.472s | Train Loss: 1.110548 || Validation Loss: 0.012278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.447s | Train Loss: 1.111768 || Validation Loss: 0.012585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.477s | Train Loss: 1.043364 || Validation Loss: 0.011536 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.462s | Train Loss: 1.017503 || Validation Loss: 0.010698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.466s | Train Loss: 0.951102 || Validation Loss: 0.011465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.459s | Train Loss: 0.927310 || Validation Loss: 0.011747 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.470s | Train Loss: 0.903035 || Validation Loss: 0.010576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.472s | Train Loss: 0.888858 || Validation Loss: 0.010733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.461s | Train Loss: 0.839233 || Validation Loss: 0.010934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.520s | Train Loss: 0.825738 || Validation Loss: 0.010741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.466s | Train Loss: 0.802214 || Validation Loss: 0.010592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.500s | Train Loss: 0.800676 || Validation Loss: 0.010361 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.460s | Train Loss: 0.786292 || Validation Loss: 0.011394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.465s | Train Loss: 0.779845 || Validation Loss: 0.011002 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.527s | Train Loss: 0.780170 || Validation Loss: 0.010667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.506s | Train Loss: 0.766456 || Validation Loss: 0.010712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.471s | Train Loss: 0.740448 || Validation Loss: 0.009980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.519s | Train Loss: 0.739985 || Validation Loss: 0.011144 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.482s | Train Loss: 0.722751 || Validation Loss: 0.011476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.540s | Train Loss: 0.729585 || Validation Loss: 0.010822 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.520s | Train Loss: 0.709280 || Validation Loss: 0.010961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.536s | Train Loss: 0.695185 || Validation Loss: 0.011693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.541s | Train Loss: 0.691353 || Validation Loss: 0.012606 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.518s | Train Loss: 0.699436 || Validation Loss: 0.011895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.519s | Train Loss: 0.658652 || Validation Loss: 0.010682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.543s | Train Loss: 0.663432 || Validation Loss: 0.010899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.571s | Train Loss: 0.659772 || Validation Loss: 0.012423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.502s | Train Loss: 0.668763 || Validation Loss: 0.011521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.593s | Train Loss: 0.655145 || Validation Loss: 0.011356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.506s | Train Loss: 0.645762 || Validation Loss: 0.011891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.574s | Train Loss: 0.652247 || Validation Loss: 0.010728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.480s | Train Loss: 0.649245 || Validation Loss: 0.011776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.537s | Train Loss: 0.625264 || Validation Loss: 0.011636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.483s | Train Loss: 0.620831 || Validation Loss: 0.012158 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.584s | Train Loss: 0.617528 || Validation Loss: 0.012691 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.553s | Train Loss: 0.640096 || Validation Loss: 0.011730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.524s | Train Loss: 0.627817 || Validation Loss: 0.012241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.495s | Train Loss: 0.619544 || Validation Loss: 0.011194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.552s | Train Loss: 0.599297 || Validation Loss: 0.012620 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.528s | Train Loss: 0.602015 || Validation Loss: 0.011609 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.513s | Train Loss: 0.617304 || Validation Loss: 0.011871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.522s | Train Loss: 0.591391 || Validation Loss: 0.012417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.552s | Train Loss: 0.582445 || Validation Loss: 0.013181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.519s | Train Loss: 0.585515 || Validation Loss: 0.012289 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.533s | Train Loss: 0.597421 || Validation Loss: 0.013550 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.530s | Train Loss: 0.588659 || Validation Loss: 0.013344 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.526s | Train Loss: 0.582277 || Validation Loss: 0.013140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.521s | Train Loss: 0.579463 || Validation Loss: 0.013009 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.559s | Train Loss: 0.577274 || Validation Loss: 0.013606 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.527s | Train Loss: 0.586470 || Validation Loss: 0.012961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.528s | Train Loss: 0.583130 || Validation Loss: 0.011654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.526s | Train Loss: 0.585775 || Validation Loss: 0.012940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.556s | Train Loss: 0.580628 || Validation Loss: 0.013160 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.567s | Train Loss: 0.601160 || Validation Loss: 0.012771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.533s | Train Loss: 0.566066 || Validation Loss: 0.012599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.572s | Train Loss: 0.580375 || Validation Loss: 0.014501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.530s | Train Loss: 0.573795 || Validation Loss: 0.013281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.572s | Train Loss: 0.592834 || Validation Loss: 0.013513 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.542s | Train Loss: 0.576508 || Validation Loss: 0.012093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.574s | Train Loss: 0.556998 || Validation Loss: 0.013856 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.546s | Train Loss: 0.582074 || Validation Loss: 0.012933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.624s | Train Loss: 0.583391 || Validation Loss: 0.012639 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.548s | Train Loss: 0.565902 || Validation Loss: 0.012759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.590s | Train Loss: 0.584351 || Validation Loss: 0.013420 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.583s | Train Loss: 0.579626 || Validation Loss: 0.012597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.559s | Train Loss: 0.582322 || Validation Loss: 0.013299 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 36.586s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.870%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.011773\n",
      "INFO:root:Test AUC: 83.50%\n",
      "INFO:root:Test Time: 0.204s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5171818590208731\n",
      "validation AUC 0.5473255592807473\n",
      "validation AUC 0.5806120335637626\n",
      "validation AUC 0.6134219024815226\n",
      "validation AUC 0.6425877647352312\n",
      "validation AUC 0.6657600329541471\n",
      "validation AUC 0.6853959936084025\n",
      "validation AUC 0.7024892944785733\n",
      "validation AUC 0.7164847522681251\n",
      "validation AUC 0.727916174351746\n",
      "validation AUC 0.7372840324151992\n",
      "validation AUC 0.7464388975366978\n",
      "validation AUC 0.7537921587776016\n",
      "validation AUC 0.7602772367132442\n",
      "validation AUC 0.7663983945017105\n",
      "validation AUC 0.7717333100186609\n",
      "validation AUC 0.7765168683002688\n",
      "validation AUC 0.7811255000220305\n",
      "validation AUC 0.7857461553672455\n",
      "validation AUC 0.7894757375571062\n",
      "validation AUC 0.7936703564654475\n",
      "validation AUC 0.7969298732472223\n",
      "validation AUC 0.800003900560297\n",
      "validation AUC 0.8026499464777144\n",
      "validation AUC 0.804999898361935\n",
      "validation AUC 0.8074193552506863\n",
      "validation AUC 0.8086010680404585\n",
      "validation AUC 0.8100370276517265\n",
      "validation AUC 0.8117865859252459\n",
      "validation AUC 0.8140258321390417\n",
      "validation AUC 0.8152328852673336\n",
      "validation AUC 0.8162061415785253\n",
      "validation AUC 0.8177890347108361\n",
      "validation AUC 0.8195441750958856\n",
      "validation AUC 0.8210525079272369\n",
      "validation AUC 0.8211691070090249\n",
      "validation AUC 0.8223145227704386\n",
      "validation AUC 0.8232164036174211\n",
      "validation AUC 0.8242502383439232\n",
      "validation AUC 0.8249401319868721\n",
      "validation AUC 0.8255104439231399\n",
      "validation AUC 0.8260561631726231\n",
      "validation AUC 0.8268395132824455\n",
      "validation AUC 0.827866847962013\n",
      "validation AUC 0.8285758931963799\n",
      "validation AUC 0.8292097315839808\n",
      "validation AUC 0.8296996483429377\n",
      "validation AUC 0.8297870570788731\n",
      "validation AUC 0.8306459439600702\n",
      "validation AUC 0.8307035158043466\n",
      "validation AUC 0.8307723210494243\n",
      "validation AUC 0.8308386358958401\n",
      "validation AUC 0.8309195658702996\n",
      "validation AUC 0.8309428149125071\n",
      "validation AUC 0.8309654333730024\n",
      "validation AUC 0.8309713773373297\n",
      "validation AUC 0.8310222549048615\n",
      "validation AUC 0.8310963868359655\n",
      "validation AUC 0.8311392663924106\n",
      "validation AUC 0.8311538522529488\n",
      "validation AUC 0.8311982643624165\n",
      "validation AUC 0.8312363812974892\n",
      "validation AUC 0.8312713932162819\n",
      "validation AUC 0.8312884960823048\n",
      "validation AUC 0.831250852748687\n",
      "validation AUC 0.8312037347252887\n",
      "validation AUC 0.831280987636767\n",
      "validation AUC 0.831353036253606\n",
      "validation AUC 0.8314158762488977\n",
      "validation AUC 0.8314572764659668\n",
      "validation AUC 0.8314572764659668\n",
      "train auc 0.8704821502464373\n",
      "Test AUC 0.8350146860443589\n",
      "test false positive rates [0.         0.         0.         ... 0.99649455 0.99649455 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.95968645e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.75339317e+01 2.65339317e+01 1.75542812e+01 ... 5.13701253e-02\n",
      " 5.13325408e-02 1.38932616e-02]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.060s | Train Loss: 0.216932 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.044s | Train Loss: 0.210940 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.045s | Train Loss: 0.205467 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.043s | Train Loss: 0.200054 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.194349 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.043s | Train Loss: 0.188826 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.045s | Train Loss: 0.183438 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.044s | Train Loss: 0.178644 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.044s | Train Loss: 0.173717 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.043s | Train Loss: 0.168501 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.043s | Train Loss: 0.163432 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.043s | Train Loss: 0.158501 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.154100 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.043s | Train Loss: 0.150015 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.044s | Train Loss: 0.145511 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.041s | Train Loss: 0.141608 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.044s | Train Loss: 0.136909 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.043s | Train Loss: 0.133141 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.046s | Train Loss: 0.129536 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.045s | Train Loss: 0.125941 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.045s | Train Loss: 0.122255 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.043s | Train Loss: 0.118957 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.043s | Train Loss: 0.116002 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.043s | Train Loss: 0.113019 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.043s | Train Loss: 0.110356 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.043s | Train Loss: 0.107362 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.043s | Train Loss: 0.104783 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.041s | Train Loss: 0.102569 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.042s | Train Loss: 0.100196 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.043s | Train Loss: 0.097640 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.095715 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.042s | Train Loss: 0.092763 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.043s | Train Loss: 0.090508 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.042s | Train Loss: 0.089503 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.087002 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.043s | Train Loss: 0.085422 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.043s | Train Loss: 0.083679 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.042s | Train Loss: 0.082463 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.043s | Train Loss: 0.080770 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.043s | Train Loss: 0.078979 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.042s | Train Loss: 0.077420 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.043s | Train Loss: 0.075856 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.043s | Train Loss: 0.074185 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.043s | Train Loss: 0.073659 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.042s | Train Loss: 0.072389 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.070441 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.044s | Train Loss: 0.069597 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.043s | Train Loss: 0.068684 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.043s | Train Loss: 0.067644 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.043s | Train Loss: 0.066324 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.043s | Train Loss: 0.065444 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.042s | Train Loss: 0.064707 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.042s | Train Loss: 0.063729 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.042s | Train Loss: 0.062070 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.042s | Train Loss: 0.061743 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.044s | Train Loss: 0.060954 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.046s | Train Loss: 0.060731 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.059164 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.043s | Train Loss: 0.058384 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.046s | Train Loss: 0.058322 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.047s | Train Loss: 0.056571 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.047s | Train Loss: 0.056591 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.046s | Train Loss: 0.055483 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.047s | Train Loss: 0.054930 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.042s | Train Loss: 0.054282 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.042s | Train Loss: 0.054364 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.042s | Train Loss: 0.053718 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.042s | Train Loss: 0.052016 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.042s | Train Loss: 0.052035 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.041s | Train Loss: 0.051602 |\n",
      "INFO:root:Pretraining Time: 3.059s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.054139\n",
      "INFO:root:Test AUC: 52.52%\n",
      "INFO:root:Test AUC: 52.52%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.427s | Train Loss: 1.692124 || Validation Loss: 0.017733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.429s | Train Loss: 1.522859 || Validation Loss: 0.014833 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.431s | Train Loss: 1.440390 || Validation Loss: 0.014628 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.443s | Train Loss: 1.327011 || Validation Loss: 0.014197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.450s | Train Loss: 1.259065 || Validation Loss: 0.012016 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.455s | Train Loss: 1.199996 || Validation Loss: 0.012907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.480s | Train Loss: 1.155087 || Validation Loss: 0.012735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.453s | Train Loss: 1.122349 || Validation Loss: 0.012146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.445s | Train Loss: 1.076020 || Validation Loss: 0.011521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.446s | Train Loss: 1.050776 || Validation Loss: 0.012375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.438s | Train Loss: 0.994034 || Validation Loss: 0.010968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.431s | Train Loss: 0.975055 || Validation Loss: 0.011726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.434s | Train Loss: 0.965553 || Validation Loss: 0.010652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.454s | Train Loss: 0.922102 || Validation Loss: 0.010924 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.430s | Train Loss: 0.920833 || Validation Loss: 0.010844 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.469s | Train Loss: 0.894938 || Validation Loss: 0.010245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.465s | Train Loss: 0.883708 || Validation Loss: 0.010644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.467s | Train Loss: 0.856129 || Validation Loss: 0.010899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.485s | Train Loss: 0.862056 || Validation Loss: 0.010970 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.466s | Train Loss: 0.846804 || Validation Loss: 0.010500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.449s | Train Loss: 0.838179 || Validation Loss: 0.010513 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.494s | Train Loss: 0.814937 || Validation Loss: 0.010479 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.453s | Train Loss: 0.819891 || Validation Loss: 0.009895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.478s | Train Loss: 0.808206 || Validation Loss: 0.010898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.495s | Train Loss: 0.798187 || Validation Loss: 0.009873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.492s | Train Loss: 0.771398 || Validation Loss: 0.010297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.500s | Train Loss: 0.771023 || Validation Loss: 0.010214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.498s | Train Loss: 0.771559 || Validation Loss: 0.010795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.482s | Train Loss: 0.770017 || Validation Loss: 0.009883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.526s | Train Loss: 0.747220 || Validation Loss: 0.010680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.529s | Train Loss: 0.735501 || Validation Loss: 0.010318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.464s | Train Loss: 0.739469 || Validation Loss: 0.011226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.535s | Train Loss: 0.734131 || Validation Loss: 0.011320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.463s | Train Loss: 0.722681 || Validation Loss: 0.010613 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.519s | Train Loss: 0.713736 || Validation Loss: 0.010586 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.459s | Train Loss: 0.713368 || Validation Loss: 0.010305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.523s | Train Loss: 0.718222 || Validation Loss: 0.009962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.466s | Train Loss: 0.710797 || Validation Loss: 0.010431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.526s | Train Loss: 0.699496 || Validation Loss: 0.010383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.502s | Train Loss: 0.694771 || Validation Loss: 0.010757 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.516s | Train Loss: 0.678544 || Validation Loss: 0.010398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.495s | Train Loss: 0.667853 || Validation Loss: 0.010775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.544s | Train Loss: 0.660931 || Validation Loss: 0.010880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.518s | Train Loss: 0.669811 || Validation Loss: 0.010662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.505s | Train Loss: 0.684734 || Validation Loss: 0.010819 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.501s | Train Loss: 0.665114 || Validation Loss: 0.010884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.541s | Train Loss: 0.653414 || Validation Loss: 0.011374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.502s | Train Loss: 0.658500 || Validation Loss: 0.011000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.524s | Train Loss: 0.647822 || Validation Loss: 0.011329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.516s | Train Loss: 0.640772 || Validation Loss: 0.010046 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.522s | Train Loss: 0.642580 || Validation Loss: 0.010921 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.513s | Train Loss: 0.653620 || Validation Loss: 0.010637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.542s | Train Loss: 0.638627 || Validation Loss: 0.010931 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.513s | Train Loss: 0.654171 || Validation Loss: 0.011549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.515s | Train Loss: 0.667951 || Validation Loss: 0.011074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.516s | Train Loss: 0.647050 || Validation Loss: 0.010728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.550s | Train Loss: 0.658957 || Validation Loss: 0.010466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.547s | Train Loss: 0.657695 || Validation Loss: 0.010627 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.519s | Train Loss: 0.645859 || Validation Loss: 0.011615 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.553s | Train Loss: 0.651623 || Validation Loss: 0.010200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.522s | Train Loss: 0.636458 || Validation Loss: 0.010835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.555s | Train Loss: 0.636962 || Validation Loss: 0.010706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.525s | Train Loss: 0.642614 || Validation Loss: 0.010421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.556s | Train Loss: 0.651560 || Validation Loss: 0.009986 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.524s | Train Loss: 0.633888 || Validation Loss: 0.011814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.596s | Train Loss: 0.654239 || Validation Loss: 0.011066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.530s | Train Loss: 0.653869 || Validation Loss: 0.011055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.569s | Train Loss: 0.646693 || Validation Loss: 0.010988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.566s | Train Loss: 0.645602 || Validation Loss: 0.011572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.541s | Train Loss: 0.645915 || Validation Loss: 0.010647 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 35.028s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.825%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009847\n",
      "INFO:root:Test AUC: 77.39%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.47101368058998183\n",
      "validation AUC 0.4847237722174957\n",
      "validation AUC 0.4999114418492977\n",
      "validation AUC 0.515843048903554\n",
      "validation AUC 0.5310330625964896\n",
      "validation AUC 0.5467777673597283\n",
      "validation AUC 0.5618033375812014\n",
      "validation AUC 0.5758901312738091\n",
      "validation AUC 0.589439304833949\n",
      "validation AUC 0.6025150552049013\n",
      "validation AUC 0.6147565778984995\n",
      "validation AUC 0.6274774225140128\n",
      "validation AUC 0.6388749049338203\n",
      "validation AUC 0.6491520458625006\n",
      "validation AUC 0.6581330114475326\n",
      "validation AUC 0.6680623080716801\n",
      "validation AUC 0.6767563483348279\n",
      "validation AUC 0.684494594344922\n",
      "validation AUC 0.6912912754310678\n",
      "validation AUC 0.6976513757962625\n",
      "validation AUC 0.7030799127423908\n",
      "validation AUC 0.7082387709095042\n",
      "validation AUC 0.7138021884857163\n",
      "validation AUC 0.7173796674487654\n",
      "validation AUC 0.720828087354671\n",
      "validation AUC 0.725571679526986\n",
      "validation AUC 0.7296549515404606\n",
      "validation AUC 0.7332313263203424\n",
      "validation AUC 0.7369024453373457\n",
      "validation AUC 0.7389770086182692\n",
      "validation AUC 0.7414145155759536\n",
      "validation AUC 0.7452991649927427\n",
      "validation AUC 0.7483765873364718\n",
      "validation AUC 0.7504610377129372\n",
      "validation AUC 0.7536208853345957\n",
      "validation AUC 0.7554235854163106\n",
      "validation AUC 0.7563622315717416\n",
      "validation AUC 0.7571682666591174\n",
      "validation AUC 0.759358287589109\n",
      "validation AUC 0.7603493412044121\n",
      "validation AUC 0.7621835943178895\n",
      "validation AUC 0.7652103120384383\n",
      "validation AUC 0.7676935694288025\n",
      "validation AUC 0.7683116592376867\n",
      "validation AUC 0.7687296683959681\n",
      "validation AUC 0.7698215379254725\n",
      "validation AUC 0.7711434872988604\n",
      "validation AUC 0.7709844689586707\n",
      "validation AUC 0.7720740955329698\n",
      "validation AUC 0.7721875097780077\n",
      "validation AUC 0.7723267406237023\n",
      "validation AUC 0.7726018525160372\n",
      "validation AUC 0.7727134708005013\n",
      "validation AUC 0.7727627998509167\n",
      "validation AUC 0.772892066441282\n",
      "validation AUC 0.7730712873424105\n",
      "validation AUC 0.7731591377516127\n",
      "validation AUC 0.7732594188686651\n",
      "validation AUC 0.7735274134357647\n",
      "validation AUC 0.7736578347622617\n",
      "validation AUC 0.773805223259983\n",
      "validation AUC 0.7740240510782894\n",
      "validation AUC 0.774230368368276\n",
      "validation AUC 0.7744051060196768\n",
      "validation AUC 0.7744918362816177\n",
      "validation AUC 0.7746716638183175\n",
      "validation AUC 0.7748881395934434\n",
      "validation AUC 0.7749309765789713\n",
      "validation AUC 0.7750593784475792\n",
      "validation AUC 0.7753531177769126\n",
      "validation AUC 0.7753531177769126\n",
      "train auc 0.8252653379450311\n",
      "Test AUC 0.7738835980463874\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99848413e-01\n",
      " 9.99848413e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [18.86007309 17.86007309 15.04959488 ...  0.0377191   0.03732061\n",
      "  0.03065634]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.060s | Train Loss: 0.214277 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.047s | Train Loss: 0.207539 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.045s | Train Loss: 0.201985 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.045s | Train Loss: 0.195682 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.045s | Train Loss: 0.189895 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.045s | Train Loss: 0.184395 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.045s | Train Loss: 0.179094 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.045s | Train Loss: 0.173588 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.044s | Train Loss: 0.168224 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.045s | Train Loss: 0.163583 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.045s | Train Loss: 0.158746 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.046s | Train Loss: 0.154601 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.045s | Train Loss: 0.150181 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.045s | Train Loss: 0.145586 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.045s | Train Loss: 0.141795 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.043s | Train Loss: 0.137176 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.043s | Train Loss: 0.133482 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.042s | Train Loss: 0.129640 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.043s | Train Loss: 0.126213 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.043s | Train Loss: 0.122831 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.042s | Train Loss: 0.119723 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.043s | Train Loss: 0.116364 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.043s | Train Loss: 0.113552 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.043s | Train Loss: 0.111223 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.043s | Train Loss: 0.108578 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.043s | Train Loss: 0.105621 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.044s | Train Loss: 0.102975 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.043s | Train Loss: 0.100530 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.042s | Train Loss: 0.098006 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.043s | Train Loss: 0.095682 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.093504 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.091524 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.043s | Train Loss: 0.089423 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.043s | Train Loss: 0.087494 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.084695 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.043s | Train Loss: 0.083825 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.043s | Train Loss: 0.081628 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.042s | Train Loss: 0.080673 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.043s | Train Loss: 0.078891 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.043s | Train Loss: 0.077051 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.043s | Train Loss: 0.075985 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.043s | Train Loss: 0.074692 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.043s | Train Loss: 0.072776 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.043s | Train Loss: 0.071908 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.043s | Train Loss: 0.070513 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.068668 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.043s | Train Loss: 0.068127 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.042s | Train Loss: 0.066919 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.043s | Train Loss: 0.066141 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.043s | Train Loss: 0.065175 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.044s | Train Loss: 0.064381 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.043s | Train Loss: 0.063067 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.043s | Train Loss: 0.062050 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.044s | Train Loss: 0.060666 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.043s | Train Loss: 0.060094 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.042s | Train Loss: 0.059500 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.043s | Train Loss: 0.058070 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.057324 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.042s | Train Loss: 0.056923 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.042s | Train Loss: 0.056871 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.044s | Train Loss: 0.055670 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.044s | Train Loss: 0.055542 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.044s | Train Loss: 0.055024 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.043s | Train Loss: 0.053961 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.044s | Train Loss: 0.053473 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.044s | Train Loss: 0.052955 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.043s | Train Loss: 0.051776 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.043s | Train Loss: 0.051225 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.040s | Train Loss: 0.050721 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.041s | Train Loss: 0.050733 |\n",
      "INFO:root:Pretraining Time: 3.060s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.054939\n",
      "INFO:root:Test AUC: 52.82%\n",
      "INFO:root:Test AUC: 52.82%\n",
      "INFO:root:Test Time: 0.217s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.436s | Train Loss: 1.577313 || Validation Loss: 0.015114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.411s | Train Loss: 1.443120 || Validation Loss: 0.013548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.443s | Train Loss: 1.308866 || Validation Loss: 0.012525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.441s | Train Loss: 1.250000 || Validation Loss: 0.011446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.426s | Train Loss: 1.179493 || Validation Loss: 0.012152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.443s | Train Loss: 1.128614 || Validation Loss: 0.011556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.467s | Train Loss: 1.076798 || Validation Loss: 0.011456 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.436s | Train Loss: 1.050945 || Validation Loss: 0.013143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.439s | Train Loss: 1.032225 || Validation Loss: 0.011072 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.431s | Train Loss: 1.004246 || Validation Loss: 0.011269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.445s | Train Loss: 0.983670 || Validation Loss: 0.010126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.443s | Train Loss: 0.974879 || Validation Loss: 0.009919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.432s | Train Loss: 0.949244 || Validation Loss: 0.009899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.463s | Train Loss: 0.911820 || Validation Loss: 0.010269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.435s | Train Loss: 0.880564 || Validation Loss: 0.009889 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.459s | Train Loss: 0.867034 || Validation Loss: 0.010256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.441s | Train Loss: 0.867001 || Validation Loss: 0.009758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.445s | Train Loss: 0.852347 || Validation Loss: 0.010109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.475s | Train Loss: 0.830617 || Validation Loss: 0.009411 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.483s | Train Loss: 0.817759 || Validation Loss: 0.010338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.444s | Train Loss: 0.798453 || Validation Loss: 0.010068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.474s | Train Loss: 0.809734 || Validation Loss: 0.010046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.444s | Train Loss: 0.782719 || Validation Loss: 0.010698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.477s | Train Loss: 0.773371 || Validation Loss: 0.010078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.477s | Train Loss: 0.773902 || Validation Loss: 0.010342 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.478s | Train Loss: 0.754254 || Validation Loss: 0.009356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.482s | Train Loss: 0.751147 || Validation Loss: 0.009273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.479s | Train Loss: 0.741219 || Validation Loss: 0.009886 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.488s | Train Loss: 0.733552 || Validation Loss: 0.010376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.490s | Train Loss: 0.718570 || Validation Loss: 0.009677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.537s | Train Loss: 0.705695 || Validation Loss: 0.010351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.471s | Train Loss: 0.709124 || Validation Loss: 0.010213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.535s | Train Loss: 0.716569 || Validation Loss: 0.009206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.458s | Train Loss: 0.700798 || Validation Loss: 0.009963 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.520s | Train Loss: 0.675605 || Validation Loss: 0.010484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.459s | Train Loss: 0.686643 || Validation Loss: 0.010483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.526s | Train Loss: 0.663019 || Validation Loss: 0.010327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.462s | Train Loss: 0.679038 || Validation Loss: 0.009985 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.529s | Train Loss: 0.659356 || Validation Loss: 0.010190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.494s | Train Loss: 0.651134 || Validation Loss: 0.010472 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.498s | Train Loss: 0.653994 || Validation Loss: 0.010798 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.465s | Train Loss: 0.649717 || Validation Loss: 0.010753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.533s | Train Loss: 0.638079 || Validation Loss: 0.011028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.505s | Train Loss: 0.631736 || Validation Loss: 0.009193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.509s | Train Loss: 0.624389 || Validation Loss: 0.010726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.511s | Train Loss: 0.625140 || Validation Loss: 0.011754 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.532s | Train Loss: 0.627604 || Validation Loss: 0.010544 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.508s | Train Loss: 0.615214 || Validation Loss: 0.011123 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.505s | Train Loss: 0.609151 || Validation Loss: 0.010661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.504s | Train Loss: 0.595331 || Validation Loss: 0.010813 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.510s | Train Loss: 0.591490 || Validation Loss: 0.009913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.513s | Train Loss: 0.599519 || Validation Loss: 0.010450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.540s | Train Loss: 0.602798 || Validation Loss: 0.010494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.512s | Train Loss: 0.614754 || Validation Loss: 0.010741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.544s | Train Loss: 0.590426 || Validation Loss: 0.010562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.534s | Train Loss: 0.604996 || Validation Loss: 0.011285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.575s | Train Loss: 0.602625 || Validation Loss: 0.010898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.552s | Train Loss: 0.618843 || Validation Loss: 0.010506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.523s | Train Loss: 0.602110 || Validation Loss: 0.011039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.557s | Train Loss: 0.615842 || Validation Loss: 0.010938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.522s | Train Loss: 0.594935 || Validation Loss: 0.009783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.553s | Train Loss: 0.593835 || Validation Loss: 0.010546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.539s | Train Loss: 0.599512 || Validation Loss: 0.009858 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.558s | Train Loss: 0.604106 || Validation Loss: 0.010656 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.524s | Train Loss: 0.589637 || Validation Loss: 0.010079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.630s | Train Loss: 0.606693 || Validation Loss: 0.011590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.542s | Train Loss: 0.600569 || Validation Loss: 0.010429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.580s | Train Loss: 0.591679 || Validation Loss: 0.010175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.573s | Train Loss: 0.598112 || Validation Loss: 0.010212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.538s | Train Loss: 0.590793 || Validation Loss: 0.010951 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 34.852s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.845%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008899\n",
      "INFO:root:Test AUC: 79.71%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5178827492638957\n",
      "validation AUC 0.5322138680935871\n",
      "validation AUC 0.5466317278279488\n",
      "validation AUC 0.5612377004637888\n",
      "validation AUC 0.5753887336705943\n",
      "validation AUC 0.5896541203433003\n",
      "validation AUC 0.6014027037428138\n",
      "validation AUC 0.6123902920854492\n",
      "validation AUC 0.6229940184668382\n",
      "validation AUC 0.633976809599231\n",
      "validation AUC 0.6436931744771174\n",
      "validation AUC 0.6523436407457318\n",
      "validation AUC 0.6609131956646629\n",
      "validation AUC 0.6699808516014009\n",
      "validation AUC 0.6789964805558399\n",
      "validation AUC 0.6867783245066084\n",
      "validation AUC 0.6932387325956788\n",
      "validation AUC 0.6977839948463648\n",
      "validation AUC 0.7035869642955588\n",
      "validation AUC 0.7094518824114636\n",
      "validation AUC 0.7160495524413037\n",
      "validation AUC 0.7211636073786469\n",
      "validation AUC 0.7252798452462016\n",
      "validation AUC 0.7297531520039302\n",
      "validation AUC 0.7343894468398863\n",
      "validation AUC 0.738773708542046\n",
      "validation AUC 0.742290835247783\n",
      "validation AUC 0.7460210639834501\n",
      "validation AUC 0.7494167411835099\n",
      "validation AUC 0.7523889707906036\n",
      "validation AUC 0.7569543132108413\n",
      "validation AUC 0.7590579949991945\n",
      "validation AUC 0.7602280992319569\n",
      "validation AUC 0.7635845819205996\n",
      "validation AUC 0.766832740249504\n",
      "validation AUC 0.7687030988222117\n",
      "validation AUC 0.7700506279955291\n",
      "validation AUC 0.7724100332840718\n",
      "validation AUC 0.7737364419610464\n",
      "validation AUC 0.777001037985391\n",
      "validation AUC 0.7793160471285595\n",
      "validation AUC 0.7807764557497664\n",
      "validation AUC 0.7838654467402704\n",
      "validation AUC 0.7860007374347149\n",
      "validation AUC 0.7882614473728742\n",
      "validation AUC 0.789248188022119\n",
      "validation AUC 0.7907708326637282\n",
      "validation AUC 0.7922664740403611\n",
      "validation AUC 0.7937975876338297\n",
      "validation AUC 0.7939250422995277\n",
      "validation AUC 0.7941824207441269\n",
      "validation AUC 0.7943358463300569\n",
      "validation AUC 0.7945267901975354\n",
      "validation AUC 0.7946689983467585\n",
      "validation AUC 0.7947452322169039\n",
      "validation AUC 0.7949076801766183\n",
      "validation AUC 0.7950989805755291\n",
      "validation AUC 0.7952175459015594\n",
      "validation AUC 0.7953588494187899\n",
      "validation AUC 0.7955222259566376\n",
      "validation AUC 0.7955691683750021\n",
      "validation AUC 0.7957108337450296\n",
      "validation AUC 0.7958751228969169\n",
      "validation AUC 0.7960646060497957\n",
      "validation AUC 0.7962906868881362\n",
      "validation AUC 0.7964127403687025\n",
      "validation AUC 0.7965144050405668\n",
      "validation AUC 0.7966764033449673\n",
      "validation AUC 0.7970050162376121\n",
      "validation AUC 0.7972613995870196\n",
      "validation AUC 0.7972613995870196\n",
      "train auc 0.8453738394197237\n",
      "Test AUC 0.7970574572587571\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.68450971e-05 ... 9.91283752e-01\n",
      " 9.91283752e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [1.57180595e+01 1.47180595e+01 1.29967728e+01 ... 3.61909345e-02\n",
      " 3.61502729e-02 1.38387345e-02]\n",
      "Itration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.058s | Train Loss: 0.208278 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.045s | Train Loss: 0.201960 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.043s | Train Loss: 0.196836 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.044s | Train Loss: 0.191705 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.185779 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.044s | Train Loss: 0.180941 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.044s | Train Loss: 0.175685 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.044s | Train Loss: 0.170771 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.044s | Train Loss: 0.165729 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.043s | Train Loss: 0.160771 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.043s | Train Loss: 0.156389 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.044s | Train Loss: 0.152041 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.147401 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.043s | Train Loss: 0.143235 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.045s | Train Loss: 0.139245 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.043s | Train Loss: 0.135431 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.044s | Train Loss: 0.131420 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.043s | Train Loss: 0.128178 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.043s | Train Loss: 0.124673 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.044s | Train Loss: 0.120865 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.044s | Train Loss: 0.118100 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.042s | Train Loss: 0.114747 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.044s | Train Loss: 0.112243 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.045s | Train Loss: 0.109324 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.044s | Train Loss: 0.106740 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.044s | Train Loss: 0.104222 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.044s | Train Loss: 0.101375 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.044s | Train Loss: 0.098847 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.044s | Train Loss: 0.097056 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.044s | Train Loss: 0.094297 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.044s | Train Loss: 0.091922 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.090359 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.045s | Train Loss: 0.088421 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.044s | Train Loss: 0.086938 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.044s | Train Loss: 0.084931 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.044s | Train Loss: 0.082729 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.044s | Train Loss: 0.081234 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.044s | Train Loss: 0.080030 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.044s | Train Loss: 0.078359 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.044s | Train Loss: 0.076728 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.043s | Train Loss: 0.075710 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.044s | Train Loss: 0.074314 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.045s | Train Loss: 0.072363 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.044s | Train Loss: 0.071623 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.044s | Train Loss: 0.070684 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.069163 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.044s | Train Loss: 0.068250 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.045s | Train Loss: 0.066343 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.044s | Train Loss: 0.065986 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.044s | Train Loss: 0.064615 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.044s | Train Loss: 0.064434 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.042s | Train Loss: 0.062984 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.044s | Train Loss: 0.062497 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.044s | Train Loss: 0.060548 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.044s | Train Loss: 0.060487 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.044s | Train Loss: 0.058956 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.044s | Train Loss: 0.058659 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.057876 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.045s | Train Loss: 0.057094 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.044s | Train Loss: 0.056356 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.043s | Train Loss: 0.055832 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.046s | Train Loss: 0.054651 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.043s | Train Loss: 0.054115 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.043s | Train Loss: 0.053060 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.045s | Train Loss: 0.053036 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.044s | Train Loss: 0.051877 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.043s | Train Loss: 0.051998 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.045s | Train Loss: 0.050950 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.043s | Train Loss: 0.050680 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.043s | Train Loss: 0.050318 |\n",
      "INFO:root:Pretraining Time: 3.096s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.053407\n",
      "INFO:root:Test AUC: 51.71%\n",
      "INFO:root:Test AUC: 51.71%\n",
      "INFO:root:Test Time: 0.237s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.451s | Train Loss: 1.481858 || Validation Loss: 0.017960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.420s | Train Loss: 1.380237 || Validation Loss: 0.016030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.453s | Train Loss: 1.273391 || Validation Loss: 0.016646 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.450s | Train Loss: 1.237931 || Validation Loss: 0.014908 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.451s | Train Loss: 1.173806 || Validation Loss: 0.013627 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.452s | Train Loss: 1.116795 || Validation Loss: 0.013645 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.480s | Train Loss: 1.039195 || Validation Loss: 0.013363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.444s | Train Loss: 1.010351 || Validation Loss: 0.013887 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.457s | Train Loss: 0.976140 || Validation Loss: 0.013450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.466s | Train Loss: 0.964246 || Validation Loss: 0.012829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.478s | Train Loss: 0.920108 || Validation Loss: 0.012771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.464s | Train Loss: 0.899564 || Validation Loss: 0.012078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.463s | Train Loss: 0.877543 || Validation Loss: 0.012333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.508s | Train Loss: 0.858118 || Validation Loss: 0.011945 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.474s | Train Loss: 0.820945 || Validation Loss: 0.012843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.504s | Train Loss: 0.810373 || Validation Loss: 0.012167 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.464s | Train Loss: 0.795232 || Validation Loss: 0.011661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.465s | Train Loss: 0.794478 || Validation Loss: 0.011704 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.505s | Train Loss: 0.753416 || Validation Loss: 0.012666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.495s | Train Loss: 0.749804 || Validation Loss: 0.012308 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.465s | Train Loss: 0.738779 || Validation Loss: 0.012168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.490s | Train Loss: 0.739866 || Validation Loss: 0.012228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.471s | Train Loss: 0.706405 || Validation Loss: 0.013132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.499s | Train Loss: 0.700425 || Validation Loss: 0.012790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.499s | Train Loss: 0.686943 || Validation Loss: 0.011616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.494s | Train Loss: 0.692354 || Validation Loss: 0.012254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.509s | Train Loss: 0.666087 || Validation Loss: 0.013060 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.502s | Train Loss: 0.691179 || Validation Loss: 0.012614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.505s | Train Loss: 0.645831 || Validation Loss: 0.012699 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.522s | Train Loss: 0.649586 || Validation Loss: 0.012099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.556s | Train Loss: 0.645681 || Validation Loss: 0.012729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.477s | Train Loss: 0.619196 || Validation Loss: 0.013238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.547s | Train Loss: 0.619618 || Validation Loss: 0.012877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.480s | Train Loss: 0.629280 || Validation Loss: 0.013726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.553s | Train Loss: 0.596742 || Validation Loss: 0.012550 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.486s | Train Loss: 0.593707 || Validation Loss: 0.012501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.543s | Train Loss: 0.592291 || Validation Loss: 0.013209 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.488s | Train Loss: 0.583401 || Validation Loss: 0.013962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.556s | Train Loss: 0.605536 || Validation Loss: 0.013826 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.519s | Train Loss: 0.581056 || Validation Loss: 0.013454 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.529s | Train Loss: 0.568119 || Validation Loss: 0.013651 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.488s | Train Loss: 0.574328 || Validation Loss: 0.013102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.564s | Train Loss: 0.552964 || Validation Loss: 0.013312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.530s | Train Loss: 0.543594 || Validation Loss: 0.013138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.531s | Train Loss: 0.528442 || Validation Loss: 0.013996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.528s | Train Loss: 0.562149 || Validation Loss: 0.015140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.558s | Train Loss: 0.555320 || Validation Loss: 0.014484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.530s | Train Loss: 0.550927 || Validation Loss: 0.013999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.521s | Train Loss: 0.523272 || Validation Loss: 0.015688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.526s | Train Loss: 0.548108 || Validation Loss: 0.013551 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.520s | Train Loss: 0.532359 || Validation Loss: 0.013810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.533s | Train Loss: 0.549782 || Validation Loss: 0.015239 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.567s | Train Loss: 0.529850 || Validation Loss: 0.014508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.534s | Train Loss: 0.546130 || Validation Loss: 0.013847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.530s | Train Loss: 0.555442 || Validation Loss: 0.015039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.538s | Train Loss: 0.533834 || Validation Loss: 0.014834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.566s | Train Loss: 0.542317 || Validation Loss: 0.014327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.566s | Train Loss: 0.535548 || Validation Loss: 0.014480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.539s | Train Loss: 0.537806 || Validation Loss: 0.014996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.576s | Train Loss: 0.556810 || Validation Loss: 0.014146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.541s | Train Loss: 0.531545 || Validation Loss: 0.014785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.577s | Train Loss: 0.526481 || Validation Loss: 0.015074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.542s | Train Loss: 0.555495 || Validation Loss: 0.014513 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.579s | Train Loss: 0.537964 || Validation Loss: 0.013612 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.542s | Train Loss: 0.551592 || Validation Loss: 0.015373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.611s | Train Loss: 0.547248 || Validation Loss: 0.014223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.550s | Train Loss: 0.550308 || Validation Loss: 0.014823 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.586s | Train Loss: 0.539038 || Validation Loss: 0.015562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.583s | Train Loss: 0.532584 || Validation Loss: 0.015286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.552s | Train Loss: 0.548687 || Validation Loss: 0.013820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 36.179s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.877%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.012380\n",
      "INFO:root:Test AUC: 83.27%\n",
      "INFO:root:Test Time: 0.200s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5110931269679737\n",
      "validation AUC 0.5367048071931227\n",
      "validation AUC 0.5603232558748398\n",
      "validation AUC 0.5830853160560685\n",
      "validation AUC 0.6022071876523907\n",
      "validation AUC 0.6191891762165651\n",
      "validation AUC 0.634753699572354\n",
      "validation AUC 0.6500585110651393\n",
      "validation AUC 0.6634482299331871\n",
      "validation AUC 0.676740493328818\n",
      "validation AUC 0.6889270968784237\n",
      "validation AUC 0.70044753741079\n",
      "validation AUC 0.7105502120776672\n",
      "validation AUC 0.7197927573246988\n",
      "validation AUC 0.7275760087764203\n",
      "validation AUC 0.7353225454726574\n",
      "validation AUC 0.7427294992829994\n",
      "validation AUC 0.7507774753232357\n",
      "validation AUC 0.7568938678296474\n",
      "validation AUC 0.7614947090735441\n",
      "validation AUC 0.7657005001869928\n",
      "validation AUC 0.7700324848027146\n",
      "validation AUC 0.7752207035993498\n",
      "validation AUC 0.7789907733922402\n",
      "validation AUC 0.7825244335779812\n",
      "validation AUC 0.7846467002324158\n",
      "validation AUC 0.7887715480679934\n",
      "validation AUC 0.7925833639666559\n",
      "validation AUC 0.7948725591698501\n",
      "validation AUC 0.7971043501517547\n",
      "validation AUC 0.7995949802928581\n",
      "validation AUC 0.8018616368560272\n",
      "validation AUC 0.8039817483577737\n",
      "validation AUC 0.8066656424813481\n",
      "validation AUC 0.8096789556843136\n",
      "validation AUC 0.8107015170997798\n",
      "validation AUC 0.8106663136118593\n",
      "validation AUC 0.8115623063688433\n",
      "validation AUC 0.8134959279853334\n",
      "validation AUC 0.8158200552868502\n",
      "validation AUC 0.8168853312964568\n",
      "validation AUC 0.8178802748293278\n",
      "validation AUC 0.8184974094532556\n",
      "validation AUC 0.8194175000101105\n",
      "validation AUC 0.8211979754123153\n",
      "validation AUC 0.8213135953630054\n",
      "validation AUC 0.8234468453141022\n",
      "validation AUC 0.8231722229873162\n",
      "validation AUC 0.8239651834817178\n",
      "validation AUC 0.8241848573970056\n",
      "validation AUC 0.8242360994780167\n",
      "validation AUC 0.8242195713193771\n",
      "validation AUC 0.8243309607851611\n",
      "validation AUC 0.8244401259205163\n",
      "validation AUC 0.8244875738685236\n",
      "validation AUC 0.8244984055062926\n",
      "validation AUC 0.8245333934789443\n",
      "validation AUC 0.8246074322861667\n",
      "validation AUC 0.8245401516120648\n",
      "validation AUC 0.824528221112493\n",
      "validation AUC 0.824546303109614\n",
      "validation AUC 0.8245720611752595\n",
      "validation AUC 0.8244968676319053\n",
      "validation AUC 0.8245910531257377\n",
      "validation AUC 0.8248199170377963\n",
      "validation AUC 0.8250030996949155\n",
      "validation AUC 0.8250975113467458\n",
      "validation AUC 0.825300609211112\n",
      "validation AUC 0.8253473520783015\n",
      "validation AUC 0.8253566804305538\n",
      "validation AUC 0.8253566804305538\n",
      "train auc 0.8766044315096647\n",
      "Test AUC 0.8327292807949401\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.91738513e-01\n",
      " 9.91738513e-01 1.00000000e+00] true positive rate [0.         0.         0.0055991  ... 0.99972004 1.         1.        ] tresholds [2.47148819e+01 2.37148819e+01 2.02040348e+01 ... 3.73390652e-02\n",
      " 3.73108909e-02 1.43492594e-02]\n",
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.068s | Train Loss: 0.222460 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.046s | Train Loss: 0.216538 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.044s | Train Loss: 0.210838 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.044s | Train Loss: 0.205723 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.200136 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.045s | Train Loss: 0.194663 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.046s | Train Loss: 0.190109 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.045s | Train Loss: 0.184808 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.046s | Train Loss: 0.180041 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.042s | Train Loss: 0.175081 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.043s | Train Loss: 0.170708 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.044s | Train Loss: 0.165720 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.161147 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.044s | Train Loss: 0.156399 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.044s | Train Loss: 0.152009 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.044s | Train Loss: 0.147530 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.044s | Train Loss: 0.143418 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.039s | Train Loss: 0.139580 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.040s | Train Loss: 0.135952 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.043s | Train Loss: 0.132021 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.042s | Train Loss: 0.127899 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.043s | Train Loss: 0.124756 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.042s | Train Loss: 0.121810 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.043s | Train Loss: 0.118219 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.043s | Train Loss: 0.115048 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.042s | Train Loss: 0.112370 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.043s | Train Loss: 0.108698 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.042s | Train Loss: 0.106698 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.042s | Train Loss: 0.103670 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.043s | Train Loss: 0.101327 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.042s | Train Loss: 0.098814 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.096166 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.043s | Train Loss: 0.094252 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.043s | Train Loss: 0.092224 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.090240 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.043s | Train Loss: 0.087934 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.043s | Train Loss: 0.085770 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.043s | Train Loss: 0.084316 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.043s | Train Loss: 0.082319 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.043s | Train Loss: 0.081150 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.042s | Train Loss: 0.079356 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.043s | Train Loss: 0.077840 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.042s | Train Loss: 0.076858 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.042s | Train Loss: 0.074947 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.042s | Train Loss: 0.074334 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.072348 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.042s | Train Loss: 0.071004 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.044s | Train Loss: 0.069852 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.042s | Train Loss: 0.069188 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.044s | Train Loss: 0.067717 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.043s | Train Loss: 0.066649 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.044s | Train Loss: 0.066126 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.044s | Train Loss: 0.065015 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.044s | Train Loss: 0.063714 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.043s | Train Loss: 0.063283 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.044s | Train Loss: 0.061635 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.043s | Train Loss: 0.061231 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.060248 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.043s | Train Loss: 0.059975 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.043s | Train Loss: 0.058402 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.044s | Train Loss: 0.057832 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.043s | Train Loss: 0.057224 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.043s | Train Loss: 0.056391 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.045s | Train Loss: 0.055483 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.043s | Train Loss: 0.055460 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.044s | Train Loss: 0.054341 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.044s | Train Loss: 0.054122 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.043s | Train Loss: 0.053475 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.042s | Train Loss: 0.053220 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.043s | Train Loss: 0.051957 |\n",
      "INFO:root:Pretraining Time: 3.069s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.056167\n",
      "INFO:root:Test AUC: 53.08%\n",
      "INFO:root:Test AUC: 53.08%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.436s | Train Loss: 1.282755 || Validation Loss: 0.013339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.416s | Train Loss: 1.205829 || Validation Loss: 0.014157 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.442s | Train Loss: 1.139123 || Validation Loss: 0.012846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.439s | Train Loss: 1.053184 || Validation Loss: 0.013250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.436s | Train Loss: 1.040028 || Validation Loss: 0.012969 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.432s | Train Loss: 0.977704 || Validation Loss: 0.012728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.467s | Train Loss: 0.965940 || Validation Loss: 0.012019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.440s | Train Loss: 0.927273 || Validation Loss: 0.012100 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.440s | Train Loss: 0.900139 || Validation Loss: 0.011988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.441s | Train Loss: 0.883573 || Validation Loss: 0.012956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.439s | Train Loss: 0.868764 || Validation Loss: 0.011872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.437s | Train Loss: 0.841237 || Validation Loss: 0.011283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.437s | Train Loss: 0.827391 || Validation Loss: 0.011543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.474s | Train Loss: 0.820629 || Validation Loss: 0.010677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.446s | Train Loss: 0.800827 || Validation Loss: 0.011489 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.474s | Train Loss: 0.794689 || Validation Loss: 0.011701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.447s | Train Loss: 0.770667 || Validation Loss: 0.012267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.450s | Train Loss: 0.759514 || Validation Loss: 0.011591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.481s | Train Loss: 0.740144 || Validation Loss: 0.011620 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.480s | Train Loss: 0.723224 || Validation Loss: 0.011547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.455s | Train Loss: 0.732154 || Validation Loss: 0.011256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.480s | Train Loss: 0.725830 || Validation Loss: 0.011214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.454s | Train Loss: 0.694712 || Validation Loss: 0.012130 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.487s | Train Loss: 0.700852 || Validation Loss: 0.011503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.490s | Train Loss: 0.716882 || Validation Loss: 0.011965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.487s | Train Loss: 0.707961 || Validation Loss: 0.011255 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.488s | Train Loss: 0.669011 || Validation Loss: 0.011591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.490s | Train Loss: 0.681040 || Validation Loss: 0.013095 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.492s | Train Loss: 0.669444 || Validation Loss: 0.012298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.495s | Train Loss: 0.650613 || Validation Loss: 0.013452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.530s | Train Loss: 0.650721 || Validation Loss: 0.012237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.465s | Train Loss: 0.629618 || Validation Loss: 0.013144 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.528s | Train Loss: 0.661230 || Validation Loss: 0.013240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.469s | Train Loss: 0.635992 || Validation Loss: 0.012048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.525s | Train Loss: 0.639973 || Validation Loss: 0.012630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.470s | Train Loss: 0.648172 || Validation Loss: 0.013335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.534s | Train Loss: 0.637671 || Validation Loss: 0.012106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.473s | Train Loss: 0.620012 || Validation Loss: 0.012818 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.539s | Train Loss: 0.630537 || Validation Loss: 0.012938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.506s | Train Loss: 0.612589 || Validation Loss: 0.013362 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.503s | Train Loss: 0.624178 || Validation Loss: 0.013810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.478s | Train Loss: 0.607705 || Validation Loss: 0.012702 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.541s | Train Loss: 0.589128 || Validation Loss: 0.013987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.515s | Train Loss: 0.606663 || Validation Loss: 0.012603 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.513s | Train Loss: 0.608890 || Validation Loss: 0.013327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.514s | Train Loss: 0.608182 || Validation Loss: 0.013942 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.544s | Train Loss: 0.578789 || Validation Loss: 0.013418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.513s | Train Loss: 0.566494 || Validation Loss: 0.014080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.510s | Train Loss: 0.559668 || Validation Loss: 0.014414 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.509s | Train Loss: 0.582711 || Validation Loss: 0.013835 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.517s | Train Loss: 0.572703 || Validation Loss: 0.014413 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.523s | Train Loss: 0.570493 || Validation Loss: 0.013686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.553s | Train Loss: 0.573234 || Validation Loss: 0.014378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.519s | Train Loss: 0.583384 || Validation Loss: 0.012615 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.522s | Train Loss: 0.586577 || Validation Loss: 0.012732 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.523s | Train Loss: 0.581316 || Validation Loss: 0.015294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.550s | Train Loss: 0.579605 || Validation Loss: 0.015180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.557s | Train Loss: 0.571859 || Validation Loss: 0.012999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.528s | Train Loss: 0.552531 || Validation Loss: 0.013972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.563s | Train Loss: 0.584019 || Validation Loss: 0.014037 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.528s | Train Loss: 0.583791 || Validation Loss: 0.013071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.569s | Train Loss: 0.568192 || Validation Loss: 0.013795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.526s | Train Loss: 0.593937 || Validation Loss: 0.014776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.611s | Train Loss: 0.575377 || Validation Loss: 0.014466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.538s | Train Loss: 0.562427 || Validation Loss: 0.014491 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.648s | Train Loss: 0.580299 || Validation Loss: 0.014533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.535s | Train Loss: 0.543593 || Validation Loss: 0.014006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.588s | Train Loss: 0.578570 || Validation Loss: 0.014053 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.590s | Train Loss: 0.590062 || Validation Loss: 0.013573 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.570s | Train Loss: 0.580362 || Validation Loss: 0.014190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 35.269s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.879%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.012814\n",
      "INFO:root:Test AUC: 82.66%\n",
      "INFO:root:Test Time: 0.198s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6099828588201512\n",
      "validation AUC 0.6363809542611679\n",
      "validation AUC 0.6602902288250662\n",
      "validation AUC 0.6786650047690069\n",
      "validation AUC 0.6945396120256881\n",
      "validation AUC 0.7080314638456906\n",
      "validation AUC 0.7200008785573057\n",
      "validation AUC 0.7310632400554871\n",
      "validation AUC 0.7395045846749381\n",
      "validation AUC 0.7481865454403036\n",
      "validation AUC 0.7557378998021091\n",
      "validation AUC 0.7610716073692811\n",
      "validation AUC 0.7661474229801644\n",
      "validation AUC 0.7707357962666583\n",
      "validation AUC 0.7749007725131508\n",
      "validation AUC 0.7786403189583406\n",
      "validation AUC 0.7826658701293283\n",
      "validation AUC 0.7853565144039762\n",
      "validation AUC 0.7875234539147471\n",
      "validation AUC 0.7899392204371054\n",
      "validation AUC 0.7914713663753183\n",
      "validation AUC 0.7936760157367653\n",
      "validation AUC 0.7962423928431477\n",
      "validation AUC 0.7984812053657235\n",
      "validation AUC 0.8013111336997129\n",
      "validation AUC 0.8031186948480464\n",
      "validation AUC 0.8043263785580506\n",
      "validation AUC 0.8055284402462898\n",
      "validation AUC 0.8069389424490155\n",
      "validation AUC 0.8091954325449979\n",
      "validation AUC 0.8104896017341691\n",
      "validation AUC 0.8111045466165379\n",
      "validation AUC 0.8118843899470481\n",
      "validation AUC 0.813147804309156\n",
      "validation AUC 0.8146958158961082\n",
      "validation AUC 0.8150372160280355\n",
      "validation AUC 0.8160852508161378\n",
      "validation AUC 0.8163903140094716\n",
      "validation AUC 0.8158543674461899\n",
      "validation AUC 0.8156321552399584\n",
      "validation AUC 0.816508629231363\n",
      "validation AUC 0.817419013619075\n",
      "validation AUC 0.8186814169218971\n",
      "validation AUC 0.8200639048003605\n",
      "validation AUC 0.8204783193768128\n",
      "validation AUC 0.8207349687944533\n",
      "validation AUC 0.8217618591400717\n",
      "validation AUC 0.8224878635636158\n",
      "validation AUC 0.8229150361118448\n",
      "validation AUC 0.8229769475290031\n",
      "validation AUC 0.8230417032155729\n",
      "validation AUC 0.8231123496528235\n",
      "validation AUC 0.8231355933736664\n",
      "validation AUC 0.8231607926960227\n",
      "validation AUC 0.8232010355162779\n",
      "validation AUC 0.8231693574324457\n",
      "validation AUC 0.82321311501406\n",
      "validation AUC 0.8233272715894683\n",
      "validation AUC 0.823408613969689\n",
      "validation AUC 0.8234711772540183\n",
      "validation AUC 0.8235141153454747\n",
      "validation AUC 0.8235356775150846\n",
      "validation AUC 0.8235739115201798\n",
      "validation AUC 0.8235946621816786\n",
      "validation AUC 0.8236721066622846\n",
      "validation AUC 0.8236253238848602\n",
      "validation AUC 0.8235909984221088\n",
      "validation AUC 0.8236768985511627\n",
      "validation AUC 0.8236331516122777\n",
      "validation AUC 0.8236491928660508\n",
      "validation AUC 0.8236491928660508\n",
      "train auc 0.8793459696338123\n",
      "Test AUC 0.826594248757243\n",
      "test false positive rates [0.         0.         0.         ... 0.99314069 0.99314069 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 4.75923852e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.89068680e+01 2.79068680e+01 2.15951996e+01 ... 2.94563677e-02\n",
      " 2.93657053e-02 8.34867544e-03]\n",
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.061s | Train Loss: 0.219715 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.046s | Train Loss: 0.213844 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.044s | Train Loss: 0.208435 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.044s | Train Loss: 0.203502 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.198405 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.045s | Train Loss: 0.193443 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.043s | Train Loss: 0.188947 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.045s | Train Loss: 0.183832 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.044s | Train Loss: 0.179227 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.044s | Train Loss: 0.174165 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.044s | Train Loss: 0.170101 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.044s | Train Loss: 0.164783 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.160251 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.044s | Train Loss: 0.155866 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.044s | Train Loss: 0.151448 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.044s | Train Loss: 0.147121 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.043s | Train Loss: 0.143091 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.045s | Train Loss: 0.138746 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.044s | Train Loss: 0.134404 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.044s | Train Loss: 0.130730 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.042s | Train Loss: 0.126921 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.044s | Train Loss: 0.123897 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.042s | Train Loss: 0.120779 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.044s | Train Loss: 0.116717 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.044s | Train Loss: 0.113438 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.044s | Train Loss: 0.110406 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.044s | Train Loss: 0.107725 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.043s | Train Loss: 0.104828 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.043s | Train Loss: 0.101658 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.044s | Train Loss: 0.099222 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.097087 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.094730 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.043s | Train Loss: 0.092520 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.043s | Train Loss: 0.090320 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.044s | Train Loss: 0.088676 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.043s | Train Loss: 0.086275 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.044s | Train Loss: 0.084963 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.044s | Train Loss: 0.082675 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.044s | Train Loss: 0.080788 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.045s | Train Loss: 0.078986 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.043s | Train Loss: 0.078295 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.043s | Train Loss: 0.076004 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.044s | Train Loss: 0.075245 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.043s | Train Loss: 0.073532 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.045s | Train Loss: 0.072374 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.071020 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.044s | Train Loss: 0.069656 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.044s | Train Loss: 0.068647 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.044s | Train Loss: 0.067162 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.044s | Train Loss: 0.066702 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.045s | Train Loss: 0.065591 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.045s | Train Loss: 0.064713 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.043s | Train Loss: 0.063814 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.044s | Train Loss: 0.062754 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.044s | Train Loss: 0.062308 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.043s | Train Loss: 0.060067 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.044s | Train Loss: 0.060386 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.044s | Train Loss: 0.058848 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.044s | Train Loss: 0.058347 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.045s | Train Loss: 0.057578 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.044s | Train Loss: 0.056954 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.044s | Train Loss: 0.055406 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.044s | Train Loss: 0.055496 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.045s | Train Loss: 0.054643 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.044s | Train Loss: 0.053808 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.044s | Train Loss: 0.053208 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.045s | Train Loss: 0.052959 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.045s | Train Loss: 0.052231 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.043s | Train Loss: 0.051585 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.043s | Train Loss: 0.051313 |\n",
      "INFO:root:Pretraining Time: 3.097s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.052542\n",
      "INFO:root:Test AUC: 54.40%\n",
      "INFO:root:Test AUC: 54.40%\n",
      "INFO:root:Test Time: 0.236s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.452s | Train Loss: 1.298453 || Validation Loss: 0.012957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.425s | Train Loss: 1.247231 || Validation Loss: 0.012030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.450s | Train Loss: 1.169731 || Validation Loss: 0.011899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.454s | Train Loss: 1.090066 || Validation Loss: 0.011372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.450s | Train Loss: 1.047014 || Validation Loss: 0.011169 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.451s | Train Loss: 0.987673 || Validation Loss: 0.011116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.485s | Train Loss: 0.975748 || Validation Loss: 0.011047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.454s | Train Loss: 0.933630 || Validation Loss: 0.011141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.457s | Train Loss: 0.931635 || Validation Loss: 0.010636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.457s | Train Loss: 0.897604 || Validation Loss: 0.010769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.460s | Train Loss: 0.875071 || Validation Loss: 0.010635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.452s | Train Loss: 0.862136 || Validation Loss: 0.010176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.461s | Train Loss: 0.848260 || Validation Loss: 0.010505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.492s | Train Loss: 0.816389 || Validation Loss: 0.010181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.463s | Train Loss: 0.817122 || Validation Loss: 0.010039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.496s | Train Loss: 0.798808 || Validation Loss: 0.010208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.466s | Train Loss: 0.788783 || Validation Loss: 0.010709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.467s | Train Loss: 0.775874 || Validation Loss: 0.010013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.496s | Train Loss: 0.763732 || Validation Loss: 0.009708 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.500s | Train Loss: 0.756183 || Validation Loss: 0.010071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.468s | Train Loss: 0.733970 || Validation Loss: 0.010636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.497s | Train Loss: 0.736518 || Validation Loss: 0.009109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.470s | Train Loss: 0.734963 || Validation Loss: 0.010335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.500s | Train Loss: 0.728922 || Validation Loss: 0.010704 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.502s | Train Loss: 0.726152 || Validation Loss: 0.009863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.513s | Train Loss: 0.701909 || Validation Loss: 0.009909 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.510s | Train Loss: 0.708722 || Validation Loss: 0.009368 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.505s | Train Loss: 0.694312 || Validation Loss: 0.010227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.508s | Train Loss: 0.695105 || Validation Loss: 0.009938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.507s | Train Loss: 0.677176 || Validation Loss: 0.010418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.541s | Train Loss: 0.668999 || Validation Loss: 0.009944 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.477s | Train Loss: 0.672221 || Validation Loss: 0.010194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.551s | Train Loss: 0.666381 || Validation Loss: 0.009798 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.480s | Train Loss: 0.675375 || Validation Loss: 0.009922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.552s | Train Loss: 0.643015 || Validation Loss: 0.010460 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.483s | Train Loss: 0.650292 || Validation Loss: 0.010532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.552s | Train Loss: 0.645066 || Validation Loss: 0.010537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.485s | Train Loss: 0.645535 || Validation Loss: 0.009795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.554s | Train Loss: 0.645576 || Validation Loss: 0.010698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.523s | Train Loss: 0.629390 || Validation Loss: 0.009157 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.525s | Train Loss: 0.637939 || Validation Loss: 0.009940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.494s | Train Loss: 0.606726 || Validation Loss: 0.011372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.555s | Train Loss: 0.620895 || Validation Loss: 0.010244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.524s | Train Loss: 0.609473 || Validation Loss: 0.009610 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.527s | Train Loss: 0.618883 || Validation Loss: 0.009951 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.534s | Train Loss: 0.614559 || Validation Loss: 0.010291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.563s | Train Loss: 0.598613 || Validation Loss: 0.010514 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.700s | Train Loss: 0.612004 || Validation Loss: 0.010003 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.531s | Train Loss: 0.609687 || Validation Loss: 0.010685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.530s | Train Loss: 0.592930 || Validation Loss: 0.009907 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.534s | Train Loss: 0.591887 || Validation Loss: 0.010240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.536s | Train Loss: 0.569500 || Validation Loss: 0.009886 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.566s | Train Loss: 0.584539 || Validation Loss: 0.009997 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.534s | Train Loss: 0.586932 || Validation Loss: 0.009723 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.536s | Train Loss: 0.588584 || Validation Loss: 0.009794 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.544s | Train Loss: 0.577214 || Validation Loss: 0.009912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.573s | Train Loss: 0.603662 || Validation Loss: 0.010259 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.572s | Train Loss: 0.589078 || Validation Loss: 0.010566 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.539s | Train Loss: 0.606144 || Validation Loss: 0.010376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.571s | Train Loss: 0.585043 || Validation Loss: 0.011070 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.540s | Train Loss: 0.601785 || Validation Loss: 0.009079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.576s | Train Loss: 0.602595 || Validation Loss: 0.010884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.541s | Train Loss: 0.580457 || Validation Loss: 0.010793 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.581s | Train Loss: 0.583789 || Validation Loss: 0.011159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.541s | Train Loss: 0.604716 || Validation Loss: 0.010574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.616s | Train Loss: 0.586676 || Validation Loss: 0.010759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.550s | Train Loss: 0.584281 || Validation Loss: 0.010170 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.586s | Train Loss: 0.562679 || Validation Loss: 0.010955 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.585s | Train Loss: 0.583099 || Validation Loss: 0.010692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.554s | Train Loss: 0.587368 || Validation Loss: 0.010236 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 36.360s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.871%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009121\n",
      "INFO:root:Test AUC: 79.74%\n",
      "INFO:root:Test Time: 0.201s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5566038534555772\n",
      "validation AUC 0.5725667581160921\n",
      "validation AUC 0.5889001335236821\n",
      "validation AUC 0.6036748200261257\n",
      "validation AUC 0.6188275415955111\n",
      "validation AUC 0.6325917461802713\n",
      "validation AUC 0.6455619898540734\n",
      "validation AUC 0.6572077751310493\n",
      "validation AUC 0.6677729375827339\n",
      "validation AUC 0.6773548129508393\n",
      "validation AUC 0.6857829552645645\n",
      "validation AUC 0.6935775545264273\n",
      "validation AUC 0.7004016007303466\n",
      "validation AUC 0.7077670691817848\n",
      "validation AUC 0.7150052638939235\n",
      "validation AUC 0.7211420984226837\n",
      "validation AUC 0.7261463895711469\n",
      "validation AUC 0.7311142426750883\n",
      "validation AUC 0.7350436687148244\n",
      "validation AUC 0.7386216212793285\n",
      "validation AUC 0.7412968841494213\n",
      "validation AUC 0.7450322905729044\n",
      "validation AUC 0.7483024820121911\n",
      "validation AUC 0.7519304527438339\n",
      "validation AUC 0.7553569885375675\n",
      "validation AUC 0.758458106918136\n",
      "validation AUC 0.7608394867820494\n",
      "validation AUC 0.7629710738066893\n",
      "validation AUC 0.7644333023346104\n",
      "validation AUC 0.7660551132673753\n",
      "validation AUC 0.7686016124160634\n",
      "validation AUC 0.7707102031633172\n",
      "validation AUC 0.7715827020953193\n",
      "validation AUC 0.7728443364609476\n",
      "validation AUC 0.7743391769721989\n",
      "validation AUC 0.7755821147230879\n",
      "validation AUC 0.7777303443157075\n",
      "validation AUC 0.7793108162270971\n",
      "validation AUC 0.7808811003858415\n",
      "validation AUC 0.7826087186728603\n",
      "validation AUC 0.78395633032733\n",
      "validation AUC 0.7862024623869981\n",
      "validation AUC 0.787277992666308\n",
      "validation AUC 0.7883576177357252\n",
      "validation AUC 0.789461689154399\n",
      "validation AUC 0.790727460881052\n",
      "validation AUC 0.791587859029813\n",
      "validation AUC 0.7933213121931834\n",
      "validation AUC 0.7939300470429921\n",
      "validation AUC 0.7941383225951741\n",
      "validation AUC 0.7943028937793885\n",
      "validation AUC 0.7945115232023207\n",
      "validation AUC 0.7947055508006206\n",
      "validation AUC 0.7948380953516176\n",
      "validation AUC 0.7950528762720989\n",
      "validation AUC 0.7952253417007891\n",
      "validation AUC 0.7953611775158295\n",
      "validation AUC 0.7953547067364007\n",
      "validation AUC 0.7954287269188469\n",
      "validation AUC 0.7955080711266371\n",
      "validation AUC 0.7956286931601093\n",
      "validation AUC 0.7956563695777157\n",
      "validation AUC 0.7957666734851085\n",
      "validation AUC 0.7959089561334369\n",
      "validation AUC 0.7958783529652667\n",
      "validation AUC 0.7959411610323703\n",
      "validation AUC 0.7959139582162189\n",
      "validation AUC 0.795974454150377\n",
      "validation AUC 0.7960876821476517\n",
      "validation AUC 0.7960793834794615\n",
      "validation AUC 0.7960793834794615\n",
      "train auc 0.8707351625296842\n",
      "Test AUC 0.7973841149887831\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.93709143e-01\n",
      " 9.93709143e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.39449453e+01 1.29449453e+01 1.16033077e+01 ... 4.86339107e-02\n",
      " 4.84098159e-02 1.04858018e-02]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.058s | Train Loss: 0.220111 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.047s | Train Loss: 0.214543 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.045s | Train Loss: 0.208903 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.043s | Train Loss: 0.203609 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.045s | Train Loss: 0.198253 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.044s | Train Loss: 0.192983 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.045s | Train Loss: 0.187964 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.044s | Train Loss: 0.182706 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.045s | Train Loss: 0.178065 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.045s | Train Loss: 0.173046 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.045s | Train Loss: 0.167813 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.043s | Train Loss: 0.163388 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.043s | Train Loss: 0.158814 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.043s | Train Loss: 0.154408 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.043s | Train Loss: 0.150121 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.043s | Train Loss: 0.145596 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.043s | Train Loss: 0.141572 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.043s | Train Loss: 0.137730 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.043s | Train Loss: 0.133973 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.042s | Train Loss: 0.130738 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.043s | Train Loss: 0.126016 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.043s | Train Loss: 0.123093 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.043s | Train Loss: 0.119811 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.043s | Train Loss: 0.117025 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.043s | Train Loss: 0.113174 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.043s | Train Loss: 0.110815 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.042s | Train Loss: 0.108058 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.043s | Train Loss: 0.105805 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.042s | Train Loss: 0.102605 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.041s | Train Loss: 0.100367 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.097847 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.095077 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.043s | Train Loss: 0.093149 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.043s | Train Loss: 0.090772 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.089680 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.044s | Train Loss: 0.087740 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.043s | Train Loss: 0.085196 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.044s | Train Loss: 0.083638 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.043s | Train Loss: 0.081656 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.043s | Train Loss: 0.080211 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.043s | Train Loss: 0.078810 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.043s | Train Loss: 0.077150 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.043s | Train Loss: 0.075822 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.043s | Train Loss: 0.074572 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.044s | Train Loss: 0.073325 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.071519 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.044s | Train Loss: 0.070814 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.043s | Train Loss: 0.068366 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.043s | Train Loss: 0.067776 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.043s | Train Loss: 0.067094 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.044s | Train Loss: 0.065660 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.042s | Train Loss: 0.064688 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.043s | Train Loss: 0.063327 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.043s | Train Loss: 0.063480 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.043s | Train Loss: 0.061722 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.043s | Train Loss: 0.061020 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.043s | Train Loss: 0.059815 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.059948 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.044s | Train Loss: 0.057955 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.043s | Train Loss: 0.057743 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.042s | Train Loss: 0.057242 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.044s | Train Loss: 0.055853 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.043s | Train Loss: 0.055382 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.044s | Train Loss: 0.054635 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.044s | Train Loss: 0.053929 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.044s | Train Loss: 0.053636 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.043s | Train Loss: 0.052438 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.044s | Train Loss: 0.051514 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.042s | Train Loss: 0.051295 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.042s | Train Loss: 0.051782 |\n",
      "INFO:root:Pretraining Time: 3.056s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.053274\n",
      "INFO:root:Test AUC: 53.34%\n",
      "INFO:root:Test AUC: 53.34%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.426s | Train Loss: 1.695067 || Validation Loss: 0.013332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.400s | Train Loss: 1.547316 || Validation Loss: 0.014148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.422s | Train Loss: 1.330432 || Validation Loss: 0.014492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.428s | Train Loss: 1.305073 || Validation Loss: 0.013668 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.428s | Train Loss: 1.265463 || Validation Loss: 0.013537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.431s | Train Loss: 1.151511 || Validation Loss: 0.014141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.460s | Train Loss: 1.098289 || Validation Loss: 0.012908 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.434s | Train Loss: 1.089798 || Validation Loss: 0.012829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.434s | Train Loss: 1.060698 || Validation Loss: 0.012888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.436s | Train Loss: 1.047251 || Validation Loss: 0.012484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.438s | Train Loss: 0.961781 || Validation Loss: 0.012572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.436s | Train Loss: 0.969189 || Validation Loss: 0.011961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.438s | Train Loss: 0.941739 || Validation Loss: 0.011801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.468s | Train Loss: 0.942000 || Validation Loss: 0.012383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.439s | Train Loss: 0.916930 || Validation Loss: 0.011827 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.471s | Train Loss: 0.905841 || Validation Loss: 0.011214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.441s | Train Loss: 0.874045 || Validation Loss: 0.011753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.445s | Train Loss: 0.865302 || Validation Loss: 0.011068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.470s | Train Loss: 0.856834 || Validation Loss: 0.011613 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.476s | Train Loss: 0.839362 || Validation Loss: 0.011666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.446s | Train Loss: 0.827198 || Validation Loss: 0.011899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.472s | Train Loss: 0.794483 || Validation Loss: 0.012427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.445s | Train Loss: 0.803959 || Validation Loss: 0.012186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.477s | Train Loss: 0.803751 || Validation Loss: 0.012276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.478s | Train Loss: 0.775515 || Validation Loss: 0.010962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.482s | Train Loss: 0.786205 || Validation Loss: 0.011803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.480s | Train Loss: 0.748091 || Validation Loss: 0.012472 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.485s | Train Loss: 0.769810 || Validation Loss: 0.013048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.487s | Train Loss: 0.742382 || Validation Loss: 0.011606 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.483s | Train Loss: 0.741196 || Validation Loss: 0.012640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.521s | Train Loss: 0.749503 || Validation Loss: 0.012530 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.459s | Train Loss: 0.727099 || Validation Loss: 0.011535 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.519s | Train Loss: 0.721809 || Validation Loss: 0.011701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.462s | Train Loss: 0.713975 || Validation Loss: 0.012077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.524s | Train Loss: 0.706326 || Validation Loss: 0.012261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.464s | Train Loss: 0.697815 || Validation Loss: 0.012200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.523s | Train Loss: 0.694122 || Validation Loss: 0.012899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.464s | Train Loss: 0.694669 || Validation Loss: 0.011492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.526s | Train Loss: 0.685763 || Validation Loss: 0.011440 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.493s | Train Loss: 0.697843 || Validation Loss: 0.013313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.500s | Train Loss: 0.687805 || Validation Loss: 0.011764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.471s | Train Loss: 0.655296 || Validation Loss: 0.012422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.533s | Train Loss: 0.675734 || Validation Loss: 0.012901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.510s | Train Loss: 0.666650 || Validation Loss: 0.011915 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.504s | Train Loss: 0.658095 || Validation Loss: 0.012641 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.506s | Train Loss: 0.651520 || Validation Loss: 0.011804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.534s | Train Loss: 0.639763 || Validation Loss: 0.011986 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.506s | Train Loss: 0.651862 || Validation Loss: 0.013970 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.506s | Train Loss: 0.649801 || Validation Loss: 0.012852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.505s | Train Loss: 0.645303 || Validation Loss: 0.013658 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.513s | Train Loss: 0.634376 || Validation Loss: 0.013522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.517s | Train Loss: 0.634702 || Validation Loss: 0.013908 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.545s | Train Loss: 0.632807 || Validation Loss: 0.012950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.513s | Train Loss: 0.634349 || Validation Loss: 0.013623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.515s | Train Loss: 0.650519 || Validation Loss: 0.012844 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.517s | Train Loss: 0.638239 || Validation Loss: 0.013375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.546s | Train Loss: 0.641554 || Validation Loss: 0.012980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.543s | Train Loss: 0.644081 || Validation Loss: 0.013019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.503s | Train Loss: 0.634452 || Validation Loss: 0.012158 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.553s | Train Loss: 0.632922 || Validation Loss: 0.012203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.522s | Train Loss: 0.648776 || Validation Loss: 0.012569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.557s | Train Loss: 0.638597 || Validation Loss: 0.012262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.542s | Train Loss: 0.635809 || Validation Loss: 0.012254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.567s | Train Loss: 0.637614 || Validation Loss: 0.013276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.522s | Train Loss: 0.631299 || Validation Loss: 0.012122 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.580s | Train Loss: 0.627857 || Validation Loss: 0.012933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.525s | Train Loss: 0.641375 || Validation Loss: 0.014178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.565s | Train Loss: 0.634478 || Validation Loss: 0.013366 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.567s | Train Loss: 0.632347 || Validation Loss: 0.011860 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.540s | Train Loss: 0.645454 || Validation Loss: 0.011558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 34.572s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.836%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010965\n",
      "INFO:root:Test AUC: 80.91%\n",
      "INFO:root:Test Time: 0.184s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5818294739958745\n",
      "validation AUC 0.59119292863007\n",
      "validation AUC 0.6023018148194897\n",
      "validation AUC 0.6140683897529249\n",
      "validation AUC 0.6253876082019684\n",
      "validation AUC 0.6355160169883503\n",
      "validation AUC 0.6459069526396417\n",
      "validation AUC 0.6560743670289948\n",
      "validation AUC 0.6658810806967923\n",
      "validation AUC 0.6750280781806381\n",
      "validation AUC 0.6835486366344453\n",
      "validation AUC 0.6911309560172183\n",
      "validation AUC 0.6984159840597457\n",
      "validation AUC 0.7051985688509024\n",
      "validation AUC 0.7120990750829654\n",
      "validation AUC 0.7177794802005857\n",
      "validation AUC 0.7224876347449353\n",
      "validation AUC 0.7271954567039935\n",
      "validation AUC 0.7306674263092101\n",
      "validation AUC 0.7341815730507362\n",
      "validation AUC 0.7378413628823746\n",
      "validation AUC 0.7413272106066291\n",
      "validation AUC 0.7450701999747555\n",
      "validation AUC 0.7478781484120091\n",
      "validation AUC 0.7503764014079904\n",
      "validation AUC 0.7534740289733407\n",
      "validation AUC 0.7559379086142888\n",
      "validation AUC 0.7580244316622899\n",
      "validation AUC 0.7613682069321205\n",
      "validation AUC 0.7644396161337816\n",
      "validation AUC 0.7653094863116408\n",
      "validation AUC 0.7675074653424842\n",
      "validation AUC 0.769841072655146\n",
      "validation AUC 0.7724024556607932\n",
      "validation AUC 0.775026130561172\n",
      "validation AUC 0.7765198003721975\n",
      "validation AUC 0.7775647381260665\n",
      "validation AUC 0.7788603627084725\n",
      "validation AUC 0.7808893458403852\n",
      "validation AUC 0.7818212418447426\n",
      "validation AUC 0.7834245610353078\n",
      "validation AUC 0.7852448748340533\n",
      "validation AUC 0.7873441877820191\n",
      "validation AUC 0.7874806807855952\n",
      "validation AUC 0.7892296084774022\n",
      "validation AUC 0.7906247532217138\n",
      "validation AUC 0.7908423358406894\n",
      "validation AUC 0.7915293559467207\n",
      "validation AUC 0.7921362309795802\n",
      "validation AUC 0.7923025661961799\n",
      "validation AUC 0.7925419983384571\n",
      "validation AUC 0.7926045376766453\n",
      "validation AUC 0.7926582621742713\n",
      "validation AUC 0.7927723176437509\n",
      "validation AUC 0.7928907871852173\n",
      "validation AUC 0.7930493026564465\n",
      "validation AUC 0.7931176130146105\n",
      "validation AUC 0.7931423546996057\n",
      "validation AUC 0.7932869148920094\n",
      "validation AUC 0.7934650901449817\n",
      "validation AUC 0.7936473601880614\n",
      "validation AUC 0.793800870915826\n",
      "validation AUC 0.7939715510266723\n",
      "validation AUC 0.7940858273327852\n",
      "validation AUC 0.7941678721331413\n",
      "validation AUC 0.7943838370572598\n",
      "validation AUC 0.7944161324193925\n",
      "validation AUC 0.7944816783286146\n",
      "validation AUC 0.7945843407563532\n",
      "validation AUC 0.7946859096436537\n",
      "validation AUC 0.7946859096436537\n",
      "train auc 0.836021675293895\n",
      "Test AUC 0.8091361200303642\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.97991473e-01\n",
      " 9.97991473e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.96072350e+01 1.86072350e+01 1.85991745e+01 ... 6.11047707e-02\n",
      " 6.10195883e-02 1.62200797e-02]\n",
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.058s | Train Loss: 0.215382 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.045s | Train Loss: 0.209453 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.043s | Train Loss: 0.204479 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.043s | Train Loss: 0.198788 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.192751 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.041s | Train Loss: 0.187475 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.043s | Train Loss: 0.182557 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.043s | Train Loss: 0.177208 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.043s | Train Loss: 0.171553 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.043s | Train Loss: 0.166765 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.044s | Train Loss: 0.161674 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.044s | Train Loss: 0.156807 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.152144 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.043s | Train Loss: 0.147758 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.043s | Train Loss: 0.143837 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.046s | Train Loss: 0.139877 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.045s | Train Loss: 0.135657 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.045s | Train Loss: 0.132166 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.045s | Train Loss: 0.128489 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.044s | Train Loss: 0.125220 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.043s | Train Loss: 0.121272 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.044s | Train Loss: 0.118955 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.045s | Train Loss: 0.115014 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.044s | Train Loss: 0.112953 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.045s | Train Loss: 0.109756 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.044s | Train Loss: 0.107477 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.044s | Train Loss: 0.104628 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.044s | Train Loss: 0.101963 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.044s | Train Loss: 0.100062 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.044s | Train Loss: 0.098013 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.095839 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.093931 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.044s | Train Loss: 0.090745 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.044s | Train Loss: 0.089483 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.087166 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.046s | Train Loss: 0.085833 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.044s | Train Loss: 0.084238 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.043s | Train Loss: 0.082814 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.044s | Train Loss: 0.080982 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.043s | Train Loss: 0.079841 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.042s | Train Loss: 0.078273 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.044s | Train Loss: 0.077114 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.043s | Train Loss: 0.075004 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.043s | Train Loss: 0.074256 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.043s | Train Loss: 0.072661 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.042s | Train Loss: 0.072174 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.043s | Train Loss: 0.071028 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.043s | Train Loss: 0.069480 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.043s | Train Loss: 0.068529 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.043s | Train Loss: 0.068088 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.045s | Train Loss: 0.065933 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.043s | Train Loss: 0.065224 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.046s | Train Loss: 0.063831 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.043s | Train Loss: 0.064165 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.044s | Train Loss: 0.063026 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.043s | Train Loss: 0.062335 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.044s | Train Loss: 0.061806 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.044s | Train Loss: 0.059943 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.043s | Train Loss: 0.059299 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.043s | Train Loss: 0.059236 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.043s | Train Loss: 0.058271 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.044s | Train Loss: 0.057829 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.043s | Train Loss: 0.057249 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.043s | Train Loss: 0.056417 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.043s | Train Loss: 0.055472 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.045s | Train Loss: 0.055299 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.042s | Train Loss: 0.054469 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.044s | Train Loss: 0.053623 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.043s | Train Loss: 0.053390 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.042s | Train Loss: 0.053195 |\n",
      "INFO:root:Pretraining Time: 3.093s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.055904\n",
      "INFO:root:Test AUC: 52.75%\n",
      "INFO:root:Test AUC: 52.75%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.432s | Train Loss: 1.195629 || Validation Loss: 0.012575 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.426s | Train Loss: 1.153251 || Validation Loss: 0.011775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.436s | Train Loss: 1.082073 || Validation Loss: 0.012159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.442s | Train Loss: 1.029040 || Validation Loss: 0.011446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.432s | Train Loss: 1.002196 || Validation Loss: 0.012332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.419s | Train Loss: 1.001036 || Validation Loss: 0.011582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.462s | Train Loss: 0.949537 || Validation Loss: 0.011485 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.433s | Train Loss: 0.937381 || Validation Loss: 0.011334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.436s | Train Loss: 0.911197 || Validation Loss: 0.010882 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.435s | Train Loss: 0.874563 || Validation Loss: 0.010868 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.436s | Train Loss: 0.876344 || Validation Loss: 0.010640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.434s | Train Loss: 0.861380 || Validation Loss: 0.011421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.438s | Train Loss: 0.817511 || Validation Loss: 0.010601 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.466s | Train Loss: 0.827459 || Validation Loss: 0.010738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.439s | Train Loss: 0.821582 || Validation Loss: 0.010964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.469s | Train Loss: 0.809815 || Validation Loss: 0.011010 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.443s | Train Loss: 0.788340 || Validation Loss: 0.010504 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.444s | Train Loss: 0.793702 || Validation Loss: 0.010740 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.472s | Train Loss: 0.811525 || Validation Loss: 0.010884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.476s | Train Loss: 0.764714 || Validation Loss: 0.011388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.446s | Train Loss: 0.761721 || Validation Loss: 0.010791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.477s | Train Loss: 0.746408 || Validation Loss: 0.010695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.450s | Train Loss: 0.738339 || Validation Loss: 0.011528 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.478s | Train Loss: 0.717391 || Validation Loss: 0.011127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.476s | Train Loss: 0.730329 || Validation Loss: 0.010625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.483s | Train Loss: 0.715142 || Validation Loss: 0.011227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.482s | Train Loss: 0.707905 || Validation Loss: 0.011746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.485s | Train Loss: 0.724067 || Validation Loss: 0.011050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.484s | Train Loss: 0.707210 || Validation Loss: 0.011039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.484s | Train Loss: 0.687928 || Validation Loss: 0.012037 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.518s | Train Loss: 0.687191 || Validation Loss: 0.011789 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.459s | Train Loss: 0.699989 || Validation Loss: 0.011710 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.523s | Train Loss: 0.690740 || Validation Loss: 0.011659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.460s | Train Loss: 0.661432 || Validation Loss: 0.011422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.522s | Train Loss: 0.674503 || Validation Loss: 0.011020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.470s | Train Loss: 0.662759 || Validation Loss: 0.011861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.530s | Train Loss: 0.649037 || Validation Loss: 0.011392 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.472s | Train Loss: 0.643359 || Validation Loss: 0.011392 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.533s | Train Loss: 0.658867 || Validation Loss: 0.011992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.501s | Train Loss: 0.656906 || Validation Loss: 0.011631 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.500s | Train Loss: 0.624247 || Validation Loss: 0.011335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.467s | Train Loss: 0.651505 || Validation Loss: 0.011475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.536s | Train Loss: 0.641296 || Validation Loss: 0.011906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.507s | Train Loss: 0.618796 || Validation Loss: 0.011572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.508s | Train Loss: 0.627482 || Validation Loss: 0.011983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.505s | Train Loss: 0.632866 || Validation Loss: 0.012064 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.543s | Train Loss: 0.620096 || Validation Loss: 0.012097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.510s | Train Loss: 0.616895 || Validation Loss: 0.011712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.509s | Train Loss: 0.602888 || Validation Loss: 0.011701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.508s | Train Loss: 0.616171 || Validation Loss: 0.010880 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.514s | Train Loss: 0.601322 || Validation Loss: 0.011796 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.517s | Train Loss: 0.621459 || Validation Loss: 0.011580 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.547s | Train Loss: 0.614698 || Validation Loss: 0.011956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.515s | Train Loss: 0.592245 || Validation Loss: 0.012625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.517s | Train Loss: 0.602554 || Validation Loss: 0.012319 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.518s | Train Loss: 0.595260 || Validation Loss: 0.012990 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.548s | Train Loss: 0.601957 || Validation Loss: 0.012471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.547s | Train Loss: 0.602094 || Validation Loss: 0.012054 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.521s | Train Loss: 0.624897 || Validation Loss: 0.011693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.554s | Train Loss: 0.594004 || Validation Loss: 0.012780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.521s | Train Loss: 0.604067 || Validation Loss: 0.012447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.555s | Train Loss: 0.610078 || Validation Loss: 0.011844 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.526s | Train Loss: 0.607380 || Validation Loss: 0.010521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.560s | Train Loss: 0.596795 || Validation Loss: 0.011528 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.526s | Train Loss: 0.607691 || Validation Loss: 0.011912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.592s | Train Loss: 0.622048 || Validation Loss: 0.011806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.534s | Train Loss: 0.593061 || Validation Loss: 0.012044 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.568s | Train Loss: 0.609735 || Validation Loss: 0.011230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.566s | Train Loss: 0.605627 || Validation Loss: 0.011879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.538s | Train Loss: 0.606172 || Validation Loss: 0.012506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 34.716s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.872%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010214\n",
      "INFO:root:Test AUC: 81.83%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6434127784696735\n",
      "validation AUC 0.6551284039971537\n",
      "validation AUC 0.6656918742548759\n",
      "validation AUC 0.6755131897473056\n",
      "validation AUC 0.6838937590609537\n",
      "validation AUC 0.692005599459179\n",
      "validation AUC 0.6993073844790267\n",
      "validation AUC 0.7053742004915664\n",
      "validation AUC 0.7112861169001646\n",
      "validation AUC 0.7174053510153058\n",
      "validation AUC 0.723298581451894\n",
      "validation AUC 0.7285714392141579\n",
      "validation AUC 0.7334230125181911\n",
      "validation AUC 0.7376736866818802\n",
      "validation AUC 0.7419531973206717\n",
      "validation AUC 0.7461177185904855\n",
      "validation AUC 0.7492643745491474\n",
      "validation AUC 0.7521944270199314\n",
      "validation AUC 0.7554242426048463\n",
      "validation AUC 0.7584554515571695\n",
      "validation AUC 0.7620275479598633\n",
      "validation AUC 0.764998449886474\n",
      "validation AUC 0.7679393100488863\n",
      "validation AUC 0.7702040801882827\n",
      "validation AUC 0.7727909392272485\n",
      "validation AUC 0.7761321336352178\n",
      "validation AUC 0.7787208924013679\n",
      "validation AUC 0.7808509069627502\n",
      "validation AUC 0.7827335232989565\n",
      "validation AUC 0.7846074737928111\n",
      "validation AUC 0.7871034332806234\n",
      "validation AUC 0.789499832696295\n",
      "validation AUC 0.7910635662551407\n",
      "validation AUC 0.7931183792911217\n",
      "validation AUC 0.794937442569172\n",
      "validation AUC 0.7965370447865207\n",
      "validation AUC 0.7980034692104777\n",
      "validation AUC 0.7996630937605298\n",
      "validation AUC 0.8009449227050497\n",
      "validation AUC 0.8015897098004424\n",
      "validation AUC 0.8028409275947455\n",
      "validation AUC 0.8042122060185486\n",
      "validation AUC 0.805297886800951\n",
      "validation AUC 0.8065862450599111\n",
      "validation AUC 0.8070970481751658\n",
      "validation AUC 0.8080511528949393\n",
      "validation AUC 0.8085798943304692\n",
      "validation AUC 0.8096280222424529\n",
      "validation AUC 0.8110006123826453\n",
      "validation AUC 0.8111314514362683\n",
      "validation AUC 0.8112083158881263\n",
      "validation AUC 0.8112218614218729\n",
      "validation AUC 0.811179819980362\n",
      "validation AUC 0.811174964235108\n",
      "validation AUC 0.8112356889879467\n",
      "validation AUC 0.8113386893223412\n",
      "validation AUC 0.8114373873334066\n",
      "validation AUC 0.811430852697602\n",
      "validation AUC 0.8114565442461893\n",
      "validation AUC 0.8115571127169334\n",
      "validation AUC 0.8117351708998832\n",
      "validation AUC 0.8118333793453998\n",
      "validation AUC 0.8119043716713533\n",
      "validation AUC 0.8119999620054562\n",
      "validation AUC 0.8121314343014739\n",
      "validation AUC 0.8122086898736346\n",
      "validation AUC 0.8122500155915985\n",
      "validation AUC 0.8123044398486775\n",
      "validation AUC 0.8123562619584368\n",
      "validation AUC 0.8123848403473531\n",
      "validation AUC 0.8123848403473531\n",
      "train auc 0.8718949256316737\n",
      "Test AUC 0.8182678239032678\n",
      "test false positive rates [0.         0.         0.         ... 0.99909048 0.99909048 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 8.39865622e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [15.45551777 14.45551777 13.30648994 ...  0.02942057  0.02933195\n",
      "  0.01579805]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.058s | Train Loss: 0.221528 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.044s | Train Loss: 0.215788 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.042s | Train Loss: 0.210422 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.043s | Train Loss: 0.205224 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.200001 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.043s | Train Loss: 0.194573 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.043s | Train Loss: 0.189481 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.044s | Train Loss: 0.185103 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.043s | Train Loss: 0.179950 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.043s | Train Loss: 0.174938 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.043s | Train Loss: 0.170565 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.043s | Train Loss: 0.165758 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.043s | Train Loss: 0.161340 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.042s | Train Loss: 0.156559 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.043s | Train Loss: 0.152880 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.043s | Train Loss: 0.148169 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.043s | Train Loss: 0.144005 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.043s | Train Loss: 0.140225 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.042s | Train Loss: 0.136210 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.043s | Train Loss: 0.132208 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.043s | Train Loss: 0.128911 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.043s | Train Loss: 0.125153 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.043s | Train Loss: 0.121689 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.043s | Train Loss: 0.117971 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.043s | Train Loss: 0.115760 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.043s | Train Loss: 0.112157 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.042s | Train Loss: 0.109917 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.043s | Train Loss: 0.106955 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.042s | Train Loss: 0.104027 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.043s | Train Loss: 0.101841 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.098821 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.043s | Train Loss: 0.096797 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.043s | Train Loss: 0.094820 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.042s | Train Loss: 0.092083 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.090449 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.043s | Train Loss: 0.088494 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.043s | Train Loss: 0.086858 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.043s | Train Loss: 0.085285 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.043s | Train Loss: 0.084038 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.043s | Train Loss: 0.081764 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.043s | Train Loss: 0.080157 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.043s | Train Loss: 0.078843 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.043s | Train Loss: 0.077819 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.042s | Train Loss: 0.076211 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.043s | Train Loss: 0.074856 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.073342 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.043s | Train Loss: 0.072302 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.043s | Train Loss: 0.069959 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.042s | Train Loss: 0.070407 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.043s | Train Loss: 0.068443 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.043s | Train Loss: 0.067710 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.043s | Train Loss: 0.067191 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.041s | Train Loss: 0.065183 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.043s | Train Loss: 0.064432 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.043s | Train Loss: 0.063357 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.044s | Train Loss: 0.062302 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.043s | Train Loss: 0.062402 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.061572 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.043s | Train Loss: 0.060372 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.043s | Train Loss: 0.059550 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.043s | Train Loss: 0.059171 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.044s | Train Loss: 0.058810 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.043s | Train Loss: 0.057675 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.044s | Train Loss: 0.056765 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.043s | Train Loss: 0.055889 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.043s | Train Loss: 0.055154 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.044s | Train Loss: 0.055620 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.043s | Train Loss: 0.054613 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.042s | Train Loss: 0.053603 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.041s | Train Loss: 0.053198 |\n",
      "INFO:root:Pretraining Time: 3.031s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.056149\n",
      "INFO:root:Test AUC: 51.22%\n",
      "INFO:root:Test AUC: 51.22%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.434s | Train Loss: 1.742312 || Validation Loss: 0.014312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.412s | Train Loss: 1.565061 || Validation Loss: 0.014444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.427s | Train Loss: 1.426624 || Validation Loss: 0.015081 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.443s | Train Loss: 1.372130 || Validation Loss: 0.012827 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.437s | Train Loss: 1.312849 || Validation Loss: 0.013888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.439s | Train Loss: 1.228727 || Validation Loss: 0.012408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.465s | Train Loss: 1.166202 || Validation Loss: 0.012442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.441s | Train Loss: 1.116109 || Validation Loss: 0.011590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.439s | Train Loss: 1.097224 || Validation Loss: 0.011667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.438s | Train Loss: 1.051458 || Validation Loss: 0.012382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.441s | Train Loss: 1.030851 || Validation Loss: 0.011846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.443s | Train Loss: 0.990667 || Validation Loss: 0.010763 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.440s | Train Loss: 0.991293 || Validation Loss: 0.011049 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.472s | Train Loss: 0.955508 || Validation Loss: 0.011801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.446s | Train Loss: 0.925662 || Validation Loss: 0.011479 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.478s | Train Loss: 0.894159 || Validation Loss: 0.011137 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.442s | Train Loss: 0.899791 || Validation Loss: 0.010850 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.449s | Train Loss: 0.887326 || Validation Loss: 0.010816 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.478s | Train Loss: 0.868773 || Validation Loss: 0.010913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.477s | Train Loss: 0.855967 || Validation Loss: 0.010914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.450s | Train Loss: 0.852777 || Validation Loss: 0.011015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.483s | Train Loss: 0.832448 || Validation Loss: 0.011256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.457s | Train Loss: 0.817468 || Validation Loss: 0.010706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.482s | Train Loss: 0.823706 || Validation Loss: 0.011138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.488s | Train Loss: 0.794992 || Validation Loss: 0.010023 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.486s | Train Loss: 0.785723 || Validation Loss: 0.010222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.488s | Train Loss: 0.778338 || Validation Loss: 0.010514 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.488s | Train Loss: 0.781414 || Validation Loss: 0.010770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.492s | Train Loss: 0.765845 || Validation Loss: 0.010333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.491s | Train Loss: 0.754221 || Validation Loss: 0.010627 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.522s | Train Loss: 0.753939 || Validation Loss: 0.009460 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.469s | Train Loss: 0.745245 || Validation Loss: 0.010686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.529s | Train Loss: 0.726772 || Validation Loss: 0.010669 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.471s | Train Loss: 0.725313 || Validation Loss: 0.010771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.537s | Train Loss: 0.716243 || Validation Loss: 0.010568 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.537s | Train Loss: 0.718145 || Validation Loss: 0.010734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.530s | Train Loss: 0.704957 || Validation Loss: 0.010934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.471s | Train Loss: 0.686052 || Validation Loss: 0.011155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.533s | Train Loss: 0.705834 || Validation Loss: 0.011392 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.502s | Train Loss: 0.688224 || Validation Loss: 0.010932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.506s | Train Loss: 0.680530 || Validation Loss: 0.011222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.478s | Train Loss: 0.687905 || Validation Loss: 0.010184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.540s | Train Loss: 0.667969 || Validation Loss: 0.011475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.510s | Train Loss: 0.663156 || Validation Loss: 0.011485 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.511s | Train Loss: 0.664047 || Validation Loss: 0.010907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.512s | Train Loss: 0.658456 || Validation Loss: 0.012034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.544s | Train Loss: 0.639613 || Validation Loss: 0.012010 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.512s | Train Loss: 0.654815 || Validation Loss: 0.012202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.511s | Train Loss: 0.633811 || Validation Loss: 0.011356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.515s | Train Loss: 0.640132 || Validation Loss: 0.012747 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.519s | Train Loss: 0.652179 || Validation Loss: 0.012102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.521s | Train Loss: 0.632413 || Validation Loss: 0.010797 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.552s | Train Loss: 0.637329 || Validation Loss: 0.011058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.521s | Train Loss: 0.642413 || Validation Loss: 0.010690 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.521s | Train Loss: 0.614720 || Validation Loss: 0.011091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.524s | Train Loss: 0.633448 || Validation Loss: 0.010682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.552s | Train Loss: 0.624571 || Validation Loss: 0.010585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.558s | Train Loss: 0.629903 || Validation Loss: 0.012429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.528s | Train Loss: 0.645686 || Validation Loss: 0.010983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.559s | Train Loss: 0.632580 || Validation Loss: 0.012024 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.526s | Train Loss: 0.623201 || Validation Loss: 0.011378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.565s | Train Loss: 0.631044 || Validation Loss: 0.010822 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.531s | Train Loss: 0.620634 || Validation Loss: 0.011468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.565s | Train Loss: 0.623931 || Validation Loss: 0.012626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.529s | Train Loss: 0.655250 || Validation Loss: 0.011897 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.602s | Train Loss: 0.623685 || Validation Loss: 0.012675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.538s | Train Loss: 0.639227 || Validation Loss: 0.011643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.574s | Train Loss: 0.633411 || Validation Loss: 0.012626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.570s | Train Loss: 0.643770 || Validation Loss: 0.011253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.545s | Train Loss: 0.636352 || Validation Loss: 0.012308 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 35.124s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.820%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010509\n",
      "INFO:root:Test AUC: 82.55%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4724556932535526\n",
      "validation AUC 0.49164773236430576\n",
      "validation AUC 0.5110561674297149\n",
      "validation AUC 0.5318342445992938\n",
      "validation AUC 0.5531752928825897\n",
      "validation AUC 0.5752590679778793\n",
      "validation AUC 0.596080288111454\n",
      "validation AUC 0.6154958937157451\n",
      "validation AUC 0.6337976871433488\n",
      "validation AUC 0.6504958474198724\n",
      "validation AUC 0.6655663033522682\n",
      "validation AUC 0.6790502077141483\n",
      "validation AUC 0.690619697925285\n",
      "validation AUC 0.7001486017688641\n",
      "validation AUC 0.7092577111363177\n",
      "validation AUC 0.7182594672930835\n",
      "validation AUC 0.7255514104489891\n",
      "validation AUC 0.7329301530445762\n",
      "validation AUC 0.7391872317899175\n",
      "validation AUC 0.7444328574685247\n",
      "validation AUC 0.7494390643082661\n",
      "validation AUC 0.7532049914187674\n",
      "validation AUC 0.7577024944216133\n",
      "validation AUC 0.762390065927451\n",
      "validation AUC 0.7664477900691842\n",
      "validation AUC 0.7695719765655614\n",
      "validation AUC 0.774004181102642\n",
      "validation AUC 0.7777434109266348\n",
      "validation AUC 0.7807387538811373\n",
      "validation AUC 0.7835918940077815\n",
      "validation AUC 0.785977871530763\n",
      "validation AUC 0.7886430504148217\n",
      "validation AUC 0.7908072707082502\n",
      "validation AUC 0.7932878567735545\n",
      "validation AUC 0.7950166377787519\n",
      "validation AUC 0.7972612532494913\n",
      "validation AUC 0.7988000083226142\n",
      "validation AUC 0.8006574572646525\n",
      "validation AUC 0.8024526169726417\n",
      "validation AUC 0.8046089483429589\n",
      "validation AUC 0.8065848535230521\n",
      "validation AUC 0.8082141436338492\n",
      "validation AUC 0.8095611193852419\n",
      "validation AUC 0.811103743090474\n",
      "validation AUC 0.8124544251723537\n",
      "validation AUC 0.8140545834723096\n",
      "validation AUC 0.815296382451161\n",
      "validation AUC 0.8159279565980985\n",
      "validation AUC 0.8174641707195953\n",
      "validation AUC 0.8176093801184876\n",
      "validation AUC 0.8176979968042013\n",
      "validation AUC 0.8177596900454082\n",
      "validation AUC 0.8179019833364658\n",
      "validation AUC 0.818010688173778\n",
      "validation AUC 0.8181632836268208\n",
      "validation AUC 0.8182512856949159\n",
      "validation AUC 0.818387174723603\n",
      "validation AUC 0.818521286416493\n",
      "validation AUC 0.8186238211314797\n",
      "validation AUC 0.8187522496069108\n",
      "validation AUC 0.8188342491756675\n",
      "validation AUC 0.8189030411173335\n",
      "validation AUC 0.819018014522217\n",
      "validation AUC 0.8191004823710639\n",
      "validation AUC 0.8192301613671906\n",
      "validation AUC 0.8193163275644775\n",
      "validation AUC 0.8194133865952269\n",
      "validation AUC 0.8195645745473168\n",
      "validation AUC 0.8196626792262225\n",
      "validation AUC 0.819756199549472\n",
      "validation AUC 0.819756199549472\n",
      "train auc 0.8202735335153448\n",
      "Test AUC 0.8254761042117675\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.98237802e-01\n",
      " 9.98237802e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [26.40491104 25.40491104 23.95436668 ...  0.05473024  0.05472672\n",
      "  0.02835968]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.01\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.01\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.057s | Train Loss: 0.215827 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.045s | Train Loss: 0.210090 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.042s | Train Loss: 0.204209 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.044s | Train Loss: 0.199111 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.044s | Train Loss: 0.193963 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.042s | Train Loss: 0.187957 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.043s | Train Loss: 0.183096 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.043s | Train Loss: 0.178228 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.043s | Train Loss: 0.173249 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.043s | Train Loss: 0.167987 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.044s | Train Loss: 0.163872 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.043s | Train Loss: 0.159388 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.044s | Train Loss: 0.154753 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.043s | Train Loss: 0.150539 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.043s | Train Loss: 0.146366 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.043s | Train Loss: 0.142539 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.043s | Train Loss: 0.138351 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.043s | Train Loss: 0.134357 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.043s | Train Loss: 0.131340 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.043s | Train Loss: 0.127207 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.043s | Train Loss: 0.124161 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.044s | Train Loss: 0.120593 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.043s | Train Loss: 0.117381 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.043s | Train Loss: 0.114277 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.043s | Train Loss: 0.111689 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.043s | Train Loss: 0.108923 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.044s | Train Loss: 0.106077 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.043s | Train Loss: 0.103744 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.043s | Train Loss: 0.101903 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.043s | Train Loss: 0.099339 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.043s | Train Loss: 0.097173 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.044s | Train Loss: 0.094643 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.044s | Train Loss: 0.092940 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.043s | Train Loss: 0.090030 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.043s | Train Loss: 0.088764 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.043s | Train Loss: 0.087029 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.043s | Train Loss: 0.085525 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.044s | Train Loss: 0.083563 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.044s | Train Loss: 0.081778 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.044s | Train Loss: 0.080364 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.044s | Train Loss: 0.078753 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.042s | Train Loss: 0.077963 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.043s | Train Loss: 0.075983 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.043s | Train Loss: 0.075180 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.043s | Train Loss: 0.073772 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.043s | Train Loss: 0.071857 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.044s | Train Loss: 0.071087 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.044s | Train Loss: 0.069929 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.044s | Train Loss: 0.068907 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.043s | Train Loss: 0.068673 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.043s | Train Loss: 0.067006 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.044s | Train Loss: 0.065305 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.042s | Train Loss: 0.065142 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.044s | Train Loss: 0.063613 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.044s | Train Loss: 0.063044 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.044s | Train Loss: 0.061842 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.044s | Train Loss: 0.061415 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.043s | Train Loss: 0.060558 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.042s | Train Loss: 0.060073 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.044s | Train Loss: 0.058954 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.042s | Train Loss: 0.058173 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.043s | Train Loss: 0.057237 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.044s | Train Loss: 0.056535 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.044s | Train Loss: 0.056564 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.043s | Train Loss: 0.056422 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.043s | Train Loss: 0.055078 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.043s | Train Loss: 0.054480 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.043s | Train Loss: 0.054505 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.042s | Train Loss: 0.053077 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.042s | Train Loss: 0.052975 |\n",
      "INFO:root:Pretraining Time: 3.059s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.056378\n",
      "INFO:root:Test AUC: 50.95%\n",
      "INFO:root:Test AUC: 50.95%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.430s | Train Loss: 1.347014 || Validation Loss: 0.010125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.414s | Train Loss: 1.238362 || Validation Loss: 0.010737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.443s | Train Loss: 1.143173 || Validation Loss: 0.010375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.436s | Train Loss: 1.100254 || Validation Loss: 0.009873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.439s | Train Loss: 1.053912 || Validation Loss: 0.010548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.435s | Train Loss: 1.003743 || Validation Loss: 0.011178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.480s | Train Loss: 0.986669 || Validation Loss: 0.009829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.439s | Train Loss: 0.957880 || Validation Loss: 0.010742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.449s | Train Loss: 0.937087 || Validation Loss: 0.009494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.435s | Train Loss: 0.908943 || Validation Loss: 0.009845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.439s | Train Loss: 0.874304 || Validation Loss: 0.010282 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.444s | Train Loss: 0.851130 || Validation Loss: 0.009866 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.445s | Train Loss: 0.849023 || Validation Loss: 0.009753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.473s | Train Loss: 0.840147 || Validation Loss: 0.010846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.443s | Train Loss: 0.804025 || Validation Loss: 0.009773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.481s | Train Loss: 0.808075 || Validation Loss: 0.010082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.459s | Train Loss: 0.791841 || Validation Loss: 0.010338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.448s | Train Loss: 0.759718 || Validation Loss: 0.010381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.474s | Train Loss: 0.777268 || Validation Loss: 0.009847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.472s | Train Loss: 0.759002 || Validation Loss: 0.010647 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.451s | Train Loss: 0.730333 || Validation Loss: 0.010354 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.492s | Train Loss: 0.748705 || Validation Loss: 0.010246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.466s | Train Loss: 0.734827 || Validation Loss: 0.011257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.479s | Train Loss: 0.731038 || Validation Loss: 0.010590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.488s | Train Loss: 0.731412 || Validation Loss: 0.010997 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.498s | Train Loss: 0.695289 || Validation Loss: 0.010655 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.483s | Train Loss: 0.699062 || Validation Loss: 0.010446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.499s | Train Loss: 0.707450 || Validation Loss: 0.011050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.489s | Train Loss: 0.695990 || Validation Loss: 0.011136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.489s | Train Loss: 0.681564 || Validation Loss: 0.011143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.524s | Train Loss: 0.691012 || Validation Loss: 0.011107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.460s | Train Loss: 0.679311 || Validation Loss: 0.011759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.533s | Train Loss: 0.655424 || Validation Loss: 0.010386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.469s | Train Loss: 0.663805 || Validation Loss: 0.010083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.528s | Train Loss: 0.637336 || Validation Loss: 0.011139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.463s | Train Loss: 0.664660 || Validation Loss: 0.010885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.529s | Train Loss: 0.633053 || Validation Loss: 0.011724 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.469s | Train Loss: 0.617631 || Validation Loss: 0.011235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.548s | Train Loss: 0.654803 || Validation Loss: 0.011589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.502s | Train Loss: 0.619957 || Validation Loss: 0.011683 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.508s | Train Loss: 0.633209 || Validation Loss: 0.010927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.485s | Train Loss: 0.637360 || Validation Loss: 0.011666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.549s | Train Loss: 0.596082 || Validation Loss: 0.011556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.516s | Train Loss: 0.606554 || Validation Loss: 0.010955 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.516s | Train Loss: 0.611951 || Validation Loss: 0.012024 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.514s | Train Loss: 0.619486 || Validation Loss: 0.012257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.549s | Train Loss: 0.606342 || Validation Loss: 0.011543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.514s | Train Loss: 0.609288 || Validation Loss: 0.012142 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.511s | Train Loss: 0.606478 || Validation Loss: 0.012007 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.520s | Train Loss: 0.590898 || Validation Loss: 0.012263 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.540s | Train Loss: 0.607837 || Validation Loss: 0.012371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.515s | Train Loss: 0.598932 || Validation Loss: 0.012457 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.548s | Train Loss: 0.601579 || Validation Loss: 0.012655 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.503s | Train Loss: 0.586713 || Validation Loss: 0.011375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.526s | Train Loss: 0.593861 || Validation Loss: 0.011175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.533s | Train Loss: 0.610994 || Validation Loss: 0.011576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.553s | Train Loss: 0.593264 || Validation Loss: 0.013396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.563s | Train Loss: 0.599953 || Validation Loss: 0.012735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.526s | Train Loss: 0.613188 || Validation Loss: 0.012379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.570s | Train Loss: 0.599568 || Validation Loss: 0.011888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.534s | Train Loss: 0.585759 || Validation Loss: 0.012335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.573s | Train Loss: 0.593115 || Validation Loss: 0.012257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.527s | Train Loss: 0.595563 || Validation Loss: 0.012781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.563s | Train Loss: 0.585527 || Validation Loss: 0.012815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.530s | Train Loss: 0.595528 || Validation Loss: 0.013076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.601s | Train Loss: 0.600964 || Validation Loss: 0.012279 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.535s | Train Loss: 0.597168 || Validation Loss: 0.012653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.570s | Train Loss: 0.579360 || Validation Loss: 0.012388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.571s | Train Loss: 0.565238 || Validation Loss: 0.012420 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.538s | Train Loss: 0.592935 || Validation Loss: 0.011499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 35.181s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.876%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010853\n",
      "INFO:root:Test AUC: 80.58%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 2462 random outlier 167 unlabeled 0 0\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/1_per_labeled_0_unlabeled\n",
      "final sizes 2462 167 0 0\n",
      "len of selected to training 2629\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({1.0: 2462, -1.0: 590})\n",
      "class weights 0.19331585845347313 0.8066841415465269\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5211633625958724\n",
      "validation AUC 0.5488259419932938\n",
      "validation AUC 0.5762512550438554\n",
      "validation AUC 0.5999185325677097\n",
      "validation AUC 0.620897017651818\n",
      "validation AUC 0.6378292221888986\n",
      "validation AUC 0.65368139989353\n",
      "validation AUC 0.6669895661874529\n",
      "validation AUC 0.678366984401803\n",
      "validation AUC 0.6883096456332989\n",
      "validation AUC 0.6973596798709589\n",
      "validation AUC 0.7030914388182483\n",
      "validation AUC 0.7094520074635332\n",
      "validation AUC 0.7165713468352674\n",
      "validation AUC 0.722223774878178\n",
      "validation AUC 0.7255363243801728\n",
      "validation AUC 0.7313905305890134\n",
      "validation AUC 0.7349946004112776\n",
      "validation AUC 0.7373282715803156\n",
      "validation AUC 0.7413367890630205\n",
      "validation AUC 0.7442239460132656\n",
      "validation AUC 0.7466951664767649\n",
      "validation AUC 0.7508060803189754\n",
      "validation AUC 0.7537140438050483\n",
      "validation AUC 0.7550040357229595\n",
      "validation AUC 0.7570722905260764\n",
      "validation AUC 0.7594516083611836\n",
      "validation AUC 0.7613840592774481\n",
      "validation AUC 0.7651607887582127\n",
      "validation AUC 0.7680022325253303\n",
      "validation AUC 0.770009009602509\n",
      "validation AUC 0.7703024908456564\n",
      "validation AUC 0.7721297011330037\n",
      "validation AUC 0.7732163903140096\n",
      "validation AUC 0.7758209349084287\n",
      "validation AUC 0.777013902384461\n",
      "validation AUC 0.7769171785996851\n",
      "validation AUC 0.7768688260196853\n",
      "validation AUC 0.7780429585254967\n",
      "validation AUC 0.7784255513412712\n",
      "validation AUC 0.7801419734770414\n",
      "validation AUC 0.7816215682785365\n",
      "validation AUC 0.7835217557608563\n",
      "validation AUC 0.7847390392127105\n",
      "validation AUC 0.78509224479214\n",
      "validation AUC 0.7858002310749391\n",
      "validation AUC 0.7866646202471965\n",
      "validation AUC 0.7870569777671256\n",
      "validation AUC 0.7874532358473518\n",
      "validation AUC 0.7875708779166134\n",
      "validation AUC 0.7876946129484128\n",
      "validation AUC 0.7877781078206394\n",
      "validation AUC 0.7877734117163253\n",
      "validation AUC 0.7878143489746687\n",
      "validation AUC 0.7878868046759043\n",
      "validation AUC 0.78794549400676\n",
      "validation AUC 0.7880079535244782\n",
      "validation AUC 0.788092560561919\n",
      "validation AUC 0.7882070976149004\n",
      "validation AUC 0.7883562235381839\n",
      "validation AUC 0.7884039934287532\n",
      "validation AUC 0.788359014593949\n",
      "validation AUC 0.7883503168234092\n",
      "validation AUC 0.7883407170815593\n",
      "validation AUC 0.7883778894744046\n",
      "validation AUC 0.788308554753543\n",
      "validation AUC 0.7881243397516796\n",
      "validation AUC 0.7881351873535427\n",
      "validation AUC 0.7882774008241303\n",
      "validation AUC 0.7883463497460538\n",
      "validation AUC 0.7883463497460538\n",
      "train auc 0.8756616642128885\n",
      "Test AUC 0.8058064274851031\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 7.57934628e-05 ... 9.99886310e-01\n",
      " 9.99886310e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [16.52113342 15.52113342 13.37652016 ...  0.03055762  0.03020658\n",
      "  0.02250504]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.01 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.01 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f77b63",
   "metadata": {},
   "source": [
    "# Restriction on the number of labeled samples (k means active selection) 6.1 Optimal construction of a dataset-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49602e1",
   "metadata": {},
   "source": [
    "# Random sampling of 1k points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29981ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test 28.06 after previous bad results. 1k samples selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2e63be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.036s | Train Loss: 0.215099 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.019s | Train Loss: 0.212934 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.209853 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.208055 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.205840 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.203471 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.201539 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.198756 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.197014 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.194866 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.192642 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.190366 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.019s | Train Loss: 0.188265 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.186153 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.184007 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.182366 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.020s | Train Loss: 0.179658 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.178072 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.175801 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.174115 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.172328 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.170402 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.168531 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.166300 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.019s | Train Loss: 0.164434 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.162297 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.160558 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.158077 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.019s | Train Loss: 0.157719 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.155169 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.153715 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.152001 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.019s | Train Loss: 0.150402 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.147600 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.146039 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.145514 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.019s | Train Loss: 0.144430 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.141920 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.139338 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.139031 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.137427 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.135289 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.134010 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.132087 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.131413 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.129914 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.128132 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.126972 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.125971 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.123647 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.122447 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.121122 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.120276 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.119571 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.118112 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.116356 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.115024 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.113938 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.112663 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.111331 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.110031 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.019s | Train Loss: 0.108836 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.107749 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.106923 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.106042 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.018s | Train Loss: 0.105005 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.018s | Train Loss: 0.103104 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.102074 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.018s | Train Loss: 0.101828 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.100184 |\n",
      "INFO:root:Pretraining Time: 1.264s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.108042\n",
      "INFO:root:Test AUC: 54.03%\n",
      "INFO:root:Test AUC: 54.03%\n",
      "INFO:root:Test Time: 0.234s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.417s | Train Loss: 1.580337 || Validation Loss: 0.011808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.397s | Train Loss: 1.527872 || Validation Loss: 0.010851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.464s | Train Loss: 1.479270 || Validation Loss: 0.011371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.398s | Train Loss: 1.439139 || Validation Loss: 0.010591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.422s | Train Loss: 1.329112 || Validation Loss: 0.011378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.418s | Train Loss: 1.346154 || Validation Loss: 0.011186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.422s | Train Loss: 1.296589 || Validation Loss: 0.010447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.433s | Train Loss: 1.198736 || Validation Loss: 0.010552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.421s | Train Loss: 1.245709 || Validation Loss: 0.011526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.426s | Train Loss: 1.165464 || Validation Loss: 0.010523 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.429s | Train Loss: 1.150196 || Validation Loss: 0.010506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.425s | Train Loss: 1.129935 || Validation Loss: 0.011671 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.441s | Train Loss: 1.113800 || Validation Loss: 0.009577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.431s | Train Loss: 1.081624 || Validation Loss: 0.010475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.430s | Train Loss: 1.096652 || Validation Loss: 0.010247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.430s | Train Loss: 1.044715 || Validation Loss: 0.010534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.428s | Train Loss: 1.087667 || Validation Loss: 0.010262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.432s | Train Loss: 1.019474 || Validation Loss: 0.009261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.426s | Train Loss: 1.007668 || Validation Loss: 0.010177 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.430s | Train Loss: 1.015850 || Validation Loss: 0.009891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.446s | Train Loss: 1.028393 || Validation Loss: 0.010605 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.434s | Train Loss: 1.004834 || Validation Loss: 0.010971 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.442s | Train Loss: 0.987892 || Validation Loss: 0.010607 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.434s | Train Loss: 0.968910 || Validation Loss: 0.009533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.465s | Train Loss: 0.970934 || Validation Loss: 0.010567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.436s | Train Loss: 0.959674 || Validation Loss: 0.009602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.433s | Train Loss: 0.916011 || Validation Loss: 0.010206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.462s | Train Loss: 0.904049 || Validation Loss: 0.010828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.447s | Train Loss: 0.906737 || Validation Loss: 0.010561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.441s | Train Loss: 0.922674 || Validation Loss: 0.009973 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.430s | Train Loss: 0.908866 || Validation Loss: 0.010539 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.465s | Train Loss: 0.878665 || Validation Loss: 0.009640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.439s | Train Loss: 0.866408 || Validation Loss: 0.009883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.426s | Train Loss: 0.871808 || Validation Loss: 0.010013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.464s | Train Loss: 0.837498 || Validation Loss: 0.010330 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.434s | Train Loss: 0.826337 || Validation Loss: 0.009767 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.475s | Train Loss: 0.822139 || Validation Loss: 0.009842 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.473s | Train Loss: 0.820808 || Validation Loss: 0.010058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.433s | Train Loss: 0.811240 || Validation Loss: 0.009276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.467s | Train Loss: 0.804528 || Validation Loss: 0.009982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.437s | Train Loss: 0.820832 || Validation Loss: 0.010126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.473s | Train Loss: 0.801585 || Validation Loss: 0.010109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.447s | Train Loss: 0.784794 || Validation Loss: 0.009749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.467s | Train Loss: 0.758494 || Validation Loss: 0.010776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.435s | Train Loss: 0.789448 || Validation Loss: 0.011082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.473s | Train Loss: 0.773413 || Validation Loss: 0.009384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.429s | Train Loss: 0.737011 || Validation Loss: 0.008685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.468s | Train Loss: 0.772604 || Validation Loss: 0.009722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.476s | Train Loss: 0.752055 || Validation Loss: 0.009431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.447s | Train Loss: 0.744895 || Validation Loss: 0.010451 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.485s | Train Loss: 0.742678 || Validation Loss: 0.009386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.449s | Train Loss: 0.777516 || Validation Loss: 0.010044 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.474s | Train Loss: 0.781722 || Validation Loss: 0.010401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.440s | Train Loss: 0.741430 || Validation Loss: 0.010300 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.471s | Train Loss: 0.744715 || Validation Loss: 0.009296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.452s | Train Loss: 0.771501 || Validation Loss: 0.010349 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.484s | Train Loss: 0.771633 || Validation Loss: 0.009983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.467s | Train Loss: 0.760623 || Validation Loss: 0.009878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.477s | Train Loss: 0.723527 || Validation Loss: 0.009928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.449s | Train Loss: 0.749529 || Validation Loss: 0.009981 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.496s | Train Loss: 0.729950 || Validation Loss: 0.009375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.486s | Train Loss: 0.731261 || Validation Loss: 0.010285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.486s | Train Loss: 0.758837 || Validation Loss: 0.008988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.489s | Train Loss: 0.734074 || Validation Loss: 0.010055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.491s | Train Loss: 0.734851 || Validation Loss: 0.009660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.474s | Train Loss: 0.725438 || Validation Loss: 0.009343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.478s | Train Loss: 0.780253 || Validation Loss: 0.009897 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.477s | Train Loss: 0.759641 || Validation Loss: 0.010116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.479s | Train Loss: 0.749053 || Validation Loss: 0.009304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.483s | Train Loss: 0.737557 || Validation Loss: 0.009486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.636s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.734%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008384\n",
      "INFO:root:Test AUC: 70.85%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 941 random outlier 59\n",
      "final sizes 941 59 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 941, -1.0: 59})\n",
      "Resampled dataset shape Counter({1.0: 941, -1.0: 225})\n",
      "class weights 0.19296740994854203 0.807032590051458\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.47984393448506113\n",
      "validation AUC 0.48632685460733327\n",
      "validation AUC 0.49338781328736236\n",
      "validation AUC 0.5003147267915597\n",
      "validation AUC 0.5079380765565578\n",
      "validation AUC 0.5151339371521291\n",
      "validation AUC 0.5213768637281524\n",
      "validation AUC 0.5285157032406897\n",
      "validation AUC 0.5360421509551956\n",
      "validation AUC 0.5438257270101615\n",
      "validation AUC 0.5505199026488263\n",
      "validation AUC 0.5567335484157978\n",
      "validation AUC 0.5631412776553982\n",
      "validation AUC 0.5687814745416336\n",
      "validation AUC 0.5745705392649663\n",
      "validation AUC 0.5805891171068464\n",
      "validation AUC 0.586526435049658\n",
      "validation AUC 0.5927548768710716\n",
      "validation AUC 0.5987426094226894\n",
      "validation AUC 0.6040584850968796\n",
      "validation AUC 0.6092244712319578\n",
      "validation AUC 0.6134673256502761\n",
      "validation AUC 0.6176617849176774\n",
      "validation AUC 0.6219503445370764\n",
      "validation AUC 0.6265478546066947\n",
      "validation AUC 0.6303355514200275\n",
      "validation AUC 0.6332505098399482\n",
      "validation AUC 0.6366405357507371\n",
      "validation AUC 0.6407041639465624\n",
      "validation AUC 0.6442490229442345\n",
      "validation AUC 0.6476638038434301\n",
      "validation AUC 0.6509016626710314\n",
      "validation AUC 0.6541410141414201\n",
      "validation AUC 0.6574352368635195\n",
      "validation AUC 0.6601334082084818\n",
      "validation AUC 0.6630334029509733\n",
      "validation AUC 0.665954100462682\n",
      "validation AUC 0.6688748698128136\n",
      "validation AUC 0.6708363753801316\n",
      "validation AUC 0.6731399249645011\n",
      "validation AUC 0.675547084179519\n",
      "validation AUC 0.6774767786288887\n",
      "validation AUC 0.6794650479997736\n",
      "validation AUC 0.6812956054254931\n",
      "validation AUC 0.6828236140026815\n",
      "validation AUC 0.6846580746493807\n",
      "validation AUC 0.6865075920973904\n",
      "validation AUC 0.6881732351108749\n",
      "validation AUC 0.6901113160309517\n",
      "validation AUC 0.6903423483991101\n",
      "validation AUC 0.6905180731636555\n",
      "validation AUC 0.6906607841818392\n",
      "validation AUC 0.6907756937303469\n",
      "validation AUC 0.6909463046634524\n",
      "validation AUC 0.6911518876051155\n",
      "validation AUC 0.6914066878485361\n",
      "validation AUC 0.6916031207036633\n",
      "validation AUC 0.691768644412152\n",
      "validation AUC 0.6919505499417524\n",
      "validation AUC 0.6921825428162321\n",
      "validation AUC 0.6923641530100937\n",
      "validation AUC 0.6926243491172814\n",
      "validation AUC 0.6928335426048675\n",
      "validation AUC 0.6930355283040193\n",
      "validation AUC 0.6932565751313898\n",
      "validation AUC 0.6935001659201502\n",
      "validation AUC 0.6936749674279269\n",
      "validation AUC 0.69382759746984\n",
      "validation AUC 0.6940045887191751\n",
      "validation AUC 0.694223693248444\n",
      "validation AUC 0.694223693248444\n",
      "train auc 0.7340707355483626\n",
      "Test AUC 0.7084971219384623\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.97953577e-01\n",
      " 9.97953577e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [12.27800751 11.27800751 11.10867596 ...  0.09638588  0.09634621\n",
      "  0.03269988]\n",
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.030s | Train Loss: 0.224825 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.221493 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.019s | Train Loss: 0.220697 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.217315 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.215945 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.213100 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.210633 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.208705 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.206228 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.204308 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.202325 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.200163 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.019s | Train Loss: 0.198400 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.195687 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.193951 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.191569 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.189002 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.187658 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.185639 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.183598 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.021s | Train Loss: 0.181767 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.020s | Train Loss: 0.179228 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.177058 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.175674 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.020s | Train Loss: 0.173800 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.020s | Train Loss: 0.171530 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.019s | Train Loss: 0.169738 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.168826 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.166269 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.164225 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.019s | Train Loss: 0.162174 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.161084 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.158698 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.157155 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.154926 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.153479 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.151526 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.150046 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.015s | Train Loss: 0.148878 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.146911 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.145006 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.143989 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.142160 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.140592 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.139340 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.137449 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.136399 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.134935 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.133018 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.132170 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.130748 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.129363 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.127724 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.127180 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.019s | Train Loss: 0.124387 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.123011 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.122520 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.121773 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.119247 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.118068 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.117314 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.016s | Train Loss: 0.116726 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.115003 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.019s | Train Loss: 0.114344 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.113060 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.112379 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.110373 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.109756 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.108932 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.108040 |\n",
      "INFO:root:Pretraining Time: 1.254s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.112209\n",
      "INFO:root:Test AUC: 52.98%\n",
      "INFO:root:Test AUC: 52.98%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.406s | Train Loss: 1.664502 || Validation Loss: 0.016424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.384s | Train Loss: 1.550367 || Validation Loss: 0.016622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.445s | Train Loss: 1.445642 || Validation Loss: 0.015692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.382s | Train Loss: 1.382727 || Validation Loss: 0.015110 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.413s | Train Loss: 1.367813 || Validation Loss: 0.014980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.407s | Train Loss: 1.296182 || Validation Loss: 0.015652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.415s | Train Loss: 1.336706 || Validation Loss: 0.013871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.414s | Train Loss: 1.259862 || Validation Loss: 0.014200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.418s | Train Loss: 1.224334 || Validation Loss: 0.015337 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.415s | Train Loss: 1.189682 || Validation Loss: 0.013578 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.414s | Train Loss: 1.132114 || Validation Loss: 0.013749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.415s | Train Loss: 1.125092 || Validation Loss: 0.013344 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.414s | Train Loss: 1.154159 || Validation Loss: 0.013539 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.415s | Train Loss: 1.083416 || Validation Loss: 0.012646 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.415s | Train Loss: 1.075063 || Validation Loss: 0.013181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.414s | Train Loss: 1.051458 || Validation Loss: 0.013193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.413s | Train Loss: 1.034098 || Validation Loss: 0.012828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.415s | Train Loss: 1.060112 || Validation Loss: 0.013313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.420s | Train Loss: 1.022964 || Validation Loss: 0.013336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.415s | Train Loss: 0.974648 || Validation Loss: 0.012839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.422s | Train Loss: 1.006262 || Validation Loss: 0.013597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.422s | Train Loss: 0.947816 || Validation Loss: 0.012996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.419s | Train Loss: 0.924372 || Validation Loss: 0.013850 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.421s | Train Loss: 0.919575 || Validation Loss: 0.011626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.448s | Train Loss: 0.915386 || Validation Loss: 0.012307 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.423s | Train Loss: 0.890945 || Validation Loss: 0.012310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 0.907904 || Validation Loss: 0.013286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.443s | Train Loss: 0.882051 || Validation Loss: 0.012410 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.420s | Train Loss: 0.865585 || Validation Loss: 0.012622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.421s | Train Loss: 0.853203 || Validation Loss: 0.011923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.424s | Train Loss: 0.811550 || Validation Loss: 0.012505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.451s | Train Loss: 0.841185 || Validation Loss: 0.013449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.422s | Train Loss: 0.826462 || Validation Loss: 0.012006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.429s | Train Loss: 0.812450 || Validation Loss: 0.012092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.450s | Train Loss: 0.817919 || Validation Loss: 0.012164 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.430s | Train Loss: 0.796416 || Validation Loss: 0.012317 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.419s | Train Loss: 0.790670 || Validation Loss: 0.012057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.451s | Train Loss: 0.781022 || Validation Loss: 0.012679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.424s | Train Loss: 0.791865 || Validation Loss: 0.011946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.470s | Train Loss: 0.745794 || Validation Loss: 0.011820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.426s | Train Loss: 0.730790 || Validation Loss: 0.011718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.459s | Train Loss: 0.732960 || Validation Loss: 0.012127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.434s | Train Loss: 0.757174 || Validation Loss: 0.010717 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.450s | Train Loss: 0.755374 || Validation Loss: 0.011679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.430s | Train Loss: 0.738559 || Validation Loss: 0.011862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.462s | Train Loss: 0.722078 || Validation Loss: 0.011464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.430s | Train Loss: 0.742156 || Validation Loss: 0.011631 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.458s | Train Loss: 0.678298 || Validation Loss: 0.011995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.462s | Train Loss: 0.705290 || Validation Loss: 0.012777 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.428s | Train Loss: 0.699582 || Validation Loss: 0.012438 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.461s | Train Loss: 0.740146 || Validation Loss: 0.012185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.713293 || Validation Loss: 0.011652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.464s | Train Loss: 0.697633 || Validation Loss: 0.011033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.436s | Train Loss: 0.688156 || Validation Loss: 0.013134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.465s | Train Loss: 0.710557 || Validation Loss: 0.011843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.437s | Train Loss: 0.709504 || Validation Loss: 0.011652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.460s | Train Loss: 0.724413 || Validation Loss: 0.011607 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.458s | Train Loss: 0.714016 || Validation Loss: 0.011495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.463s | Train Loss: 0.698260 || Validation Loss: 0.011506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.433s | Train Loss: 0.674763 || Validation Loss: 0.011947 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.464s | Train Loss: 0.669249 || Validation Loss: 0.011407 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.468s | Train Loss: 0.702239 || Validation Loss: 0.012132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.464s | Train Loss: 0.691277 || Validation Loss: 0.010891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.460s | Train Loss: 0.691933 || Validation Loss: 0.013773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.466s | Train Loss: 0.682131 || Validation Loss: 0.011593 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.465s | Train Loss: 0.719784 || Validation Loss: 0.012738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.470s | Train Loss: 0.674466 || Validation Loss: 0.012786 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.464s | Train Loss: 0.671735 || Validation Loss: 0.011497 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.468s | Train Loss: 0.688080 || Validation Loss: 0.011389 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.471s | Train Loss: 0.739104 || Validation Loss: 0.012027 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.616s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.773%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009926\n",
      "INFO:root:Test AUC: 72.35%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 934 random outlier 66\n",
      "final sizes 934 66 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 934, -1.0: 66})\n",
      "Resampled dataset shape Counter({1.0: 934, -1.0: 224})\n",
      "class weights 0.19343696027633853 0.8065630397236615\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4853541570394311\n",
      "validation AUC 0.4936321836560458\n",
      "validation AUC 0.5013882801838979\n",
      "validation AUC 0.5094445176427717\n",
      "validation AUC 0.5177049039355324\n",
      "validation AUC 0.5260831238666823\n",
      "validation AUC 0.5347606359711906\n",
      "validation AUC 0.543238397349705\n",
      "validation AUC 0.551144870321536\n",
      "validation AUC 0.559519197674436\n",
      "validation AUC 0.5675114270984748\n",
      "validation AUC 0.5747659850069486\n",
      "validation AUC 0.5814116697101325\n",
      "validation AUC 0.5878817014871938\n",
      "validation AUC 0.5943199487531298\n",
      "validation AUC 0.6006388750592535\n",
      "validation AUC 0.6072984831130749\n",
      "validation AUC 0.6131874165476987\n",
      "validation AUC 0.618283535825449\n",
      "validation AUC 0.6230371401453754\n",
      "validation AUC 0.6274525132060306\n",
      "validation AUC 0.6320306960791973\n",
      "validation AUC 0.6362556999797576\n",
      "validation AUC 0.6408187941234255\n",
      "validation AUC 0.645526530940649\n",
      "validation AUC 0.6502954155592019\n",
      "validation AUC 0.6547878898724426\n",
      "validation AUC 0.6587577955331614\n",
      "validation AUC 0.6623100831559013\n",
      "validation AUC 0.6658948736739957\n",
      "validation AUC 0.6691733797455663\n",
      "validation AUC 0.6723720626865272\n",
      "validation AUC 0.6753537776261308\n",
      "validation AUC 0.6783482159486407\n",
      "validation AUC 0.6809679902887223\n",
      "validation AUC 0.6836474517155335\n",
      "validation AUC 0.6864743787998535\n",
      "validation AUC 0.6891564184278434\n",
      "validation AUC 0.6917578074530182\n",
      "validation AUC 0.6942523993501123\n",
      "validation AUC 0.6963218887098309\n",
      "validation AUC 0.6985807281883115\n",
      "validation AUC 0.7007955812664975\n",
      "validation AUC 0.7028145150012463\n",
      "validation AUC 0.7047519493755166\n",
      "validation AUC 0.7064999272037314\n",
      "validation AUC 0.7083164442087906\n",
      "validation AUC 0.7098154726340309\n",
      "validation AUC 0.7114186587904794\n",
      "validation AUC 0.7116169035702739\n",
      "validation AUC 0.7118221592480103\n",
      "validation AUC 0.7119786738861266\n",
      "validation AUC 0.7121285634252388\n",
      "validation AUC 0.7123201777846648\n",
      "validation AUC 0.7125270857461287\n",
      "validation AUC 0.7127524082900051\n",
      "validation AUC 0.7129290137404021\n",
      "validation AUC 0.7130869917538005\n",
      "validation AUC 0.7132848933055742\n",
      "validation AUC 0.713471441725842\n",
      "validation AUC 0.7136515300094103\n",
      "validation AUC 0.713866396071726\n",
      "validation AUC 0.714034325037042\n",
      "validation AUC 0.714174851635032\n",
      "validation AUC 0.7143283756662082\n",
      "validation AUC 0.7145076604237126\n",
      "validation AUC 0.7147079007152554\n",
      "validation AUC 0.7148854852967502\n",
      "validation AUC 0.71505897774882\n",
      "validation AUC 0.7152400797523565\n",
      "validation AUC 0.7152400797523565\n",
      "train auc 0.7732652903782612\n",
      "Test AUC 0.7234608988379008\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.32638560e-04 ... 9.99829465e-01\n",
      " 9.99829465e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [21.41058731 20.41058731 14.84651661 ...  0.05783394  0.05619647\n",
      "  0.04019863]\n",
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.029s | Train Loss: 0.222365 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.219684 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.019s | Train Loss: 0.217547 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.022s | Train Loss: 0.215352 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.212576 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.210406 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.208246 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.206389 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.204010 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.201948 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.200259 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.198380 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.195263 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.194008 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.192062 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.190466 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.187921 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.185779 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.184258 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.182019 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.180100 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.177867 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.176361 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.174170 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.172021 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.171068 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.168767 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.167117 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.165217 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.163165 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.161139 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.160188 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.158130 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.156516 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.154382 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.152549 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.151346 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.149964 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.148469 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.147946 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.145601 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.143698 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.143285 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.015s | Train Loss: 0.141461 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.139848 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.138375 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.136846 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.135500 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.134260 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.133537 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.131934 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.130434 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.128423 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.018s | Train Loss: 0.127439 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.126057 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.125107 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.124422 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.122389 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.121378 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.121080 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.118706 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.117704 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.116837 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.114943 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.114866 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.113857 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.111796 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.110933 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.110937 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.109719 |\n",
      "INFO:root:Pretraining Time: 1.230s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.114730\n",
      "INFO:root:Test AUC: 51.99%\n",
      "INFO:root:Test AUC: 51.99%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.409s | Train Loss: 1.227952 || Validation Loss: 0.013632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.377s | Train Loss: 1.248420 || Validation Loss: 0.013892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.436s | Train Loss: 1.180367 || Validation Loss: 0.014329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.378s | Train Loss: 1.085904 || Validation Loss: 0.013618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.408s | Train Loss: 1.065277 || Validation Loss: 0.013528 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.409s | Train Loss: 1.075056 || Validation Loss: 0.014029 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.419s | Train Loss: 1.029132 || Validation Loss: 0.013450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.410s | Train Loss: 1.031627 || Validation Loss: 0.012015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.409s | Train Loss: 0.965148 || Validation Loss: 0.013099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.408s | Train Loss: 0.983333 || Validation Loss: 0.013020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.413s | Train Loss: 0.951300 || Validation Loss: 0.012918 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.404s | Train Loss: 0.931044 || Validation Loss: 0.013648 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.410s | Train Loss: 0.915575 || Validation Loss: 0.012224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.406s | Train Loss: 0.879564 || Validation Loss: 0.011650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.410s | Train Loss: 0.883313 || Validation Loss: 0.011589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.416s | Train Loss: 0.837192 || Validation Loss: 0.011586 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.411s | Train Loss: 0.834227 || Validation Loss: 0.011863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.416s | Train Loss: 0.836472 || Validation Loss: 0.011129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.419s | Train Loss: 0.823739 || Validation Loss: 0.011358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.414s | Train Loss: 0.818007 || Validation Loss: 0.011795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.414s | Train Loss: 0.802032 || Validation Loss: 0.011565 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.411s | Train Loss: 0.764527 || Validation Loss: 0.012140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.414s | Train Loss: 0.777651 || Validation Loss: 0.011713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.414s | Train Loss: 0.756750 || Validation Loss: 0.011897 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.484s | Train Loss: 0.765346 || Validation Loss: 0.011551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.412s | Train Loss: 0.740997 || Validation Loss: 0.011973 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.413s | Train Loss: 0.753151 || Validation Loss: 0.010581 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.457s | Train Loss: 0.761437 || Validation Loss: 0.011034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.411s | Train Loss: 0.728950 || Validation Loss: 0.011230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.407s | Train Loss: 0.728169 || Validation Loss: 0.011736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.419s | Train Loss: 0.725060 || Validation Loss: 0.011015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.442s | Train Loss: 0.703806 || Validation Loss: 0.011005 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.415s | Train Loss: 0.680319 || Validation Loss: 0.011201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.416s | Train Loss: 0.703892 || Validation Loss: 0.010502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.442s | Train Loss: 0.701341 || Validation Loss: 0.012930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.420s | Train Loss: 0.683099 || Validation Loss: 0.010551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.421s | Train Loss: 0.665669 || Validation Loss: 0.011000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.458s | Train Loss: 0.696562 || Validation Loss: 0.012083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.420s | Train Loss: 0.685214 || Validation Loss: 0.011621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.453s | Train Loss: 0.656849 || Validation Loss: 0.009376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.426s | Train Loss: 0.651422 || Validation Loss: 0.010446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.453s | Train Loss: 0.656254 || Validation Loss: 0.011013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.417s | Train Loss: 0.663426 || Validation Loss: 0.011192 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.446s | Train Loss: 0.634711 || Validation Loss: 0.011785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.417s | Train Loss: 0.637410 || Validation Loss: 0.011812 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.456s | Train Loss: 0.641802 || Validation Loss: 0.010373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.419s | Train Loss: 0.626922 || Validation Loss: 0.010700 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.454s | Train Loss: 0.630243 || Validation Loss: 0.011519 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.460s | Train Loss: 0.624458 || Validation Loss: 0.010055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.426s | Train Loss: 0.643641 || Validation Loss: 0.011382 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.459s | Train Loss: 0.632105 || Validation Loss: 0.011240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.423s | Train Loss: 0.619235 || Validation Loss: 0.011459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.448s | Train Loss: 0.619392 || Validation Loss: 0.010261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.423s | Train Loss: 0.609678 || Validation Loss: 0.010042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.454s | Train Loss: 0.605493 || Validation Loss: 0.011385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.432s | Train Loss: 0.627780 || Validation Loss: 0.011410 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.457s | Train Loss: 0.607662 || Validation Loss: 0.010703 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.455s | Train Loss: 0.629718 || Validation Loss: 0.011525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.462s | Train Loss: 0.622186 || Validation Loss: 0.011673 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.434s | Train Loss: 0.627791 || Validation Loss: 0.010659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.463s | Train Loss: 0.621877 || Validation Loss: 0.011035 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.457s | Train Loss: 0.618953 || Validation Loss: 0.011558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.460s | Train Loss: 0.616001 || Validation Loss: 0.011443 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.464s | Train Loss: 0.632144 || Validation Loss: 0.010102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.462s | Train Loss: 0.596063 || Validation Loss: 0.011217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.467s | Train Loss: 0.623350 || Validation Loss: 0.011522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.467s | Train Loss: 0.611885 || Validation Loss: 0.010425 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.467s | Train Loss: 0.625647 || Validation Loss: 0.010979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.468s | Train Loss: 0.607595 || Validation Loss: 0.011467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.466s | Train Loss: 0.622093 || Validation Loss: 0.010895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.317s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.860%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009643\n",
      "INFO:root:Test AUC: 71.19%\n",
      "INFO:root:Test Time: 0.184s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 935 random outlier 65\n",
      "final sizes 935 65 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 935, -1.0: 65})\n",
      "Resampled dataset shape Counter({1.0: 935, -1.0: 224})\n",
      "class weights 0.19327006039689387 0.8067299396031061\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5174407780005419\n",
      "validation AUC 0.5263247830000705\n",
      "validation AUC 0.5352402745143031\n",
      "validation AUC 0.5442262874137167\n",
      "validation AUC 0.5533915637851761\n",
      "validation AUC 0.562257191451845\n",
      "validation AUC 0.5700673477232967\n",
      "validation AUC 0.5774266167849038\n",
      "validation AUC 0.584107616724666\n",
      "validation AUC 0.5909240294309779\n",
      "validation AUC 0.5972288142105128\n",
      "validation AUC 0.6031286404785993\n",
      "validation AUC 0.6090028470365214\n",
      "validation AUC 0.6147439928710742\n",
      "validation AUC 0.6204360437952569\n",
      "validation AUC 0.6257145262399685\n",
      "validation AUC 0.6311570849819808\n",
      "validation AUC 0.6361200131629277\n",
      "validation AUC 0.6406516873089497\n",
      "validation AUC 0.645003712183988\n",
      "validation AUC 0.6491374919301505\n",
      "validation AUC 0.6527364638850687\n",
      "validation AUC 0.6556531570698693\n",
      "validation AUC 0.6587589236624696\n",
      "validation AUC 0.6619429888147043\n",
      "validation AUC 0.6652078189790941\n",
      "validation AUC 0.6679776371778632\n",
      "validation AUC 0.6705815724760289\n",
      "validation AUC 0.6728246527330635\n",
      "validation AUC 0.6752732068437859\n",
      "validation AUC 0.6777717392114121\n",
      "validation AUC 0.6804005598714188\n",
      "validation AUC 0.6828258569578866\n",
      "validation AUC 0.6854735950692661\n",
      "validation AUC 0.68794100809635\n",
      "validation AUC 0.6898113400622344\n",
      "validation AUC 0.6920572539459515\n",
      "validation AUC 0.6937584197292362\n",
      "validation AUC 0.695394042263981\n",
      "validation AUC 0.6965492147475023\n",
      "validation AUC 0.6978594039049877\n",
      "validation AUC 0.699546680926505\n",
      "validation AUC 0.7008956282435048\n",
      "validation AUC 0.7023591578450856\n",
      "validation AUC 0.7041919342798693\n",
      "validation AUC 0.70602050683657\n",
      "validation AUC 0.7070551743566417\n",
      "validation AUC 0.7078803104824711\n",
      "validation AUC 0.7090496165105343\n",
      "validation AUC 0.7092629659839215\n",
      "validation AUC 0.7094695307173647\n",
      "validation AUC 0.7096048131104803\n",
      "validation AUC 0.7097554210338219\n",
      "validation AUC 0.7098850840658546\n",
      "validation AUC 0.7100483728011856\n",
      "validation AUC 0.7101654853946633\n",
      "validation AUC 0.710302111432356\n",
      "validation AUC 0.7104051170881152\n",
      "validation AUC 0.7104949896158891\n",
      "validation AUC 0.7106415905516403\n",
      "validation AUC 0.7107563111917026\n",
      "validation AUC 0.7108074654701968\n",
      "validation AUC 0.7108443052777508\n",
      "validation AUC 0.7109394752240454\n",
      "validation AUC 0.7110706654877361\n",
      "validation AUC 0.7112434901085409\n",
      "validation AUC 0.7113941911557642\n",
      "validation AUC 0.711569684440947\n",
      "validation AUC 0.7117135768020854\n",
      "validation AUC 0.7118383202324882\n",
      "validation AUC 0.7118383202324882\n",
      "train auc 0.8596733585054177\n",
      "Test AUC 0.7119135780529972\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.13690194e-04 ... 9.99962103e-01\n",
      " 9.99962103e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [12.70996284 11.70996284  9.25666237 ...  0.01585466  0.01474642\n",
      "  0.01432474]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.034s | Train Loss: 0.211965 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.019s | Train Loss: 0.209015 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.207265 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.204577 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.202550 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.199887 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.198126 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.019s | Train Loss: 0.195788 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.193683 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.191160 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.019s | Train Loss: 0.188927 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.187461 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.184891 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.182795 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.181112 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.178668 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.019s | Train Loss: 0.176993 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.174539 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.172770 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.170967 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.168384 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.019s | Train Loss: 0.166886 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.165106 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.163091 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.161497 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.159077 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.157786 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.155124 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.154219 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.152083 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.150391 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.148197 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.147232 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.145425 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.143663 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.019s | Train Loss: 0.141979 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.141417 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.138914 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.136591 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.136041 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.133711 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.133571 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.131261 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.130220 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.128796 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.126630 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.125486 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.124289 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.123305 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.121670 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.121117 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.119390 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.117869 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.116268 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.115417 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.114236 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.112244 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.111555 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.110902 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.109393 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.107822 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.106793 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.106853 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.105842 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.103954 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.018s | Train Loss: 0.103214 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.102180 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.101225 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.100435 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.099997 |\n",
      "INFO:root:Pretraining Time: 1.271s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.103855\n",
      "INFO:root:Test AUC: 51.33%\n",
      "INFO:root:Test AUC: 51.33%\n",
      "INFO:root:Test Time: 0.237s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.418s | Train Loss: 1.721540 || Validation Loss: 0.014804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.397s | Train Loss: 1.725913 || Validation Loss: 0.014774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.453s | Train Loss: 1.536449 || Validation Loss: 0.012902 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.401s | Train Loss: 1.495707 || Validation Loss: 0.014311 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.451s | Train Loss: 1.514922 || Validation Loss: 0.013979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.433s | Train Loss: 1.430649 || Validation Loss: 0.013593 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.444s | Train Loss: 1.299959 || Validation Loss: 0.014591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.440s | Train Loss: 1.298534 || Validation Loss: 0.012754 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.443s | Train Loss: 1.245514 || Validation Loss: 0.012672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.436s | Train Loss: 1.234051 || Validation Loss: 0.013146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.426s | Train Loss: 1.242677 || Validation Loss: 0.013694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.422s | Train Loss: 1.200825 || Validation Loss: 0.013880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.425s | Train Loss: 1.107454 || Validation Loss: 0.011493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.433s | Train Loss: 1.123525 || Validation Loss: 0.012071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.443s | Train Loss: 1.087073 || Validation Loss: 0.012470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.438s | Train Loss: 1.025482 || Validation Loss: 0.011795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.441s | Train Loss: 1.049569 || Validation Loss: 0.012938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.439s | Train Loss: 1.045358 || Validation Loss: 0.012108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.444s | Train Loss: 1.051979 || Validation Loss: 0.011864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.441s | Train Loss: 1.006123 || Validation Loss: 0.011642 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.445s | Train Loss: 0.991287 || Validation Loss: 0.011784 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.436s | Train Loss: 0.971785 || Validation Loss: 0.012168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.430s | Train Loss: 0.931820 || Validation Loss: 0.011418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.439s | Train Loss: 0.939477 || Validation Loss: 0.011151 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.471s | Train Loss: 0.915408 || Validation Loss: 0.011538 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.423s | Train Loss: 0.904064 || Validation Loss: 0.011463 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.434s | Train Loss: 0.875074 || Validation Loss: 0.011203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.462s | Train Loss: 0.881053 || Validation Loss: 0.010756 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.429s | Train Loss: 0.887348 || Validation Loss: 0.011009 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.437s | Train Loss: 0.873870 || Validation Loss: 0.011543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.437s | Train Loss: 0.876372 || Validation Loss: 0.010907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.459s | Train Loss: 0.833115 || Validation Loss: 0.010650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.431s | Train Loss: 0.829807 || Validation Loss: 0.010839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.456s | Train Loss: 0.811432 || Validation Loss: 0.010144 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.483s | Train Loss: 0.813152 || Validation Loss: 0.010297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.446s | Train Loss: 0.784254 || Validation Loss: 0.011168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.452s | Train Loss: 0.818534 || Validation Loss: 0.011542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.489s | Train Loss: 0.778032 || Validation Loss: 0.010900 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.446s | Train Loss: 0.758818 || Validation Loss: 0.011541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.472s | Train Loss: 0.742156 || Validation Loss: 0.010778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.452s | Train Loss: 0.753382 || Validation Loss: 0.012015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.484s | Train Loss: 0.741231 || Validation Loss: 0.010691 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.435s | Train Loss: 0.754894 || Validation Loss: 0.010020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.472s | Train Loss: 0.727419 || Validation Loss: 0.011242 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.436s | Train Loss: 0.699890 || Validation Loss: 0.011148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.467s | Train Loss: 0.710215 || Validation Loss: 0.011088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.435s | Train Loss: 0.701839 || Validation Loss: 0.010586 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.470s | Train Loss: 0.704603 || Validation Loss: 0.010845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.468s | Train Loss: 0.706881 || Validation Loss: 0.011254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.442s | Train Loss: 0.699975 || Validation Loss: 0.010886 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.482s | Train Loss: 0.695047 || Validation Loss: 0.011090 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.449s | Train Loss: 0.677417 || Validation Loss: 0.011270 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.466s | Train Loss: 0.680783 || Validation Loss: 0.010431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.450s | Train Loss: 0.689581 || Validation Loss: 0.010434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.473s | Train Loss: 0.692231 || Validation Loss: 0.011517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.445s | Train Loss: 0.678366 || Validation Loss: 0.010292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.473s | Train Loss: 0.667643 || Validation Loss: 0.010739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.475s | Train Loss: 0.696077 || Validation Loss: 0.010140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.478s | Train Loss: 0.677393 || Validation Loss: 0.011441 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.448s | Train Loss: 0.679168 || Validation Loss: 0.011237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.476s | Train Loss: 0.680498 || Validation Loss: 0.011455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.476s | Train Loss: 0.688376 || Validation Loss: 0.010653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.477s | Train Loss: 0.691877 || Validation Loss: 0.010727 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.477s | Train Loss: 0.676769 || Validation Loss: 0.010975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.483s | Train Loss: 0.685526 || Validation Loss: 0.010803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.481s | Train Loss: 0.692805 || Validation Loss: 0.011716 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.479s | Train Loss: 0.680236 || Validation Loss: 0.011299 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.481s | Train Loss: 0.667281 || Validation Loss: 0.010852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.479s | Train Loss: 0.681872 || Validation Loss: 0.011192 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.481s | Train Loss: 0.690011 || Validation Loss: 0.010992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.768s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.765%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009128\n",
      "INFO:root:Test AUC: 73.96%\n",
      "INFO:root:Test Time: 0.197s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 935 random outlier 65\n",
      "final sizes 935 65 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 935, -1.0: 65})\n",
      "Resampled dataset shape Counter({1.0: 935, -1.0: 224})\n",
      "class weights 0.19327006039689387 0.8067299396031061\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.47375293553081566\n",
      "validation AUC 0.47921276742252034\n",
      "validation AUC 0.48487696366338073\n",
      "validation AUC 0.49065748227506645\n",
      "validation AUC 0.4969877829981548\n",
      "validation AUC 0.5035062604790974\n",
      "validation AUC 0.5100964992231872\n",
      "validation AUC 0.5164335660614431\n",
      "validation AUC 0.5228675402715556\n",
      "validation AUC 0.5294793571493492\n",
      "validation AUC 0.5361292962835802\n",
      "validation AUC 0.5431773373508927\n",
      "validation AUC 0.5498743758837457\n",
      "validation AUC 0.5561469052113827\n",
      "validation AUC 0.5623624533662207\n",
      "validation AUC 0.5686156262299005\n",
      "validation AUC 0.575535691137778\n",
      "validation AUC 0.5821498254698818\n",
      "validation AUC 0.5892661955201347\n",
      "validation AUC 0.5963854204825286\n",
      "validation AUC 0.6031437771003799\n",
      "validation AUC 0.6098276213095538\n",
      "validation AUC 0.6160207984473535\n",
      "validation AUC 0.6226535309489504\n",
      "validation AUC 0.6289201668652245\n",
      "validation AUC 0.634802610895643\n",
      "validation AUC 0.6401852377681515\n",
      "validation AUC 0.6457841275611994\n",
      "validation AUC 0.650895372818001\n",
      "validation AUC 0.6558218177228263\n",
      "validation AUC 0.661324247138649\n",
      "validation AUC 0.6662542546971154\n",
      "validation AUC 0.6706602116030015\n",
      "validation AUC 0.675120845530788\n",
      "validation AUC 0.6790065751846035\n",
      "validation AUC 0.682358247359592\n",
      "validation AUC 0.6862381128695502\n",
      "validation AUC 0.6900232394637511\n",
      "validation AUC 0.6935116893353254\n",
      "validation AUC 0.6964199667819132\n",
      "validation AUC 0.6990115086217815\n",
      "validation AUC 0.7024251746950805\n",
      "validation AUC 0.7055796849879833\n",
      "validation AUC 0.7080566099544001\n",
      "validation AUC 0.7101269799999702\n",
      "validation AUC 0.7125019502801486\n",
      "validation AUC 0.7153318573286792\n",
      "validation AUC 0.7176787494282194\n",
      "validation AUC 0.7198541419267555\n",
      "validation AUC 0.7200604405919656\n",
      "validation AUC 0.7202923296998346\n",
      "validation AUC 0.7205114448718327\n",
      "validation AUC 0.7207323427009927\n",
      "validation AUC 0.7209841496767909\n",
      "validation AUC 0.7212113719478515\n",
      "validation AUC 0.7214495562301153\n",
      "validation AUC 0.7216512971464928\n",
      "validation AUC 0.7219091518532291\n",
      "validation AUC 0.7221463383796188\n",
      "validation AUC 0.7223412413424057\n",
      "validation AUC 0.7225466300542588\n",
      "validation AUC 0.7227437520261095\n",
      "validation AUC 0.7229899343194603\n",
      "validation AUC 0.7232535733495842\n",
      "validation AUC 0.7235097943973693\n",
      "validation AUC 0.7237048117694963\n",
      "validation AUC 0.7239127307902461\n",
      "validation AUC 0.7241036613543129\n",
      "validation AUC 0.7242904040043907\n",
      "validation AUC 0.7244713490276696\n",
      "validation AUC 0.7244713490276696\n",
      "train auc 0.7646542850056552\n",
      "Test AUC 0.7395827619736219\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99412601e-01\n",
      " 9.99412601e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [18.97175217 17.97175217 14.93687916 ...  0.06440983  0.06386431\n",
      "  0.03212282]\n",
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.229695 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.226682 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.223828 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.221426 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.219280 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.215857 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.214202 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.211706 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.209551 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.207980 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.205276 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.203358 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.201354 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.199162 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.197579 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.194537 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.193233 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.191375 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.188474 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.186752 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.184800 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.183036 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.180782 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.179064 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.177223 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.174768 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.172973 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.171317 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.170028 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.167207 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.166598 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.163893 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.161852 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.160537 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.158724 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.156492 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.155333 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.152838 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.152179 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.149946 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.015s | Train Loss: 0.148351 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.015s | Train Loss: 0.146726 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.144950 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.015s | Train Loss: 0.143327 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.015s | Train Loss: 0.141039 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.015s | Train Loss: 0.139316 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.016s | Train Loss: 0.138739 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.137206 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.135807 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.134090 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.132955 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.132000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.129998 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.018s | Train Loss: 0.128668 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.127102 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.126836 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.124611 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.123107 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.122096 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.120787 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.119143 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.117366 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.116703 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.115440 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.019s | Train Loss: 0.115140 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.018s | Train Loss: 0.112563 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.018s | Train Loss: 0.112338 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.110791 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.109988 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.109334 |\n",
      "INFO:root:Pretraining Time: 1.241s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.120721\n",
      "INFO:root:Test AUC: 56.24%\n",
      "INFO:root:Test AUC: 56.24%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.405s | Train Loss: 1.608468 || Validation Loss: 0.013493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.377s | Train Loss: 1.520125 || Validation Loss: 0.013377 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.440s | Train Loss: 1.473862 || Validation Loss: 0.012157 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.379s | Train Loss: 1.327811 || Validation Loss: 0.011785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.409s | Train Loss: 1.334537 || Validation Loss: 0.012174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.412s | Train Loss: 1.312535 || Validation Loss: 0.012911 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.413s | Train Loss: 1.252295 || Validation Loss: 0.011015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.412s | Train Loss: 1.229342 || Validation Loss: 0.011412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.414s | Train Loss: 1.148720 || Validation Loss: 0.012219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.410s | Train Loss: 1.129666 || Validation Loss: 0.010886 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.412s | Train Loss: 1.107589 || Validation Loss: 0.012150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.412s | Train Loss: 1.123274 || Validation Loss: 0.011993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.409s | Train Loss: 1.075534 || Validation Loss: 0.011481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.414s | Train Loss: 1.037119 || Validation Loss: 0.011181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.414s | Train Loss: 0.999252 || Validation Loss: 0.010583 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.414s | Train Loss: 1.006059 || Validation Loss: 0.011515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.413s | Train Loss: 0.989368 || Validation Loss: 0.012136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.410s | Train Loss: 0.943461 || Validation Loss: 0.010518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.416s | Train Loss: 0.948524 || Validation Loss: 0.010948 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.417s | Train Loss: 0.945192 || Validation Loss: 0.011841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.414s | Train Loss: 0.969359 || Validation Loss: 0.010474 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.413s | Train Loss: 0.915897 || Validation Loss: 0.010684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.414s | Train Loss: 0.915922 || Validation Loss: 0.011399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.417s | Train Loss: 0.901105 || Validation Loss: 0.011727 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.444s | Train Loss: 0.885843 || Validation Loss: 0.011108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.416s | Train Loss: 0.885337 || Validation Loss: 0.011505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 0.875488 || Validation Loss: 0.010207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.449s | Train Loss: 0.867872 || Validation Loss: 0.010181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.419s | Train Loss: 0.821799 || Validation Loss: 0.010508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.420s | Train Loss: 0.831828 || Validation Loss: 0.010351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.418s | Train Loss: 0.831214 || Validation Loss: 0.010793 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.449s | Train Loss: 0.834667 || Validation Loss: 0.010115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.421s | Train Loss: 0.803746 || Validation Loss: 0.010027 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.419s | Train Loss: 0.829444 || Validation Loss: 0.010543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.450s | Train Loss: 0.811284 || Validation Loss: 0.010332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.425s | Train Loss: 0.783146 || Validation Loss: 0.010981 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.423s | Train Loss: 0.780261 || Validation Loss: 0.010061 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.448s | Train Loss: 0.765875 || Validation Loss: 0.009292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.422s | Train Loss: 0.753912 || Validation Loss: 0.010827 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.458s | Train Loss: 0.771222 || Validation Loss: 0.010220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.425s | Train Loss: 0.757176 || Validation Loss: 0.009602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.455s | Train Loss: 0.745459 || Validation Loss: 0.010209 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.425s | Train Loss: 0.748574 || Validation Loss: 0.010460 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.455s | Train Loss: 0.733913 || Validation Loss: 0.010391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.431s | Train Loss: 0.733045 || Validation Loss: 0.009255 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.611s | Train Loss: 0.722431 || Validation Loss: 0.009197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.431s | Train Loss: 0.709654 || Validation Loss: 0.010005 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.459s | Train Loss: 0.723736 || Validation Loss: 0.010081 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.456s | Train Loss: 0.698525 || Validation Loss: 0.010073 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.431s | Train Loss: 0.705263 || Validation Loss: 0.010662 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.457s | Train Loss: 0.704424 || Validation Loss: 0.010571 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.430s | Train Loss: 0.683895 || Validation Loss: 0.011281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.458s | Train Loss: 0.718346 || Validation Loss: 0.010685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.434s | Train Loss: 0.707664 || Validation Loss: 0.009424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.459s | Train Loss: 0.709934 || Validation Loss: 0.009562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.434s | Train Loss: 0.713228 || Validation Loss: 0.010232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.461s | Train Loss: 0.711014 || Validation Loss: 0.010649 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.464s | Train Loss: 0.691508 || Validation Loss: 0.011014 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.459s | Train Loss: 0.666356 || Validation Loss: 0.010953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.431s | Train Loss: 0.692900 || Validation Loss: 0.010472 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.462s | Train Loss: 0.735548 || Validation Loss: 0.009947 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.461s | Train Loss: 0.681510 || Validation Loss: 0.010467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.463s | Train Loss: 0.696693 || Validation Loss: 0.010688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.463s | Train Loss: 0.699595 || Validation Loss: 0.010627 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.463s | Train Loss: 0.700137 || Validation Loss: 0.010517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.463s | Train Loss: 0.676937 || Validation Loss: 0.010216 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.464s | Train Loss: 0.697739 || Validation Loss: 0.010365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.450s | Train Loss: 0.701721 || Validation Loss: 0.009851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.464s | Train Loss: 0.688079 || Validation Loss: 0.008932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.465s | Train Loss: 0.697155 || Validation Loss: 0.010354 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.567s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.775%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009076\n",
      "INFO:root:Test AUC: 70.60%\n",
      "INFO:root:Test Time: 0.183s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 940 random outlier 60\n",
      "final sizes 940 60 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 940, -1.0: 60})\n",
      "Resampled dataset shape Counter({1.0: 940, -1.0: 225})\n",
      "class weights 0.19313304721030042 0.8068669527896996\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5692367385739125\n",
      "validation AUC 0.5722810274405747\n",
      "validation AUC 0.5758953462111778\n",
      "validation AUC 0.5797767549168877\n",
      "validation AUC 0.583479269985396\n",
      "validation AUC 0.5875210699433786\n",
      "validation AUC 0.5909227363393652\n",
      "validation AUC 0.5947453253407855\n",
      "validation AUC 0.5985664988592059\n",
      "validation AUC 0.6021278540873295\n",
      "validation AUC 0.6054272864254329\n",
      "validation AUC 0.6089464778206586\n",
      "validation AUC 0.6128085061162701\n",
      "validation AUC 0.6165179655656621\n",
      "validation AUC 0.6200999490638974\n",
      "validation AUC 0.6237478775737046\n",
      "validation AUC 0.627542037184419\n",
      "validation AUC 0.6309822169571457\n",
      "validation AUC 0.6344533803756159\n",
      "validation AUC 0.6368698812462977\n",
      "validation AUC 0.6388959323062816\n",
      "validation AUC 0.6418479221028841\n",
      "validation AUC 0.6442821963315363\n",
      "validation AUC 0.6461699690700999\n",
      "validation AUC 0.6482408792341834\n",
      "validation AUC 0.6507678862241151\n",
      "validation AUC 0.6532266493729411\n",
      "validation AUC 0.6555885716669686\n",
      "validation AUC 0.6583936705134458\n",
      "validation AUC 0.6612638017574551\n",
      "validation AUC 0.6637080003312017\n",
      "validation AUC 0.6650713632250704\n",
      "validation AUC 0.6668309602530245\n",
      "validation AUC 0.6691800208086643\n",
      "validation AUC 0.6714666245072682\n",
      "validation AUC 0.673908252861883\n",
      "validation AUC 0.6760034178060951\n",
      "validation AUC 0.6774435280817992\n",
      "validation AUC 0.6796918419009788\n",
      "validation AUC 0.6821928433818081\n",
      "validation AUC 0.6840589129345984\n",
      "validation AUC 0.6861995329757518\n",
      "validation AUC 0.6888920237639374\n",
      "validation AUC 0.691275630618962\n",
      "validation AUC 0.6924514606401005\n",
      "validation AUC 0.6941410524552457\n",
      "validation AUC 0.6957242356019304\n",
      "validation AUC 0.6978925932563835\n",
      "validation AUC 0.699838640259359\n",
      "validation AUC 0.7000876348939336\n",
      "validation AUC 0.7003080458182268\n",
      "validation AUC 0.7005193172987135\n",
      "validation AUC 0.7006396892280466\n",
      "validation AUC 0.7006747916100385\n",
      "validation AUC 0.7007604735631411\n",
      "validation AUC 0.7007597205900414\n",
      "validation AUC 0.7008021478304902\n",
      "validation AUC 0.7009442495524202\n",
      "validation AUC 0.7011368004720263\n",
      "validation AUC 0.7012890739825071\n",
      "validation AUC 0.7014544353893739\n",
      "validation AUC 0.7016611730671686\n",
      "validation AUC 0.7018311826860675\n",
      "validation AUC 0.7020261255590892\n",
      "validation AUC 0.7022665261897347\n",
      "validation AUC 0.7024679770917379\n",
      "validation AUC 0.7025482019853798\n",
      "validation AUC 0.702651359300032\n",
      "validation AUC 0.702845644984518\n",
      "validation AUC 0.7029790941675076\n",
      "validation AUC 0.7029790941675076\n",
      "train auc 0.7747657879752367\n",
      "Test AUC 0.7059809174255471\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99867361e-01\n",
      " 9.99867361e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [11.0417757  10.0417757   8.55544376 ...  0.04761942  0.04617468\n",
      "  0.03452227]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.227877 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.224673 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.222859 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.220207 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.217828 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.215088 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.212946 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.210232 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.208917 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.205571 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.203877 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.201947 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.199677 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.197716 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.195790 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.193604 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.190989 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.188863 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.187785 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.184856 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.183409 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.181548 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.179709 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.177185 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.175121 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.173611 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.171565 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.170046 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.167962 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.166348 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.164570 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.162187 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.160758 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.158551 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.157518 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.155518 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.153854 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.152507 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.149931 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.148900 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.146862 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.145120 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.144171 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.141883 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.140755 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.139177 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.138078 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.136118 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.134776 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.132966 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.131555 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.129828 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.128830 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.018s | Train Loss: 0.127282 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.125705 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.124409 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.123267 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.121839 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.120479 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.119062 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.018s | Train Loss: 0.118162 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.116817 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.019s | Train Loss: 0.115067 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.114023 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.113046 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.018s | Train Loss: 0.111648 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.018s | Train Loss: 0.110703 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.109896 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.109011 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.107538 |\n",
      "INFO:root:Pretraining Time: 1.260s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.114378\n",
      "INFO:root:Test AUC: 49.15%\n",
      "INFO:root:Test AUC: 49.15%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.407s | Train Loss: 1.575785 || Validation Loss: 0.018759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.383s | Train Loss: 1.527925 || Validation Loss: 0.018748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.438s | Train Loss: 1.414587 || Validation Loss: 0.019468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.381s | Train Loss: 1.462804 || Validation Loss: 0.016377 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.411s | Train Loss: 1.383958 || Validation Loss: 0.017657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.410s | Train Loss: 1.342624 || Validation Loss: 0.017298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.411s | Train Loss: 1.305305 || Validation Loss: 0.016069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.409s | Train Loss: 1.249105 || Validation Loss: 0.015629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.409s | Train Loss: 1.208425 || Validation Loss: 0.016151 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.412s | Train Loss: 1.196042 || Validation Loss: 0.016014 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.411s | Train Loss: 1.177915 || Validation Loss: 0.015577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.412s | Train Loss: 1.108576 || Validation Loss: 0.015545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.412s | Train Loss: 1.078041 || Validation Loss: 0.015681 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.412s | Train Loss: 1.043077 || Validation Loss: 0.015986 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.412s | Train Loss: 1.078957 || Validation Loss: 0.015715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.413s | Train Loss: 1.020334 || Validation Loss: 0.014445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.412s | Train Loss: 1.014486 || Validation Loss: 0.013762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.413s | Train Loss: 0.982105 || Validation Loss: 0.015503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.416s | Train Loss: 0.994975 || Validation Loss: 0.014933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.414s | Train Loss: 0.982727 || Validation Loss: 0.014331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.415s | Train Loss: 0.919356 || Validation Loss: 0.014371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.414s | Train Loss: 0.943777 || Validation Loss: 0.015793 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.416s | Train Loss: 0.912511 || Validation Loss: 0.013036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.419s | Train Loss: 0.882343 || Validation Loss: 0.013423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.443s | Train Loss: 0.895803 || Validation Loss: 0.014137 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.418s | Train Loss: 0.900072 || Validation Loss: 0.012927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.417s | Train Loss: 0.873487 || Validation Loss: 0.012886 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.444s | Train Loss: 0.862460 || Validation Loss: 0.013896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.418s | Train Loss: 0.873191 || Validation Loss: 0.013286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.417s | Train Loss: 0.831370 || Validation Loss: 0.012835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.419s | Train Loss: 0.830208 || Validation Loss: 0.013475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.447s | Train Loss: 0.842189 || Validation Loss: 0.012861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.421s | Train Loss: 0.822199 || Validation Loss: 0.013584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.418s | Train Loss: 0.809150 || Validation Loss: 0.012849 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.450s | Train Loss: 0.798719 || Validation Loss: 0.013416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.421s | Train Loss: 0.798429 || Validation Loss: 0.012467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.421s | Train Loss: 0.765208 || Validation Loss: 0.014537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.447s | Train Loss: 0.783042 || Validation Loss: 0.013826 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.426s | Train Loss: 0.734255 || Validation Loss: 0.012870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.451s | Train Loss: 0.777807 || Validation Loss: 0.012365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.424s | Train Loss: 0.744517 || Validation Loss: 0.013146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.452s | Train Loss: 0.715632 || Validation Loss: 0.012783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.427s | Train Loss: 0.747585 || Validation Loss: 0.012324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.455s | Train Loss: 0.691611 || Validation Loss: 0.013201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.424s | Train Loss: 0.701082 || Validation Loss: 0.013561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.453s | Train Loss: 0.711024 || Validation Loss: 0.012334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.425s | Train Loss: 0.727662 || Validation Loss: 0.012091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.453s | Train Loss: 0.720097 || Validation Loss: 0.012593 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.453s | Train Loss: 0.665442 || Validation Loss: 0.014342 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.427s | Train Loss: 0.724817 || Validation Loss: 0.012920 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.455s | Train Loss: 0.709262 || Validation Loss: 0.012031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.428s | Train Loss: 0.686312 || Validation Loss: 0.013555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.452s | Train Loss: 0.697184 || Validation Loss: 0.012061 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.428s | Train Loss: 0.697207 || Validation Loss: 0.013531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.458s | Train Loss: 0.677423 || Validation Loss: 0.013372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.409s | Train Loss: 0.672740 || Validation Loss: 0.013870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.456s | Train Loss: 0.691379 || Validation Loss: 0.013094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.461s | Train Loss: 0.705243 || Validation Loss: 0.012462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.460s | Train Loss: 0.709348 || Validation Loss: 0.011425 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.432s | Train Loss: 0.695549 || Validation Loss: 0.012718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.462s | Train Loss: 0.682142 || Validation Loss: 0.012276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.459s | Train Loss: 0.665949 || Validation Loss: 0.013650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.461s | Train Loss: 0.717189 || Validation Loss: 0.012892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.460s | Train Loss: 0.673491 || Validation Loss: 0.013199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.461s | Train Loss: 0.726178 || Validation Loss: 0.011810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.464s | Train Loss: 0.672179 || Validation Loss: 0.012049 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.463s | Train Loss: 0.682768 || Validation Loss: 0.012597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.466s | Train Loss: 0.683935 || Validation Loss: 0.012988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.464s | Train Loss: 0.711523 || Validation Loss: 0.012724 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.582s | Train Loss: 0.680397 || Validation Loss: 0.012551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.452s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.792%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010331\n",
      "INFO:root:Test AUC: 73.37%\n",
      "INFO:root:Test Time: 0.183s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 934 random outlier 66\n",
      "final sizes 934 66 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 934, -1.0: 66})\n",
      "Resampled dataset shape Counter({1.0: 934, -1.0: 224})\n",
      "class weights 0.19343696027633853 0.8065630397236615\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5235901682892217\n",
      "validation AUC 0.5291290384102487\n",
      "validation AUC 0.5355507708209568\n",
      "validation AUC 0.5421031320913836\n",
      "validation AUC 0.5480948875946857\n",
      "validation AUC 0.5543813003627255\n",
      "validation AUC 0.5603982711524778\n",
      "validation AUC 0.5665795710043523\n",
      "validation AUC 0.5733195423072969\n",
      "validation AUC 0.5801913264735976\n",
      "validation AUC 0.5865056870488414\n",
      "validation AUC 0.5927356694053247\n",
      "validation AUC 0.5998618653554256\n",
      "validation AUC 0.6065573207822917\n",
      "validation AUC 0.6126821742585157\n",
      "validation AUC 0.6184196563334989\n",
      "validation AUC 0.6244470836048116\n",
      "validation AUC 0.6302655435465363\n",
      "validation AUC 0.636425659045692\n",
      "validation AUC 0.6414974330801144\n",
      "validation AUC 0.6462423476115482\n",
      "validation AUC 0.6508149563552312\n",
      "validation AUC 0.655163381327076\n",
      "validation AUC 0.6598171089536642\n",
      "validation AUC 0.663698674639632\n",
      "validation AUC 0.6676051150447112\n",
      "validation AUC 0.6717783633525705\n",
      "validation AUC 0.6754912497608047\n",
      "validation AUC 0.6788493740904458\n",
      "validation AUC 0.6824492559987211\n",
      "validation AUC 0.6857594481361708\n",
      "validation AUC 0.6890374726242395\n",
      "validation AUC 0.6914956903331876\n",
      "validation AUC 0.6942015217825805\n",
      "validation AUC 0.6970717381684247\n",
      "validation AUC 0.6996193069114097\n",
      "validation AUC 0.7021068666251032\n",
      "validation AUC 0.7047048020846127\n",
      "validation AUC 0.707546783309561\n",
      "validation AUC 0.7106700465491695\n",
      "validation AUC 0.7141010177003487\n",
      "validation AUC 0.7171806430890479\n",
      "validation AUC 0.7206256573215698\n",
      "validation AUC 0.723255792358648\n",
      "validation AUC 0.7254763472790692\n",
      "validation AUC 0.7274283595318305\n",
      "validation AUC 0.7294029662989462\n",
      "validation AUC 0.7313943007758763\n",
      "validation AUC 0.7332021120283488\n",
      "validation AUC 0.7334030254725212\n",
      "validation AUC 0.7335927401047627\n",
      "validation AUC 0.7338275852307015\n",
      "validation AUC 0.7340746349064919\n",
      "validation AUC 0.7343056459891917\n",
      "validation AUC 0.7345419704545063\n",
      "validation AUC 0.7347601410842769\n",
      "validation AUC 0.7349346499169973\n",
      "validation AUC 0.7350475586323924\n",
      "validation AUC 0.7352133617125258\n",
      "validation AUC 0.7353934659601881\n",
      "validation AUC 0.7355326116640482\n",
      "validation AUC 0.7356681867322202\n",
      "validation AUC 0.7357279190505494\n",
      "validation AUC 0.7358729129341729\n",
      "validation AUC 0.7360120612987152\n",
      "validation AUC 0.7361310443518716\n",
      "validation AUC 0.7362806651620685\n",
      "validation AUC 0.7364190153219117\n",
      "validation AUC 0.7365318229313781\n",
      "validation AUC 0.7366726927573886\n",
      "validation AUC 0.7366726927573886\n",
      "train auc 0.7923246688475757\n",
      "Test AUC 0.7337297141884112\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.99507342e-01\n",
      " 9.99507342e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [15.09868813 14.09868813 12.9883337  ...  0.06559095  0.0655795\n",
      "  0.02487288]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.203676 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.201364 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.199267 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.196894 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.195266 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.192229 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.190217 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.188258 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.186438 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.184095 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.182158 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.180015 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.178227 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.176084 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.174581 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.171726 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.170572 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.168377 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.166965 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.164010 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.162340 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.160634 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.159648 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.156845 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.155589 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.153396 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.152066 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.150166 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.148366 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.147326 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.145015 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.143147 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.142207 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.140892 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.138555 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.137475 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.135478 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.134706 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.133204 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.131442 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.130249 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.128471 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.126986 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.126156 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.123865 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.018s | Train Loss: 0.122425 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.019s | Train Loss: 0.122026 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.119674 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.118955 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.117229 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.116097 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.115597 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.114515 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.019s | Train Loss: 0.113187 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.111539 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.110016 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.109334 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.108265 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.107051 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.105783 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.104608 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.104842 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.019s | Train Loss: 0.102292 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.101770 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.101263 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.018s | Train Loss: 0.100016 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.018s | Train Loss: 0.098957 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.097970 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.016s | Train Loss: 0.097209 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.018s | Train Loss: 0.095862 |\n",
      "INFO:root:Pretraining Time: 1.271s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.101436\n",
      "INFO:root:Test AUC: 54.07%\n",
      "INFO:root:Test AUC: 54.07%\n",
      "INFO:root:Test Time: 0.237s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.422s | Train Loss: 1.351797 || Validation Loss: 0.012375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.394s | Train Loss: 1.282674 || Validation Loss: 0.012412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.454s | Train Loss: 1.299883 || Validation Loss: 0.010961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.396s | Train Loss: 1.198719 || Validation Loss: 0.011628 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.424s | Train Loss: 1.163123 || Validation Loss: 0.012369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.427s | Train Loss: 1.125471 || Validation Loss: 0.012293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.429s | Train Loss: 1.106994 || Validation Loss: 0.011502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.434s | Train Loss: 1.098812 || Validation Loss: 0.011055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.428s | Train Loss: 1.060118 || Validation Loss: 0.011051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.429s | Train Loss: 1.089827 || Validation Loss: 0.010602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.425s | Train Loss: 1.028273 || Validation Loss: 0.010930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.426s | Train Loss: 1.024169 || Validation Loss: 0.011505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.429s | Train Loss: 1.028595 || Validation Loss: 0.011313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.429s | Train Loss: 0.984839 || Validation Loss: 0.010747 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.428s | Train Loss: 1.011546 || Validation Loss: 0.011188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.431s | Train Loss: 0.968416 || Validation Loss: 0.010342 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.432s | Train Loss: 0.957499 || Validation Loss: 0.011245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.431s | Train Loss: 0.932510 || Validation Loss: 0.010912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.429s | Train Loss: 0.978041 || Validation Loss: 0.010021 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.432s | Train Loss: 0.924822 || Validation Loss: 0.010864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.431s | Train Loss: 0.907182 || Validation Loss: 0.010769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.430s | Train Loss: 0.892709 || Validation Loss: 0.010524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.431s | Train Loss: 0.914783 || Validation Loss: 0.010058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.430s | Train Loss: 0.904583 || Validation Loss: 0.010370 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.463s | Train Loss: 0.875337 || Validation Loss: 0.010575 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.432s | Train Loss: 0.879075 || Validation Loss: 0.010476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.432s | Train Loss: 0.886859 || Validation Loss: 0.009714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.464s | Train Loss: 0.850562 || Validation Loss: 0.009495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.433s | Train Loss: 0.860245 || Validation Loss: 0.010312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.437s | Train Loss: 0.829384 || Validation Loss: 0.010408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.434s | Train Loss: 0.861584 || Validation Loss: 0.010471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.469s | Train Loss: 0.806244 || Validation Loss: 0.010126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.444s | Train Loss: 0.805394 || Validation Loss: 0.009682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.434s | Train Loss: 0.818403 || Validation Loss: 0.009795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.472s | Train Loss: 0.762826 || Validation Loss: 0.010086 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.437s | Train Loss: 0.789919 || Validation Loss: 0.009980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.443s | Train Loss: 0.798654 || Validation Loss: 0.010689 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.465s | Train Loss: 0.784994 || Validation Loss: 0.009952 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.442s | Train Loss: 0.784319 || Validation Loss: 0.009660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.474s | Train Loss: 0.756241 || Validation Loss: 0.010369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.427s | Train Loss: 0.770480 || Validation Loss: 0.010464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.474s | Train Loss: 0.778347 || Validation Loss: 0.009982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.442s | Train Loss: 0.755787 || Validation Loss: 0.009934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.468s | Train Loss: 0.755707 || Validation Loss: 0.009452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.444s | Train Loss: 0.758667 || Validation Loss: 0.009639 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.476s | Train Loss: 0.731444 || Validation Loss: 0.009836 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.445s | Train Loss: 0.737408 || Validation Loss: 0.009492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.475s | Train Loss: 0.710355 || Validation Loss: 0.009527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.470s | Train Loss: 0.728526 || Validation Loss: 0.008758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.441s | Train Loss: 0.707948 || Validation Loss: 0.010270 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.476s | Train Loss: 0.708784 || Validation Loss: 0.010075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.449s | Train Loss: 0.715345 || Validation Loss: 0.010043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.475s | Train Loss: 0.714826 || Validation Loss: 0.009338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.449s | Train Loss: 0.706808 || Validation Loss: 0.009541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.478s | Train Loss: 0.724682 || Validation Loss: 0.010449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.444s | Train Loss: 0.688955 || Validation Loss: 0.009466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.479s | Train Loss: 0.704835 || Validation Loss: 0.010193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.478s | Train Loss: 0.722882 || Validation Loss: 0.009395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.480s | Train Loss: 0.693589 || Validation Loss: 0.009217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.450s | Train Loss: 0.695170 || Validation Loss: 0.008862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.484s | Train Loss: 0.721119 || Validation Loss: 0.009611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.480s | Train Loss: 0.718665 || Validation Loss: 0.009672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.481s | Train Loss: 0.693478 || Validation Loss: 0.009665 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.477s | Train Loss: 0.709026 || Validation Loss: 0.009508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.487s | Train Loss: 0.705331 || Validation Loss: 0.009845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.484s | Train Loss: 0.688884 || Validation Loss: 0.010236 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.474s | Train Loss: 0.696865 || Validation Loss: 0.009195 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.482s | Train Loss: 0.723097 || Validation Loss: 0.009999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.478s | Train Loss: 0.707374 || Validation Loss: 0.009696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.487s | Train Loss: 0.696650 || Validation Loss: 0.010174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.599s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.788%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008643\n",
      "INFO:root:Test AUC: 72.34%\n",
      "INFO:root:Test Time: 0.197s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 1\n",
      "experiment_method type 1\n",
      "method num 1\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 1\n",
      "normal random 935 random outlier 65\n",
      "final sizes 935 65 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 935, -1.0: 65})\n",
      "Resampled dataset shape Counter({1.0: 935, -1.0: 224})\n",
      "class weights 0.19327006039689387 0.8067299396031061\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5367345749070411\n",
      "validation AUC 0.5414823842608507\n",
      "validation AUC 0.5458579056215535\n",
      "validation AUC 0.5503069709025652\n",
      "validation AUC 0.5551061074792206\n",
      "validation AUC 0.5599411872134844\n",
      "validation AUC 0.5645705403292393\n",
      "validation AUC 0.5685148289404758\n",
      "validation AUC 0.5724697815664948\n",
      "validation AUC 0.5762107035844499\n",
      "validation AUC 0.580309947673957\n",
      "validation AUC 0.5843334235127264\n",
      "validation AUC 0.5878030916277267\n",
      "validation AUC 0.5916223095446338\n",
      "validation AUC 0.5951227138619208\n",
      "validation AUC 0.5988698219662953\n",
      "validation AUC 0.6023910647475981\n",
      "validation AUC 0.6061802036507541\n",
      "validation AUC 0.6100973293560745\n",
      "validation AUC 0.6141664385575015\n",
      "validation AUC 0.6178208005971848\n",
      "validation AUC 0.621484054637218\n",
      "validation AUC 0.6251084361083965\n",
      "validation AUC 0.6289410718462984\n",
      "validation AUC 0.6328080676358219\n",
      "validation AUC 0.6363709979878855\n",
      "validation AUC 0.6395879066028132\n",
      "validation AUC 0.6426955223696333\n",
      "validation AUC 0.6460758022010442\n",
      "validation AUC 0.6496073498050146\n",
      "validation AUC 0.6529395537673879\n",
      "validation AUC 0.65631339740824\n",
      "validation AUC 0.6597846220224038\n",
      "validation AUC 0.6629655369267587\n",
      "validation AUC 0.6663120101284727\n",
      "validation AUC 0.6695736634381593\n",
      "validation AUC 0.6727137714576048\n",
      "validation AUC 0.6757059641642276\n",
      "validation AUC 0.6786965764255457\n",
      "validation AUC 0.6816742817168765\n",
      "validation AUC 0.6847509324627297\n",
      "validation AUC 0.6878757522015017\n",
      "validation AUC 0.6905033090374013\n",
      "validation AUC 0.6932994121169176\n",
      "validation AUC 0.6963236873310865\n",
      "validation AUC 0.6992543517588068\n",
      "validation AUC 0.7017388782946431\n",
      "validation AUC 0.7043627687102907\n",
      "validation AUC 0.7072345415952983\n",
      "validation AUC 0.7075150067804828\n",
      "validation AUC 0.7077379426923083\n",
      "validation AUC 0.7079826403249183\n",
      "validation AUC 0.7082220538424191\n",
      "validation AUC 0.7084380613374548\n",
      "validation AUC 0.7086894798536326\n",
      "validation AUC 0.7089455040109254\n",
      "validation AUC 0.709184039503257\n",
      "validation AUC 0.7094576215032515\n",
      "validation AUC 0.7097310545050354\n",
      "validation AUC 0.7099793972724389\n",
      "validation AUC 0.7102404554407016\n",
      "validation AUC 0.7104866670015579\n",
      "validation AUC 0.7107440507675218\n",
      "validation AUC 0.7110014238907563\n",
      "validation AUC 0.7112609122564437\n",
      "validation AUC 0.7114881717770569\n",
      "validation AUC 0.7117341944294678\n",
      "validation AUC 0.7119899338937511\n",
      "validation AUC 0.7122366376808386\n",
      "validation AUC 0.7124393790137722\n",
      "validation AUC 0.7124393790137722\n",
      "train auc 0.7881887130641168\n",
      "Test AUC 0.7233906355182129\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99886310e-01\n",
      " 9.99886310e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [14.50993729 13.50993729 13.46764565 ...  0.04752819  0.0465686\n",
      "  0.02425424]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {4..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1k_points_random\" --iteration_num $i --experiment_method 1 --balanced_train 1 --balanced_batches 1 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 1000\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e1d2ac",
   "metadata": {},
   "source": [
    "# k means sampling of 10K points for k=5, k=10, k=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deee5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test 28.06 after previous bad results. 1k samples selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1deba004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.036s | Train Loss: 0.207309 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.205846 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.016s | Train Loss: 0.204019 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.016s | Train Loss: 0.202083 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.200060 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.015s | Train Loss: 0.198669 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.195945 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.015s | Train Loss: 0.194713 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.015s | Train Loss: 0.192603 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.015s | Train Loss: 0.190563 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.189017 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.187120 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.015s | Train Loss: 0.185087 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.015s | Train Loss: 0.183835 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.015s | Train Loss: 0.182232 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.015s | Train Loss: 0.179967 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.016s | Train Loss: 0.178692 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.015s | Train Loss: 0.176942 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.015s | Train Loss: 0.175687 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.173332 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.172147 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.015s | Train Loss: 0.170176 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.168309 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.167377 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.015s | Train Loss: 0.165522 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.015s | Train Loss: 0.164049 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.015s | Train Loss: 0.162206 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.015s | Train Loss: 0.160475 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.015s | Train Loss: 0.158828 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.157603 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.156036 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.015s | Train Loss: 0.154775 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.153248 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.015s | Train Loss: 0.151347 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.150704 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.148342 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.016s | Train Loss: 0.146891 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.016s | Train Loss: 0.145141 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.144143 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.142919 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.015s | Train Loss: 0.141836 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.014s | Train Loss: 0.140486 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.138809 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.137805 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.015s | Train Loss: 0.137085 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.015s | Train Loss: 0.134128 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.016s | Train Loss: 0.133558 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.132699 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.015s | Train Loss: 0.130946 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.130127 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.128320 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.015s | Train Loss: 0.128455 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.125907 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.124865 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.123991 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.015s | Train Loss: 0.122829 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.122602 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.120446 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.118767 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.118211 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.116576 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.016s | Train Loss: 0.116351 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.115197 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.016s | Train Loss: 0.113901 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.016s | Train Loss: 0.113015 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.111142 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.016s | Train Loss: 0.111252 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.109829 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.110024 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.109148 |\n",
      "INFO:root:Pretraining Time: 1.120s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.111951\n",
      "INFO:root:Test AUC: 47.57%\n",
      "INFO:root:Test AUC: 47.57%\n",
      "INFO:root:Test Time: 0.233s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.415s | Train Loss: 1.216747 || Validation Loss: 0.010388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.391s | Train Loss: 1.163250 || Validation Loss: 0.010794 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.420s | Train Loss: 1.176811 || Validation Loss: 0.009954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.427s | Train Loss: 1.102863 || Validation Loss: 0.010437 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.414s | Train Loss: 1.074916 || Validation Loss: 0.010375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.423s | Train Loss: 1.091125 || Validation Loss: 0.011064 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.416s | Train Loss: 1.068013 || Validation Loss: 0.010735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.391s | Train Loss: 1.039218 || Validation Loss: 0.010051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.443s | Train Loss: 1.008343 || Validation Loss: 0.011117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.396s | Train Loss: 1.026410 || Validation Loss: 0.010863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.462s | Train Loss: 1.002143 || Validation Loss: 0.010985 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.424s | Train Loss: 0.976784 || Validation Loss: 0.010117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.415s | Train Loss: 0.891862 || Validation Loss: 0.011279 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.423s | Train Loss: 0.925801 || Validation Loss: 0.010709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.427s | Train Loss: 0.939377 || Validation Loss: 0.009369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.425s | Train Loss: 0.902049 || Validation Loss: 0.010630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.417s | Train Loss: 0.916611 || Validation Loss: 0.010363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.422s | Train Loss: 0.869048 || Validation Loss: 0.011093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.420s | Train Loss: 0.880069 || Validation Loss: 0.010477 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.414s | Train Loss: 0.887491 || Validation Loss: 0.011016 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.424s | Train Loss: 0.848069 || Validation Loss: 0.011317 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.406s | Train Loss: 0.850874 || Validation Loss: 0.010884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.426s | Train Loss: 0.826809 || Validation Loss: 0.010558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.424s | Train Loss: 0.857412 || Validation Loss: 0.010556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.423s | Train Loss: 0.842929 || Validation Loss: 0.010084 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.428s | Train Loss: 0.813399 || Validation Loss: 0.010244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.426s | Train Loss: 0.808720 || Validation Loss: 0.010481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.425s | Train Loss: 0.808220 || Validation Loss: 0.010730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.459s | Train Loss: 0.790445 || Validation Loss: 0.011013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.431s | Train Loss: 0.771314 || Validation Loss: 0.009413 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.426s | Train Loss: 0.796136 || Validation Loss: 0.010998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.459s | Train Loss: 0.771548 || Validation Loss: 0.011223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.432s | Train Loss: 0.775777 || Validation Loss: 0.011176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.423s | Train Loss: 0.763347 || Validation Loss: 0.011133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.456s | Train Loss: 0.793746 || Validation Loss: 0.010632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.432s | Train Loss: 0.759119 || Validation Loss: 0.010498 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.433s | Train Loss: 0.701884 || Validation Loss: 0.010960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.456s | Train Loss: 0.748273 || Validation Loss: 0.010431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.431s | Train Loss: 0.721625 || Validation Loss: 0.009983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.434s | Train Loss: 0.716570 || Validation Loss: 0.010570 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.464s | Train Loss: 0.705035 || Validation Loss: 0.010867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.429s | Train Loss: 0.741574 || Validation Loss: 0.009416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.561s | Train Loss: 0.725629 || Validation Loss: 0.010480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.428s | Train Loss: 0.694596 || Validation Loss: 0.010966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.434s | Train Loss: 0.696477 || Validation Loss: 0.010867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.459s | Train Loss: 0.707898 || Validation Loss: 0.010658 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.440s | Train Loss: 0.690649 || Validation Loss: 0.010387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.456s | Train Loss: 0.626677 || Validation Loss: 0.010401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.437s | Train Loss: 0.680599 || Validation Loss: 0.010787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.469s | Train Loss: 0.685361 || Validation Loss: 0.010844 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.470s | Train Loss: 0.693404 || Validation Loss: 0.011094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.438s | Train Loss: 0.681748 || Validation Loss: 0.010618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.437s | Train Loss: 0.672274 || Validation Loss: 0.010580 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.459s | Train Loss: 0.675941 || Validation Loss: 0.009946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.474s | Train Loss: 0.670403 || Validation Loss: 0.010521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.442s | Train Loss: 0.670674 || Validation Loss: 0.010553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.444s | Train Loss: 0.698148 || Validation Loss: 0.011043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.475s | Train Loss: 0.675857 || Validation Loss: 0.010767 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.483s | Train Loss: 0.655098 || Validation Loss: 0.010464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.459s | Train Loss: 0.667147 || Validation Loss: 0.010661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.473s | Train Loss: 0.678343 || Validation Loss: 0.011509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.445s | Train Loss: 0.678067 || Validation Loss: 0.011478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.466s | Train Loss: 0.677874 || Validation Loss: 0.011105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.447s | Train Loss: 0.686967 || Validation Loss: 0.011407 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.473s | Train Loss: 0.670671 || Validation Loss: 0.010210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.442s | Train Loss: 0.691802 || Validation Loss: 0.010983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.474s | Train Loss: 0.660650 || Validation Loss: 0.010334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.451s | Train Loss: 0.660579 || Validation Loss: 0.010668 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.498s | Train Loss: 0.670920 || Validation Loss: 0.010879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.477s | Train Loss: 0.682509 || Validation Loss: 0.010486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.983s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.829%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009020\n",
      "INFO:root:Test AUC: 73.96%\n",
      "INFO:root:Test Time: 0.191s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 925\n",
      "len all outlier 16676 len chosen outlier 75\n",
      "final sizes 925 75 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 925, -1.0: 75})\n",
      "Resampled dataset shape Counter({1.0: 925, -1.0: 222})\n",
      "class weights 0.1935483870967742 0.8064516129032258\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5996294574145021\n",
      "validation AUC 0.6036474788544933\n",
      "validation AUC 0.607707363470279\n",
      "validation AUC 0.6118196768313955\n",
      "validation AUC 0.6158162372077054\n",
      "validation AUC 0.619577712996667\n",
      "validation AUC 0.6232615474145299\n",
      "validation AUC 0.6270760479416643\n",
      "validation AUC 0.6302455538401841\n",
      "validation AUC 0.6334812255869091\n",
      "validation AUC 0.6374592729355606\n",
      "validation AUC 0.641130599485786\n",
      "validation AUC 0.6447790787568355\n",
      "validation AUC 0.6482467886096402\n",
      "validation AUC 0.651565175648669\n",
      "validation AUC 0.6546683028443971\n",
      "validation AUC 0.6576280751368175\n",
      "validation AUC 0.6607716127757877\n",
      "validation AUC 0.6638040376599362\n",
      "validation AUC 0.6663696272043487\n",
      "validation AUC 0.668786897012224\n",
      "validation AUC 0.6709960136593045\n",
      "validation AUC 0.6734627668371702\n",
      "validation AUC 0.676065914573366\n",
      "validation AUC 0.6786370303549797\n",
      "validation AUC 0.681059943470079\n",
      "validation AUC 0.6836337305767528\n",
      "validation AUC 0.6859034282998261\n",
      "validation AUC 0.6880123197041832\n",
      "validation AUC 0.690282110551138\n",
      "validation AUC 0.6922864664073956\n",
      "validation AUC 0.6941735181010475\n",
      "validation AUC 0.6960385074168117\n",
      "validation AUC 0.6978782495179375\n",
      "validation AUC 0.6997483925753766\n",
      "validation AUC 0.7022946735481135\n",
      "validation AUC 0.7050662956895031\n",
      "validation AUC 0.7075404429035665\n",
      "validation AUC 0.7095757158885092\n",
      "validation AUC 0.7111454439646466\n",
      "validation AUC 0.7130966926015789\n",
      "validation AUC 0.7149553149045251\n",
      "validation AUC 0.7169654683875819\n",
      "validation AUC 0.7191208631977187\n",
      "validation AUC 0.7212700399932481\n",
      "validation AUC 0.7231551307640223\n",
      "validation AUC 0.724594328425687\n",
      "validation AUC 0.7265650186684115\n",
      "validation AUC 0.728611312199569\n",
      "validation AUC 0.7287846210645582\n",
      "validation AUC 0.728894603029389\n",
      "validation AUC 0.7289719224579255\n",
      "validation AUC 0.7290852409183994\n",
      "validation AUC 0.7292401910753052\n",
      "validation AUC 0.7294023091104105\n",
      "validation AUC 0.7295891768125579\n",
      "validation AUC 0.7297071275209699\n",
      "validation AUC 0.7298115459990469\n",
      "validation AUC 0.7299316784669702\n",
      "validation AUC 0.7300302354618721\n",
      "validation AUC 0.7301831555181593\n",
      "validation AUC 0.7303231606224123\n",
      "validation AUC 0.7304980472720236\n",
      "validation AUC 0.7306707894116762\n",
      "validation AUC 0.7308619647585175\n",
      "validation AUC 0.7310504634589339\n",
      "validation AUC 0.7312428281307769\n",
      "validation AUC 0.7314222778505859\n",
      "validation AUC 0.7315498575683534\n",
      "validation AUC 0.7316622793788817\n",
      "validation AUC 0.7316622793788817\n",
      "train auc 0.8286175026999552\n",
      "Test AUC 0.7395891912623208\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.94675509e-01\n",
      " 9.94713406e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [13.71190453 12.71190453 12.31481171 ...  0.0864759   0.08644857\n",
      "  0.05323758]\n",
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.034s | Train Loss: 0.224699 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.222553 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.219855 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.019s | Train Loss: 0.216720 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.214292 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.212715 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.210223 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.019s | Train Loss: 0.207657 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.205549 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.203795 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.201686 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.199062 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.196819 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.194528 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.192010 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.191012 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.188721 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.186357 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.184875 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.182086 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.180680 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.178622 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.176482 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.173839 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.173071 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.170848 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.169089 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.167367 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.164823 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.163328 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.161717 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.159899 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.157972 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.156887 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.154903 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.153228 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.151150 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.149682 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.148327 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.146103 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.016s | Train Loss: 0.144178 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.143338 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.141703 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.140156 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.139106 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.136857 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.135107 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.133858 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.132408 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.131156 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.129689 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.128175 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.126892 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.125747 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.125062 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.123643 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.122527 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.120339 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.118989 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.118526 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.116594 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.115010 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.018s | Train Loss: 0.114132 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.113795 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.111608 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.110961 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.109583 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.108617 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.018s | Train Loss: 0.108315 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.106820 |\n",
      "INFO:root:Pretraining Time: 1.248s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.108237\n",
      "INFO:root:Test AUC: 53.02%\n",
      "INFO:root:Test AUC: 53.02%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.412s | Train Loss: 1.826670 || Validation Loss: 0.014848 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.387s | Train Loss: 1.831790 || Validation Loss: 0.013699 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.411s | Train Loss: 1.743433 || Validation Loss: 0.013659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.407s | Train Loss: 1.590928 || Validation Loss: 0.014082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.420s | Train Loss: 1.562094 || Validation Loss: 0.012718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.444s | Train Loss: 1.523816 || Validation Loss: 0.014997 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.419s | Train Loss: 1.486696 || Validation Loss: 0.012803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.413s | Train Loss: 1.387258 || Validation Loss: 0.012531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.411s | Train Loss: 1.456586 || Validation Loss: 0.013608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.381s | Train Loss: 1.336559 || Validation Loss: 0.013142 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.441s | Train Loss: 1.315122 || Validation Loss: 0.012956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.417s | Train Loss: 1.302912 || Validation Loss: 0.012756 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.422s | Train Loss: 1.260367 || Validation Loss: 0.011853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.422s | Train Loss: 1.174722 || Validation Loss: 0.012459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.416s | Train Loss: 1.109700 || Validation Loss: 0.011912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.425s | Train Loss: 1.177977 || Validation Loss: 0.012575 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.422s | Train Loss: 1.182645 || Validation Loss: 0.011743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.423s | Train Loss: 1.141138 || Validation Loss: 0.012335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.445s | Train Loss: 1.115570 || Validation Loss: 0.011778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.455s | Train Loss: 1.123196 || Validation Loss: 0.011747 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.449s | Train Loss: 1.074766 || Validation Loss: 0.012686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.425s | Train Loss: 1.062532 || Validation Loss: 0.011457 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.414s | Train Loss: 1.021426 || Validation Loss: 0.011377 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.415s | Train Loss: 1.050787 || Validation Loss: 0.011499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.443s | Train Loss: 1.046378 || Validation Loss: 0.012140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.417s | Train Loss: 1.006024 || Validation Loss: 0.011638 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.420s | Train Loss: 0.992084 || Validation Loss: 0.011241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.457s | Train Loss: 0.983125 || Validation Loss: 0.010853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.424s | Train Loss: 0.958841 || Validation Loss: 0.011540 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.418s | Train Loss: 0.983700 || Validation Loss: 0.011679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.435s | Train Loss: 0.926073 || Validation Loss: 0.011522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.485s | Train Loss: 0.930991 || Validation Loss: 0.011080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.436s | Train Loss: 0.921312 || Validation Loss: 0.011667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.426s | Train Loss: 0.905168 || Validation Loss: 0.011527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.448s | Train Loss: 0.901819 || Validation Loss: 0.011263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.416s | Train Loss: 0.891686 || Validation Loss: 0.011215 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.420s | Train Loss: 0.893176 || Validation Loss: 0.010743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.459s | Train Loss: 0.866226 || Validation Loss: 0.011492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.423s | Train Loss: 0.840120 || Validation Loss: 0.010925 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.464s | Train Loss: 0.830696 || Validation Loss: 0.011019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.424s | Train Loss: 0.842751 || Validation Loss: 0.010445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.467s | Train Loss: 0.824894 || Validation Loss: 0.011044 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.430s | Train Loss: 0.841963 || Validation Loss: 0.012065 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.459s | Train Loss: 0.821296 || Validation Loss: 0.010446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.436s | Train Loss: 0.859040 || Validation Loss: 0.011126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.461s | Train Loss: 0.823555 || Validation Loss: 0.011318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.430s | Train Loss: 0.822207 || Validation Loss: 0.011824 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.465s | Train Loss: 0.808526 || Validation Loss: 0.010874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.462s | Train Loss: 0.794505 || Validation Loss: 0.010775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.429s | Train Loss: 0.801385 || Validation Loss: 0.010135 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.458s | Train Loss: 0.797059 || Validation Loss: 0.011470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.427s | Train Loss: 0.776594 || Validation Loss: 0.011553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.475s | Train Loss: 0.783908 || Validation Loss: 0.010937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.449s | Train Loss: 0.780583 || Validation Loss: 0.011151 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.474s | Train Loss: 0.762296 || Validation Loss: 0.010338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.446s | Train Loss: 0.798857 || Validation Loss: 0.010640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.492s | Train Loss: 0.760563 || Validation Loss: 0.011311 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.487s | Train Loss: 0.780328 || Validation Loss: 0.010363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.480s | Train Loss: 0.749866 || Validation Loss: 0.010545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.446s | Train Loss: 0.795625 || Validation Loss: 0.011859 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.463s | Train Loss: 0.777777 || Validation Loss: 0.011189 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.486s | Train Loss: 0.772718 || Validation Loss: 0.011110 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.506s | Train Loss: 0.780594 || Validation Loss: 0.010903 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.456s | Train Loss: 0.798041 || Validation Loss: 0.011281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.466s | Train Loss: 0.791874 || Validation Loss: 0.010403 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.467s | Train Loss: 0.764261 || Validation Loss: 0.011505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.469s | Train Loss: 0.780421 || Validation Loss: 0.010833 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.470s | Train Loss: 0.798923 || Validation Loss: 0.011256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.468s | Train Loss: 0.776931 || Validation Loss: 0.011384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.473s | Train Loss: 0.785232 || Validation Loss: 0.011787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.039s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.688%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008456\n",
      "INFO:root:Test AUC: 71.62%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 936\n",
      "len all outlier 16676 len chosen outlier 64\n",
      "final sizes 936 64 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 936, -1.0: 64})\n",
      "Resampled dataset shape Counter({1.0: 936, -1.0: 224})\n",
      "class weights 0.19310344827586207 0.8068965517241379\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5371694953622179\n",
      "validation AUC 0.5402768610248991\n",
      "validation AUC 0.5435322830165668\n",
      "validation AUC 0.5470678110013042\n",
      "validation AUC 0.550813939974581\n",
      "validation AUC 0.5543240398182818\n",
      "validation AUC 0.5578020837612597\n",
      "validation AUC 0.5615756896009679\n",
      "validation AUC 0.5657812891452888\n",
      "validation AUC 0.5701683312604844\n",
      "validation AUC 0.5743117932296491\n",
      "validation AUC 0.5783296736534765\n",
      "validation AUC 0.5821309745355673\n",
      "validation AUC 0.585835655399493\n",
      "validation AUC 0.5894769215607434\n",
      "validation AUC 0.5935543028235374\n",
      "validation AUC 0.5973270971551347\n",
      "validation AUC 0.6014475202759957\n",
      "validation AUC 0.6052894790447853\n",
      "validation AUC 0.6096204818687271\n",
      "validation AUC 0.6138456879811445\n",
      "validation AUC 0.6178213779652506\n",
      "validation AUC 0.6220076396703905\n",
      "validation AUC 0.6262114778430082\n",
      "validation AUC 0.6302933370341651\n",
      "validation AUC 0.6338863091504271\n",
      "validation AUC 0.6373676150899428\n",
      "validation AUC 0.6414838715822737\n",
      "validation AUC 0.6460309111687568\n",
      "validation AUC 0.6502202140338009\n",
      "validation AUC 0.6540944548613113\n",
      "validation AUC 0.6579279553209709\n",
      "validation AUC 0.6614431982636174\n",
      "validation AUC 0.6649159527701215\n",
      "validation AUC 0.6687776325163477\n",
      "validation AUC 0.6724881775241308\n",
      "validation AUC 0.6757355509517474\n",
      "validation AUC 0.678704997485123\n",
      "validation AUC 0.6817925517071682\n",
      "validation AUC 0.6847122807305084\n",
      "validation AUC 0.6877718685205734\n",
      "validation AUC 0.6905610059337474\n",
      "validation AUC 0.6934761053698316\n",
      "validation AUC 0.6963056425835182\n",
      "validation AUC 0.698974695421051\n",
      "validation AUC 0.701632344574113\n",
      "validation AUC 0.7043479912806248\n",
      "validation AUC 0.7066196632299874\n",
      "validation AUC 0.7086719432963895\n",
      "validation AUC 0.708914738541133\n",
      "validation AUC 0.7091173468399501\n",
      "validation AUC 0.7093028389693325\n",
      "validation AUC 0.7095243727015695\n",
      "validation AUC 0.7097470930981262\n",
      "validation AUC 0.7099813315884933\n",
      "validation AUC 0.7102248691636072\n",
      "validation AUC 0.7104519238116809\n",
      "validation AUC 0.7106972493653209\n",
      "validation AUC 0.7109655924818058\n",
      "validation AUC 0.711245461674148\n",
      "validation AUC 0.7115290425183423\n",
      "validation AUC 0.7118062297428951\n",
      "validation AUC 0.7120818950571547\n",
      "validation AUC 0.7123270795946313\n",
      "validation AUC 0.7125529236322443\n",
      "validation AUC 0.7128098870103999\n",
      "validation AUC 0.713062755598448\n",
      "validation AUC 0.7132780633340308\n",
      "validation AUC 0.713525272650761\n",
      "validation AUC 0.7138102716386375\n",
      "validation AUC 0.7138102716386375\n",
      "train auc 0.6877912145400817\n",
      "Test AUC 0.7161922139828542\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 2.84225486e-04 ... 9.99431549e-01\n",
      " 9.99431549e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [11.0344305  10.0344305   7.65148449 ...  0.09065115  0.09062242\n",
      "  0.02984434]\n",
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.034s | Train Loss: 0.226376 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.224026 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.016s | Train Loss: 0.221674 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.015s | Train Loss: 0.219871 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.015s | Train Loss: 0.218003 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.016s | Train Loss: 0.216210 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.214870 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.212139 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.015s | Train Loss: 0.210956 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.015s | Train Loss: 0.209070 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.014s | Train Loss: 0.206814 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.205177 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.015s | Train Loss: 0.203992 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.202487 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.200281 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.198664 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.016s | Train Loss: 0.197426 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.195688 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.016s | Train Loss: 0.193399 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.015s | Train Loss: 0.192381 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.190160 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.188517 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.015s | Train Loss: 0.186660 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.015s | Train Loss: 0.185229 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.184041 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.015s | Train Loss: 0.182750 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.015s | Train Loss: 0.180696 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.015s | Train Loss: 0.179871 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.177647 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.175766 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.174187 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.172667 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.170779 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.016s | Train Loss: 0.168991 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.167949 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.165960 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.016s | Train Loss: 0.164814 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.015s | Train Loss: 0.162737 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.015s | Train Loss: 0.161636 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.159379 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.016s | Train Loss: 0.158885 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.016s | Train Loss: 0.156672 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.155146 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.153711 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.152146 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.151092 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.016s | Train Loss: 0.149866 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.147964 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.015s | Train Loss: 0.146346 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.145335 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.015s | Train Loss: 0.144791 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.016s | Train Loss: 0.142783 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.140962 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.139437 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.015s | Train Loss: 0.139024 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.016s | Train Loss: 0.137558 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.135942 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.015s | Train Loss: 0.134548 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.133182 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.132126 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.130965 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.016s | Train Loss: 0.128693 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.015s | Train Loss: 0.128575 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.016s | Train Loss: 0.127356 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.125314 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.015s | Train Loss: 0.124774 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.123670 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.015s | Train Loss: 0.122366 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.016s | Train Loss: 0.120739 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.015s | Train Loss: 0.119491 |\n",
      "INFO:root:Pretraining Time: 1.123s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.123387\n",
      "INFO:root:Test AUC: 51.51%\n",
      "INFO:root:Test AUC: 51.51%\n",
      "INFO:root:Test Time: 0.226s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.421s | Train Loss: 2.036958 || Validation Loss: 0.021117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.410s | Train Loss: 1.935861 || Validation Loss: 0.019730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.419s | Train Loss: 1.774384 || Validation Loss: 0.020922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.424s | Train Loss: 1.764835 || Validation Loss: 0.020318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.412s | Train Loss: 1.677348 || Validation Loss: 0.020151 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.413s | Train Loss: 1.615789 || Validation Loss: 0.018785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.421s | Train Loss: 1.504808 || Validation Loss: 0.018383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.392s | Train Loss: 1.519230 || Validation Loss: 0.018725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.445s | Train Loss: 1.490888 || Validation Loss: 0.019406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.397s | Train Loss: 1.403365 || Validation Loss: 0.017993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.441s | Train Loss: 1.379844 || Validation Loss: 0.018724 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.422s | Train Loss: 1.386005 || Validation Loss: 0.017843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.437s | Train Loss: 1.355387 || Validation Loss: 0.018131 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.442s | Train Loss: 1.277119 || Validation Loss: 0.018103 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.445s | Train Loss: 1.285104 || Validation Loss: 0.017840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.438s | Train Loss: 1.245774 || Validation Loss: 0.018319 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.451s | Train Loss: 1.209332 || Validation Loss: 0.015791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.420s | Train Loss: 1.147710 || Validation Loss: 0.017066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.405s | Train Loss: 1.177315 || Validation Loss: 0.016730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.422s | Train Loss: 1.129969 || Validation Loss: 0.016327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.412s | Train Loss: 1.133437 || Validation Loss: 0.016588 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.420s | Train Loss: 1.094893 || Validation Loss: 0.015822 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.421s | Train Loss: 1.097794 || Validation Loss: 0.015705 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.417s | Train Loss: 1.107753 || Validation Loss: 0.015654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.425s | Train Loss: 1.074958 || Validation Loss: 0.016230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.422s | Train Loss: 1.021162 || Validation Loss: 0.015717 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 1.027891 || Validation Loss: 0.016218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.443s | Train Loss: 1.028264 || Validation Loss: 0.015505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.451s | Train Loss: 1.006171 || Validation Loss: 0.015368 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.429s | Train Loss: 0.975449 || Validation Loss: 0.014972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.419s | Train Loss: 0.965598 || Validation Loss: 0.014525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.454s | Train Loss: 0.980728 || Validation Loss: 0.014916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.429s | Train Loss: 0.943061 || Validation Loss: 0.016055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.438s | Train Loss: 0.950739 || Validation Loss: 0.014127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.472s | Train Loss: 0.924094 || Validation Loss: 0.013676 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.432s | Train Loss: 0.894129 || Validation Loss: 0.014312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.439s | Train Loss: 0.890466 || Validation Loss: 0.015010 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.459s | Train Loss: 0.915976 || Validation Loss: 0.014323 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.425s | Train Loss: 0.912571 || Validation Loss: 0.015449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.433s | Train Loss: 0.892420 || Validation Loss: 0.014212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.457s | Train Loss: 0.883839 || Validation Loss: 0.015099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.427s | Train Loss: 0.889168 || Validation Loss: 0.013674 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.474s | Train Loss: 0.849809 || Validation Loss: 0.014832 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.451s | Train Loss: 0.853973 || Validation Loss: 0.013989 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.465s | Train Loss: 0.818939 || Validation Loss: 0.014092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.492s | Train Loss: 0.822625 || Validation Loss: 0.013088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.439s | Train Loss: 0.844371 || Validation Loss: 0.013694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.456s | Train Loss: 0.833698 || Validation Loss: 0.013992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.439s | Train Loss: 0.811875 || Validation Loss: 0.012904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.476s | Train Loss: 0.810394 || Validation Loss: 0.013909 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.473s | Train Loss: 0.823935 || Validation Loss: 0.014378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.434s | Train Loss: 0.819476 || Validation Loss: 0.014179 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.448s | Train Loss: 0.818212 || Validation Loss: 0.013332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.477s | Train Loss: 0.829369 || Validation Loss: 0.013141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.471s | Train Loss: 0.800061 || Validation Loss: 0.014039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.454s | Train Loss: 0.782307 || Validation Loss: 0.013472 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.442s | Train Loss: 0.795495 || Validation Loss: 0.013643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.469s | Train Loss: 0.799294 || Validation Loss: 0.014341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.472s | Train Loss: 0.779515 || Validation Loss: 0.013769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.432s | Train Loss: 0.781350 || Validation Loss: 0.014168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.476s | Train Loss: 0.811815 || Validation Loss: 0.013885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.442s | Train Loss: 0.782458 || Validation Loss: 0.013147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.485s | Train Loss: 0.812892 || Validation Loss: 0.013283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.438s | Train Loss: 0.764225 || Validation Loss: 0.012854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.485s | Train Loss: 0.781748 || Validation Loss: 0.014080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.430s | Train Loss: 0.791883 || Validation Loss: 0.013889 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.492s | Train Loss: 0.807529 || Validation Loss: 0.013621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.456s | Train Loss: 0.792463 || Validation Loss: 0.013167 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.521s | Train Loss: 0.793091 || Validation Loss: 0.013677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.480s | Train Loss: 0.752833 || Validation Loss: 0.012939 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.127s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.690%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010360\n",
      "INFO:root:Test AUC: 69.58%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 926\n",
      "len all outlier 16676 len chosen outlier 74\n",
      "final sizes 926 74 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 926, -1.0: 74})\n",
      "Resampled dataset shape Counter({1.0: 926, -1.0: 222})\n",
      "class weights 0.19337979094076654 0.8066202090592335\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.41466271966859397\n",
      "validation AUC 0.4221208064719715\n",
      "validation AUC 0.4303021438926804\n",
      "validation AUC 0.43915640246374926\n",
      "validation AUC 0.4482288609320946\n",
      "validation AUC 0.45751654731555247\n",
      "validation AUC 0.4673193311555471\n",
      "validation AUC 0.477289032901572\n",
      "validation AUC 0.48711486889328176\n",
      "validation AUC 0.4970777114002575\n",
      "validation AUC 0.5074297478758708\n",
      "validation AUC 0.5176512353122353\n",
      "validation AUC 0.5265617726359678\n",
      "validation AUC 0.5346995387228257\n",
      "validation AUC 0.5428300012366851\n",
      "validation AUC 0.5505570643989424\n",
      "validation AUC 0.5575992759112677\n",
      "validation AUC 0.5643532769921433\n",
      "validation AUC 0.5710949591138268\n",
      "validation AUC 0.577710487643472\n",
      "validation AUC 0.5842964267249151\n",
      "validation AUC 0.5908287557180724\n",
      "validation AUC 0.5968676824584195\n",
      "validation AUC 0.6022621280818417\n",
      "validation AUC 0.606892505560294\n",
      "validation AUC 0.611681661917526\n",
      "validation AUC 0.6165733157401923\n",
      "validation AUC 0.6217020869115076\n",
      "validation AUC 0.6267061119917379\n",
      "validation AUC 0.6311549963463511\n",
      "validation AUC 0.6357958755379635\n",
      "validation AUC 0.6402760787842938\n",
      "validation AUC 0.6445964681464176\n",
      "validation AUC 0.6487085287427126\n",
      "validation AUC 0.6526047920378762\n",
      "validation AUC 0.6565773956304785\n",
      "validation AUC 0.6603677611081891\n",
      "validation AUC 0.6636582615357076\n",
      "validation AUC 0.6666032643805091\n",
      "validation AUC 0.6696605826085458\n",
      "validation AUC 0.6727493793692395\n",
      "validation AUC 0.6754689745283302\n",
      "validation AUC 0.6782693054320703\n",
      "validation AUC 0.6810725923538801\n",
      "validation AUC 0.6841446906722649\n",
      "validation AUC 0.6870472396697944\n",
      "validation AUC 0.6896766031089967\n",
      "validation AUC 0.6922519839643868\n",
      "validation AUC 0.6948843034216587\n",
      "validation AUC 0.6951558686244463\n",
      "validation AUC 0.695406166993363\n",
      "validation AUC 0.6956650833123494\n",
      "validation AUC 0.6959254523638885\n",
      "validation AUC 0.6961729250881697\n",
      "validation AUC 0.6964148635932025\n",
      "validation AUC 0.6966570655057861\n",
      "validation AUC 0.6969001348008095\n",
      "validation AUC 0.6971945419614082\n",
      "validation AUC 0.6974553659896259\n",
      "validation AUC 0.6976851425157241\n",
      "validation AUC 0.6979067587291133\n",
      "validation AUC 0.6981188470390756\n",
      "validation AUC 0.698353931626424\n",
      "validation AUC 0.6985997094960603\n",
      "validation AUC 0.6988764683507579\n",
      "validation AUC 0.6991482570508614\n",
      "validation AUC 0.6994720807153106\n",
      "validation AUC 0.6997881937223648\n",
      "validation AUC 0.7000671210331622\n",
      "validation AUC 0.7003455667604576\n",
      "validation AUC 0.7003455667604576\n",
      "train auc 0.6898883038986767\n",
      "Test AUC 0.6957564705326921\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.51586926e-04 ... 9.98427286e-01\n",
      " 9.98427286e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [16.61972713 15.61972713  9.6125288  ...  0.07403748  0.07379897\n",
      "  0.02986466]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.035s | Train Loss: 0.204145 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.201880 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.200496 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.198036 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.196011 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.193906 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.191934 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.189719 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.187915 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.016s | Train Loss: 0.185612 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.183157 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.182015 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.016s | Train Loss: 0.179296 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.177498 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.176274 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.173850 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.172123 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.169562 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.168047 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.165713 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.164135 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.161945 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.160246 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.158138 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.156025 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.016s | Train Loss: 0.154445 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.152667 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.150938 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.149024 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.147822 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.145675 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.144640 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.142638 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.141499 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.139729 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.138167 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.135857 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.134840 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.133390 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.133138 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.130285 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.129334 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.127925 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.127461 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.126350 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.123738 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.122984 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.120948 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.121138 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.119256 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.118787 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.116655 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.115412 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.115511 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.114372 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.112981 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.111848 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.110473 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.110427 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.108202 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.107046 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.106991 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.106653 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.105125 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.103877 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.103334 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.102967 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.101302 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.100815 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.099125 |\n",
      "INFO:root:Pretraining Time: 1.223s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.103764\n",
      "INFO:root:Test AUC: 49.77%\n",
      "INFO:root:Test AUC: 49.77%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.417s | Train Loss: 1.353475 || Validation Loss: 0.015073 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.384s | Train Loss: 1.293655 || Validation Loss: 0.014830 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.409s | Train Loss: 1.244005 || Validation Loss: 0.012355 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.415s | Train Loss: 1.261008 || Validation Loss: 0.014249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.412s | Train Loss: 1.194117 || Validation Loss: 0.013868 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.414s | Train Loss: 1.164929 || Validation Loss: 0.012946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.417s | Train Loss: 1.142517 || Validation Loss: 0.012528 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.415s | Train Loss: 1.119877 || Validation Loss: 0.014416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.415s | Train Loss: 1.104271 || Validation Loss: 0.013089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.388s | Train Loss: 1.057611 || Validation Loss: 0.012902 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.440s | Train Loss: 1.064351 || Validation Loss: 0.012616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.415s | Train Loss: 1.050583 || Validation Loss: 0.013351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.412s | Train Loss: 1.038410 || Validation Loss: 0.012751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.400s | Train Loss: 0.999296 || Validation Loss: 0.012940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.418s | Train Loss: 0.978528 || Validation Loss: 0.013249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.416s | Train Loss: 0.975766 || Validation Loss: 0.012664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.422s | Train Loss: 0.940855 || Validation Loss: 0.011208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.414s | Train Loss: 0.925459 || Validation Loss: 0.011655 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.418s | Train Loss: 0.883420 || Validation Loss: 0.012416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.416s | Train Loss: 0.875563 || Validation Loss: 0.011943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.420s | Train Loss: 0.892344 || Validation Loss: 0.011620 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.419s | Train Loss: 0.888410 || Validation Loss: 0.012046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.414s | Train Loss: 0.838505 || Validation Loss: 0.011792 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.417s | Train Loss: 0.842582 || Validation Loss: 0.011950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.445s | Train Loss: 0.842156 || Validation Loss: 0.011491 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.419s | Train Loss: 0.832455 || Validation Loss: 0.012056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 0.821161 || Validation Loss: 0.010933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.445s | Train Loss: 0.799122 || Validation Loss: 0.011198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.419s | Train Loss: 0.792589 || Validation Loss: 0.012391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.419s | Train Loss: 0.768725 || Validation Loss: 0.012124 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.419s | Train Loss: 0.802381 || Validation Loss: 0.012297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.449s | Train Loss: 0.777491 || Validation Loss: 0.011560 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.421s | Train Loss: 0.729809 || Validation Loss: 0.010762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.422s | Train Loss: 0.748310 || Validation Loss: 0.012190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.451s | Train Loss: 0.727435 || Validation Loss: 0.012523 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.425s | Train Loss: 0.742709 || Validation Loss: 0.012241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.425s | Train Loss: 0.732465 || Validation Loss: 0.012146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.453s | Train Loss: 0.719621 || Validation Loss: 0.012576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.426s | Train Loss: 0.711705 || Validation Loss: 0.010802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.452s | Train Loss: 0.695706 || Validation Loss: 0.011524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.428s | Train Loss: 0.712329 || Validation Loss: 0.011536 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.456s | Train Loss: 0.686765 || Validation Loss: 0.011853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.430s | Train Loss: 0.687512 || Validation Loss: 0.011394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.457s | Train Loss: 0.678603 || Validation Loss: 0.012381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.426s | Train Loss: 0.688433 || Validation Loss: 0.011640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.457s | Train Loss: 0.691079 || Validation Loss: 0.011681 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.428s | Train Loss: 0.648761 || Validation Loss: 0.012355 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.456s | Train Loss: 0.671066 || Validation Loss: 0.012845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.460s | Train Loss: 0.652869 || Validation Loss: 0.012005 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.430s | Train Loss: 0.662302 || Validation Loss: 0.012343 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.461s | Train Loss: 0.634038 || Validation Loss: 0.012663 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.434s | Train Loss: 0.647989 || Validation Loss: 0.011467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.462s | Train Loss: 0.661804 || Validation Loss: 0.012316 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.430s | Train Loss: 0.632058 || Validation Loss: 0.012451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.462s | Train Loss: 0.633102 || Validation Loss: 0.012662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.431s | Train Loss: 0.650584 || Validation Loss: 0.012214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.461s | Train Loss: 0.655037 || Validation Loss: 0.012186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.459s | Train Loss: 0.643570 || Validation Loss: 0.011333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.464s | Train Loss: 0.652187 || Validation Loss: 0.012877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.434s | Train Loss: 0.628432 || Validation Loss: 0.012524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.463s | Train Loss: 0.660409 || Validation Loss: 0.012086 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.466s | Train Loss: 0.639036 || Validation Loss: 0.012118 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.464s | Train Loss: 0.679728 || Validation Loss: 0.012917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.465s | Train Loss: 0.651333 || Validation Loss: 0.011257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.466s | Train Loss: 0.657060 || Validation Loss: 0.012264 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.467s | Train Loss: 0.644169 || Validation Loss: 0.012468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.470s | Train Loss: 0.667310 || Validation Loss: 0.011985 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.463s | Train Loss: 0.631247 || Validation Loss: 0.011818 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.472s | Train Loss: 0.653025 || Validation Loss: 0.011863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.470s | Train Loss: 0.640562 || Validation Loss: 0.011884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.573s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.821%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009276\n",
      "INFO:root:Test AUC: 78.17%\n",
      "INFO:root:Test Time: 0.182s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 945\n",
      "len all outlier 16676 len chosen outlier 55\n",
      "final sizes 945 55 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 945, -1.0: 55})\n",
      "Resampled dataset shape Counter({1.0: 945, -1.0: 226})\n",
      "class weights 0.19299743808710504 0.807002561912895\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5906850575867441\n",
      "validation AUC 0.599640060233591\n",
      "validation AUC 0.6091860323543229\n",
      "validation AUC 0.6184822142964634\n",
      "validation AUC 0.6269859040242927\n",
      "validation AUC 0.6352136517268999\n",
      "validation AUC 0.6434624560694739\n",
      "validation AUC 0.6511985761943855\n",
      "validation AUC 0.6584845966714226\n",
      "validation AUC 0.6653790232443594\n",
      "validation AUC 0.67192282776041\n",
      "validation AUC 0.6778624099438852\n",
      "validation AUC 0.6839532492571907\n",
      "validation AUC 0.6899643133321683\n",
      "validation AUC 0.695183443936124\n",
      "validation AUC 0.7000798816656212\n",
      "validation AUC 0.7049510482343392\n",
      "validation AUC 0.7099083384294268\n",
      "validation AUC 0.7143094369293768\n",
      "validation AUC 0.718656411829351\n",
      "validation AUC 0.7229404110392348\n",
      "validation AUC 0.7264364438554521\n",
      "validation AUC 0.7297897390466626\n",
      "validation AUC 0.7332239216414154\n",
      "validation AUC 0.736507251530052\n",
      "validation AUC 0.7396279658359874\n",
      "validation AUC 0.7427238054228111\n",
      "validation AUC 0.7454821108491344\n",
      "validation AUC 0.7482747337561619\n",
      "validation AUC 0.7506239725775179\n",
      "validation AUC 0.7527127572055002\n",
      "validation AUC 0.7544857081980731\n",
      "validation AUC 0.7563769850552666\n",
      "validation AUC 0.7583371762455132\n",
      "validation AUC 0.7602438291858972\n",
      "validation AUC 0.7619151341313819\n",
      "validation AUC 0.7634022852706968\n",
      "validation AUC 0.765072725494424\n",
      "validation AUC 0.7664877694818886\n",
      "validation AUC 0.767984919465403\n",
      "validation AUC 0.7695419640688687\n",
      "validation AUC 0.7711856431497115\n",
      "validation AUC 0.772741889548478\n",
      "validation AUC 0.7742622672887411\n",
      "validation AUC 0.7755200995393188\n",
      "validation AUC 0.7765971836571101\n",
      "validation AUC 0.7776087245263613\n",
      "validation AUC 0.7787005860738183\n",
      "validation AUC 0.7798171148071654\n",
      "validation AUC 0.7799330247722298\n",
      "validation AUC 0.7800541443532977\n",
      "validation AUC 0.7801919357698514\n",
      "validation AUC 0.7803269015417696\n",
      "validation AUC 0.7804507403401799\n",
      "validation AUC 0.780570460402342\n",
      "validation AUC 0.7806879827408987\n",
      "validation AUC 0.7807934601705434\n",
      "validation AUC 0.7808910140882066\n",
      "validation AUC 0.7809952569612496\n",
      "validation AUC 0.7810924649902098\n",
      "validation AUC 0.7811912295183335\n",
      "validation AUC 0.781315262546554\n",
      "validation AUC 0.7814561722827994\n",
      "validation AUC 0.7815816660256196\n",
      "validation AUC 0.7817133298907653\n",
      "validation AUC 0.7818201988572688\n",
      "validation AUC 0.781898287222999\n",
      "validation AUC 0.7819870050146414\n",
      "validation AUC 0.7820915272593291\n",
      "validation AUC 0.7822083791059384\n",
      "validation AUC 0.7822083791059384\n",
      "train auc 0.8205265278114362\n",
      "Test AUC 0.7817022072299792\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 3.78967314e-05 ... 9.99583136e-01\n",
      " 9.99583136e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.22170067e+01 2.12170067e+01 2.05559559e+01 ... 4.23675403e-02\n",
      " 4.23363186e-02 1.36642549e-02]\n",
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.035s | Train Loss: 0.208058 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.206541 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.019s | Train Loss: 0.203825 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.202029 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.199424 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.197650 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.019s | Train Loss: 0.195166 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.193562 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.191150 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.188894 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.186877 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.184402 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.182633 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.180402 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.178736 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.176861 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.175386 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.172873 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.171565 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.169316 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.168022 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.165245 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.163854 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.162199 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.160511 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.158288 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.156851 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.155885 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.153263 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.151997 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.149892 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.149027 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.146804 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.145146 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.143912 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.142423 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.141035 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.137924 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.137909 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.135832 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.134621 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.132771 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.131953 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.130286 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.129761 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.128248 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.126374 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.124825 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.123991 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.122890 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.121887 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.120064 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.119906 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.117945 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.116434 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.115854 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.114892 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.112197 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.111704 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.110428 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.109955 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.108837 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.018s | Train Loss: 0.107090 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.106756 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.105423 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.018s | Train Loss: 0.105541 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.018s | Train Loss: 0.103243 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.103798 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.101373 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.018s | Train Loss: 0.101217 |\n",
      "INFO:root:Pretraining Time: 1.257s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.109385\n",
      "INFO:root:Test AUC: 49.25%\n",
      "INFO:root:Test AUC: 49.25%\n",
      "INFO:root:Test Time: 0.239s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.423s | Train Loss: 1.244372 || Validation Loss: 0.014636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.403s | Train Loss: 1.215397 || Validation Loss: 0.013795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.431s | Train Loss: 1.143352 || Validation Loss: 0.014397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.430s | Train Loss: 1.044406 || Validation Loss: 0.015188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.431s | Train Loss: 1.060721 || Validation Loss: 0.014286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.431s | Train Loss: 0.978578 || Validation Loss: 0.014122 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.433s | Train Loss: 0.973720 || Validation Loss: 0.013870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.435s | Train Loss: 0.924245 || Validation Loss: 0.014163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.435s | Train Loss: 0.924693 || Validation Loss: 0.013347 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.406s | Train Loss: 0.905777 || Validation Loss: 0.014128 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.459s | Train Loss: 0.905047 || Validation Loss: 0.014211 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.434s | Train Loss: 0.865683 || Validation Loss: 0.014353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.434s | Train Loss: 0.843245 || Validation Loss: 0.013809 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.438s | Train Loss: 0.814510 || Validation Loss: 0.013308 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.437s | Train Loss: 0.786516 || Validation Loss: 0.013820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.438s | Train Loss: 0.809674 || Validation Loss: 0.014738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.435s | Train Loss: 0.802095 || Validation Loss: 0.012878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.432s | Train Loss: 0.770838 || Validation Loss: 0.013380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.436s | Train Loss: 0.763690 || Validation Loss: 0.013930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.439s | Train Loss: 0.705906 || Validation Loss: 0.013286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.439s | Train Loss: 0.701745 || Validation Loss: 0.013262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.437s | Train Loss: 0.702676 || Validation Loss: 0.013660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.434s | Train Loss: 0.663507 || Validation Loss: 0.014150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.435s | Train Loss: 0.655417 || Validation Loss: 0.013281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.470s | Train Loss: 0.683420 || Validation Loss: 0.014022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.443s | Train Loss: 0.657910 || Validation Loss: 0.013923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.433s | Train Loss: 0.644655 || Validation Loss: 0.013857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.468s | Train Loss: 0.637855 || Validation Loss: 0.014782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.435s | Train Loss: 0.626091 || Validation Loss: 0.012964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.439s | Train Loss: 0.620747 || Validation Loss: 0.013936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.437s | Train Loss: 0.627367 || Validation Loss: 0.014218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.470s | Train Loss: 0.614488 || Validation Loss: 0.012992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.443s | Train Loss: 0.622137 || Validation Loss: 0.012623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.440s | Train Loss: 0.622349 || Validation Loss: 0.014729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.467s | Train Loss: 0.584714 || Validation Loss: 0.012982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.441s | Train Loss: 0.624165 || Validation Loss: 0.013737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.443s | Train Loss: 0.592174 || Validation Loss: 0.013662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.475s | Train Loss: 0.562285 || Validation Loss: 0.014136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.441s | Train Loss: 0.568453 || Validation Loss: 0.012791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.470s | Train Loss: 0.556307 || Validation Loss: 0.013802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.450s | Train Loss: 0.531535 || Validation Loss: 0.012701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.475s | Train Loss: 0.524710 || Validation Loss: 0.014874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.445s | Train Loss: 0.542289 || Validation Loss: 0.014215 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.474s | Train Loss: 0.532075 || Validation Loss: 0.013654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.444s | Train Loss: 0.532600 || Validation Loss: 0.012868 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.482s | Train Loss: 0.521072 || Validation Loss: 0.015677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.447s | Train Loss: 0.534154 || Validation Loss: 0.013363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.475s | Train Loss: 0.528629 || Validation Loss: 0.013220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.481s | Train Loss: 0.534180 || Validation Loss: 0.015202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.446s | Train Loss: 0.514972 || Validation Loss: 0.014196 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.487s | Train Loss: 0.511408 || Validation Loss: 0.013547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.450s | Train Loss: 0.488971 || Validation Loss: 0.014807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.476s | Train Loss: 0.507871 || Validation Loss: 0.014149 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.456s | Train Loss: 0.521143 || Validation Loss: 0.013543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.488s | Train Loss: 0.488177 || Validation Loss: 0.014625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.437s | Train Loss: 0.501309 || Validation Loss: 0.014341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.482s | Train Loss: 0.508418 || Validation Loss: 0.013069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.482s | Train Loss: 0.530071 || Validation Loss: 0.014982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.487s | Train Loss: 0.486220 || Validation Loss: 0.013969 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.453s | Train Loss: 0.493140 || Validation Loss: 0.014293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.482s | Train Loss: 0.517576 || Validation Loss: 0.014561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.490s | Train Loss: 0.510773 || Validation Loss: 0.014953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.486s | Train Loss: 0.503882 || Validation Loss: 0.014520 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.483s | Train Loss: 0.501775 || Validation Loss: 0.013901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.484s | Train Loss: 0.504070 || Validation Loss: 0.013710 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.486s | Train Loss: 0.517659 || Validation Loss: 0.014184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.487s | Train Loss: 0.475250 || Validation Loss: 0.015394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.489s | Train Loss: 0.519410 || Validation Loss: 0.014843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.485s | Train Loss: 0.518072 || Validation Loss: 0.014235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.490s | Train Loss: 0.495018 || Validation Loss: 0.014650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.916s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.903%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.011692\n",
      "INFO:root:Test AUC: 79.93%\n",
      "INFO:root:Test Time: 0.199s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 945\n",
      "len all outlier 16676 len chosen outlier 55\n",
      "final sizes 945 55 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 945, -1.0: 55})\n",
      "Resampled dataset shape Counter({1.0: 945, -1.0: 226})\n",
      "class weights 0.19299743808710504 0.807002561912895\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6648618105453696\n",
      "validation AUC 0.6745396806712923\n",
      "validation AUC 0.6837257396324383\n",
      "validation AUC 0.6920705280901004\n",
      "validation AUC 0.6991931161549607\n",
      "validation AUC 0.7057617102482885\n",
      "validation AUC 0.7116403122534213\n",
      "validation AUC 0.7167209729668289\n",
      "validation AUC 0.7213890416711809\n",
      "validation AUC 0.7257946047960822\n",
      "validation AUC 0.7299074210261592\n",
      "validation AUC 0.7334943933037649\n",
      "validation AUC 0.7371964161460421\n",
      "validation AUC 0.7407117123023352\n",
      "validation AUC 0.7435648790358027\n",
      "validation AUC 0.7464201689937705\n",
      "validation AUC 0.7490848689550096\n",
      "validation AUC 0.751656625961065\n",
      "validation AUC 0.7540449369024507\n",
      "validation AUC 0.7561654794347349\n",
      "validation AUC 0.7582291977744775\n",
      "validation AUC 0.7603860932094492\n",
      "validation AUC 0.7625848624629448\n",
      "validation AUC 0.7644508841234532\n",
      "validation AUC 0.7660289374746039\n",
      "validation AUC 0.7678441055137214\n",
      "validation AUC 0.7696965364088835\n",
      "validation AUC 0.7712076682780468\n",
      "validation AUC 0.7724498025028721\n",
      "validation AUC 0.773522624207569\n",
      "validation AUC 0.7746100557189451\n",
      "validation AUC 0.7756926767166669\n",
      "validation AUC 0.7767396685172954\n",
      "validation AUC 0.7776487491706653\n",
      "validation AUC 0.7786663351101726\n",
      "validation AUC 0.7793711285741743\n",
      "validation AUC 0.7801550746768389\n",
      "validation AUC 0.7808414083268288\n",
      "validation AUC 0.7816529430445827\n",
      "validation AUC 0.7827225719304826\n",
      "validation AUC 0.7837216315389195\n",
      "validation AUC 0.784758073734106\n",
      "validation AUC 0.7855697388252938\n",
      "validation AUC 0.7863047310337518\n",
      "validation AUC 0.7873259009123587\n",
      "validation AUC 0.788401282193458\n",
      "validation AUC 0.7892325272459192\n",
      "validation AUC 0.7897940243416247\n",
      "validation AUC 0.7904591842816253\n",
      "validation AUC 0.7905168678745598\n",
      "validation AUC 0.7905951052385004\n",
      "validation AUC 0.7906626227133298\n",
      "validation AUC 0.7907411234848212\n",
      "validation AUC 0.7908245917502243\n",
      "validation AUC 0.790925389039649\n",
      "validation AUC 0.7910094320124359\n",
      "validation AUC 0.7910775774082952\n",
      "validation AUC 0.791158331777721\n",
      "validation AUC 0.7912241384338117\n",
      "validation AUC 0.7912985284511018\n",
      "validation AUC 0.7913913037832987\n",
      "validation AUC 0.7914782655246024\n",
      "validation AUC 0.7915433857246517\n",
      "validation AUC 0.7916117386537331\n",
      "validation AUC 0.7916799000136865\n",
      "validation AUC 0.7917443364183767\n",
      "validation AUC 0.7918236673227554\n",
      "validation AUC 0.7919066220764688\n",
      "validation AUC 0.7919938605287351\n",
      "validation AUC 0.7920811628373773\n",
      "validation AUC 0.7920811628373773\n",
      "train auc 0.9034988939308424\n",
      "Test AUC 0.7993405682281741\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.99242065e-01\n",
      " 9.99242065e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [23.75415421 22.75415421 22.51363754 ...  0.06141547  0.06137002\n",
      "  0.04583392]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.035s | Train Loss: 0.221219 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.019s | Train Loss: 0.218924 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.216833 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.214080 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.211624 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.208787 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.206804 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.019s | Train Loss: 0.204631 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.202240 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.200360 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.198637 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.195737 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.193613 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.191427 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.190063 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.187786 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.185727 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.183551 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.181668 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.180154 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.177621 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.176244 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.173783 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.172421 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.170806 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.168824 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.167155 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.165464 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.163341 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.161887 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.160711 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.159081 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.157457 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.155421 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.154168 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.152708 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.151617 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.149198 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.147421 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.146227 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.144653 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.142985 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.142075 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.140534 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.139431 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.137728 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.136000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.134768 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.133200 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.131837 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.130658 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.128694 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.128448 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.126395 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.125600 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.123579 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.122269 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.121390 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.120471 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.119243 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.117871 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.117348 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.115132 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.114787 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.018s | Train Loss: 0.113830 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.112835 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.112358 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.110917 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.016s | Train Loss: 0.109338 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.019s | Train Loss: 0.108515 |\n",
      "INFO:root:Pretraining Time: 1.230s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.109496\n",
      "INFO:root:Test AUC: 55.63%\n",
      "INFO:root:Test AUC: 55.63%\n",
      "INFO:root:Test Time: 0.228s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.412s | Train Loss: 1.624483 || Validation Loss: 0.014318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.385s | Train Loss: 1.380457 || Validation Loss: 0.014825 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.413s | Train Loss: 1.403975 || Validation Loss: 0.013793 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.420s | Train Loss: 1.339873 || Validation Loss: 0.013184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.428s | Train Loss: 1.343011 || Validation Loss: 0.014199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.422s | Train Loss: 1.288171 || Validation Loss: 0.014333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.422s | Train Loss: 1.279081 || Validation Loss: 0.014419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.423s | Train Loss: 1.211491 || Validation Loss: 0.014587 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.419s | Train Loss: 1.138775 || Validation Loss: 0.013275 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.389s | Train Loss: 1.151255 || Validation Loss: 0.013321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.451s | Train Loss: 1.191672 || Validation Loss: 0.013196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.422s | Train Loss: 1.170805 || Validation Loss: 0.013206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.417s | Train Loss: 1.136838 || Validation Loss: 0.013293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.425s | Train Loss: 1.052819 || Validation Loss: 0.012449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.426s | Train Loss: 1.069528 || Validation Loss: 0.012068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.425s | Train Loss: 1.063904 || Validation Loss: 0.012987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.425s | Train Loss: 1.021714 || Validation Loss: 0.011994 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.420s | Train Loss: 0.949435 || Validation Loss: 0.011314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.424s | Train Loss: 0.990477 || Validation Loss: 0.012199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.420s | Train Loss: 0.957928 || Validation Loss: 0.012715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.421s | Train Loss: 0.940946 || Validation Loss: 0.011893 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.426s | Train Loss: 0.935112 || Validation Loss: 0.011877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.422s | Train Loss: 0.937178 || Validation Loss: 0.011429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.421s | Train Loss: 0.901964 || Validation Loss: 0.011734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.454s | Train Loss: 0.891298 || Validation Loss: 0.012913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.426s | Train Loss: 0.863786 || Validation Loss: 0.010750 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.427s | Train Loss: 0.867796 || Validation Loss: 0.011606 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.460s | Train Loss: 0.813436 || Validation Loss: 0.011601 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.429s | Train Loss: 0.850661 || Validation Loss: 0.011326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.429s | Train Loss: 0.857895 || Validation Loss: 0.011443 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.413s | Train Loss: 0.802905 || Validation Loss: 0.011515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.456s | Train Loss: 0.811533 || Validation Loss: 0.012362 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.428s | Train Loss: 0.803253 || Validation Loss: 0.011952 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.431s | Train Loss: 0.801016 || Validation Loss: 0.011494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.461s | Train Loss: 0.796867 || Validation Loss: 0.010698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.427s | Train Loss: 0.772290 || Validation Loss: 0.009898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.429s | Train Loss: 0.792476 || Validation Loss: 0.011098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.465s | Train Loss: 0.767477 || Validation Loss: 0.011459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.433s | Train Loss: 0.761249 || Validation Loss: 0.011080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.459s | Train Loss: 0.763041 || Validation Loss: 0.010859 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.432s | Train Loss: 0.752475 || Validation Loss: 0.010725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.460s | Train Loss: 0.734901 || Validation Loss: 0.010313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.432s | Train Loss: 0.731530 || Validation Loss: 0.010785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.465s | Train Loss: 0.716742 || Validation Loss: 0.010488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.436s | Train Loss: 0.693964 || Validation Loss: 0.010610 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.466s | Train Loss: 0.711304 || Validation Loss: 0.010888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.433s | Train Loss: 0.697616 || Validation Loss: 0.011189 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.462s | Train Loss: 0.700561 || Validation Loss: 0.011078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.465s | Train Loss: 0.686224 || Validation Loss: 0.010226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.436s | Train Loss: 0.703201 || Validation Loss: 0.010013 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.466s | Train Loss: 0.718307 || Validation Loss: 0.010990 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.437s | Train Loss: 0.676357 || Validation Loss: 0.010310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.467s | Train Loss: 0.679185 || Validation Loss: 0.010341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.436s | Train Loss: 0.678723 || Validation Loss: 0.010051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.471s | Train Loss: 0.685468 || Validation Loss: 0.010569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.437s | Train Loss: 0.712825 || Validation Loss: 0.010147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.467s | Train Loss: 0.688740 || Validation Loss: 0.010972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.471s | Train Loss: 0.668829 || Validation Loss: 0.010408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.471s | Train Loss: 0.684821 || Validation Loss: 0.011033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.439s | Train Loss: 0.684828 || Validation Loss: 0.010806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.471s | Train Loss: 0.692294 || Validation Loss: 0.011104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.472s | Train Loss: 0.671922 || Validation Loss: 0.011082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.472s | Train Loss: 0.679837 || Validation Loss: 0.011194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.472s | Train Loss: 0.695649 || Validation Loss: 0.010102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.479s | Train Loss: 0.669144 || Validation Loss: 0.010906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.479s | Train Loss: 0.657961 || Validation Loss: 0.010753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.479s | Train Loss: 0.682028 || Validation Loss: 0.010502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.475s | Train Loss: 0.660561 || Validation Loss: 0.010347 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.474s | Train Loss: 0.697447 || Validation Loss: 0.011066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.474s | Train Loss: 0.704019 || Validation Loss: 0.010421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.030s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.786%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008250\n",
      "INFO:root:Test AUC: 74.24%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 952\n",
      "len all outlier 16676 len chosen outlier 48\n",
      "final sizes 952 48 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 952, -1.0: 48})\n",
      "Resampled dataset shape Counter({1.0: 952, -1.0: 228})\n",
      "class weights 0.19322033898305085 0.8067796610169492\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5719099713433871\n",
      "validation AUC 0.5757253232888673\n",
      "validation AUC 0.5798333902009837\n",
      "validation AUC 0.5841596277428708\n",
      "validation AUC 0.5886690053709598\n",
      "validation AUC 0.5934799952873995\n",
      "validation AUC 0.5980650746491677\n",
      "validation AUC 0.602383080039923\n",
      "validation AUC 0.6064377896285751\n",
      "validation AUC 0.6104035659103162\n",
      "validation AUC 0.6143623924898091\n",
      "validation AUC 0.618922376295832\n",
      "validation AUC 0.623203289114212\n",
      "validation AUC 0.627178133001337\n",
      "validation AUC 0.6309919204656066\n",
      "validation AUC 0.6347927557282894\n",
      "validation AUC 0.6386673530872322\n",
      "validation AUC 0.6425236555944678\n",
      "validation AUC 0.646464676887802\n",
      "validation AUC 0.6504007280265421\n",
      "validation AUC 0.654324933807545\n",
      "validation AUC 0.6583617024152397\n",
      "validation AUC 0.6624189343307416\n",
      "validation AUC 0.6663528968338518\n",
      "validation AUC 0.6701592264055479\n",
      "validation AUC 0.6733308581892503\n",
      "validation AUC 0.6764289034817264\n",
      "validation AUC 0.6792346462133275\n",
      "validation AUC 0.6822797598915122\n",
      "validation AUC 0.6850161386347464\n",
      "validation AUC 0.6876887567866025\n",
      "validation AUC 0.6907308984826235\n",
      "validation AUC 0.6933407138887404\n",
      "validation AUC 0.6960162029168315\n",
      "validation AUC 0.6991094323742886\n",
      "validation AUC 0.702312164873757\n",
      "validation AUC 0.7048459060506684\n",
      "validation AUC 0.707074836799067\n",
      "validation AUC 0.7090439332930754\n",
      "validation AUC 0.711103027366927\n",
      "validation AUC 0.7136581710725083\n",
      "validation AUC 0.7159267167201322\n",
      "validation AUC 0.7178962176378547\n",
      "validation AUC 0.7196069405920721\n",
      "validation AUC 0.7212914638073769\n",
      "validation AUC 0.723151791607697\n",
      "validation AUC 0.7253658358384544\n",
      "validation AUC 0.7272354281346509\n",
      "validation AUC 0.7293001069807152\n",
      "validation AUC 0.7294657238130855\n",
      "validation AUC 0.7295757244026928\n",
      "validation AUC 0.729734939633375\n",
      "validation AUC 0.7299131734213586\n",
      "validation AUC 0.7301381287910733\n",
      "validation AUC 0.7303565521856654\n",
      "validation AUC 0.7305523970299974\n",
      "validation AUC 0.7307335628899098\n",
      "validation AUC 0.7309497167224736\n",
      "validation AUC 0.7311765664980077\n",
      "validation AUC 0.7313657756006064\n",
      "validation AUC 0.7315802186144316\n",
      "validation AUC 0.7317825209347807\n",
      "validation AUC 0.7319716981091915\n",
      "validation AUC 0.7321408004354153\n",
      "validation AUC 0.7323124064637127\n",
      "validation AUC 0.7324981832861045\n",
      "validation AUC 0.7326697520648492\n",
      "validation AUC 0.7328744303745198\n",
      "validation AUC 0.7331100790265224\n",
      "validation AUC 0.733304779777452\n",
      "validation AUC 0.733304779777452\n",
      "train auc 0.7862015759887369\n",
      "Test AUC 0.742406339002813\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.68450971e-05 ... 9.99867361e-01\n",
      " 9.99867361e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [10.12211037  9.12211037  8.19059753 ...  0.04756184  0.0472561\n",
      "  0.04066684]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.035s | Train Loss: 0.209954 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.207928 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.206124 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.203850 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.201804 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.199681 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.196691 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.195042 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.192658 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.190884 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.188563 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.186304 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.184837 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.181564 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.179781 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.177945 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.176242 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.173856 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.171659 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.169800 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.168262 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.166135 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.165117 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.162761 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.160382 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.158969 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.157212 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.155822 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.153793 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.152659 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.150510 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.148890 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.147438 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.146872 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.144208 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.143202 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.141150 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.139402 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.138097 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.136523 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.135414 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.133755 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.132859 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.131689 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.130444 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.018s | Train Loss: 0.129102 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.128177 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.125943 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.124874 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.124282 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.122469 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.121077 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.119879 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.018s | Train Loss: 0.119082 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.117919 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.117430 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.116116 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.114942 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.112919 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.112150 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.018s | Train Loss: 0.111796 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.109985 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.109549 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.108760 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.107005 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.107363 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.105363 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.104352 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.103017 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.018s | Train Loss: 0.102824 |\n",
      "INFO:root:Pretraining Time: 1.243s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.104469\n",
      "INFO:root:Test AUC: 54.69%\n",
      "INFO:root:Test AUC: 54.69%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.418s | Train Loss: 1.407656 || Validation Loss: 0.013625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.382s | Train Loss: 1.349110 || Validation Loss: 0.014133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.415s | Train Loss: 1.331024 || Validation Loss: 0.013632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.416s | Train Loss: 1.317676 || Validation Loss: 0.013543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.406s | Train Loss: 1.272500 || Validation Loss: 0.013982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.410s | Train Loss: 1.196369 || Validation Loss: 0.012649 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.407s | Train Loss: 1.248877 || Validation Loss: 0.013270 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.416s | Train Loss: 1.161517 || Validation Loss: 0.012220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.420s | Train Loss: 1.138302 || Validation Loss: 0.013846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.387s | Train Loss: 1.146448 || Validation Loss: 0.012806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.448s | Train Loss: 1.120799 || Validation Loss: 0.012525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.422s | Train Loss: 1.078125 || Validation Loss: 0.013062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.419s | Train Loss: 1.049646 || Validation Loss: 0.012066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.419s | Train Loss: 1.039777 || Validation Loss: 0.012698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.421s | Train Loss: 1.031276 || Validation Loss: 0.011173 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.419s | Train Loss: 1.022111 || Validation Loss: 0.013050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.418s | Train Loss: 1.008187 || Validation Loss: 0.012735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.422s | Train Loss: 0.974279 || Validation Loss: 0.011504 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.422s | Train Loss: 0.945538 || Validation Loss: 0.011919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.421s | Train Loss: 0.922764 || Validation Loss: 0.012360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.422s | Train Loss: 0.965779 || Validation Loss: 0.012287 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.419s | Train Loss: 0.933741 || Validation Loss: 0.012155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.423s | Train Loss: 0.898808 || Validation Loss: 0.011892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.420s | Train Loss: 0.890323 || Validation Loss: 0.011197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.452s | Train Loss: 0.887444 || Validation Loss: 0.012697 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.424s | Train Loss: 0.873514 || Validation Loss: 0.011241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.423s | Train Loss: 0.883788 || Validation Loss: 0.011671 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.453s | Train Loss: 0.857763 || Validation Loss: 0.011165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.422s | Train Loss: 0.835262 || Validation Loss: 0.010252 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.423s | Train Loss: 0.845001 || Validation Loss: 0.011404 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.423s | Train Loss: 0.839087 || Validation Loss: 0.011278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.454s | Train Loss: 0.842876 || Validation Loss: 0.010485 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.425s | Train Loss: 0.852294 || Validation Loss: 0.012755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.430s | Train Loss: 0.802981 || Validation Loss: 0.010766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.457s | Train Loss: 0.802838 || Validation Loss: 0.011234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.428s | Train Loss: 0.803195 || Validation Loss: 0.011265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.428s | Train Loss: 0.815084 || Validation Loss: 0.010953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.458s | Train Loss: 0.780743 || Validation Loss: 0.010756 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.432s | Train Loss: 0.791503 || Validation Loss: 0.010180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.459s | Train Loss: 0.765463 || Validation Loss: 0.010228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.428s | Train Loss: 0.791174 || Validation Loss: 0.011154 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.458s | Train Loss: 0.783360 || Validation Loss: 0.010592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.429s | Train Loss: 0.749688 || Validation Loss: 0.009926 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.459s | Train Loss: 0.747640 || Validation Loss: 0.011142 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.430s | Train Loss: 0.738682 || Validation Loss: 0.009870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.465s | Train Loss: 0.745720 || Validation Loss: 0.010851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.434s | Train Loss: 0.733043 || Validation Loss: 0.010215 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.463s | Train Loss: 0.746521 || Validation Loss: 0.010198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.462s | Train Loss: 0.709301 || Validation Loss: 0.009668 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.431s | Train Loss: 0.736626 || Validation Loss: 0.011057 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.469s | Train Loss: 0.703260 || Validation Loss: 0.011401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.435s | Train Loss: 0.738265 || Validation Loss: 0.010861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.464s | Train Loss: 0.739345 || Validation Loss: 0.011481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.437s | Train Loss: 0.690340 || Validation Loss: 0.011027 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.466s | Train Loss: 0.732559 || Validation Loss: 0.011171 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.436s | Train Loss: 0.738751 || Validation Loss: 0.011138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.468s | Train Loss: 0.730735 || Validation Loss: 0.011772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.470s | Train Loss: 0.717493 || Validation Loss: 0.010701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.470s | Train Loss: 0.730128 || Validation Loss: 0.010655 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.443s | Train Loss: 0.734604 || Validation Loss: 0.011176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.470s | Train Loss: 0.729688 || Validation Loss: 0.011763 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.469s | Train Loss: 0.720832 || Validation Loss: 0.010351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.468s | Train Loss: 0.724502 || Validation Loss: 0.011501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.466s | Train Loss: 0.724905 || Validation Loss: 0.010193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.467s | Train Loss: 0.712667 || Validation Loss: 0.010482 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.473s | Train Loss: 0.716282 || Validation Loss: 0.010791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.472s | Train Loss: 0.711996 || Validation Loss: 0.010795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.473s | Train Loss: 0.726890 || Validation Loss: 0.011186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.470s | Train Loss: 0.715841 || Validation Loss: 0.011663 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.467s | Train Loss: 0.712238 || Validation Loss: 0.010966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.821s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.770%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008651\n",
      "INFO:root:Test AUC: 67.30%\n",
      "INFO:root:Test Time: 0.182s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 50\n",
      "i index 1\n",
      "n cluster 50\n",
      "i index 2\n",
      "n cluster 50\n",
      "i index 3\n",
      "n cluster 50\n",
      "i index 4\n",
      "n cluster 50\n",
      "i index 5\n",
      "n cluster 50\n",
      "i index 6\n",
      "n cluster 50\n",
      "i index 7\n",
      "n cluster 50\n",
      "i index 8\n",
      "n cluster 50\n",
      "i index 9\n",
      "n cluster 50\n",
      "i index 10\n",
      "n cluster 50\n",
      "i index 11\n",
      "n cluster 50\n",
      "i index 12\n",
      "n cluster 50\n",
      "i index 13\n",
      "n cluster 50\n",
      "i index 14\n",
      "n cluster 50\n",
      "i index 15\n",
      "n cluster 50\n",
      "i index 16\n",
      "n cluster 50\n",
      "i index 17\n",
      "n cluster 50\n",
      "i index 18\n",
      "n cluster 50\n",
      "i index 19\n",
      "n cluster 50\n",
      "size of chosen indexes of normal 944\n",
      "len all outlier 16676 len chosen outlier 56\n",
      "final sizes 944 56 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 944, -1.0: 56})\n",
      "Resampled dataset shape Counter({1.0: 944, -1.0: 226})\n",
      "class weights 0.19316239316239317 0.8068376068376069\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4803088408301074\n",
      "validation AUC 0.4859834908918458\n",
      "validation AUC 0.4922447282836705\n",
      "validation AUC 0.4985261682364337\n",
      "validation AUC 0.5054408133344033\n",
      "validation AUC 0.5123336115697537\n",
      "validation AUC 0.5191590998634963\n",
      "validation AUC 0.5258597968345543\n",
      "validation AUC 0.53272379052299\n",
      "validation AUC 0.5393407611424587\n",
      "validation AUC 0.5451330639161881\n",
      "validation AUC 0.5510130867256856\n",
      "validation AUC 0.5571757139089617\n",
      "validation AUC 0.5631782717825273\n",
      "validation AUC 0.5686567949676067\n",
      "validation AUC 0.5743211747955478\n",
      "validation AUC 0.5795369609218136\n",
      "validation AUC 0.5847467977623875\n",
      "validation AUC 0.589699045964458\n",
      "validation AUC 0.5944914909250512\n",
      "validation AUC 0.5985258276690955\n",
      "validation AUC 0.602878770479538\n",
      "validation AUC 0.607570340990919\n",
      "validation AUC 0.6117703797091681\n",
      "validation AUC 0.6156328789455525\n",
      "validation AUC 0.6193322570695926\n",
      "validation AUC 0.6230026177921317\n",
      "validation AUC 0.6265552193753867\n",
      "validation AUC 0.629971564755793\n",
      "validation AUC 0.6338604739249939\n",
      "validation AUC 0.6373955841826054\n",
      "validation AUC 0.6406300000872703\n",
      "validation AUC 0.6437883524054591\n",
      "validation AUC 0.646460318690144\n",
      "validation AUC 0.6489347293117581\n",
      "validation AUC 0.6514319872125478\n",
      "validation AUC 0.653867860511281\n",
      "validation AUC 0.6567130079482031\n",
      "validation AUC 0.6598234307348826\n",
      "validation AUC 0.6625968381941162\n",
      "validation AUC 0.6650684817061061\n",
      "validation AUC 0.6673463184564297\n",
      "validation AUC 0.6694745290751916\n",
      "validation AUC 0.671442904524288\n",
      "validation AUC 0.672964452964777\n",
      "validation AUC 0.674812701267315\n",
      "validation AUC 0.676422669503025\n",
      "validation AUC 0.6781596081602489\n",
      "validation AUC 0.6800101446495913\n",
      "validation AUC 0.6802165976343767\n",
      "validation AUC 0.6804520254497458\n",
      "validation AUC 0.6806782792324378\n",
      "validation AUC 0.6808909475711482\n",
      "validation AUC 0.6811030145956519\n",
      "validation AUC 0.6812926813356115\n",
      "validation AUC 0.6814632151089294\n",
      "validation AUC 0.6816298430005855\n",
      "validation AUC 0.6818558706252795\n",
      "validation AUC 0.6820639679117454\n",
      "validation AUC 0.682290402620836\n",
      "validation AUC 0.6825224327448683\n",
      "validation AUC 0.6827237745588961\n",
      "validation AUC 0.6829037856826767\n",
      "validation AUC 0.6830616891969699\n",
      "validation AUC 0.683290002347786\n",
      "validation AUC 0.683488138039605\n",
      "validation AUC 0.6836727415010889\n",
      "validation AUC 0.6839058172732774\n",
      "validation AUC 0.6841150320463223\n",
      "validation AUC 0.6842766445517846\n",
      "validation AUC 0.6842766445517846\n",
      "train auc 0.7704687168549793\n",
      "Test AUC 0.6730386743994955\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 9.47418285e-05 ... 9.98863098e-01\n",
      " 9.98863098e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [12.18774509 11.18774509 10.65212536 ...  0.09666063  0.0956937\n",
      "  0.04671801]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {4..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1k_points_by_20_k_means\" --iteration_num $i --experiment_method 2 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.00 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 1000 --k_means_chosen_k 20 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4ed1692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.033s | Train Loss: 0.213569 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.019s | Train Loss: 0.211325 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.016s | Train Loss: 0.209387 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.019s | Train Loss: 0.206351 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.204050 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.202080 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.199809 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.197886 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.195420 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.193216 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.191005 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.188918 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.187008 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.185401 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.183039 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.180769 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.178893 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.176498 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.175040 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.173015 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.170913 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.169036 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.167882 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.165250 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.163409 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.161872 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.159540 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.157608 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.154941 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.155576 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.152858 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.151059 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.150176 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.147390 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.145634 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.145042 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.142666 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.141020 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.140303 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.138312 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.137414 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.135909 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.134092 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.133140 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.131648 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.130846 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.128394 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.127109 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.016s | Train Loss: 0.125835 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.124288 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.123033 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.121599 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.120101 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.119697 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.118897 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.117070 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.116352 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.116016 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.113527 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.016s | Train Loss: 0.112860 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.111870 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.016s | Train Loss: 0.110989 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.110082 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.108767 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.107349 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.106027 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.105580 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.103505 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.103687 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.101686 |\n",
      "INFO:root:Pretraining Time: 1.225s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.108421\n",
      "INFO:root:Test AUC: 54.87%\n",
      "INFO:root:Test AUC: 54.87%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.408s | Train Loss: 1.471161 || Validation Loss: 0.014767 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.382s | Train Loss: 1.322858 || Validation Loss: 0.015771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.414s | Train Loss: 1.291646 || Validation Loss: 0.015547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.411s | Train Loss: 1.309700 || Validation Loss: 0.014722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.409s | Train Loss: 1.218545 || Validation Loss: 0.014845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.410s | Train Loss: 1.219725 || Validation Loss: 0.015453 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.415s | Train Loss: 1.204304 || Validation Loss: 0.013923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.424s | Train Loss: 1.161950 || Validation Loss: 0.013719 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.402s | Train Loss: 1.117416 || Validation Loss: 0.013820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.383s | Train Loss: 1.087565 || Validation Loss: 0.013417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.451s | Train Loss: 1.107330 || Validation Loss: 0.013518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.414s | Train Loss: 1.036107 || Validation Loss: 0.013481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.420s | Train Loss: 1.063895 || Validation Loss: 0.013068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.423s | Train Loss: 1.003874 || Validation Loss: 0.012592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.457s | Train Loss: 0.964198 || Validation Loss: 0.012068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.418s | Train Loss: 0.977388 || Validation Loss: 0.013103 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.414s | Train Loss: 0.989863 || Validation Loss: 0.013209 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.410s | Train Loss: 0.946259 || Validation Loss: 0.012961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.419s | Train Loss: 0.899889 || Validation Loss: 0.013681 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.420s | Train Loss: 0.879526 || Validation Loss: 0.013770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.420s | Train Loss: 0.900141 || Validation Loss: 0.012864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.419s | Train Loss: 0.891733 || Validation Loss: 0.011532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.422s | Train Loss: 0.882248 || Validation Loss: 0.013114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.420s | Train Loss: 0.838036 || Validation Loss: 0.012559 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.447s | Train Loss: 0.815417 || Validation Loss: 0.012132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.424s | Train Loss: 0.847220 || Validation Loss: 0.013209 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.421s | Train Loss: 0.800979 || Validation Loss: 0.011935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.450s | Train Loss: 0.786307 || Validation Loss: 0.013778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.422s | Train Loss: 0.789439 || Validation Loss: 0.012770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.298s | Train Loss: 0.808031 || Validation Loss: 0.012690 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.482s | Train Loss: 0.785315 || Validation Loss: 0.011823 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.439s | Train Loss: 0.781736 || Validation Loss: 0.012545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.416s | Train Loss: 0.748073 || Validation Loss: 0.012982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.413s | Train Loss: 0.740999 || Validation Loss: 0.013302 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.444s | Train Loss: 0.733251 || Validation Loss: 0.013337 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.400s | Train Loss: 0.773508 || Validation Loss: 0.012936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.426s | Train Loss: 0.731570 || Validation Loss: 0.013299 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.453s | Train Loss: 0.733436 || Validation Loss: 0.012244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.427s | Train Loss: 0.714789 || Validation Loss: 0.012984 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.454s | Train Loss: 0.718502 || Validation Loss: 0.012698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.595s | Train Loss: 0.690322 || Validation Loss: 0.012839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.454s | Train Loss: 0.665756 || Validation Loss: 0.012622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.428s | Train Loss: 0.674936 || Validation Loss: 0.012336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.466s | Train Loss: 0.658243 || Validation Loss: 0.012167 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.427s | Train Loss: 0.670931 || Validation Loss: 0.013835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.461s | Train Loss: 0.684720 || Validation Loss: 0.012229 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.429s | Train Loss: 0.663853 || Validation Loss: 0.013532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.631s | Train Loss: 0.640654 || Validation Loss: 0.012869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.458s | Train Loss: 0.667829 || Validation Loss: 0.012805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.434s | Train Loss: 0.635788 || Validation Loss: 0.013658 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.458s | Train Loss: 0.663687 || Validation Loss: 0.011932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.631941 || Validation Loss: 0.013220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.459s | Train Loss: 0.624125 || Validation Loss: 0.012845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.432s | Train Loss: 0.639543 || Validation Loss: 0.013359 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.461s | Train Loss: 0.646483 || Validation Loss: 0.012936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.432s | Train Loss: 0.670230 || Validation Loss: 0.012920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.464s | Train Loss: 0.626404 || Validation Loss: 0.012473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.465s | Train Loss: 0.657645 || Validation Loss: 0.012928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.465s | Train Loss: 0.672643 || Validation Loss: 0.012419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.435s | Train Loss: 0.656778 || Validation Loss: 0.012632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.465s | Train Loss: 0.636908 || Validation Loss: 0.012744 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.467s | Train Loss: 0.641000 || Validation Loss: 0.012865 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.472s | Train Loss: 0.648639 || Validation Loss: 0.013001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.464s | Train Loss: 0.621411 || Validation Loss: 0.011407 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.468s | Train Loss: 0.631178 || Validation Loss: 0.014501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.468s | Train Loss: 0.627437 || Validation Loss: 0.013128 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.468s | Train Loss: 0.655373 || Validation Loss: 0.012696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.473s | Train Loss: 0.645126 || Validation Loss: 0.013038 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.473s | Train Loss: 0.612091 || Validation Loss: 0.012250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.467s | Train Loss: 0.636836 || Validation Loss: 0.014057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.911s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.823%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010755\n",
      "INFO:root:Test AUC: 77.28%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 945\n",
      "len all outlier 16676 len chosen outlier 55\n",
      "final sizes 945 55 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 945, -1.0: 55})\n",
      "Resampled dataset shape Counter({1.0: 945, -1.0: 226})\n",
      "class weights 0.19299743808710504 0.807002561912895\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6157704229186494\n",
      "validation AUC 0.6218120608942915\n",
      "validation AUC 0.6282790169225781\n",
      "validation AUC 0.6343905563444183\n",
      "validation AUC 0.6404306298388499\n",
      "validation AUC 0.6463873814054065\n",
      "validation AUC 0.6517764817180259\n",
      "validation AUC 0.6570394390813367\n",
      "validation AUC 0.6618776956703035\n",
      "validation AUC 0.6665386736562436\n",
      "validation AUC 0.6712699628334606\n",
      "validation AUC 0.6756386888114903\n",
      "validation AUC 0.6799060321499825\n",
      "validation AUC 0.6835944589055486\n",
      "validation AUC 0.687617524999239\n",
      "validation AUC 0.6914739552192264\n",
      "validation AUC 0.6956271020720756\n",
      "validation AUC 0.6989659018659472\n",
      "validation AUC 0.7017873639087595\n",
      "validation AUC 0.704135038248905\n",
      "validation AUC 0.7070038098842433\n",
      "validation AUC 0.7100524037349167\n",
      "validation AUC 0.7125886007216197\n",
      "validation AUC 0.7152431182515784\n",
      "validation AUC 0.7179241814091526\n",
      "validation AUC 0.720772707912635\n",
      "validation AUC 0.7233875440263106\n",
      "validation AUC 0.7260015021148167\n",
      "validation AUC 0.7280875063297632\n",
      "validation AUC 0.72998416308663\n",
      "validation AUC 0.7323484826554376\n",
      "validation AUC 0.7345667626924658\n",
      "validation AUC 0.7366402936286449\n",
      "validation AUC 0.7383784136288237\n",
      "validation AUC 0.7402841007415215\n",
      "validation AUC 0.7418967110347862\n",
      "validation AUC 0.7434104876221864\n",
      "validation AUC 0.7447302765215218\n",
      "validation AUC 0.746179802951995\n",
      "validation AUC 0.7475112376578928\n",
      "validation AUC 0.7489934958024012\n",
      "validation AUC 0.7506448669158626\n",
      "validation AUC 0.7526106029680869\n",
      "validation AUC 0.7542806068399119\n",
      "validation AUC 0.7557810480874696\n",
      "validation AUC 0.7565669817198352\n",
      "validation AUC 0.7568704910746877\n",
      "validation AUC 0.7574757484126902\n",
      "validation AUC 0.7583374981880754\n",
      "validation AUC 0.7584051513577036\n",
      "validation AUC 0.7584449684687858\n",
      "validation AUC 0.7585486073669399\n",
      "validation AUC 0.7586503545199564\n",
      "validation AUC 0.7587207827812559\n",
      "validation AUC 0.7588004595743376\n",
      "validation AUC 0.75883579077501\n",
      "validation AUC 0.7588702705573364\n",
      "validation AUC 0.7589269085021146\n",
      "validation AUC 0.7589943887273914\n",
      "validation AUC 0.7590454951136036\n",
      "validation AUC 0.7591337366431087\n",
      "validation AUC 0.7591998998306316\n",
      "validation AUC 0.7593238849665702\n",
      "validation AUC 0.7594466674740952\n",
      "validation AUC 0.7595536455285743\n",
      "validation AUC 0.7596013089918505\n",
      "validation AUC 0.7596147693837625\n",
      "validation AUC 0.7596470115322488\n",
      "validation AUC 0.759713781355343\n",
      "validation AUC 0.7597633817953561\n",
      "validation AUC 0.7597633817953561\n",
      "train auc 0.822562143253751\n",
      "Test AUC 0.7728369740329942\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 3.78967314e-05 ... 9.99848413e-01\n",
      " 9.99848413e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [16.79652882 15.79652882 14.49783325 ...  0.06757697  0.06632917\n",
      "  0.02161697]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.206576 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.204197 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.202721 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.200022 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.198164 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.195705 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.193907 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.191552 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.189158 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.186527 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.184739 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.182767 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.019s | Train Loss: 0.180544 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.179168 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.176241 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.174206 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.172513 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.170697 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.019s | Train Loss: 0.168763 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.166703 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.164259 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.162841 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.160988 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.159211 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.158027 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.156509 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.154174 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.019s | Train Loss: 0.152832 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.151598 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.148790 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.148669 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.146974 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.144916 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.142688 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.141516 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.140699 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.139284 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.016s | Train Loss: 0.137728 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.135864 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.134974 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.133860 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.131364 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.130726 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.129293 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.128636 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.127087 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.126428 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.124171 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.122461 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.121693 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.120979 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.120223 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.118616 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.118008 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.116671 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.115563 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.115271 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.113337 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.112425 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.110630 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.110106 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.108957 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.108617 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.106807 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.105383 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.105066 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.104219 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.103616 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.103026 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.101629 |\n",
      "INFO:root:Pretraining Time: 1.238s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.103304\n",
      "INFO:root:Test AUC: 51.80%\n",
      "INFO:root:Test AUC: 51.80%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.419s | Train Loss: 1.616795 || Validation Loss: 0.023692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.384s | Train Loss: 1.669901 || Validation Loss: 0.021625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.414s | Train Loss: 1.535063 || Validation Loss: 0.020136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.416s | Train Loss: 1.459047 || Validation Loss: 0.019754 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.418s | Train Loss: 1.417888 || Validation Loss: 0.019414 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.415s | Train Loss: 1.373877 || Validation Loss: 0.018358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.418s | Train Loss: 1.376101 || Validation Loss: 0.018470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.417s | Train Loss: 1.293144 || Validation Loss: 0.017148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.417s | Train Loss: 1.218486 || Validation Loss: 0.017252 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.387s | Train Loss: 1.202379 || Validation Loss: 0.017956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.447s | Train Loss: 1.153034 || Validation Loss: 0.016780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.421s | Train Loss: 1.160877 || Validation Loss: 0.017374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.418s | Train Loss: 1.157698 || Validation Loss: 0.016749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.413s | Train Loss: 1.139022 || Validation Loss: 0.016006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.420s | Train Loss: 1.040903 || Validation Loss: 0.016292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.423s | Train Loss: 1.062806 || Validation Loss: 0.015185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.426s | Train Loss: 1.045850 || Validation Loss: 0.016748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.413s | Train Loss: 1.022333 || Validation Loss: 0.015454 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.427s | Train Loss: 0.989841 || Validation Loss: 0.014899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.420s | Train Loss: 0.988131 || Validation Loss: 0.015423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.424s | Train Loss: 0.949126 || Validation Loss: 0.014743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.420s | Train Loss: 0.934175 || Validation Loss: 0.014502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.418s | Train Loss: 0.942514 || Validation Loss: 0.015027 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.426s | Train Loss: 0.857185 || Validation Loss: 0.014967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.449s | Train Loss: 0.861317 || Validation Loss: 0.013896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.423s | Train Loss: 0.908353 || Validation Loss: 0.014258 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.431s | Train Loss: 0.853128 || Validation Loss: 0.014058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.449s | Train Loss: 0.881825 || Validation Loss: 0.014376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.425s | Train Loss: 0.863150 || Validation Loss: 0.014640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.429s | Train Loss: 0.828800 || Validation Loss: 0.014183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.427s | Train Loss: 0.790597 || Validation Loss: 0.013313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.457s | Train Loss: 0.803393 || Validation Loss: 0.013571 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.428s | Train Loss: 0.799601 || Validation Loss: 0.014223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.425s | Train Loss: 0.778331 || Validation Loss: 0.012787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.456s | Train Loss: 0.787259 || Validation Loss: 0.013232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.426s | Train Loss: 0.780214 || Validation Loss: 0.013681 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.427s | Train Loss: 0.754184 || Validation Loss: 0.013056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.460s | Train Loss: 0.751042 || Validation Loss: 0.012989 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.427s | Train Loss: 0.714476 || Validation Loss: 0.014938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.454s | Train Loss: 0.739252 || Validation Loss: 0.012784 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.430s | Train Loss: 0.731922 || Validation Loss: 0.013722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.457s | Train Loss: 0.709508 || Validation Loss: 0.014332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.430s | Train Loss: 0.707278 || Validation Loss: 0.013406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.463s | Train Loss: 0.719728 || Validation Loss: 0.013186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.608s | Train Loss: 0.717784 || Validation Loss: 0.012657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.465s | Train Loss: 0.679269 || Validation Loss: 0.013069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.432s | Train Loss: 0.665561 || Validation Loss: 0.013399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.473s | Train Loss: 0.693433 || Validation Loss: 0.013251 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.458s | Train Loss: 0.683459 || Validation Loss: 0.013662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.433s | Train Loss: 0.664147 || Validation Loss: 0.013074 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.467s | Train Loss: 0.675205 || Validation Loss: 0.012862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.435s | Train Loss: 0.682021 || Validation Loss: 0.013314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.464s | Train Loss: 0.700727 || Validation Loss: 0.012957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.434s | Train Loss: 0.670453 || Validation Loss: 0.012802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.466s | Train Loss: 0.688324 || Validation Loss: 0.013248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.436s | Train Loss: 0.673168 || Validation Loss: 0.013382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.465s | Train Loss: 0.685378 || Validation Loss: 0.013422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.462s | Train Loss: 0.677957 || Validation Loss: 0.014084 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.466s | Train Loss: 0.672962 || Validation Loss: 0.013839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.441s | Train Loss: 0.661378 || Validation Loss: 0.013729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.471s | Train Loss: 0.678788 || Validation Loss: 0.012381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.462s | Train Loss: 0.626978 || Validation Loss: 0.014419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.466s | Train Loss: 0.640821 || Validation Loss: 0.013544 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.467s | Train Loss: 0.656276 || Validation Loss: 0.013130 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.472s | Train Loss: 0.671685 || Validation Loss: 0.013726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.469s | Train Loss: 0.659449 || Validation Loss: 0.013164 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.471s | Train Loss: 0.672522 || Validation Loss: 0.014064 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.478s | Train Loss: 0.669188 || Validation Loss: 0.012845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.472s | Train Loss: 0.667922 || Validation Loss: 0.013093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.476s | Train Loss: 0.688133 || Validation Loss: 0.013400 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.035s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.786%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010472\n",
      "INFO:root:Test AUC: 74.05%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 942\n",
      "len all outlier 16676 len chosen outlier 58\n",
      "final sizes 942 58 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 942, -1.0: 58})\n",
      "Resampled dataset shape Counter({1.0: 942, -1.0: 226})\n",
      "class weights 0.1934931506849315 0.8065068493150684\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5002257136641789\n",
      "validation AUC 0.5099954465082588\n",
      "validation AUC 0.5196124774720027\n",
      "validation AUC 0.5289634348812899\n",
      "validation AUC 0.5380210414208639\n",
      "validation AUC 0.547128336202968\n",
      "validation AUC 0.5556709570346632\n",
      "validation AUC 0.5636927732462219\n",
      "validation AUC 0.5716992532848252\n",
      "validation AUC 0.5793669965345145\n",
      "validation AUC 0.5865331852007314\n",
      "validation AUC 0.5933707077606569\n",
      "validation AUC 0.600533881873793\n",
      "validation AUC 0.6073294986870065\n",
      "validation AUC 0.6134972849333222\n",
      "validation AUC 0.6192629250094136\n",
      "validation AUC 0.6251859577487903\n",
      "validation AUC 0.6308462853575414\n",
      "validation AUC 0.6362384906864411\n",
      "validation AUC 0.6413980273275489\n",
      "validation AUC 0.6464769878649472\n",
      "validation AUC 0.6512191326260743\n",
      "validation AUC 0.6559694190751341\n",
      "validation AUC 0.6604702372881643\n",
      "validation AUC 0.6647328339693784\n",
      "validation AUC 0.6688322802707426\n",
      "validation AUC 0.6727229694364227\n",
      "validation AUC 0.676579439566645\n",
      "validation AUC 0.6802472247486878\n",
      "validation AUC 0.6833643098839239\n",
      "validation AUC 0.6863928182436815\n",
      "validation AUC 0.6893894596111616\n",
      "validation AUC 0.6922378450984804\n",
      "validation AUC 0.6952625939141042\n",
      "validation AUC 0.698275699583848\n",
      "validation AUC 0.7012064065824855\n",
      "validation AUC 0.703983286232161\n",
      "validation AUC 0.7067709070564063\n",
      "validation AUC 0.7093020886569152\n",
      "validation AUC 0.7118243410075217\n",
      "validation AUC 0.7139781261048483\n",
      "validation AUC 0.7160434701214953\n",
      "validation AUC 0.7183113372951249\n",
      "validation AUC 0.7206856849756377\n",
      "validation AUC 0.7227136996191819\n",
      "validation AUC 0.724286072413556\n",
      "validation AUC 0.7260667686523942\n",
      "validation AUC 0.728129667501979\n",
      "validation AUC 0.7302067797165755\n",
      "validation AUC 0.7303805142907377\n",
      "validation AUC 0.7305698830342763\n",
      "validation AUC 0.7307687716991947\n",
      "validation AUC 0.7309491047655374\n",
      "validation AUC 0.7311274183739911\n",
      "validation AUC 0.7313389293158875\n",
      "validation AUC 0.7315451029290281\n",
      "validation AUC 0.7317227965984987\n",
      "validation AUC 0.7319018685014164\n",
      "validation AUC 0.7320661496712567\n",
      "validation AUC 0.7322266074405875\n",
      "validation AUC 0.732364960261113\n",
      "validation AUC 0.732484696287369\n",
      "validation AUC 0.7326234375675148\n",
      "validation AUC 0.7327650736700366\n",
      "validation AUC 0.7329393137738416\n",
      "validation AUC 0.7331024747964204\n",
      "validation AUC 0.7333213558283735\n",
      "validation AUC 0.7335483971730357\n",
      "validation AUC 0.7337574735905993\n",
      "validation AUC 0.7339308702581053\n",
      "validation AUC 0.7339308702581053\n",
      "train auc 0.785612881682637\n",
      "Test AUC 0.7404942091311814\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99488394e-01\n",
      " 9.99488394e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.49345207e+01 2.39345207e+01 1.62647743e+01 ... 3.86615917e-02\n",
      " 3.85411158e-02 1.60844158e-02]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.031s | Train Loss: 0.212594 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.209844 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.207603 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.205779 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.203813 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.201394 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.199100 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.197568 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.194467 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.192572 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.190369 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.189000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.186320 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.184158 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.182703 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.180269 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.178587 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.019s | Train Loss: 0.176694 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.174466 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.173032 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.170755 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.168662 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.166706 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.165834 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.163631 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.161850 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.159821 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.157638 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.156178 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.154806 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.152320 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.151111 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.149772 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.148089 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.146390 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.144966 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.143407 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.141314 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.139842 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.138946 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.138360 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.135413 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.134006 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.133176 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.132056 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.131059 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.128581 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.127857 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.126514 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.124757 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.124249 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.122251 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.121940 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.120194 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.118945 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.118166 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.116233 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.115562 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.114322 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.114180 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.112327 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.112035 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.110572 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.109798 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.016s | Train Loss: 0.108226 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.106869 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.107144 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.105250 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.104470 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.103130 |\n",
      "INFO:root:Pretraining Time: 1.226s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.109245\n",
      "INFO:root:Test AUC: 50.68%\n",
      "INFO:root:Test AUC: 50.68%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.414s | Train Loss: 1.505158 || Validation Loss: 0.015297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.383s | Train Loss: 1.433502 || Validation Loss: 0.014904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.415s | Train Loss: 1.468570 || Validation Loss: 0.015555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.412s | Train Loss: 1.324788 || Validation Loss: 0.013994 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.415s | Train Loss: 1.281843 || Validation Loss: 0.013621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.416s | Train Loss: 1.305055 || Validation Loss: 0.013978 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.420s | Train Loss: 1.220009 || Validation Loss: 0.013503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.417s | Train Loss: 1.173168 || Validation Loss: 0.014172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.417s | Train Loss: 1.131477 || Validation Loss: 0.014422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.388s | Train Loss: 1.102945 || Validation Loss: 0.013892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.450s | Train Loss: 1.080956 || Validation Loss: 0.013469 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.414s | Train Loss: 1.055764 || Validation Loss: 0.013444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.419s | Train Loss: 1.060744 || Validation Loss: 0.013442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.418s | Train Loss: 0.971534 || Validation Loss: 0.013722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.417s | Train Loss: 0.930449 || Validation Loss: 0.013736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.419s | Train Loss: 0.927642 || Validation Loss: 0.012937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.417s | Train Loss: 0.925099 || Validation Loss: 0.012802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.418s | Train Loss: 0.889429 || Validation Loss: 0.012024 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.422s | Train Loss: 0.931776 || Validation Loss: 0.011592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.419s | Train Loss: 0.879929 || Validation Loss: 0.012653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.418s | Train Loss: 0.862678 || Validation Loss: 0.011890 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.420s | Train Loss: 0.870636 || Validation Loss: 0.012751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.423s | Train Loss: 0.840233 || Validation Loss: 0.012083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.422s | Train Loss: 0.845899 || Validation Loss: 0.013509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.453s | Train Loss: 0.796181 || Validation Loss: 0.012261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.421s | Train Loss: 0.808251 || Validation Loss: 0.012026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.425s | Train Loss: 0.810143 || Validation Loss: 0.012156 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.450s | Train Loss: 0.778635 || Validation Loss: 0.011850 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.428s | Train Loss: 0.797522 || Validation Loss: 0.012306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.422s | Train Loss: 0.768407 || Validation Loss: 0.012716 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.425s | Train Loss: 0.754617 || Validation Loss: 0.011145 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.458s | Train Loss: 0.750426 || Validation Loss: 0.013201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.421s | Train Loss: 0.707441 || Validation Loss: 0.011832 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.424s | Train Loss: 0.704209 || Validation Loss: 0.012120 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.476s | Train Loss: 0.736656 || Validation Loss: 0.011766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.423s | Train Loss: 0.723831 || Validation Loss: 0.011324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.427s | Train Loss: 0.696638 || Validation Loss: 0.012053 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.453s | Train Loss: 0.691913 || Validation Loss: 0.011446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.433s | Train Loss: 0.686779 || Validation Loss: 0.011080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.480s | Train Loss: 0.678954 || Validation Loss: 0.012382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.451s | Train Loss: 0.697744 || Validation Loss: 0.011540 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.458s | Train Loss: 0.672724 || Validation Loss: 0.011061 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.427s | Train Loss: 0.662565 || Validation Loss: 0.011435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.469s | Train Loss: 0.669905 || Validation Loss: 0.011790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.423s | Train Loss: 0.644378 || Validation Loss: 0.011078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.460s | Train Loss: 0.630020 || Validation Loss: 0.011607 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.438s | Train Loss: 0.644348 || Validation Loss: 0.011775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.467s | Train Loss: 0.632557 || Validation Loss: 0.011368 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.467s | Train Loss: 0.636414 || Validation Loss: 0.011695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.438s | Train Loss: 0.647337 || Validation Loss: 0.011048 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.474s | Train Loss: 0.635399 || Validation Loss: 0.012461 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.436s | Train Loss: 0.631072 || Validation Loss: 0.011892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.477s | Train Loss: 0.625802 || Validation Loss: 0.011937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.444s | Train Loss: 0.621736 || Validation Loss: 0.011633 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.475s | Train Loss: 0.602086 || Validation Loss: 0.011833 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.446s | Train Loss: 0.632577 || Validation Loss: 0.011254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.480s | Train Loss: 0.621013 || Validation Loss: 0.011434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.477s | Train Loss: 0.624802 || Validation Loss: 0.011975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.464s | Train Loss: 0.635738 || Validation Loss: 0.011691 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.446s | Train Loss: 0.622582 || Validation Loss: 0.011098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.471s | Train Loss: 0.650679 || Validation Loss: 0.011417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.495s | Train Loss: 0.629252 || Validation Loss: 0.012465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.468s | Train Loss: 0.619076 || Validation Loss: 0.012247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.472s | Train Loss: 0.615375 || Validation Loss: 0.010779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.469s | Train Loss: 0.613070 || Validation Loss: 0.011373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.469s | Train Loss: 0.617220 || Validation Loss: 0.011744 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.475s | Train Loss: 0.636120 || Validation Loss: 0.011563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.475s | Train Loss: 0.606215 || Validation Loss: 0.011845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.471s | Train Loss: 0.616378 || Validation Loss: 0.010913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.475s | Train Loss: 0.623949 || Validation Loss: 0.011591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.997s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.826%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009858\n",
      "INFO:root:Test AUC: 76.36%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 945\n",
      "len all outlier 16676 len chosen outlier 55\n",
      "final sizes 945 55 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 945, -1.0: 55})\n",
      "Resampled dataset shape Counter({1.0: 945, -1.0: 226})\n",
      "class weights 0.19299743808710504 0.807002561912895\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5136312609314133\n",
      "validation AUC 0.5216634750895213\n",
      "validation AUC 0.530374205812931\n",
      "validation AUC 0.5392965326200718\n",
      "validation AUC 0.5486084977297994\n",
      "validation AUC 0.5575777243843872\n",
      "validation AUC 0.5666306561051059\n",
      "validation AUC 0.575701667162263\n",
      "validation AUC 0.5849745628126435\n",
      "validation AUC 0.5942847319617977\n",
      "validation AUC 0.6038242652312888\n",
      "validation AUC 0.6133141954000846\n",
      "validation AUC 0.6231721990411752\n",
      "validation AUC 0.6321276699681123\n",
      "validation AUC 0.6402879268027134\n",
      "validation AUC 0.6480942224241031\n",
      "validation AUC 0.6551714192483977\n",
      "validation AUC 0.6617172937753018\n",
      "validation AUC 0.6682126924977059\n",
      "validation AUC 0.6739614771512308\n",
      "validation AUC 0.6795479761466636\n",
      "validation AUC 0.684642270196335\n",
      "validation AUC 0.6895150118889929\n",
      "validation AUC 0.6947390461304845\n",
      "validation AUC 0.699489862055328\n",
      "validation AUC 0.7037675687514993\n",
      "validation AUC 0.7075946729308352\n",
      "validation AUC 0.711044082610568\n",
      "validation AUC 0.7141810403821337\n",
      "validation AUC 0.717294123851144\n",
      "validation AUC 0.7203648253113051\n",
      "validation AUC 0.723363121623195\n",
      "validation AUC 0.7268257868223003\n",
      "validation AUC 0.729729336236386\n",
      "validation AUC 0.7322274162880161\n",
      "validation AUC 0.7347063287841342\n",
      "validation AUC 0.7371456183989802\n",
      "validation AUC 0.7391662975413379\n",
      "validation AUC 0.7409200304254346\n",
      "validation AUC 0.742507601037283\n",
      "validation AUC 0.7439568587388409\n",
      "validation AUC 0.7451973406799384\n",
      "validation AUC 0.7463385765094104\n",
      "validation AUC 0.7477038870227454\n",
      "validation AUC 0.7491530382970102\n",
      "validation AUC 0.75063424813268\n",
      "validation AUC 0.7521691319130115\n",
      "validation AUC 0.7534129237429287\n",
      "validation AUC 0.754599154371299\n",
      "validation AUC 0.7547273114571323\n",
      "validation AUC 0.7548747105975828\n",
      "validation AUC 0.7550207980216443\n",
      "validation AUC 0.7551984012279155\n",
      "validation AUC 0.7553816397593637\n",
      "validation AUC 0.7555296269106093\n",
      "validation AUC 0.7556537530627115\n",
      "validation AUC 0.7557558274796549\n",
      "validation AUC 0.7558068167958446\n",
      "validation AUC 0.7558947523468815\n",
      "validation AUC 0.7560177264235342\n",
      "validation AUC 0.7561489033838131\n",
      "validation AUC 0.7562892011631226\n",
      "validation AUC 0.7564156979831815\n",
      "validation AUC 0.7565121397356301\n",
      "validation AUC 0.7566119206444044\n",
      "validation AUC 0.7567356024625572\n",
      "validation AUC 0.7568839514666\n",
      "validation AUC 0.7569990259774124\n",
      "validation AUC 0.7571332467582779\n",
      "validation AUC 0.757274949377858\n",
      "validation AUC 0.757274949377858\n",
      "train auc 0.8261360933029552\n",
      "Test AUC 0.7635669767967395\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.68450971e-05 ... 9.98313595e-01\n",
      " 9.98313595e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [16.09504604 15.09504604 13.56680489 ...  0.07147346  0.07139775\n",
      "  0.02648706]\n",
      "Itration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.033s | Train Loss: 0.219009 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.216386 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.015s | Train Loss: 0.214565 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.016s | Train Loss: 0.212906 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.015s | Train Loss: 0.210225 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.016s | Train Loss: 0.208073 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.206017 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.203425 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.015s | Train Loss: 0.201595 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.200191 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.198589 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.196803 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.016s | Train Loss: 0.194895 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.192590 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.189937 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.189132 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.016s | Train Loss: 0.187783 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.185010 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.016s | Train Loss: 0.183715 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.181160 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.180395 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.178295 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.177145 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.174670 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.172819 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.171501 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.169640 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.016s | Train Loss: 0.167979 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.166962 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.165722 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.163931 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.161177 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.159933 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.016s | Train Loss: 0.157766 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.157484 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.015s | Train Loss: 0.155553 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.016s | Train Loss: 0.154022 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.016s | Train Loss: 0.153389 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.151903 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.150245 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.015s | Train Loss: 0.148122 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.016s | Train Loss: 0.146276 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.145592 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.015s | Train Loss: 0.143172 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.143266 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.141459 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.015s | Train Loss: 0.140091 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.138447 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.015s | Train Loss: 0.138278 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.135957 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.015s | Train Loss: 0.134457 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.015s | Train Loss: 0.134587 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.132974 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.015s | Train Loss: 0.130791 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.130055 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.015s | Train Loss: 0.128940 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.127772 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.015s | Train Loss: 0.126449 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.015s | Train Loss: 0.124888 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.016s | Train Loss: 0.124217 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.015s | Train Loss: 0.122903 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.015s | Train Loss: 0.122113 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.015s | Train Loss: 0.120095 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.016s | Train Loss: 0.119535 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.118406 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.015s | Train Loss: 0.117115 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.116291 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.115642 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.114074 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.113356 |\n",
      "INFO:root:Pretraining Time: 1.128s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.118601\n",
      "INFO:root:Test AUC: 45.86%\n",
      "INFO:root:Test AUC: 45.86%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.406s | Train Loss: 1.799501 || Validation Loss: 0.017182 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.381s | Train Loss: 1.632178 || Validation Loss: 0.015129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.404s | Train Loss: 1.720573 || Validation Loss: 0.014852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.404s | Train Loss: 1.576329 || Validation Loss: 0.016515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.411s | Train Loss: 1.499446 || Validation Loss: 0.015855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.408s | Train Loss: 1.481572 || Validation Loss: 0.015501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.408s | Train Loss: 1.359101 || Validation Loss: 0.014765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.407s | Train Loss: 1.336726 || Validation Loss: 0.015155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.411s | Train Loss: 1.374343 || Validation Loss: 0.015910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.381s | Train Loss: 1.326492 || Validation Loss: 0.014963 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.438s | Train Loss: 1.286035 || Validation Loss: 0.016198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.409s | Train Loss: 1.254975 || Validation Loss: 0.015018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.414s | Train Loss: 1.276924 || Validation Loss: 0.015914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.414s | Train Loss: 1.246580 || Validation Loss: 0.015480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.418s | Train Loss: 1.130137 || Validation Loss: 0.014800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.414s | Train Loss: 1.169827 || Validation Loss: 0.014718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.412s | Train Loss: 1.161689 || Validation Loss: 0.013803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.416s | Train Loss: 1.152447 || Validation Loss: 0.013596 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.405s | Train Loss: 1.122268 || Validation Loss: 0.014036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.412s | Train Loss: 1.091594 || Validation Loss: 0.013944 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.412s | Train Loss: 1.102449 || Validation Loss: 0.014923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.414s | Train Loss: 1.099131 || Validation Loss: 0.012906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.414s | Train Loss: 1.064628 || Validation Loss: 0.014284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.415s | Train Loss: 1.074673 || Validation Loss: 0.014124 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.419s | Train Loss: 1.016940 || Validation Loss: 0.012746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.415s | Train Loss: 1.059458 || Validation Loss: 0.013100 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.419s | Train Loss: 0.975401 || Validation Loss: 0.012289 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.415s | Train Loss: 0.957290 || Validation Loss: 0.013028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.451s | Train Loss: 1.002020 || Validation Loss: 0.013705 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.415s | Train Loss: 0.980377 || Validation Loss: 0.013140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.418s | Train Loss: 0.939672 || Validation Loss: 0.013518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.450s | Train Loss: 0.921013 || Validation Loss: 0.013217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.417s | Train Loss: 0.941554 || Validation Loss: 0.013107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.414s | Train Loss: 0.937583 || Validation Loss: 0.012398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.445s | Train Loss: 0.963088 || Validation Loss: 0.012880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.413s | Train Loss: 0.905032 || Validation Loss: 0.012807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.415s | Train Loss: 0.906499 || Validation Loss: 0.012286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.588s | Train Loss: 0.928942 || Validation Loss: 0.013097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.423s | Train Loss: 0.881822 || Validation Loss: 0.012457 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.420s | Train Loss: 0.884750 || Validation Loss: 0.011535 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.451s | Train Loss: 0.858248 || Validation Loss: 0.012007 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.421s | Train Loss: 0.893331 || Validation Loss: 0.012659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.448s | Train Loss: 0.850752 || Validation Loss: 0.012645 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.424s | Train Loss: 0.811154 || Validation Loss: 0.012329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.424s | Train Loss: 0.849357 || Validation Loss: 0.012008 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.451s | Train Loss: 0.869650 || Validation Loss: 0.013107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.422s | Train Loss: 0.846104 || Validation Loss: 0.011912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.452s | Train Loss: 0.842222 || Validation Loss: 0.012445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.421s | Train Loss: 0.843242 || Validation Loss: 0.011913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.462s | Train Loss: 0.819807 || Validation Loss: 0.011593 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.456s | Train Loss: 0.835341 || Validation Loss: 0.012521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.422s | Train Loss: 0.839693 || Validation Loss: 0.011941 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.428s | Train Loss: 0.812786 || Validation Loss: 0.011586 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.450s | Train Loss: 0.820920 || Validation Loss: 0.013366 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.461s | Train Loss: 0.838013 || Validation Loss: 0.012768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.424s | Train Loss: 0.825178 || Validation Loss: 0.012097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.432s | Train Loss: 0.826774 || Validation Loss: 0.012506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.457s | Train Loss: 0.810984 || Validation Loss: 0.012228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.461s | Train Loss: 0.820089 || Validation Loss: 0.011778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.424s | Train Loss: 0.810617 || Validation Loss: 0.010653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.462s | Train Loss: 0.824799 || Validation Loss: 0.012197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.427s | Train Loss: 0.817749 || Validation Loss: 0.012841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.459s | Train Loss: 0.802856 || Validation Loss: 0.012063 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.430s | Train Loss: 0.824696 || Validation Loss: 0.013005 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.458s | Train Loss: 0.797451 || Validation Loss: 0.012078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.432s | Train Loss: 0.767942 || Validation Loss: 0.011455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.458s | Train Loss: 0.830908 || Validation Loss: 0.012391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.431s | Train Loss: 0.819361 || Validation Loss: 0.012380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.460s | Train Loss: 0.811389 || Validation Loss: 0.011825 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.469s | Train Loss: 0.803907 || Validation Loss: 0.012249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.187s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.701%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009263\n",
      "INFO:root:Test AUC: 70.93%\n",
      "INFO:root:Test Time: 0.183s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 929\n",
      "len all outlier 16676 len chosen outlier 71\n",
      "final sizes 929 71 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 929, -1.0: 71})\n",
      "Resampled dataset shape Counter({1.0: 929, -1.0: 222})\n",
      "class weights 0.19287576020851432 0.8071242397914856\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5493253999590894\n",
      "validation AUC 0.5541866741109011\n",
      "validation AUC 0.5591976355899679\n",
      "validation AUC 0.5642215599133511\n",
      "validation AUC 0.56944571654623\n",
      "validation AUC 0.574589608375232\n",
      "validation AUC 0.5796243403902434\n",
      "validation AUC 0.5844354314126117\n",
      "validation AUC 0.5891763522598665\n",
      "validation AUC 0.5940379723003812\n",
      "validation AUC 0.5988302443166229\n",
      "validation AUC 0.6037544914978428\n",
      "validation AUC 0.6084184388052643\n",
      "validation AUC 0.6128721496376257\n",
      "validation AUC 0.6172001379084866\n",
      "validation AUC 0.6213458375114915\n",
      "validation AUC 0.6252445326703054\n",
      "validation AUC 0.6289480109058176\n",
      "validation AUC 0.6324481624582832\n",
      "validation AUC 0.6359543484382753\n",
      "validation AUC 0.6394344756955184\n",
      "validation AUC 0.6427917113572609\n",
      "validation AUC 0.6460500467960809\n",
      "validation AUC 0.6494847641879821\n",
      "validation AUC 0.6527116397189212\n",
      "validation AUC 0.6557473931166659\n",
      "validation AUC 0.6586886577027924\n",
      "validation AUC 0.6618063521342823\n",
      "validation AUC 0.6649711113755239\n",
      "validation AUC 0.6681279843543365\n",
      "validation AUC 0.6710779573537322\n",
      "validation AUC 0.6738331417996812\n",
      "validation AUC 0.6763791088119031\n",
      "validation AUC 0.6788321195655298\n",
      "validation AUC 0.6812618307239844\n",
      "validation AUC 0.6836797630417601\n",
      "validation AUC 0.685975604629417\n",
      "validation AUC 0.6882657922671206\n",
      "validation AUC 0.6902651566833254\n",
      "validation AUC 0.6923192832632652\n",
      "validation AUC 0.6943352077662975\n",
      "validation AUC 0.6963077525046066\n",
      "validation AUC 0.6984300191590413\n",
      "validation AUC 0.7004885305434624\n",
      "validation AUC 0.702488914001\n",
      "validation AUC 0.7045869950530466\n",
      "validation AUC 0.7065769273503884\n",
      "validation AUC 0.7084969342553911\n",
      "validation AUC 0.7106963899649279\n",
      "validation AUC 0.7109164842680239\n",
      "validation AUC 0.7111112781428353\n",
      "validation AUC 0.7112912014640989\n",
      "validation AUC 0.7114733065448741\n",
      "validation AUC 0.7116507873597582\n",
      "validation AUC 0.7118379131480916\n",
      "validation AUC 0.7120420247324258\n",
      "validation AUC 0.7122534638358994\n",
      "validation AUC 0.7124750321570067\n",
      "validation AUC 0.7126636931590451\n",
      "validation AUC 0.7128284665551167\n",
      "validation AUC 0.7129891717679042\n",
      "validation AUC 0.7131627147729384\n",
      "validation AUC 0.7133444313940932\n",
      "validation AUC 0.713529098711953\n",
      "validation AUC 0.7137144923960892\n",
      "validation AUC 0.7138956369705428\n",
      "validation AUC 0.7140859742024498\n",
      "validation AUC 0.7142959898408763\n",
      "validation AUC 0.7145014982834342\n",
      "validation AUC 0.7146950230127735\n",
      "validation AUC 0.7146950230127735\n",
      "train auc 0.7014781276733912\n",
      "Test AUC 0.70928343402526\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.99450497e-01\n",
      " 9.99450497e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [18.86880493 17.86880493 13.94441223 ...  0.09995959  0.09859553\n",
      "  0.03549276]\n",
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.036s | Train Loss: 0.216692 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.214158 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.211917 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.210011 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.207694 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.016s | Train Loss: 0.205608 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.015s | Train Loss: 0.202998 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.015s | Train Loss: 0.201106 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.015s | Train Loss: 0.198697 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.015s | Train Loss: 0.196622 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.194970 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.192401 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.190828 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.188266 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.186630 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.184662 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.182201 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.019s | Train Loss: 0.180464 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.178584 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.177026 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.174017 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.172694 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.170816 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.168936 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.166840 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.164830 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.163361 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.019s | Train Loss: 0.161697 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.159837 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.158666 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.155679 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.154131 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.152426 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.151164 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.149736 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.147773 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.146311 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.145099 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.143310 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.140989 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.140348 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.016s | Train Loss: 0.138541 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.137277 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.135645 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.134541 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.132658 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.131416 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.016s | Train Loss: 0.130779 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.128641 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.128262 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.126734 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.124440 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.123830 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.122487 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.121023 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.120190 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.119098 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.117458 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.116480 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.115828 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.114427 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.113421 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.112458 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.110922 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.109544 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.108943 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.108560 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.106833 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.105855 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.105942 |\n",
      "INFO:root:Pretraining Time: 1.233s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.109413\n",
      "INFO:root:Test AUC: 52.33%\n",
      "INFO:root:Test AUC: 52.33%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.416s | Train Loss: 1.424823 || Validation Loss: 0.012360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.391s | Train Loss: 1.337018 || Validation Loss: 0.013465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.420s | Train Loss: 1.309291 || Validation Loss: 0.012478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.418s | Train Loss: 1.261698 || Validation Loss: 0.013737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.417s | Train Loss: 1.128780 || Validation Loss: 0.011633 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.411s | Train Loss: 1.130779 || Validation Loss: 0.012239 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.415s | Train Loss: 1.128033 || Validation Loss: 0.012490 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.412s | Train Loss: 1.130997 || Validation Loss: 0.012185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.410s | Train Loss: 1.079717 || Validation Loss: 0.012018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.381s | Train Loss: 1.056697 || Validation Loss: 0.012024 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.444s | Train Loss: 1.022296 || Validation Loss: 0.011981 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.415s | Train Loss: 1.022086 || Validation Loss: 0.012108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.412s | Train Loss: 1.020994 || Validation Loss: 0.010860 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.415s | Train Loss: 0.945224 || Validation Loss: 0.011768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.419s | Train Loss: 0.980705 || Validation Loss: 0.012472 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.416s | Train Loss: 0.953158 || Validation Loss: 0.011600 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.415s | Train Loss: 0.916714 || Validation Loss: 0.011130 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.415s | Train Loss: 0.915085 || Validation Loss: 0.012076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.417s | Train Loss: 0.878127 || Validation Loss: 0.010320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.421s | Train Loss: 0.892507 || Validation Loss: 0.011051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.418s | Train Loss: 0.885383 || Validation Loss: 0.011520 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.417s | Train Loss: 0.842129 || Validation Loss: 0.012150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.416s | Train Loss: 0.840824 || Validation Loss: 0.010372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.419s | Train Loss: 0.855260 || Validation Loss: 0.010826 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.451s | Train Loss: 0.838924 || Validation Loss: 0.010943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.422s | Train Loss: 0.813918 || Validation Loss: 0.010191 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.421s | Train Loss: 0.838021 || Validation Loss: 0.010637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.449s | Train Loss: 0.811377 || Validation Loss: 0.010684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.422s | Train Loss: 0.801183 || Validation Loss: 0.009948 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.426s | Train Loss: 0.778113 || Validation Loss: 0.010837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.422s | Train Loss: 0.765625 || Validation Loss: 0.010959 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.455s | Train Loss: 0.746777 || Validation Loss: 0.011180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.419s | Train Loss: 0.759782 || Validation Loss: 0.010649 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.417s | Train Loss: 0.757495 || Validation Loss: 0.011230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.453s | Train Loss: 0.735779 || Validation Loss: 0.011119 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.424s | Train Loss: 0.732277 || Validation Loss: 0.010863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.420s | Train Loss: 0.722969 || Validation Loss: 0.011053 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.452s | Train Loss: 0.745068 || Validation Loss: 0.011785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.423s | Train Loss: 0.714582 || Validation Loss: 0.011219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.451s | Train Loss: 0.715571 || Validation Loss: 0.010518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.428s | Train Loss: 0.700695 || Validation Loss: 0.009644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.455s | Train Loss: 0.720738 || Validation Loss: 0.010433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.424s | Train Loss: 0.695423 || Validation Loss: 0.010572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.454s | Train Loss: 0.671822 || Validation Loss: 0.010772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.425s | Train Loss: 0.687970 || Validation Loss: 0.009857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.454s | Train Loss: 0.688121 || Validation Loss: 0.011328 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.429s | Train Loss: 0.668591 || Validation Loss: 0.009906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.460s | Train Loss: 0.653974 || Validation Loss: 0.010325 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.457s | Train Loss: 0.640084 || Validation Loss: 0.010486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.429s | Train Loss: 0.635692 || Validation Loss: 0.011031 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.462s | Train Loss: 0.666558 || Validation Loss: 0.010743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.431s | Train Loss: 0.648286 || Validation Loss: 0.010237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.464s | Train Loss: 0.657011 || Validation Loss: 0.010514 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.430s | Train Loss: 0.668199 || Validation Loss: 0.010193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.468s | Train Loss: 0.668577 || Validation Loss: 0.009881 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.434s | Train Loss: 0.664994 || Validation Loss: 0.010475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.466s | Train Loss: 0.678727 || Validation Loss: 0.010242 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.459s | Train Loss: 0.631176 || Validation Loss: 0.011121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.463s | Train Loss: 0.651607 || Validation Loss: 0.011168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.435s | Train Loss: 0.647202 || Validation Loss: 0.009889 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.465s | Train Loss: 0.650150 || Validation Loss: 0.010305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.465s | Train Loss: 0.639424 || Validation Loss: 0.009847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.462s | Train Loss: 0.654614 || Validation Loss: 0.010596 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.466s | Train Loss: 0.640831 || Validation Loss: 0.010492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.462s | Train Loss: 0.661829 || Validation Loss: 0.010567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.464s | Train Loss: 0.639526 || Validation Loss: 0.009854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.450s | Train Loss: 0.669916 || Validation Loss: 0.010955 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.463s | Train Loss: 0.654120 || Validation Loss: 0.010348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.470s | Train Loss: 0.664743 || Validation Loss: 0.010495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.474s | Train Loss: 0.639814 || Validation Loss: 0.010403 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.584s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.824%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008719\n",
      "INFO:root:Test AUC: 73.25%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 933\n",
      "len all outlier 16676 len chosen outlier 67\n",
      "final sizes 933 67 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 933, -1.0: 67})\n",
      "Resampled dataset shape Counter({1.0: 933, -1.0: 223})\n",
      "class weights 0.19290657439446368 0.8070934256055363\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.48367507758017547\n",
      "validation AUC 0.4914665638437111\n",
      "validation AUC 0.49991062235913974\n",
      "validation AUC 0.5076420729865605\n",
      "validation AUC 0.5152073985912857\n",
      "validation AUC 0.5232244495207898\n",
      "validation AUC 0.5309079759380662\n",
      "validation AUC 0.5385980955261584\n",
      "validation AUC 0.5455613965219135\n",
      "validation AUC 0.5526831810607098\n",
      "validation AUC 0.5600632327120038\n",
      "validation AUC 0.5666081307684924\n",
      "validation AUC 0.5725788163284159\n",
      "validation AUC 0.5784125086764851\n",
      "validation AUC 0.5843683848785547\n",
      "validation AUC 0.5902381826858546\n",
      "validation AUC 0.5961427502898548\n",
      "validation AUC 0.6021352161953393\n",
      "validation AUC 0.6072929143049564\n",
      "validation AUC 0.6121764078149136\n",
      "validation AUC 0.6165310853902359\n",
      "validation AUC 0.6211117293945585\n",
      "validation AUC 0.6261330196424342\n",
      "validation AUC 0.6311014447930765\n",
      "validation AUC 0.6359010629532338\n",
      "validation AUC 0.6401937093806933\n",
      "validation AUC 0.6443376768795007\n",
      "validation AUC 0.6485283100857102\n",
      "validation AUC 0.6523746084273814\n",
      "validation AUC 0.6559797717900837\n",
      "validation AUC 0.6595713922797215\n",
      "validation AUC 0.6628856792072785\n",
      "validation AUC 0.6660582342477496\n",
      "validation AUC 0.6695834840166426\n",
      "validation AUC 0.6732239493125115\n",
      "validation AUC 0.676686029161504\n",
      "validation AUC 0.679878957046583\n",
      "validation AUC 0.6830628306296899\n",
      "validation AUC 0.6862126581216903\n",
      "validation AUC 0.6891517249842114\n",
      "validation AUC 0.6920796249629899\n",
      "validation AUC 0.6946563707484159\n",
      "validation AUC 0.6974475169767497\n",
      "validation AUC 0.6997718065798886\n",
      "validation AUC 0.7019859093456574\n",
      "validation AUC 0.7041878687572677\n",
      "validation AUC 0.7065194220231701\n",
      "validation AUC 0.7084816220285765\n",
      "validation AUC 0.7107545817481875\n",
      "validation AUC 0.7109652359503733\n",
      "validation AUC 0.7111815680486535\n",
      "validation AUC 0.7114576803575446\n",
      "validation AUC 0.7117215881165838\n",
      "validation AUC 0.7119760690781246\n",
      "validation AUC 0.7121954822465439\n",
      "validation AUC 0.7123924339347256\n",
      "validation AUC 0.7126289898794029\n",
      "validation AUC 0.7128517022939126\n",
      "validation AUC 0.713099217589111\n",
      "validation AUC 0.7133454637388378\n",
      "validation AUC 0.7136352971865094\n",
      "validation AUC 0.7138821978640894\n",
      "validation AUC 0.7140930090465327\n",
      "validation AUC 0.7142638594410481\n",
      "validation AUC 0.7144438998323345\n",
      "validation AUC 0.7146649306956109\n",
      "validation AUC 0.7149466650262672\n",
      "validation AUC 0.7151945687810861\n",
      "validation AUC 0.7154538921844691\n",
      "validation AUC 0.7156720122612756\n",
      "validation AUC 0.7156720122612756\n",
      "train auc 0.82430279553617\n",
      "Test AUC 0.7324657727904228\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.68450971e-05 ... 9.99374704e-01\n",
      " 9.99374704e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [14.64426422 13.64426422 10.43581676 ...  0.06108087  0.060955\n",
      "  0.0244322 ]\n",
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.031s | Train Loss: 0.221597 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.219172 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.016s | Train Loss: 0.216372 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.215091 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.015s | Train Loss: 0.213159 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.210969 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.015s | Train Loss: 0.209122 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.207222 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.015s | Train Loss: 0.204807 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.015s | Train Loss: 0.202710 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.201750 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.198734 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.015s | Train Loss: 0.196904 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.017s | Train Loss: 0.195223 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.015s | Train Loss: 0.193567 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.191378 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.016s | Train Loss: 0.189860 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.188197 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.185826 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.183606 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.182537 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.180602 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.179116 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.176961 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.175392 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.016s | Train Loss: 0.173779 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.172011 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.169870 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.168379 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.166177 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.165171 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.015s | Train Loss: 0.163727 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.015s | Train Loss: 0.161424 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.016s | Train Loss: 0.159772 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.158207 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.157141 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.016s | Train Loss: 0.155513 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.016s | Train Loss: 0.154236 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.152289 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.150810 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.015s | Train Loss: 0.148808 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.015s | Train Loss: 0.148327 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.146104 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.144879 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.015s | Train Loss: 0.143210 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.141448 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.015s | Train Loss: 0.139928 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.138757 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.015s | Train Loss: 0.138070 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.015s | Train Loss: 0.136917 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.135128 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.015s | Train Loss: 0.134159 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.132072 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.015s | Train Loss: 0.131722 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.129850 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.015s | Train Loss: 0.128483 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.127812 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.016s | Train Loss: 0.126626 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.125074 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.016s | Train Loss: 0.124264 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.123114 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.015s | Train Loss: 0.122204 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.015s | Train Loss: 0.120823 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.016s | Train Loss: 0.120155 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.118761 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.015s | Train Loss: 0.117795 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.116987 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.115666 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.114272 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.113353 |\n",
      "INFO:root:Pretraining Time: 1.130s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.117632\n",
      "INFO:root:Test AUC: 48.76%\n",
      "INFO:root:Test AUC: 48.76%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.409s | Train Loss: 1.638153 || Validation Loss: 0.014424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.379s | Train Loss: 1.548910 || Validation Loss: 0.013578 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.407s | Train Loss: 1.539701 || Validation Loss: 0.014768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.410s | Train Loss: 1.482044 || Validation Loss: 0.013706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.413s | Train Loss: 1.427251 || Validation Loss: 0.012450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.410s | Train Loss: 1.425055 || Validation Loss: 0.013708 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.410s | Train Loss: 1.340703 || Validation Loss: 0.012314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.409s | Train Loss: 1.256261 || Validation Loss: 0.012940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.415s | Train Loss: 1.244626 || Validation Loss: 0.012953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.384s | Train Loss: 1.171082 || Validation Loss: 0.012774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.442s | Train Loss: 1.238169 || Validation Loss: 0.012320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.411s | Train Loss: 1.182087 || Validation Loss: 0.013232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.412s | Train Loss: 1.147348 || Validation Loss: 0.012427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.412s | Train Loss: 1.102738 || Validation Loss: 0.012231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.414s | Train Loss: 1.086810 || Validation Loss: 0.012211 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.410s | Train Loss: 1.062365 || Validation Loss: 0.012091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.414s | Train Loss: 1.066156 || Validation Loss: 0.012439 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.419s | Train Loss: 1.037217 || Validation Loss: 0.011278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.414s | Train Loss: 0.975154 || Validation Loss: 0.012452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.414s | Train Loss: 0.994161 || Validation Loss: 0.011388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.419s | Train Loss: 0.995735 || Validation Loss: 0.011285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.419s | Train Loss: 0.929072 || Validation Loss: 0.012315 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.419s | Train Loss: 0.951157 || Validation Loss: 0.010938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.418s | Train Loss: 0.924737 || Validation Loss: 0.011987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.416s | Train Loss: 0.899167 || Validation Loss: 0.011265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.417s | Train Loss: 0.912072 || Validation Loss: 0.012267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.417s | Train Loss: 0.890980 || Validation Loss: 0.011789 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.422s | Train Loss: 0.888776 || Validation Loss: 0.011438 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.449s | Train Loss: 0.828911 || Validation Loss: 0.011812 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.416s | Train Loss: 0.862837 || Validation Loss: 0.011698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.416s | Train Loss: 0.834384 || Validation Loss: 0.011799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.448s | Train Loss: 0.803800 || Validation Loss: 0.011395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.417s | Train Loss: 0.793206 || Validation Loss: 0.011952 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.414s | Train Loss: 0.794955 || Validation Loss: 0.010267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.446s | Train Loss: 0.817089 || Validation Loss: 0.011862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.417s | Train Loss: 0.772780 || Validation Loss: 0.012052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.419s | Train Loss: 0.810658 || Validation Loss: 0.011889 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.446s | Train Loss: 0.768253 || Validation Loss: 0.011572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.420s | Train Loss: 0.797560 || Validation Loss: 0.011207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.420s | Train Loss: 0.766750 || Validation Loss: 0.011028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.448s | Train Loss: 0.760838 || Validation Loss: 0.011480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.418s | Train Loss: 0.740422 || Validation Loss: 0.012358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.455s | Train Loss: 0.724466 || Validation Loss: 0.012248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.425s | Train Loss: 0.743006 || Validation Loss: 0.012117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.421s | Train Loss: 0.710125 || Validation Loss: 0.011698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.454s | Train Loss: 0.709879 || Validation Loss: 0.011249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.409s | Train Loss: 0.717637 || Validation Loss: 0.011225 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.460s | Train Loss: 0.696317 || Validation Loss: 0.011690 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.424s | Train Loss: 0.700475 || Validation Loss: 0.012778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.459s | Train Loss: 0.689987 || Validation Loss: 0.010821 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.457s | Train Loss: 0.690666 || Validation Loss: 0.011623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.424s | Train Loss: 0.707343 || Validation Loss: 0.011948 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.425s | Train Loss: 0.685526 || Validation Loss: 0.011906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.461s | Train Loss: 0.714712 || Validation Loss: 0.012147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.457s | Train Loss: 0.684288 || Validation Loss: 0.011274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.429s | Train Loss: 0.695946 || Validation Loss: 0.012458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.424s | Train Loss: 0.690008 || Validation Loss: 0.010541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.458s | Train Loss: 0.711464 || Validation Loss: 0.011943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.458s | Train Loss: 0.718972 || Validation Loss: 0.011075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.431s | Train Loss: 0.704359 || Validation Loss: 0.012075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.458s | Train Loss: 0.688744 || Validation Loss: 0.011209 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.428s | Train Loss: 0.670529 || Validation Loss: 0.011751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.461s | Train Loss: 0.701541 || Validation Loss: 0.011416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.427s | Train Loss: 0.705711 || Validation Loss: 0.012218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.458s | Train Loss: 0.708015 || Validation Loss: 0.011771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.430s | Train Loss: 0.694928 || Validation Loss: 0.012228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.464s | Train Loss: 0.673461 || Validation Loss: 0.012632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.433s | Train Loss: 0.697377 || Validation Loss: 0.011379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.467s | Train Loss: 0.699476 || Validation Loss: 0.012330 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.463s | Train Loss: 0.704889 || Validation Loss: 0.011907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.095s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.764%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009631\n",
      "INFO:root:Test AUC: 77.68%\n",
      "INFO:root:Test Time: 0.183s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 924\n",
      "len all outlier 16676 len chosen outlier 76\n",
      "final sizes 924 76 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 924, -1.0: 76})\n",
      "Resampled dataset shape Counter({1.0: 924, -1.0: 221})\n",
      "class weights 0.1930131004366812 0.8069868995633188\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5287986774493124\n",
      "validation AUC 0.5365463768637281\n",
      "validation AUC 0.5441069611325396\n",
      "validation AUC 0.5515499831525595\n",
      "validation AUC 0.5592177290629311\n",
      "validation AUC 0.5668861241797915\n",
      "validation AUC 0.5748747670040483\n",
      "validation AUC 0.5827715524315124\n",
      "validation AUC 0.5909549864698982\n",
      "validation AUC 0.5993619470915869\n",
      "validation AUC 0.6073642285734719\n",
      "validation AUC 0.6147784114577068\n",
      "validation AUC 0.6224271574355534\n",
      "validation AUC 0.6301914116154322\n",
      "validation AUC 0.6376971272932154\n",
      "validation AUC 0.6448655589146375\n",
      "validation AUC 0.6516604866111273\n",
      "validation AUC 0.6582031124449051\n",
      "validation AUC 0.6642319152889831\n",
      "validation AUC 0.6700686993499207\n",
      "validation AUC 0.6759429883889951\n",
      "validation AUC 0.6813611619774447\n",
      "validation AUC 0.6865074164923565\n",
      "validation AUC 0.6911272683115076\n",
      "validation AUC 0.6956992571162078\n",
      "validation AUC 0.7002830726538688\n",
      "validation AUC 0.7045758840436326\n",
      "validation AUC 0.7084052950345494\n",
      "validation AUC 0.7121731830679284\n",
      "validation AUC 0.715705680535491\n",
      "validation AUC 0.7192065398294567\n",
      "validation AUC 0.7229103533109424\n",
      "validation AUC 0.7264985282169618\n",
      "validation AUC 0.7299307312640604\n",
      "validation AUC 0.7328596662482655\n",
      "validation AUC 0.7357847592071848\n",
      "validation AUC 0.7380384316405788\n",
      "validation AUC 0.74013431496902\n",
      "validation AUC 0.7422724313081001\n",
      "validation AUC 0.7441644637990755\n",
      "validation AUC 0.7460804956446759\n",
      "validation AUC 0.7478126211275631\n",
      "validation AUC 0.7496415182875084\n",
      "validation AUC 0.7516685138897197\n",
      "validation AUC 0.7535829812541094\n",
      "validation AUC 0.7554398342033055\n",
      "validation AUC 0.756723355341788\n",
      "validation AUC 0.7579745119403973\n",
      "validation AUC 0.7594174372180076\n",
      "validation AUC 0.7595589349650482\n",
      "validation AUC 0.7597043891467149\n",
      "validation AUC 0.7598639209985945\n",
      "validation AUC 0.7599917375170895\n",
      "validation AUC 0.7601148685739999\n",
      "validation AUC 0.7602744802463495\n",
      "validation AUC 0.7604028900970043\n",
      "validation AUC 0.7605464418908043\n",
      "validation AUC 0.7606960467369072\n",
      "validation AUC 0.7608550943446027\n",
      "validation AUC 0.7610392987037369\n",
      "validation AUC 0.7611947650330149\n",
      "validation AUC 0.7613356694478957\n",
      "validation AUC 0.7614812460209495\n",
      "validation AUC 0.761639907829707\n",
      "validation AUC 0.7617790029806026\n",
      "validation AUC 0.7619558904633269\n",
      "validation AUC 0.7621272676729438\n",
      "validation AUC 0.7622647424683002\n",
      "validation AUC 0.7623789735428135\n",
      "validation AUC 0.7624998722872482\n",
      "validation AUC 0.7624998722872482\n",
      "train auc 0.7642850349425134\n",
      "Test AUC 0.7768028080926285\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99734723e-01\n",
      " 9.99734723e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [18.70422935 17.70422935 16.99831772 ...  0.03423166  0.03304358\n",
      "  0.02112224]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.034s | Train Loss: 0.216593 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.214337 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.019s | Train Loss: 0.211851 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.209448 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.206774 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.205040 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.202886 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.200775 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.019s | Train Loss: 0.198228 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.196040 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.194565 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.192335 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.019s | Train Loss: 0.190026 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.188103 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.185785 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.185023 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.019s | Train Loss: 0.182743 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.180298 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.177822 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.176568 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.174853 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.172462 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.170182 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.168531 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.167257 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.165058 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.163700 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.161731 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.159836 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.158106 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.156959 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.154920 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.154221 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.151601 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.150114 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.148514 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.147546 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.019s | Train Loss: 0.145372 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.143823 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.142948 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.141035 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.140637 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.137183 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.136670 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.135472 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.133348 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.132605 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.131547 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.129744 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.128563 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.127794 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.125271 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.124439 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.123441 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.122156 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.121333 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.119493 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.118090 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.116991 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.115854 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.114597 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.113176 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.112383 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.111510 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.110196 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.110102 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.107645 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.018s | Train Loss: 0.107387 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.016s | Train Loss: 0.107081 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.104867 |\n",
      "INFO:root:Pretraining Time: 1.250s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.106850\n",
      "INFO:root:Test AUC: 54.46%\n",
      "INFO:root:Test AUC: 54.46%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.415s | Train Loss: 1.366142 || Validation Loss: 0.011759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.380s | Train Loss: 1.311827 || Validation Loss: 0.013222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.415s | Train Loss: 1.305973 || Validation Loss: 0.013597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.415s | Train Loss: 1.233910 || Validation Loss: 0.012526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.415s | Train Loss: 1.218754 || Validation Loss: 0.013172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.411s | Train Loss: 1.133689 || Validation Loss: 0.012292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.469s | Train Loss: 1.087142 || Validation Loss: 0.012738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.415s | Train Loss: 1.139655 || Validation Loss: 0.012712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.415s | Train Loss: 1.107015 || Validation Loss: 0.012277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.389s | Train Loss: 1.038990 || Validation Loss: 0.012533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.443s | Train Loss: 1.019065 || Validation Loss: 0.012684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.415s | Train Loss: 1.046714 || Validation Loss: 0.012438 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.417s | Train Loss: 1.050915 || Validation Loss: 0.012267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.416s | Train Loss: 0.987847 || Validation Loss: 0.012450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.420s | Train Loss: 0.962798 || Validation Loss: 0.011901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.420s | Train Loss: 0.949449 || Validation Loss: 0.012418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.416s | Train Loss: 0.934248 || Validation Loss: 0.011802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.418s | Train Loss: 0.944716 || Validation Loss: 0.011617 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.419s | Train Loss: 0.920261 || Validation Loss: 0.012079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.423s | Train Loss: 0.931538 || Validation Loss: 0.011591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.423s | Train Loss: 0.881362 || Validation Loss: 0.012063 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.429s | Train Loss: 0.840704 || Validation Loss: 0.012062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 523.906s | Train Loss: 0.819036 || Validation Loss: 0.012310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.395s | Train Loss: 0.808992 || Validation Loss: 0.011829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.446s | Train Loss: 0.858719 || Validation Loss: 0.012340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.413s | Train Loss: 0.814468 || Validation Loss: 0.012254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 0.771669 || Validation Loss: 0.012039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.457s | Train Loss: 0.802959 || Validation Loss: 0.012212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.422s | Train Loss: 0.774049 || Validation Loss: 0.011117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.420s | Train Loss: 0.778428 || Validation Loss: 0.011462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.425s | Train Loss: 0.795113 || Validation Loss: 0.011869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.449s | Train Loss: 0.790498 || Validation Loss: 0.012291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.428s | Train Loss: 0.772270 || Validation Loss: 0.012396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.420s | Train Loss: 0.730336 || Validation Loss: 0.011455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.451s | Train Loss: 0.742471 || Validation Loss: 0.011733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.432s | Train Loss: 0.743117 || Validation Loss: 0.011068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.424s | Train Loss: 0.714367 || Validation Loss: 0.011277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.452s | Train Loss: 0.729938 || Validation Loss: 0.011928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 13.848s | Train Loss: 0.706069 || Validation Loss: 0.012276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.441s | Train Loss: 0.722368 || Validation Loss: 0.012184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.420s | Train Loss: 0.707465 || Validation Loss: 0.012375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.437s | Train Loss: 0.737499 || Validation Loss: 0.012025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.439s | Train Loss: 0.699577 || Validation Loss: 0.012525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.442s | Train Loss: 0.679828 || Validation Loss: 0.011920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.406s | Train Loss: 0.679950 || Validation Loss: 0.011416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.451s | Train Loss: 0.703065 || Validation Loss: 0.013332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.423s | Train Loss: 0.675086 || Validation Loss: 0.012787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.480s | Train Loss: 0.670154 || Validation Loss: 0.012905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.465s | Train Loss: 0.687857 || Validation Loss: 0.013015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.440s | Train Loss: 0.651873 || Validation Loss: 0.012989 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.481s | Train Loss: 0.653256 || Validation Loss: 0.011877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.672196 || Validation Loss: 0.012337 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.450s | Train Loss: 0.652305 || Validation Loss: 0.012249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.419s | Train Loss: 0.629438 || Validation Loss: 0.011878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.468s | Train Loss: 0.684837 || Validation Loss: 0.012858 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.441s | Train Loss: 0.673954 || Validation Loss: 0.012811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.468s | Train Loss: 0.681444 || Validation Loss: 0.011632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.463s | Train Loss: 0.652312 || Validation Loss: 0.012367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.473s | Train Loss: 0.648033 || Validation Loss: 0.012804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.433s | Train Loss: 0.662371 || Validation Loss: 0.011751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.473s | Train Loss: 0.662782 || Validation Loss: 0.011715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.465s | Train Loss: 0.648397 || Validation Loss: 0.012534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.462s | Train Loss: 0.677501 || Validation Loss: 0.012626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.472s | Train Loss: 0.650054 || Validation Loss: 0.012734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.468s | Train Loss: 0.627645 || Validation Loss: 0.012138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.480s | Train Loss: 0.641203 || Validation Loss: 0.012553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.465s | Train Loss: 0.654204 || Validation Loss: 0.012484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.464s | Train Loss: 0.674831 || Validation Loss: 0.012033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.457s | Train Loss: 0.669909 || Validation Loss: 0.011954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.471s | Train Loss: 0.662197 || Validation Loss: 0.013179 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 567.592s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.828%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010008\n",
      "INFO:root:Test AUC: 77.69%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 937\n",
      "len all outlier 16676 len chosen outlier 63\n",
      "final sizes 937 63 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 937, -1.0: 63})\n",
      "Resampled dataset shape Counter({1.0: 937, -1.0: 224})\n",
      "class weights 0.1929371231696813 0.8070628768303187\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5427794695578564\n",
      "validation AUC 0.5517399718350811\n",
      "validation AUC 0.5607808666289347\n",
      "validation AUC 0.5694461528981323\n",
      "validation AUC 0.5774599258670046\n",
      "validation AUC 0.5857399852449201\n",
      "validation AUC 0.593718554725872\n",
      "validation AUC 0.6017623428521195\n",
      "validation AUC 0.6100374453788525\n",
      "validation AUC 0.618344577199485\n",
      "validation AUC 0.6265556876554771\n",
      "validation AUC 0.6341894114208831\n",
      "validation AUC 0.6415649505549438\n",
      "validation AUC 0.6486196938810479\n",
      "validation AUC 0.6554359735532433\n",
      "validation AUC 0.6622610361378132\n",
      "validation AUC 0.6686851738053484\n",
      "validation AUC 0.6749463606442088\n",
      "validation AUC 0.6809803464974671\n",
      "validation AUC 0.6864514889497606\n",
      "validation AUC 0.6914158618812132\n",
      "validation AUC 0.6962268996899348\n",
      "validation AUC 0.7005583282230494\n",
      "validation AUC 0.7042146884351633\n",
      "validation AUC 0.707674059709543\n",
      "validation AUC 0.7115607126201271\n",
      "validation AUC 0.7152975159018341\n",
      "validation AUC 0.718560667175673\n",
      "validation AUC 0.7216987450945\n",
      "validation AUC 0.7249475499692105\n",
      "validation AUC 0.7279067156260596\n",
      "validation AUC 0.7308554035158896\n",
      "validation AUC 0.733514526686963\n",
      "validation AUC 0.7360992704834759\n",
      "validation AUC 0.738719292267014\n",
      "validation AUC 0.741274922877462\n",
      "validation AUC 0.7437080077811122\n",
      "validation AUC 0.745916153279142\n",
      "validation AUC 0.7482246650360586\n",
      "validation AUC 0.7505672973831871\n",
      "validation AUC 0.7524317944727198\n",
      "validation AUC 0.7542680723654509\n",
      "validation AUC 0.756662047899519\n",
      "validation AUC 0.7587083334486296\n",
      "validation AUC 0.76051078874757\n",
      "validation AUC 0.7622011894101438\n",
      "validation AUC 0.7638055542488649\n",
      "validation AUC 0.765337636330702\n",
      "validation AUC 0.7666376137894012\n",
      "validation AUC 0.7667842227071997\n",
      "validation AUC 0.7669591811952339\n",
      "validation AUC 0.7671443167931838\n",
      "validation AUC 0.767303218063351\n",
      "validation AUC 0.767456755397939\n",
      "validation AUC 0.7676204379142549\n",
      "validation AUC 0.7677661501821077\n",
      "validation AUC 0.7679170507805058\n",
      "validation AUC 0.7680361668677786\n",
      "validation AUC 0.7681538994002397\n",
      "validation AUC 0.7682743325252663\n",
      "validation AUC 0.7684409896844282\n",
      "validation AUC 0.7686167091276092\n",
      "validation AUC 0.7687616604403152\n",
      "validation AUC 0.7689197661664655\n",
      "validation AUC 0.7690625516837544\n",
      "validation AUC 0.7692429486064729\n",
      "validation AUC 0.7693977630685799\n",
      "validation AUC 0.7695492117675381\n",
      "validation AUC 0.7697211024888447\n",
      "validation AUC 0.7698742886133652\n",
      "validation AUC 0.7698742886133652\n",
      "train auc 0.8282563061917743\n",
      "Test AUC 0.7769076076202985\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99829465e-01\n",
      " 9.99829465e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.75343437e+01 1.65343437e+01 1.51776323e+01 ... 5.48049472e-02\n",
      " 5.44348098e-02 1.22505659e-02]\n",
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.035s | Train Loss: 0.218794 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.216141 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.019s | Train Loss: 0.213206 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.211275 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.209209 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.206541 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.204083 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.201872 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.200005 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.198389 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.196742 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.194141 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.192199 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.190527 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.188268 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.186380 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.184483 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.182873 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.180596 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.178392 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.176386 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.174004 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.172629 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.170827 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.168769 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.166899 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.164793 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.162539 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.161979 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.159838 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.157527 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.156165 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.153943 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.152474 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.150591 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.149626 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.146545 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.145671 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.144062 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.142217 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.140843 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.139289 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.137313 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.136179 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.133882 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.132299 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.132144 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.130174 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.129090 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.128396 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.126287 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.125107 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.123348 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.122167 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.121435 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.120102 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.119017 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.117281 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.116442 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.016s | Train Loss: 0.115289 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.113210 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.018s | Train Loss: 0.112798 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.111920 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.110201 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.109436 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.107808 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.107551 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.106233 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.105374 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.103873 |\n",
      "INFO:root:Pretraining Time: 1.234s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.109346\n",
      "INFO:root:Test AUC: 53.39%\n",
      "INFO:root:Test AUC: 53.39%\n",
      "INFO:root:Test Time: 0.220s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.414s | Train Loss: 1.709920 || Validation Loss: 0.012046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.385s | Train Loss: 1.609422 || Validation Loss: 0.011464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.428s | Train Loss: 1.482696 || Validation Loss: 0.011687 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.424s | Train Loss: 1.525327 || Validation Loss: 0.012864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.426s | Train Loss: 1.420393 || Validation Loss: 0.011228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.420s | Train Loss: 1.421573 || Validation Loss: 0.010865 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.420s | Train Loss: 1.372750 || Validation Loss: 0.012586 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.410s | Train Loss: 1.311192 || Validation Loss: 0.011775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.424s | Train Loss: 1.229599 || Validation Loss: 0.011633 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.394s | Train Loss: 1.147089 || Validation Loss: 0.010894 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.459s | Train Loss: 1.181792 || Validation Loss: 0.011571 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.434s | Train Loss: 1.130825 || Validation Loss: 0.011293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.417s | Train Loss: 1.125383 || Validation Loss: 0.011311 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.413s | Train Loss: 1.125220 || Validation Loss: 0.011065 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.415s | Train Loss: 1.096188 || Validation Loss: 0.011591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.418s | Train Loss: 1.106428 || Validation Loss: 0.010774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.414s | Train Loss: 1.061303 || Validation Loss: 0.011254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.418s | Train Loss: 1.036855 || Validation Loss: 0.011489 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.412s | Train Loss: 1.019031 || Validation Loss: 0.011350 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.417s | Train Loss: 0.991210 || Validation Loss: 0.011082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.417s | Train Loss: 0.991296 || Validation Loss: 0.010366 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.430s | Train Loss: 0.965776 || Validation Loss: 0.010661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.421s | Train Loss: 0.990737 || Validation Loss: 0.011592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.417s | Train Loss: 0.958005 || Validation Loss: 0.009984 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.452s | Train Loss: 0.929176 || Validation Loss: 0.009768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.424s | Train Loss: 0.927155 || Validation Loss: 0.010104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 0.892850 || Validation Loss: 0.010634 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.449s | Train Loss: 0.896821 || Validation Loss: 0.010072 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.419s | Train Loss: 0.892313 || Validation Loss: 0.011203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.448s | Train Loss: 0.886550 || Validation Loss: 0.010379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.448s | Train Loss: 0.891089 || Validation Loss: 0.011088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.471s | Train Loss: 0.866982 || Validation Loss: 0.011052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.430s | Train Loss: 0.870718 || Validation Loss: 0.010418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.426s | Train Loss: 0.812195 || Validation Loss: 0.010675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.460s | Train Loss: 0.831559 || Validation Loss: 0.010803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.421s | Train Loss: 0.778723 || Validation Loss: 0.010244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.416s | Train Loss: 0.853004 || Validation Loss: 0.009927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.464s | Train Loss: 0.831684 || Validation Loss: 0.009076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.425s | Train Loss: 0.780871 || Validation Loss: 0.010567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.456s | Train Loss: 0.823920 || Validation Loss: 0.009954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.426s | Train Loss: 0.766507 || Validation Loss: 0.010470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.447s | Train Loss: 0.758102 || Validation Loss: 0.009115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.428s | Train Loss: 0.771179 || Validation Loss: 0.010245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.454s | Train Loss: 0.751349 || Validation Loss: 0.010031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.421s | Train Loss: 0.752654 || Validation Loss: 0.009082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.455s | Train Loss: 0.750020 || Validation Loss: 0.010842 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.423s | Train Loss: 0.770712 || Validation Loss: 0.011220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.467s | Train Loss: 0.725461 || Validation Loss: 0.010133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.470s | Train Loss: 0.744908 || Validation Loss: 0.009647 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.432s | Train Loss: 0.761165 || Validation Loss: 0.010531 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.462s | Train Loss: 0.749854 || Validation Loss: 0.010258 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.429s | Train Loss: 0.727256 || Validation Loss: 0.009117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.462s | Train Loss: 0.734685 || Validation Loss: 0.009506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.432s | Train Loss: 0.716016 || Validation Loss: 0.009652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.461s | Train Loss: 0.728457 || Validation Loss: 0.009880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.435s | Train Loss: 0.751605 || Validation Loss: 0.010394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.468s | Train Loss: 0.744723 || Validation Loss: 0.010480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.459s | Train Loss: 0.724768 || Validation Loss: 0.009870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.464s | Train Loss: 0.724511 || Validation Loss: 0.009943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.433s | Train Loss: 0.746113 || Validation Loss: 0.009813 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.488s | Train Loss: 0.707379 || Validation Loss: 0.009939 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.459s | Train Loss: 0.737933 || Validation Loss: 0.010337 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.467s | Train Loss: 0.746218 || Validation Loss: 0.009757 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.459s | Train Loss: 0.740059 || Validation Loss: 0.008829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.476s | Train Loss: 0.731827 || Validation Loss: 0.010437 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.463s | Train Loss: 0.699495 || Validation Loss: 0.009820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.473s | Train Loss: 0.741732 || Validation Loss: 0.010119 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.471s | Train Loss: 0.733222 || Validation Loss: 0.009455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.468s | Train Loss: 0.709160 || Validation Loss: 0.009831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.461s | Train Loss: 0.749310 || Validation Loss: 0.010041 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.820s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.752%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008477\n",
      "INFO:root:Test AUC: 70.40%\n",
      "INFO:root:Test Time: 0.182s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 940\n",
      "len all outlier 16676 len chosen outlier 60\n",
      "final sizes 940 60 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 940, -1.0: 60})\n",
      "Resampled dataset shape Counter({1.0: 940, -1.0: 225})\n",
      "class weights 0.19313304721030042 0.8068669527896996\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5650857096241988\n",
      "validation AUC 0.5697768996580066\n",
      "validation AUC 0.574626344416175\n",
      "validation AUC 0.5794347268639304\n",
      "validation AUC 0.5844105487114528\n",
      "validation AUC 0.5890920112906587\n",
      "validation AUC 0.5939329950790149\n",
      "validation AUC 0.5988710565228968\n",
      "validation AUC 0.6036755517137667\n",
      "validation AUC 0.6081892582720081\n",
      "validation AUC 0.6127081398573833\n",
      "validation AUC 0.6169412215895\n",
      "validation AUC 0.6212170338798517\n",
      "validation AUC 0.6251734073102353\n",
      "validation AUC 0.6287643652899728\n",
      "validation AUC 0.6322306357136365\n",
      "validation AUC 0.6354346054303888\n",
      "validation AUC 0.6385125013755727\n",
      "validation AUC 0.6415891441393788\n",
      "validation AUC 0.6444343767181356\n",
      "validation AUC 0.6471456412808483\n",
      "validation AUC 0.6498422321805057\n",
      "validation AUC 0.6523527722074914\n",
      "validation AUC 0.6547917558438695\n",
      "validation AUC 0.6568650047477216\n",
      "validation AUC 0.6587900642884708\n",
      "validation AUC 0.6607010248735484\n",
      "validation AUC 0.6623097399278806\n",
      "validation AUC 0.6642134740997476\n",
      "validation AUC 0.6661638127833227\n",
      "validation AUC 0.6681220403900092\n",
      "validation AUC 0.6701815814584924\n",
      "validation AUC 0.6721046880584107\n",
      "validation AUC 0.6739389917248523\n",
      "validation AUC 0.6756028228136696\n",
      "validation AUC 0.6772318202494102\n",
      "validation AUC 0.6792405396246904\n",
      "validation AUC 0.6810236650664968\n",
      "validation AUC 0.6828096933127261\n",
      "validation AUC 0.6844680726634473\n",
      "validation AUC 0.6860918099302199\n",
      "validation AUC 0.6879889589133177\n",
      "validation AUC 0.6896863598311043\n",
      "validation AUC 0.6912237898844263\n",
      "validation AUC 0.6927079424347542\n",
      "validation AUC 0.6946215237919278\n",
      "validation AUC 0.6960119351823728\n",
      "validation AUC 0.6976398630238165\n",
      "validation AUC 0.6993826817889918\n",
      "validation AUC 0.6995506772713659\n",
      "validation AUC 0.6996957775822826\n",
      "validation AUC 0.6998233413359563\n",
      "validation AUC 0.699966182727574\n",
      "validation AUC 0.700093812998306\n",
      "validation AUC 0.7002075225790824\n",
      "validation AUC 0.7003068910820953\n",
      "validation AUC 0.7003962101879442\n",
      "validation AUC 0.7004635893072924\n",
      "validation AUC 0.7005657062951531\n",
      "validation AUC 0.7006610624892109\n",
      "validation AUC 0.7007741547916866\n",
      "validation AUC 0.7009076837951462\n",
      "validation AUC 0.7010531991725065\n",
      "validation AUC 0.7012082557567055\n",
      "validation AUC 0.7013588849655058\n",
      "validation AUC 0.7014996776317288\n",
      "validation AUC 0.7016382486282053\n",
      "validation AUC 0.7017775486516408\n",
      "validation AUC 0.7019238063593713\n",
      "validation AUC 0.7020484619872573\n",
      "validation AUC 0.7020484619872573\n",
      "train auc 0.7517432709605034\n",
      "Test AUC 0.703961927152764\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.68450971e-05 ... 9.97764093e-01\n",
      " 1.00000000e+00 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 0.99972004 1.        ] tresholds [11.87470627 10.87470627  9.0763483  ...  0.07981875  0.06191452\n",
      "  0.04184262]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.033s | Train Loss: 0.213838 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.019s | Train Loss: 0.211746 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.208789 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.206846 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.205422 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.202589 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.200074 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.198660 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.019s | Train Loss: 0.196469 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.193930 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.192682 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.190354 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.187926 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.186480 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.184226 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.182482 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.180704 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.178595 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.176448 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.174817 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.172463 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.019s | Train Loss: 0.170793 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.169196 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.167040 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.164957 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.163297 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.019s | Train Loss: 0.161338 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.159689 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.158238 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.156389 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.154845 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.152718 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.151294 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.149361 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.147543 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.019s | Train Loss: 0.145823 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.144814 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.142047 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.019s | Train Loss: 0.141594 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.140352 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.138522 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.137181 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.136037 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.134149 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.132918 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.131442 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.016s | Train Loss: 0.129748 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.016s | Train Loss: 0.129005 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.016s | Train Loss: 0.126591 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.125936 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.123323 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.016s | Train Loss: 0.123778 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.122221 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.120448 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.119110 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.016s | Train Loss: 0.118822 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.117367 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.016s | Train Loss: 0.115820 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.115254 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.114004 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.112761 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.111598 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.111144 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.109908 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.016s | Train Loss: 0.108792 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.015s | Train Loss: 0.107356 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.105852 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.105394 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.016s | Train Loss: 0.104382 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.102957 |\n",
      "INFO:root:Pretraining Time: 1.229s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.108871\n",
      "INFO:root:Test AUC: 51.16%\n",
      "INFO:root:Test AUC: 51.16%\n",
      "INFO:root:Test Time: 0.232s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.418s | Train Loss: 1.360730 || Validation Loss: 0.012397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.390s | Train Loss: 1.387583 || Validation Loss: 0.012729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.417s | Train Loss: 1.234332 || Validation Loss: 0.013033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.424s | Train Loss: 1.223104 || Validation Loss: 0.012635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.425s | Train Loss: 1.182681 || Validation Loss: 0.011785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.424s | Train Loss: 1.134178 || Validation Loss: 0.011881 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.425s | Train Loss: 1.134430 || Validation Loss: 0.012734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.419s | Train Loss: 1.094903 || Validation Loss: 0.011782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.421s | Train Loss: 1.058181 || Validation Loss: 0.012374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.395s | Train Loss: 1.059629 || Validation Loss: 0.011400 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.447s | Train Loss: 1.043402 || Validation Loss: 0.011050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.422s | Train Loss: 0.967422 || Validation Loss: 0.011774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.417s | Train Loss: 1.012418 || Validation Loss: 0.012583 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.419s | Train Loss: 0.961774 || Validation Loss: 0.012753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.421s | Train Loss: 0.954148 || Validation Loss: 0.010984 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.432s | Train Loss: 0.937900 || Validation Loss: 0.010679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.420s | Train Loss: 0.934352 || Validation Loss: 0.011463 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.429s | Train Loss: 0.917754 || Validation Loss: 0.010784 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.427s | Train Loss: 0.903650 || Validation Loss: 0.012132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.427s | Train Loss: 0.881336 || Validation Loss: 0.012168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.425s | Train Loss: 0.898690 || Validation Loss: 0.011766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.431s | Train Loss: 0.872431 || Validation Loss: 0.012000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.424s | Train Loss: 0.887416 || Validation Loss: 0.012165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.423s | Train Loss: 0.842417 || Validation Loss: 0.012200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.455s | Train Loss: 0.862369 || Validation Loss: 0.010793 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.427s | Train Loss: 0.846880 || Validation Loss: 0.011314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.426s | Train Loss: 0.816299 || Validation Loss: 0.011491 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.459s | Train Loss: 0.837297 || Validation Loss: 0.011772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.433s | Train Loss: 0.841176 || Validation Loss: 0.011841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.433s | Train Loss: 0.775517 || Validation Loss: 0.011365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.421s | Train Loss: 0.785900 || Validation Loss: 0.011449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.457s | Train Loss: 0.776695 || Validation Loss: 0.011486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.429s | Train Loss: 0.786644 || Validation Loss: 0.010579 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.425s | Train Loss: 0.746092 || Validation Loss: 0.011283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.460s | Train Loss: 0.774649 || Validation Loss: 0.011653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.427s | Train Loss: 0.774708 || Validation Loss: 0.011518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.429s | Train Loss: 0.726430 || Validation Loss: 0.011359 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.463s | Train Loss: 0.729550 || Validation Loss: 0.011106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.435s | Train Loss: 0.747356 || Validation Loss: 0.011452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.470s | Train Loss: 0.714921 || Validation Loss: 0.010898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.441s | Train Loss: 0.708792 || Validation Loss: 0.011234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.461s | Train Loss: 0.717384 || Validation Loss: 0.011858 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.433s | Train Loss: 0.731248 || Validation Loss: 0.010728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.466s | Train Loss: 0.686641 || Validation Loss: 0.011567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.435s | Train Loss: 0.696580 || Validation Loss: 0.011222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.467s | Train Loss: 0.694354 || Validation Loss: 0.011295 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.438s | Train Loss: 0.682506 || Validation Loss: 0.011638 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.453s | Train Loss: 0.659536 || Validation Loss: 0.010487 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.464s | Train Loss: 0.638022 || Validation Loss: 0.011194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.439s | Train Loss: 0.690080 || Validation Loss: 0.011618 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.466s | Train Loss: 0.684611 || Validation Loss: 0.011076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.444s | Train Loss: 0.708126 || Validation Loss: 0.010806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.469s | Train Loss: 0.673724 || Validation Loss: 0.012221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.441s | Train Loss: 0.699000 || Validation Loss: 0.010632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.470s | Train Loss: 0.657829 || Validation Loss: 0.010762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.444s | Train Loss: 0.666785 || Validation Loss: 0.011331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.475s | Train Loss: 0.669702 || Validation Loss: 0.011734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.478s | Train Loss: 0.683190 || Validation Loss: 0.012006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.475s | Train Loss: 0.663737 || Validation Loss: 0.012210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.442s | Train Loss: 0.678362 || Validation Loss: 0.011394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.474s | Train Loss: 0.668593 || Validation Loss: 0.010896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.478s | Train Loss: 0.693029 || Validation Loss: 0.012007 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.486s | Train Loss: 0.695424 || Validation Loss: 0.010995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.472s | Train Loss: 0.658601 || Validation Loss: 0.010607 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.468s | Train Loss: 0.707037 || Validation Loss: 0.011531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.477s | Train Loss: 0.657699 || Validation Loss: 0.011431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.476s | Train Loss: 0.680779 || Validation Loss: 0.011025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.479s | Train Loss: 0.687267 || Validation Loss: 0.011364 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.478s | Train Loss: 0.671739 || Validation Loss: 0.011278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.481s | Train Loss: 0.662037 || Validation Loss: 0.010221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.169s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.809%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008779\n",
      "INFO:root:Test AUC: 72.49%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 943\n",
      "len all outlier 16676 len chosen outlier 57\n",
      "final sizes 943 57 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 943, -1.0: 57})\n",
      "Resampled dataset shape Counter({1.0: 943, -1.0: 226})\n",
      "class weights 0.19332763045337895 0.806672369546621\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5516436258671962\n",
      "validation AUC 0.557595676008074\n",
      "validation AUC 0.563466013933887\n",
      "validation AUC 0.5695826006701088\n",
      "validation AUC 0.5758834183722882\n",
      "validation AUC 0.5818642650780336\n",
      "validation AUC 0.5876294634808579\n",
      "validation AUC 0.5936984905204146\n",
      "validation AUC 0.5995289315146755\n",
      "validation AUC 0.604601070062577\n",
      "validation AUC 0.6095476510112827\n",
      "validation AUC 0.6149038333621042\n",
      "validation AUC 0.620114739796975\n",
      "validation AUC 0.6245204173312165\n",
      "validation AUC 0.6289518715558797\n",
      "validation AUC 0.6333751468430578\n",
      "validation AUC 0.6382332521222134\n",
      "validation AUC 0.6422467504022419\n",
      "validation AUC 0.6460901219933493\n",
      "validation AUC 0.6502006739827626\n",
      "validation AUC 0.6540299572609275\n",
      "validation AUC 0.6576071887805199\n",
      "validation AUC 0.6610465064921713\n",
      "validation AUC 0.6645281982306248\n",
      "validation AUC 0.6677756594607585\n",
      "validation AUC 0.670870597076272\n",
      "validation AUC 0.6741520990761474\n",
      "validation AUC 0.6774073481234634\n",
      "validation AUC 0.6800911411411091\n",
      "validation AUC 0.6827952325042577\n",
      "validation AUC 0.685084983790059\n",
      "validation AUC 0.6875586999735848\n",
      "validation AUC 0.6902816635565063\n",
      "validation AUC 0.693227656175135\n",
      "validation AUC 0.6959067770346078\n",
      "validation AUC 0.6974773458863616\n",
      "validation AUC 0.69964806090536\n",
      "validation AUC 0.7019149090376566\n",
      "validation AUC 0.7040587006121484\n",
      "validation AUC 0.7061720353559982\n",
      "validation AUC 0.7082554054954371\n",
      "validation AUC 0.7102449041015589\n",
      "validation AUC 0.7120464946787418\n",
      "validation AUC 0.7139684358997991\n",
      "validation AUC 0.7156065248870648\n",
      "validation AUC 0.7171101270252315\n",
      "validation AUC 0.7186666607776897\n",
      "validation AUC 0.7203824709565237\n",
      "validation AUC 0.7219130657169379\n",
      "validation AUC 0.7220285074019119\n",
      "validation AUC 0.7221655458453659\n",
      "validation AUC 0.7223087224829573\n",
      "validation AUC 0.7224483737164601\n",
      "validation AUC 0.7225959138730744\n",
      "validation AUC 0.7227477297461945\n",
      "validation AUC 0.7229046275225662\n",
      "validation AUC 0.7230481420668138\n",
      "validation AUC 0.7231876549448355\n",
      "validation AUC 0.7233637788117308\n",
      "validation AUC 0.7235199449004616\n",
      "validation AUC 0.7236620173548858\n",
      "validation AUC 0.7237798004403111\n",
      "validation AUC 0.7239163280327574\n",
      "validation AUC 0.7240719806097985\n",
      "validation AUC 0.72425666123107\n",
      "validation AUC 0.7244456973893172\n",
      "validation AUC 0.7246317136731187\n",
      "validation AUC 0.724785011546297\n",
      "validation AUC 0.7248932188360133\n",
      "validation AUC 0.7250398783067759\n",
      "validation AUC 0.7250398783067759\n",
      "train auc 0.8085421651761651\n",
      "Test AUC 0.7248943384596125\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99621033e-01\n",
      " 9.99621033e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [19.22501945 18.22501945 13.46239567 ...  0.02757088  0.02720878\n",
      "  0.02101376]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.031s | Train Loss: 0.211906 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.015s | Train Loss: 0.210952 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.015s | Train Loss: 0.209265 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.016s | Train Loss: 0.206167 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.016s | Train Loss: 0.205353 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.015s | Train Loss: 0.202665 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.201217 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.199267 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.015s | Train Loss: 0.197422 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.016s | Train Loss: 0.195991 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.193848 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.191681 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.016s | Train Loss: 0.189943 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.015s | Train Loss: 0.188472 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.186453 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.184121 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.016s | Train Loss: 0.182933 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.015s | Train Loss: 0.181195 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.016s | Train Loss: 0.179283 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.177194 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.175783 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.015s | Train Loss: 0.174432 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.015s | Train Loss: 0.172944 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.170367 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.014s | Train Loss: 0.168751 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.016s | Train Loss: 0.166897 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.165824 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.014s | Train Loss: 0.163386 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.161974 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.160665 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.014s | Train Loss: 0.158887 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.157305 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.155150 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.015s | Train Loss: 0.153245 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.152325 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.150256 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.014s | Train Loss: 0.148920 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.016s | Train Loss: 0.147889 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.015s | Train Loss: 0.145988 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.015s | Train Loss: 0.144473 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.016s | Train Loss: 0.143348 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.015s | Train Loss: 0.141347 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.141109 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.014s | Train Loss: 0.138828 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.015s | Train Loss: 0.138445 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.015s | Train Loss: 0.136096 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.015s | Train Loss: 0.135703 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.133813 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.014s | Train Loss: 0.132296 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.015s | Train Loss: 0.130626 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.015s | Train Loss: 0.129987 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.014s | Train Loss: 0.128501 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.014s | Train Loss: 0.127735 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.014s | Train Loss: 0.125930 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.015s | Train Loss: 0.125400 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.015s | Train Loss: 0.123990 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.014s | Train Loss: 0.122123 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.014s | Train Loss: 0.121756 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.014s | Train Loss: 0.120293 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.014s | Train Loss: 0.119327 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.014s | Train Loss: 0.118815 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.014s | Train Loss: 0.117628 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.013s | Train Loss: 0.115943 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.013s | Train Loss: 0.114930 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.013s | Train Loss: 0.114030 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.013s | Train Loss: 0.113184 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.013s | Train Loss: 0.112531 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.014s | Train Loss: 0.110897 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.014s | Train Loss: 0.110509 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.014s | Train Loss: 0.109479 |\n",
      "INFO:root:Pretraining Time: 1.069s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.115254\n",
      "INFO:root:Test AUC: 50.56%\n",
      "INFO:root:Test AUC: 50.56%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.401s | Train Loss: 1.513354 || Validation Loss: 0.011855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.384s | Train Loss: 1.472054 || Validation Loss: 0.012234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.412s | Train Loss: 1.461755 || Validation Loss: 0.012846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.418s | Train Loss: 1.368369 || Validation Loss: 0.012324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.412s | Train Loss: 1.314824 || Validation Loss: 0.012176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.418s | Train Loss: 1.306591 || Validation Loss: 0.011791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.417s | Train Loss: 1.275017 || Validation Loss: 0.011709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.411s | Train Loss: 1.283517 || Validation Loss: 0.011817 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.413s | Train Loss: 1.254874 || Validation Loss: 0.011618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.388s | Train Loss: 1.204803 || Validation Loss: 0.012653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.443s | Train Loss: 1.162079 || Validation Loss: 0.012003 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.419s | Train Loss: 1.126039 || Validation Loss: 0.012178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.413s | Train Loss: 1.153773 || Validation Loss: 0.011910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.416s | Train Loss: 1.157399 || Validation Loss: 0.012470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.418s | Train Loss: 1.099746 || Validation Loss: 0.011801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.421s | Train Loss: 1.081579 || Validation Loss: 0.011892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.416s | Train Loss: 1.088404 || Validation Loss: 0.012201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.420s | Train Loss: 1.080469 || Validation Loss: 0.011879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.413s | Train Loss: 1.044727 || Validation Loss: 0.011838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.420s | Train Loss: 1.017585 || Validation Loss: 0.011262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.419s | Train Loss: 0.988071 || Validation Loss: 0.012332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.417s | Train Loss: 1.016469 || Validation Loss: 0.011661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.419s | Train Loss: 1.016725 || Validation Loss: 0.011563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.418s | Train Loss: 0.972827 || Validation Loss: 0.011375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.418s | Train Loss: 0.971408 || Validation Loss: 0.011056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.408s | Train Loss: 0.954385 || Validation Loss: 0.011241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.420s | Train Loss: 0.934537 || Validation Loss: 0.011345 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.420s | Train Loss: 0.963605 || Validation Loss: 0.012512 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.448s | Train Loss: 0.938980 || Validation Loss: 0.012307 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.420s | Train Loss: 0.931677 || Validation Loss: 0.011434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.421s | Train Loss: 0.948721 || Validation Loss: 0.012032 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.453s | Train Loss: 0.926651 || Validation Loss: 0.011765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.424s | Train Loss: 0.867830 || Validation Loss: 0.012467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.419s | Train Loss: 0.883785 || Validation Loss: 0.011214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.450s | Train Loss: 0.893893 || Validation Loss: 0.012129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.432s | Train Loss: 0.912502 || Validation Loss: 0.011871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.425s | Train Loss: 0.877185 || Validation Loss: 0.011562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.456s | Train Loss: 0.851969 || Validation Loss: 0.011199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.426s | Train Loss: 0.892137 || Validation Loss: 0.011025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.428s | Train Loss: 0.868790 || Validation Loss: 0.010781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.456s | Train Loss: 0.825417 || Validation Loss: 0.011318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.429s | Train Loss: 0.855529 || Validation Loss: 0.010613 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.458s | Train Loss: 0.883662 || Validation Loss: 0.011696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.430s | Train Loss: 0.848533 || Validation Loss: 0.012228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.429s | Train Loss: 0.856241 || Validation Loss: 0.011639 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.461s | Train Loss: 0.849424 || Validation Loss: 0.010910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.426s | Train Loss: 0.808378 || Validation Loss: 0.012139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.460s | Train Loss: 0.835791 || Validation Loss: 0.010905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.427s | Train Loss: 0.769754 || Validation Loss: 0.011070 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.459s | Train Loss: 0.832442 || Validation Loss: 0.011118 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.457s | Train Loss: 0.815285 || Validation Loss: 0.011058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.775217 || Validation Loss: 0.011603 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.431s | Train Loss: 0.801394 || Validation Loss: 0.011635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.458s | Train Loss: 0.825400 || Validation Loss: 0.012043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.458s | Train Loss: 0.810006 || Validation Loss: 0.011313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.429s | Train Loss: 0.782403 || Validation Loss: 0.011226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.427s | Train Loss: 0.786332 || Validation Loss: 0.011170 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.459s | Train Loss: 0.788710 || Validation Loss: 0.011159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.463s | Train Loss: 0.804012 || Validation Loss: 0.010573 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.435s | Train Loss: 0.785751 || Validation Loss: 0.010937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.459s | Train Loss: 0.776568 || Validation Loss: 0.011186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.431s | Train Loss: 0.788677 || Validation Loss: 0.010965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.459s | Train Loss: 0.817621 || Validation Loss: 0.011295 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.434s | Train Loss: 0.789637 || Validation Loss: 0.011597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.464s | Train Loss: 0.817660 || Validation Loss: 0.011104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.435s | Train Loss: 0.826035 || Validation Loss: 0.011700 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.462s | Train Loss: 0.785104 || Validation Loss: 0.011567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.436s | Train Loss: 0.778705 || Validation Loss: 0.010536 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.469s | Train Loss: 0.791641 || Validation Loss: 0.011715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.463s | Train Loss: 0.825263 || Validation Loss: 0.011148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.332s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.706%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008755\n",
      "INFO:root:Test AUC: 65.69%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 100\n",
      "i index 1\n",
      "n cluster 100\n",
      "i index 2\n",
      "n cluster 100\n",
      "i index 3\n",
      "n cluster 100\n",
      "i index 4\n",
      "n cluster 100\n",
      "i index 5\n",
      "n cluster 100\n",
      "i index 6\n",
      "n cluster 100\n",
      "i index 7\n",
      "n cluster 100\n",
      "i index 8\n",
      "n cluster 100\n",
      "i index 9\n",
      "n cluster 100\n",
      "size of chosen indexes of normal 921\n",
      "len all outlier 16676 len chosen outlier 79\n",
      "final sizes 921 79 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 921, -1.0: 79})\n",
      "Resampled dataset shape Counter({1.0: 921, -1.0: 221})\n",
      "class weights 0.19352014010507881 0.8064798598949212\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5225931361207209\n",
      "validation AUC 0.5258132481971749\n",
      "validation AUC 0.5288940070365469\n",
      "validation AUC 0.5316629152819589\n",
      "validation AUC 0.534952689343201\n",
      "validation AUC 0.5383772216431395\n",
      "validation AUC 0.5417710576510262\n",
      "validation AUC 0.5450729165314153\n",
      "validation AUC 0.5485382557162631\n",
      "validation AUC 0.551781031484812\n",
      "validation AUC 0.554955292022657\n",
      "validation AUC 0.558240537602572\n",
      "validation AUC 0.5615674148789187\n",
      "validation AUC 0.5646436239515048\n",
      "validation AUC 0.5676770811803978\n",
      "validation AUC 0.5705397731566101\n",
      "validation AUC 0.5735475308548686\n",
      "validation AUC 0.5769425322416163\n",
      "validation AUC 0.5798129934102348\n",
      "validation AUC 0.5825840860758406\n",
      "validation AUC 0.5856861968909183\n",
      "validation AUC 0.5888340767634523\n",
      "validation AUC 0.591436146923304\n",
      "validation AUC 0.5942490069801405\n",
      "validation AUC 0.5964323203299757\n",
      "validation AUC 0.5988046379098702\n",
      "validation AUC 0.6010562137255875\n",
      "validation AUC 0.6032696593028203\n",
      "validation AUC 0.6058456015621824\n",
      "validation AUC 0.6083961955009778\n",
      "validation AUC 0.6107127318758045\n",
      "validation AUC 0.6128797778138685\n",
      "validation AUC 0.6151100894563971\n",
      "validation AUC 0.6176165958889691\n",
      "validation AUC 0.6203390858704358\n",
      "validation AUC 0.6228815461033457\n",
      "validation AUC 0.6254033009063561\n",
      "validation AUC 0.6278968551373412\n",
      "validation AUC 0.6301504051793481\n",
      "validation AUC 0.6324065414045805\n",
      "validation AUC 0.6348600044742034\n",
      "validation AUC 0.6370003238582534\n",
      "validation AUC 0.6388109208451263\n",
      "validation AUC 0.6403830275712674\n",
      "validation AUC 0.6425316136953195\n",
      "validation AUC 0.6452817002739225\n",
      "validation AUC 0.6483276520670416\n",
      "validation AUC 0.6510063685027999\n",
      "validation AUC 0.6536563415873375\n",
      "validation AUC 0.6539054719167109\n",
      "validation AUC 0.6541444730284504\n",
      "validation AUC 0.6543443248603728\n",
      "validation AUC 0.6545484231412952\n",
      "validation AUC 0.6547623446613835\n",
      "validation AUC 0.6549734245727423\n",
      "validation AUC 0.6551600315280214\n",
      "validation AUC 0.655364733783833\n",
      "validation AUC 0.6555565796226217\n",
      "validation AUC 0.6557813114950203\n",
      "validation AUC 0.6560437851462833\n",
      "validation AUC 0.6562875063084779\n",
      "validation AUC 0.6565330260919282\n",
      "validation AUC 0.6567489617485408\n",
      "validation AUC 0.6569765033014812\n",
      "validation AUC 0.6572005300930621\n",
      "validation AUC 0.6574468826700819\n",
      "validation AUC 0.657712863100657\n",
      "validation AUC 0.6579559323956805\n",
      "validation AUC 0.6582005768146438\n",
      "validation AUC 0.6584100975661569\n",
      "validation AUC 0.6584100975661569\n",
      "train auc 0.7060005085302741\n",
      "Test AUC 0.6569182037458564\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 9.47418285e-05 ... 9.99450497e-01\n",
      " 9.99450497e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [16.74764633 15.74764633 12.26407051 ...  0.06126617  0.06090124\n",
      "  0.02062322]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "   python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1k_points_by_10_k_means\" --iteration_num $i --experiment_method 2 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.00 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 1000 --k_means_chosen_k 10 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fcac1dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.215196 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.213604 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.211412 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.017s | Train Loss: 0.209018 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.206907 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.204641 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.202543 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.017s | Train Loss: 0.200793 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.198419 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.017s | Train Loss: 0.196584 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.017s | Train Loss: 0.194419 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.017s | Train Loss: 0.192260 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.190164 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.188626 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.186281 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.183989 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.182242 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.180483 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.178149 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.176404 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.174170 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.017s | Train Loss: 0.172637 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.169959 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.168310 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.166051 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.164646 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.017s | Train Loss: 0.162255 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.159996 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.158660 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.156976 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.154612 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.154059 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.151941 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.150424 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.147590 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.147341 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.144096 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.142998 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.142015 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.140181 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.137628 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.136736 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.133600 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.132737 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.132308 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.018s | Train Loss: 0.130654 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.128566 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.127568 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.127455 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.018s | Train Loss: 0.125091 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.125043 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.122388 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.120154 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.018s | Train Loss: 0.119739 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.118289 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.019s | Train Loss: 0.116210 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.116720 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.016s | Train Loss: 0.115175 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.114677 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.112619 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.112494 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.016s | Train Loss: 0.110723 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.108917 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.016s | Train Loss: 0.108733 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.016s | Train Loss: 0.108249 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.106741 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.016s | Train Loss: 0.105642 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.104430 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.016s | Train Loss: 0.104253 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.015s | Train Loss: 0.102807 |\n",
      "INFO:root:Pretraining Time: 1.213s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.104056\n",
      "INFO:root:Test AUC: 56.50%\n",
      "INFO:root:Test AUC: 56.50%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.420s | Train Loss: 1.268238 || Validation Loss: 0.014409 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.385s | Train Loss: 1.233811 || Validation Loss: 0.015051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.415s | Train Loss: 1.176402 || Validation Loss: 0.015837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.410s | Train Loss: 1.093453 || Validation Loss: 0.014499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.416s | Train Loss: 1.059307 || Validation Loss: 0.014186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.412s | Train Loss: 1.065109 || Validation Loss: 0.013093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.418s | Train Loss: 1.012223 || Validation Loss: 0.014061 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.424s | Train Loss: 1.016767 || Validation Loss: 0.012910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.417s | Train Loss: 0.969846 || Validation Loss: 0.013840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.392s | Train Loss: 0.963614 || Validation Loss: 0.012958 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.450s | Train Loss: 0.916001 || Validation Loss: 0.012567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.418s | Train Loss: 0.909178 || Validation Loss: 0.012637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.420s | Train Loss: 0.893996 || Validation Loss: 0.012695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.419s | Train Loss: 0.907662 || Validation Loss: 0.012932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.421s | Train Loss: 0.880240 || Validation Loss: 0.011271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.423s | Train Loss: 0.851880 || Validation Loss: 0.012652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.419s | Train Loss: 0.841006 || Validation Loss: 0.012593 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.420s | Train Loss: 0.813249 || Validation Loss: 0.012626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.425s | Train Loss: 0.834736 || Validation Loss: 0.012185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.421s | Train Loss: 0.798693 || Validation Loss: 0.011763 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.419s | Train Loss: 0.806946 || Validation Loss: 0.012390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.422s | Train Loss: 0.785987 || Validation Loss: 0.011996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.421s | Train Loss: 0.770375 || Validation Loss: 0.012306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.426s | Train Loss: 0.736567 || Validation Loss: 0.011993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.451s | Train Loss: 0.755142 || Validation Loss: 0.011574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.421s | Train Loss: 0.755947 || Validation Loss: 0.011349 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.420s | Train Loss: 0.734076 || Validation Loss: 0.011402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.451s | Train Loss: 0.740366 || Validation Loss: 0.011172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.423s | Train Loss: 0.699509 || Validation Loss: 0.012547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.424s | Train Loss: 0.692720 || Validation Loss: 0.011265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.424s | Train Loss: 0.692975 || Validation Loss: 0.011045 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.456s | Train Loss: 0.699562 || Validation Loss: 0.011897 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.426s | Train Loss: 0.657479 || Validation Loss: 0.011451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.425s | Train Loss: 0.651097 || Validation Loss: 0.011485 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.457s | Train Loss: 0.657152 || Validation Loss: 0.011398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.426s | Train Loss: 0.677860 || Validation Loss: 0.011568 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.425s | Train Loss: 0.644760 || Validation Loss: 0.012551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.454s | Train Loss: 0.622227 || Validation Loss: 0.011515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.428s | Train Loss: 0.638822 || Validation Loss: 0.012203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.455s | Train Loss: 0.653116 || Validation Loss: 0.012334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.429s | Train Loss: 0.624798 || Validation Loss: 0.011740 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.457s | Train Loss: 0.614173 || Validation Loss: 0.010948 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.430s | Train Loss: 0.618291 || Validation Loss: 0.011776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.465s | Train Loss: 0.626963 || Validation Loss: 0.012023 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.429s | Train Loss: 0.620486 || Validation Loss: 0.011627 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.460s | Train Loss: 0.598873 || Validation Loss: 0.011813 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.435s | Train Loss: 0.612142 || Validation Loss: 0.011688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.465s | Train Loss: 0.590471 || Validation Loss: 0.012446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.463s | Train Loss: 0.555841 || Validation Loss: 0.011547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.432s | Train Loss: 0.572841 || Validation Loss: 0.011983 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.468s | Train Loss: 0.598490 || Validation Loss: 0.012541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.573716 || Validation Loss: 0.010734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.463s | Train Loss: 0.584222 || Validation Loss: 0.011674 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.436s | Train Loss: 0.567911 || Validation Loss: 0.010910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.467s | Train Loss: 0.597894 || Validation Loss: 0.012621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.436s | Train Loss: 0.578109 || Validation Loss: 0.011636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.465s | Train Loss: 0.570198 || Validation Loss: 0.011489 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.473s | Train Loss: 0.613902 || Validation Loss: 0.011813 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.460s | Train Loss: 0.556693 || Validation Loss: 0.010649 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.437s | Train Loss: 0.586870 || Validation Loss: 0.011424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.470s | Train Loss: 0.586025 || Validation Loss: 0.012109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.472s | Train Loss: 0.560297 || Validation Loss: 0.011946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.473s | Train Loss: 0.602611 || Validation Loss: 0.011468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.469s | Train Loss: 0.577428 || Validation Loss: 0.012558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.469s | Train Loss: 0.571455 || Validation Loss: 0.012286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.456s | Train Loss: 0.575320 || Validation Loss: 0.011666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.509s | Train Loss: 0.577765 || Validation Loss: 0.012529 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.509s | Train Loss: 0.583078 || Validation Loss: 0.012805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.504s | Train Loss: 0.593308 || Validation Loss: 0.011546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.510s | Train Loss: 0.583413 || Validation Loss: 0.011409 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.984s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.874%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.009829\n",
      "INFO:root:Test AUC: 79.76%\n",
      "INFO:root:Test Time: 0.189s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 936\n",
      "len all outlier 16676 len chosen outlier 64\n",
      "final sizes 936 64 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 936, -1.0: 64})\n",
      "Resampled dataset shape Counter({1.0: 936, -1.0: 224})\n",
      "class weights 0.19310344827586207 0.8068965517241379\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5965552677998052\n",
      "validation AUC 0.6047331303693899\n",
      "validation AUC 0.6132836507669257\n",
      "validation AUC 0.6211037819364361\n",
      "validation AUC 0.6294456223580754\n",
      "validation AUC 0.6378398516148107\n",
      "validation AUC 0.6460048657494195\n",
      "validation AUC 0.6534726826680811\n",
      "validation AUC 0.6612134855938951\n",
      "validation AUC 0.6688883195619962\n",
      "validation AUC 0.676266439557918\n",
      "validation AUC 0.6835041420438256\n",
      "validation AUC 0.6904569067375501\n",
      "validation AUC 0.6969370597501982\n",
      "validation AUC 0.7032747199206137\n",
      "validation AUC 0.70880046223502\n",
      "validation AUC 0.7141105083542232\n",
      "validation AUC 0.7193572461980445\n",
      "validation AUC 0.7242919711462836\n",
      "validation AUC 0.7284200011579289\n",
      "validation AUC 0.732824100907548\n",
      "validation AUC 0.7370925776967133\n",
      "validation AUC 0.7410423047426343\n",
      "validation AUC 0.744164870883472\n",
      "validation AUC 0.7474949383179337\n",
      "validation AUC 0.7507641825543103\n",
      "validation AUC 0.7539499186150489\n",
      "validation AUC 0.7566240746412921\n",
      "validation AUC 0.759019423087443\n",
      "validation AUC 0.761536332787929\n",
      "validation AUC 0.7640640182518551\n",
      "validation AUC 0.7663894253415731\n",
      "validation AUC 0.7684866363504974\n",
      "validation AUC 0.7705129508180322\n",
      "validation AUC 0.7721467960169799\n",
      "validation AUC 0.773579517577851\n",
      "validation AUC 0.7752837750740043\n",
      "validation AUC 0.7771436399155989\n",
      "validation AUC 0.7788390240361786\n",
      "validation AUC 0.7803712125453087\n",
      "validation AUC 0.7817931024684108\n",
      "validation AUC 0.7834192210458695\n",
      "validation AUC 0.7850274279098766\n",
      "validation AUC 0.7861719470213449\n",
      "validation AUC 0.7871796352225809\n",
      "validation AUC 0.7881598039864683\n",
      "validation AUC 0.7890098387775507\n",
      "validation AUC 0.7899517309654254\n",
      "validation AUC 0.7913143994212057\n",
      "validation AUC 0.7914274172245763\n",
      "validation AUC 0.7915362737207812\n",
      "validation AUC 0.7916300654336285\n",
      "validation AUC 0.7916981842226647\n",
      "validation AUC 0.7917951634329443\n",
      "validation AUC 0.7919015295304874\n",
      "validation AUC 0.7920009273010059\n",
      "validation AUC 0.7921253461281644\n",
      "validation AUC 0.7922260316689311\n",
      "validation AUC 0.7923086804441765\n",
      "validation AUC 0.7924220707430732\n",
      "validation AUC 0.7925490278611753\n",
      "validation AUC 0.7926619285945233\n",
      "validation AUC 0.7927896732745954\n",
      "validation AUC 0.7928779733391116\n",
      "validation AUC 0.7929644614789605\n",
      "validation AUC 0.7930526817230068\n",
      "validation AUC 0.7931148086554338\n",
      "validation AUC 0.7931801337280224\n",
      "validation AUC 0.7932482019640943\n",
      "validation AUC 0.7933201893852397\n",
      "validation AUC 0.7933201893852397\n",
      "train auc 0.8736583606901123\n",
      "Test AUC 0.7975958571403563\n",
      "test false positive rates [0.         0.         0.         ... 0.99910943 0.99910943 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [20.16041565 19.16041565 18.6000843  ...  0.04815223  0.04797972\n",
      "  0.02176261]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.037s | Train Loss: 0.204705 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.203032 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.016s | Train Loss: 0.201461 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.016s | Train Loss: 0.199292 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.016s | Train Loss: 0.198125 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.016s | Train Loss: 0.195788 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.193520 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.192089 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.016s | Train Loss: 0.190405 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.015s | Train Loss: 0.187989 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.186422 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.185336 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.015s | Train Loss: 0.183536 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.181406 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.179457 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.178158 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.016s | Train Loss: 0.176521 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.174366 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.172709 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.171265 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.169822 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.168290 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.165552 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.164462 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.162357 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.015s | Train Loss: 0.160429 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.159424 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.016s | Train Loss: 0.157577 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.015s | Train Loss: 0.156905 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.154956 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.153433 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.151037 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.150588 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.016s | Train Loss: 0.149349 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.147479 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.015s | Train Loss: 0.147017 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.015s | Train Loss: 0.145277 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.015s | Train Loss: 0.143463 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.015s | Train Loss: 0.142802 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.015s | Train Loss: 0.141715 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.015s | Train Loss: 0.140261 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.016s | Train Loss: 0.139135 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.016s | Train Loss: 0.136643 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.136354 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.134182 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.133394 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.016s | Train Loss: 0.132945 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.131203 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.015s | Train Loss: 0.129154 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.015s | Train Loss: 0.128099 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.126251 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.016s | Train Loss: 0.126204 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.015s | Train Loss: 0.124251 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.122969 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.015s | Train Loss: 0.121979 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.015s | Train Loss: 0.121588 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.121133 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.015s | Train Loss: 0.120263 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.015s | Train Loss: 0.117700 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.117058 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.014s | Train Loss: 0.115059 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.015s | Train Loss: 0.114793 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.015s | Train Loss: 0.113778 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.112517 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.111772 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.015s | Train Loss: 0.110540 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.108960 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.015s | Train Loss: 0.108455 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.107336 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.108389 |\n",
      "INFO:root:Pretraining Time: 1.128s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.109144\n",
      "INFO:root:Test AUC: 52.50%\n",
      "INFO:root:Test AUC: 52.50%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.424s | Train Loss: 1.718870 || Validation Loss: 0.011790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.395s | Train Loss: 1.634255 || Validation Loss: 0.011465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.421s | Train Loss: 1.511351 || Validation Loss: 0.011340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.429s | Train Loss: 1.431639 || Validation Loss: 0.011805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.412s | Train Loss: 1.414555 || Validation Loss: 0.010599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.421s | Train Loss: 1.371988 || Validation Loss: 0.011510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.414s | Train Loss: 1.333129 || Validation Loss: 0.011182 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.406s | Train Loss: 1.279629 || Validation Loss: 0.010367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.417s | Train Loss: 1.254797 || Validation Loss: 0.011746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.381s | Train Loss: 1.245491 || Validation Loss: 0.011729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.434s | Train Loss: 1.188753 || Validation Loss: 0.011258 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.398s | Train Loss: 1.151687 || Validation Loss: 0.010944 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.422s | Train Loss: 1.134287 || Validation Loss: 0.011559 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.417s | Train Loss: 1.153510 || Validation Loss: 0.011692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.422s | Train Loss: 1.137713 || Validation Loss: 0.011739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.421s | Train Loss: 1.072724 || Validation Loss: 0.011899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.412s | Train Loss: 1.058430 || Validation Loss: 0.011390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.410s | Train Loss: 1.090810 || Validation Loss: 0.010574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.416s | Train Loss: 1.035993 || Validation Loss: 0.010745 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.416s | Train Loss: 0.985986 || Validation Loss: 0.010800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.413s | Train Loss: 1.025672 || Validation Loss: 0.010520 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.414s | Train Loss: 0.992721 || Validation Loss: 0.011286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.422s | Train Loss: 1.002573 || Validation Loss: 0.010499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.414s | Train Loss: 0.955804 || Validation Loss: 0.011352 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.424s | Train Loss: 0.964042 || Validation Loss: 0.011132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.422s | Train Loss: 0.925909 || Validation Loss: 0.011765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.424s | Train Loss: 0.923836 || Validation Loss: 0.010998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.427s | Train Loss: 0.894512 || Validation Loss: 0.010774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.468s | Train Loss: 0.893698 || Validation Loss: 0.010919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.433s | Train Loss: 0.897545 || Validation Loss: 0.010186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.427s | Train Loss: 0.897136 || Validation Loss: 0.011283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.461s | Train Loss: 0.894273 || Validation Loss: 0.010260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.424s | Train Loss: 0.871596 || Validation Loss: 0.010638 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.422s | Train Loss: 0.857820 || Validation Loss: 0.010263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.456s | Train Loss: 0.827483 || Validation Loss: 0.009984 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.427s | Train Loss: 0.824394 || Validation Loss: 0.010458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.429s | Train Loss: 0.837394 || Validation Loss: 0.010144 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.462s | Train Loss: 0.804963 || Validation Loss: 0.011044 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.428s | Train Loss: 0.820755 || Validation Loss: 0.010201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.430s | Train Loss: 0.807100 || Validation Loss: 0.010853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.462s | Train Loss: 0.787639 || Validation Loss: 0.010979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.423s | Train Loss: 0.807831 || Validation Loss: 0.010561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.459s | Train Loss: 0.795414 || Validation Loss: 0.010160 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.422s | Train Loss: 0.776018 || Validation Loss: 0.011111 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.424s | Train Loss: 0.762295 || Validation Loss: 0.009801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.464s | Train Loss: 0.768693 || Validation Loss: 0.010155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.431s | Train Loss: 0.748419 || Validation Loss: 0.010591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.458s | Train Loss: 0.757512 || Validation Loss: 0.010879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.434s | Train Loss: 0.744793 || Validation Loss: 0.010327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.461s | Train Loss: 0.744189 || Validation Loss: 0.011675 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.454s | Train Loss: 0.744942 || Validation Loss: 0.010351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.435s | Train Loss: 0.734139 || Validation Loss: 0.010688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.413s | Train Loss: 0.752069 || Validation Loss: 0.010602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.457s | Train Loss: 0.735849 || Validation Loss: 0.010781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.455s | Train Loss: 0.764836 || Validation Loss: 0.010665 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.427s | Train Loss: 0.774422 || Validation Loss: 0.010923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.425s | Train Loss: 0.717615 || Validation Loss: 0.010654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.467s | Train Loss: 0.756324 || Validation Loss: 0.010074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.461s | Train Loss: 0.742907 || Validation Loss: 0.010841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.432s | Train Loss: 0.761584 || Validation Loss: 0.011026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.461s | Train Loss: 0.737878 || Validation Loss: 0.010809 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.437s | Train Loss: 0.749021 || Validation Loss: 0.010832 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.470s | Train Loss: 0.769981 || Validation Loss: 0.011141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.430s | Train Loss: 0.760099 || Validation Loss: 0.009919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.471s | Train Loss: 0.747886 || Validation Loss: 0.010293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.434s | Train Loss: 0.764187 || Validation Loss: 0.010767 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.470s | Train Loss: 0.730938 || Validation Loss: 0.011289 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.436s | Train Loss: 0.753582 || Validation Loss: 0.010546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.473s | Train Loss: 0.734325 || Validation Loss: 0.009920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.475s | Train Loss: 0.753055 || Validation Loss: 0.010573 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.477s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.745%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008466\n",
      "INFO:root:Test AUC: 73.33%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 914\n",
      "len all outlier 16676 len chosen outlier 86\n",
      "final sizes 914 86 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 914, -1.0: 86})\n",
      "Resampled dataset shape Counter({1.0: 914, -1.0: 219})\n",
      "class weights 0.19329214474845544 0.8067078552515445\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5285745149629324\n",
      "validation AUC 0.5351791666232089\n",
      "validation AUC 0.5415928398271366\n",
      "validation AUC 0.5480520346450639\n",
      "validation AUC 0.5545129988167413\n",
      "validation AUC 0.5607339534780759\n",
      "validation AUC 0.566973854858289\n",
      "validation AUC 0.573253629223913\n",
      "validation AUC 0.5794444170689795\n",
      "validation AUC 0.5855955394618568\n",
      "validation AUC 0.5911949321238653\n",
      "validation AUC 0.5966959646814641\n",
      "validation AUC 0.6020421614914976\n",
      "validation AUC 0.6073205188841396\n",
      "validation AUC 0.6126107562233892\n",
      "validation AUC 0.6182703441624523\n",
      "validation AUC 0.6239888263049104\n",
      "validation AUC 0.6296132080953708\n",
      "validation AUC 0.6350583051283268\n",
      "validation AUC 0.6402126321534307\n",
      "validation AUC 0.6450589293244017\n",
      "validation AUC 0.6497153894717439\n",
      "validation AUC 0.6543464267994141\n",
      "validation AUC 0.6587946539654916\n",
      "validation AUC 0.663102852081175\n",
      "validation AUC 0.667061511037681\n",
      "validation AUC 0.6709265565470559\n",
      "validation AUC 0.6748209094723058\n",
      "validation AUC 0.6782115686254895\n",
      "validation AUC 0.6814740706928395\n",
      "validation AUC 0.6847414630943138\n",
      "validation AUC 0.6879088697144747\n",
      "validation AUC 0.6910750550814457\n",
      "validation AUC 0.6944376573394495\n",
      "validation AUC 0.6979485899767202\n",
      "validation AUC 0.701364240919038\n",
      "validation AUC 0.7044665220177848\n",
      "validation AUC 0.7074292769350983\n",
      "validation AUC 0.7098509900824662\n",
      "validation AUC 0.7122463199038408\n",
      "validation AUC 0.714578176487529\n",
      "validation AUC 0.7169774787076235\n",
      "validation AUC 0.7193900790180081\n",
      "validation AUC 0.7216447279218177\n",
      "validation AUC 0.7238313006075082\n",
      "validation AUC 0.7260571236789446\n",
      "validation AUC 0.7283180438110083\n",
      "validation AUC 0.7304564368610504\n",
      "validation AUC 0.7323639332377332\n",
      "validation AUC 0.732572929834827\n",
      "validation AUC 0.7327747532323566\n",
      "validation AUC 0.7329699089599648\n",
      "validation AUC 0.7331458545611439\n",
      "validation AUC 0.7333213318822327\n",
      "validation AUC 0.7335347398906311\n",
      "validation AUC 0.7337382315359822\n",
      "validation AUC 0.733940049612147\n",
      "validation AUC 0.7341536731358141\n",
      "validation AUC 0.7343568481599678\n",
      "validation AUC 0.734560765514492\n",
      "validation AUC 0.7347625702872452\n",
      "validation AUC 0.7349817679403956\n",
      "validation AUC 0.7352049512956778\n",
      "validation AUC 0.7354203840833301\n",
      "validation AUC 0.7356278454667187\n",
      "validation AUC 0.7358286445015509\n",
      "validation AUC 0.7360327401217911\n",
      "validation AUC 0.7362428621875109\n",
      "validation AUC 0.7364380365398953\n",
      "validation AUC 0.7366555419990835\n",
      "validation AUC 0.7366555419990835\n",
      "train auc 0.7450970129714771\n",
      "Test AUC 0.7332559387371541\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 3.78967314e-05 ... 9.99829465e-01\n",
      " 9.99829465e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [11.65506554 10.65506554 10.47407532 ...  0.03450779  0.03421074\n",
      "  0.01626392]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.218568 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.017s | Train Loss: 0.216211 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.015s | Train Loss: 0.214541 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.015s | Train Loss: 0.212331 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.209782 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.015s | Train Loss: 0.208087 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.206260 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.204961 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.016s | Train Loss: 0.202848 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.016s | Train Loss: 0.200542 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.015s | Train Loss: 0.198888 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.197188 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.015s | Train Loss: 0.195255 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.193308 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.191180 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.189457 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.015s | Train Loss: 0.187773 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.186950 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.016s | Train Loss: 0.184124 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.015s | Train Loss: 0.183199 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.015s | Train Loss: 0.181297 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.178885 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.177322 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.175665 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.173995 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.016s | Train Loss: 0.172015 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.170516 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.169263 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.167159 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.165647 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.165193 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.162797 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.160270 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.016s | Train Loss: 0.159500 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.157158 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.155599 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.016s | Train Loss: 0.154374 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.015s | Train Loss: 0.152732 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.151450 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.150368 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.016s | Train Loss: 0.148346 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.147278 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.016s | Train Loss: 0.146329 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.144198 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.142556 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.141973 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.015s | Train Loss: 0.140604 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.138843 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.015s | Train Loss: 0.138187 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.136575 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.017s | Train Loss: 0.135515 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.016s | Train Loss: 0.133926 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.132691 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.131753 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.130214 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.016s | Train Loss: 0.129548 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.015s | Train Loss: 0.128423 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.016s | Train Loss: 0.126110 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.015s | Train Loss: 0.125616 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.124714 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.015s | Train Loss: 0.123460 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.016s | Train Loss: 0.122650 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.015s | Train Loss: 0.121048 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.119996 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.118975 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.015s | Train Loss: 0.117865 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.116882 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.015s | Train Loss: 0.115268 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.113998 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.015s | Train Loss: 0.113295 |\n",
      "INFO:root:Pretraining Time: 1.140s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.117493\n",
      "INFO:root:Test AUC: 49.05%\n",
      "INFO:root:Test AUC: 49.05%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.414s | Train Loss: 1.547799 || Validation Loss: 0.012046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.379s | Train Loss: 1.469473 || Validation Loss: 0.012105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.411s | Train Loss: 1.454888 || Validation Loss: 0.012511 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.410s | Train Loss: 1.397548 || Validation Loss: 0.012106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.412s | Train Loss: 1.303416 || Validation Loss: 0.012191 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.412s | Train Loss: 1.268423 || Validation Loss: 0.012011 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.412s | Train Loss: 1.218930 || Validation Loss: 0.012814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.415s | Train Loss: 1.216967 || Validation Loss: 0.012285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.415s | Train Loss: 1.178226 || Validation Loss: 0.011591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.384s | Train Loss: 1.094329 || Validation Loss: 0.011479 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.447s | Train Loss: 1.071203 || Validation Loss: 0.011814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.413s | Train Loss: 1.066075 || Validation Loss: 0.011748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.415s | Train Loss: 1.054835 || Validation Loss: 0.010896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.414s | Train Loss: 1.059492 || Validation Loss: 0.012149 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.416s | Train Loss: 1.038620 || Validation Loss: 0.011236 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.416s | Train Loss: 0.978272 || Validation Loss: 0.011839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.418s | Train Loss: 1.004396 || Validation Loss: 0.012116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.417s | Train Loss: 0.929372 || Validation Loss: 0.011800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.417s | Train Loss: 0.898990 || Validation Loss: 0.011380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.418s | Train Loss: 0.934456 || Validation Loss: 0.012074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.420s | Train Loss: 0.939710 || Validation Loss: 0.010852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.420s | Train Loss: 0.958601 || Validation Loss: 0.010912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.419s | Train Loss: 0.902058 || Validation Loss: 0.010764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.418s | Train Loss: 0.860581 || Validation Loss: 0.010724 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.419s | Train Loss: 0.878591 || Validation Loss: 0.010949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.421s | Train Loss: 0.837053 || Validation Loss: 0.010001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.421s | Train Loss: 0.820157 || Validation Loss: 0.010745 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.419s | Train Loss: 0.861930 || Validation Loss: 0.011734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.449s | Train Loss: 0.842809 || Validation Loss: 0.011449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.420s | Train Loss: 0.838738 || Validation Loss: 0.012023 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.421s | Train Loss: 0.802998 || Validation Loss: 0.010110 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.449s | Train Loss: 0.789701 || Validation Loss: 0.010720 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.409s | Train Loss: 0.773670 || Validation Loss: 0.010992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.422s | Train Loss: 0.767695 || Validation Loss: 0.010339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.451s | Train Loss: 0.801121 || Validation Loss: 0.010561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.423s | Train Loss: 0.753586 || Validation Loss: 0.010511 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.424s | Train Loss: 0.757018 || Validation Loss: 0.010634 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.454s | Train Loss: 0.782561 || Validation Loss: 0.010685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.425s | Train Loss: 0.741936 || Validation Loss: 0.011486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.424s | Train Loss: 0.775596 || Validation Loss: 0.010949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.450s | Train Loss: 0.741659 || Validation Loss: 0.010499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.424s | Train Loss: 0.736648 || Validation Loss: 0.010482 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.451s | Train Loss: 0.718722 || Validation Loss: 0.011269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.425s | Train Loss: 0.718743 || Validation Loss: 0.012002 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.424s | Train Loss: 0.710357 || Validation Loss: 0.010872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.455s | Train Loss: 0.691489 || Validation Loss: 0.010714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.426s | Train Loss: 0.699898 || Validation Loss: 0.010678 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.454s | Train Loss: 0.679806 || Validation Loss: 0.011470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.427s | Train Loss: 0.669139 || Validation Loss: 0.010768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.457s | Train Loss: 0.676389 || Validation Loss: 0.010668 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.457s | Train Loss: 0.679117 || Validation Loss: 0.010424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.430s | Train Loss: 0.677694 || Validation Loss: 0.011532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.429s | Train Loss: 0.686046 || Validation Loss: 0.011229 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.458s | Train Loss: 0.678834 || Validation Loss: 0.010369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.460s | Train Loss: 0.658699 || Validation Loss: 0.010735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.431s | Train Loss: 0.665966 || Validation Loss: 0.009867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.432s | Train Loss: 0.675502 || Validation Loss: 0.010531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.461s | Train Loss: 0.655835 || Validation Loss: 0.010733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.462s | Train Loss: 0.658570 || Validation Loss: 0.011442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.434s | Train Loss: 0.669998 || Validation Loss: 0.010574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.461s | Train Loss: 0.705744 || Validation Loss: 0.011475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.431s | Train Loss: 0.687125 || Validation Loss: 0.011339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.462s | Train Loss: 0.664699 || Validation Loss: 0.010900 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.435s | Train Loss: 0.673026 || Validation Loss: 0.009873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.464s | Train Loss: 0.689770 || Validation Loss: 0.010268 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.435s | Train Loss: 0.692683 || Validation Loss: 0.011684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.466s | Train Loss: 0.671476 || Validation Loss: 0.010542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.434s | Train Loss: 0.695614 || Validation Loss: 0.010412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.466s | Train Loss: 0.656682 || Validation Loss: 0.010451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.464s | Train Loss: 0.675702 || Validation Loss: 0.010765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.272s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.783%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008873\n",
      "INFO:root:Test AUC: 76.85%\n",
      "INFO:root:Test Time: 0.189s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 927\n",
      "len all outlier 16676 len chosen outlier 73\n",
      "final sizes 927 73 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 927, -1.0: 73})\n",
      "Resampled dataset shape Counter({1.0: 927, -1.0: 222})\n",
      "class weights 0.19321148825065274 0.8067885117493473\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5514084401739192\n",
      "validation AUC 0.5597413540063384\n",
      "validation AUC 0.5679454003507418\n",
      "validation AUC 0.5763571235938028\n",
      "validation AUC 0.5847653640036926\n",
      "validation AUC 0.5932783182273981\n",
      "validation AUC 0.601921632581907\n",
      "validation AUC 0.6105085032214477\n",
      "validation AUC 0.6191843364354052\n",
      "validation AUC 0.6273788601977461\n",
      "validation AUC 0.6350328716659256\n",
      "validation AUC 0.6422635499504794\n",
      "validation AUC 0.6492937271966222\n",
      "validation AUC 0.6561560686013304\n",
      "validation AUC 0.6630146025696231\n",
      "validation AUC 0.6691643121401826\n",
      "validation AUC 0.674427485018762\n",
      "validation AUC 0.6794063001339068\n",
      "validation AUC 0.6843011228292291\n",
      "validation AUC 0.6891161809289527\n",
      "validation AUC 0.6937218646146918\n",
      "validation AUC 0.6978488888884159\n",
      "validation AUC 0.7016553408514993\n",
      "validation AUC 0.7049221824917311\n",
      "validation AUC 0.7082027931630255\n",
      "validation AUC 0.7114050094901219\n",
      "validation AUC 0.7143784949392757\n",
      "validation AUC 0.7173844593376433\n",
      "validation AUC 0.7202782041373398\n",
      "validation AUC 0.722985911367776\n",
      "validation AUC 0.7257417849304487\n",
      "validation AUC 0.7281597092661773\n",
      "validation AUC 0.7305488290549916\n",
      "validation AUC 0.7327475823443932\n",
      "validation AUC 0.7349552888298383\n",
      "validation AUC 0.7369080780018403\n",
      "validation AUC 0.7389197188063199\n",
      "validation AUC 0.740979813296728\n",
      "validation AUC 0.7429145949707143\n",
      "validation AUC 0.7446685939230441\n",
      "validation AUC 0.7463788273117125\n",
      "validation AUC 0.7481131478575229\n",
      "validation AUC 0.7497254042800375\n",
      "validation AUC 0.7514075435239738\n",
      "validation AUC 0.7531127695084954\n",
      "validation AUC 0.7546318647998751\n",
      "validation AUC 0.7560879790797999\n",
      "validation AUC 0.7574151566939681\n",
      "validation AUC 0.7586333500991795\n",
      "validation AUC 0.7587431963692117\n",
      "validation AUC 0.7588562886716873\n",
      "validation AUC 0.7589880589641261\n",
      "validation AUC 0.7591193237269223\n",
      "validation AUC 0.7592521529709284\n",
      "validation AUC 0.7593860172203618\n",
      "validation AUC 0.7595007325390594\n",
      "validation AUC 0.7596165280947832\n",
      "validation AUC 0.7597461325918047\n",
      "validation AUC 0.7598600709912617\n",
      "validation AUC 0.7599758638863033\n",
      "validation AUC 0.7600973266953922\n",
      "validation AUC 0.7602237011240638\n",
      "validation AUC 0.7603405769168141\n",
      "validation AUC 0.7604607466342901\n",
      "validation AUC 0.7606063444928027\n",
      "validation AUC 0.760746546487548\n",
      "validation AUC 0.7609044287163826\n",
      "validation AUC 0.7610470279859082\n",
      "validation AUC 0.7611843005694074\n",
      "validation AUC 0.7613311702340739\n",
      "validation AUC 0.7613311702340739\n",
      "train auc 0.7826123048720603\n",
      "Test AUC 0.7685325705537517\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.98143060e-01\n",
      " 9.98143060e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [16.07763672 15.07763672 12.91077709 ...  0.06497254  0.06494674\n",
      "  0.02352498]\n",
      "Itration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.030s | Train Loss: 0.214356 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.212369 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.014s | Train Loss: 0.210792 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.015s | Train Loss: 0.208384 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.016s | Train Loss: 0.206095 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.016s | Train Loss: 0.203999 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.202212 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.200764 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.016s | Train Loss: 0.197871 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.016s | Train Loss: 0.196478 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.194253 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.192240 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.016s | Train Loss: 0.190588 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.188431 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.186932 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.184794 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.015s | Train Loss: 0.183178 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.181313 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.016s | Train Loss: 0.179507 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.177673 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.015s | Train Loss: 0.175704 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.174035 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.172699 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.170838 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.016s | Train Loss: 0.168818 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.016s | Train Loss: 0.167498 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.166280 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.016s | Train Loss: 0.164675 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.162875 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.161317 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.159263 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.158129 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.156352 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.017s | Train Loss: 0.154876 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.153717 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.152397 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.150125 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.016s | Train Loss: 0.148861 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.147236 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.146484 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.145192 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.016s | Train Loss: 0.143313 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.016s | Train Loss: 0.142220 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.140538 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.139437 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.138104 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.016s | Train Loss: 0.136416 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.016s | Train Loss: 0.135363 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.016s | Train Loss: 0.133923 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.015s | Train Loss: 0.133035 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.130896 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.130153 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.016s | Train Loss: 0.129109 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.128114 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.126808 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.125739 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.015s | Train Loss: 0.123631 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.015s | Train Loss: 0.122852 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.121990 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.120918 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.015s | Train Loss: 0.119391 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.015s | Train Loss: 0.118913 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.117595 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.116665 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.114960 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.115274 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.016s | Train Loss: 0.113374 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.015s | Train Loss: 0.112525 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.111948 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.110062 |\n",
      "INFO:root:Pretraining Time: 1.131s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.114744\n",
      "INFO:root:Test AUC: 45.55%\n",
      "INFO:root:Test AUC: 45.55%\n",
      "INFO:root:Test Time: 0.236s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.422s | Train Loss: 1.546585 || Validation Loss: 0.014288 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.495s | Train Loss: 1.483252 || Validation Loss: 0.014217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.423s | Train Loss: 1.463033 || Validation Loss: 0.015013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.426s | Train Loss: 1.425470 || Validation Loss: 0.013965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.422s | Train Loss: 1.323286 || Validation Loss: 0.013540 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.425s | Train Loss: 1.337763 || Validation Loss: 0.014395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.423s | Train Loss: 1.323466 || Validation Loss: 0.013694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.425s | Train Loss: 1.324349 || Validation Loss: 0.013629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.427s | Train Loss: 1.252889 || Validation Loss: 0.013270 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.399s | Train Loss: 1.243048 || Validation Loss: 0.013249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.457s | Train Loss: 1.136190 || Validation Loss: 0.012257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.411s | Train Loss: 1.173726 || Validation Loss: 0.012154 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.424s | Train Loss: 1.177859 || Validation Loss: 0.013164 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.430s | Train Loss: 1.135943 || Validation Loss: 0.012663 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.431s | Train Loss: 1.131058 || Validation Loss: 0.011152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.433s | Train Loss: 1.070029 || Validation Loss: 0.012963 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.430s | Train Loss: 1.072134 || Validation Loss: 0.012537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.432s | Train Loss: 1.044420 || Validation Loss: 0.011643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.429s | Train Loss: 1.042344 || Validation Loss: 0.011801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.433s | Train Loss: 1.049089 || Validation Loss: 0.011456 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.433s | Train Loss: 1.055071 || Validation Loss: 0.013078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.433s | Train Loss: 0.961759 || Validation Loss: 0.011383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.429s | Train Loss: 0.987554 || Validation Loss: 0.011498 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.432s | Train Loss: 0.970665 || Validation Loss: 0.011233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.431s | Train Loss: 0.947698 || Validation Loss: 0.011445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.430s | Train Loss: 0.957291 || Validation Loss: 0.010574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.432s | Train Loss: 0.920877 || Validation Loss: 0.011417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.433s | Train Loss: 0.935229 || Validation Loss: 0.011821 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.465s | Train Loss: 0.923286 || Validation Loss: 0.011389 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.435s | Train Loss: 0.891536 || Validation Loss: 0.010818 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.432s | Train Loss: 0.900711 || Validation Loss: 0.010905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.462s | Train Loss: 0.903671 || Validation Loss: 0.011079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.435s | Train Loss: 0.893449 || Validation Loss: 0.010742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.436s | Train Loss: 0.904712 || Validation Loss: 0.010862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.466s | Train Loss: 0.871947 || Validation Loss: 0.009930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.434s | Train Loss: 0.876905 || Validation Loss: 0.010789 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.437s | Train Loss: 0.838025 || Validation Loss: 0.010608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.466s | Train Loss: 0.878720 || Validation Loss: 0.010159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.437s | Train Loss: 0.827146 || Validation Loss: 0.011503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.439s | Train Loss: 0.837460 || Validation Loss: 0.010339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.467s | Train Loss: 0.812132 || Validation Loss: 0.010776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.437s | Train Loss: 0.797213 || Validation Loss: 0.010390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.465s | Train Loss: 0.814093 || Validation Loss: 0.010292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.436s | Train Loss: 0.792698 || Validation Loss: 0.009997 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.440s | Train Loss: 0.794406 || Validation Loss: 0.010255 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.470s | Train Loss: 0.785042 || Validation Loss: 0.010329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.438s | Train Loss: 0.778637 || Validation Loss: 0.010797 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.465s | Train Loss: 0.740360 || Validation Loss: 0.010066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.441s | Train Loss: 0.807162 || Validation Loss: 0.009874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.470s | Train Loss: 0.775780 || Validation Loss: 0.010031 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.474s | Train Loss: 0.753191 || Validation Loss: 0.010006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.440s | Train Loss: 0.773510 || Validation Loss: 0.010737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.444s | Train Loss: 0.769141 || Validation Loss: 0.009881 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.476s | Train Loss: 0.766463 || Validation Loss: 0.009848 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.474s | Train Loss: 0.759424 || Validation Loss: 0.010471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.443s | Train Loss: 0.783514 || Validation Loss: 0.010505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.454s | Train Loss: 0.758419 || Validation Loss: 0.010726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.475s | Train Loss: 0.763873 || Validation Loss: 0.011202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.474s | Train Loss: 0.773402 || Validation Loss: 0.010595 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.458s | Train Loss: 0.752235 || Validation Loss: 0.010867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.486s | Train Loss: 0.774461 || Validation Loss: 0.010786 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.446s | Train Loss: 0.759731 || Validation Loss: 0.010330 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.479s | Train Loss: 0.805219 || Validation Loss: 0.009374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.446s | Train Loss: 0.761095 || Validation Loss: 0.009639 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.473s | Train Loss: 0.761741 || Validation Loss: 0.010133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.445s | Train Loss: 0.780907 || Validation Loss: 0.010382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.476s | Train Loss: 0.768262 || Validation Loss: 0.010771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.445s | Train Loss: 0.738978 || Validation Loss: 0.010328 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.474s | Train Loss: 0.754727 || Validation Loss: 0.010469 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.477s | Train Loss: 0.759894 || Validation Loss: 0.011013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.307s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.715%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008149\n",
      "INFO:root:Test AUC: 73.72%\n",
      "INFO:root:Test Time: 0.196s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 924\n",
      "len all outlier 16676 len chosen outlier 76\n",
      "final sizes 924 76 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 924, -1.0: 76})\n",
      "Resampled dataset shape Counter({1.0: 924, -1.0: 221})\n",
      "class weights 0.1930131004366812 0.8069868995633188\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5161935193015474\n",
      "validation AUC 0.5229699898447077\n",
      "validation AUC 0.5296483610303269\n",
      "validation AUC 0.5362147574000493\n",
      "validation AUC 0.5427390937034846\n",
      "validation AUC 0.5498675086626496\n",
      "validation AUC 0.5566096457610116\n",
      "validation AUC 0.563345362632909\n",
      "validation AUC 0.5699460818047003\n",
      "validation AUC 0.5764666878315077\n",
      "validation AUC 0.5829364934505707\n",
      "validation AUC 0.5891274702040828\n",
      "validation AUC 0.5951960795165135\n",
      "validation AUC 0.6016126821316876\n",
      "validation AUC 0.6075342781026056\n",
      "validation AUC 0.6129488198596734\n",
      "validation AUC 0.6180472698951457\n",
      "validation AUC 0.623465459447689\n",
      "validation AUC 0.628793576921284\n",
      "validation AUC 0.6337684595479437\n",
      "validation AUC 0.6384746957403327\n",
      "validation AUC 0.6430344081567579\n",
      "validation AUC 0.6471740440647307\n",
      "validation AUC 0.6511868718528119\n",
      "validation AUC 0.6548174048215395\n",
      "validation AUC 0.6582935543586979\n",
      "validation AUC 0.6618854169704279\n",
      "validation AUC 0.6655389781447296\n",
      "validation AUC 0.6690432005411615\n",
      "validation AUC 0.6727160569837272\n",
      "validation AUC 0.6765656545714885\n",
      "validation AUC 0.6801104231059614\n",
      "validation AUC 0.6832668278046838\n",
      "validation AUC 0.6864383558217751\n",
      "validation AUC 0.6893924661821954\n",
      "validation AUC 0.6921984989281706\n",
      "validation AUC 0.694689964523526\n",
      "validation AUC 0.6971904790994887\n",
      "validation AUC 0.6998179587756008\n",
      "validation AUC 0.7020958141507007\n",
      "validation AUC 0.7044337557148797\n",
      "validation AUC 0.7069041433848093\n",
      "validation AUC 0.7095905891027392\n",
      "validation AUC 0.7121884261170024\n",
      "validation AUC 0.7146273485576866\n",
      "validation AUC 0.7166786681177674\n",
      "validation AUC 0.7186194922013271\n",
      "validation AUC 0.7209351957825844\n",
      "validation AUC 0.7227334897743594\n",
      "validation AUC 0.7229066097309029\n",
      "validation AUC 0.7230899413862325\n",
      "validation AUC 0.7232975091969146\n",
      "validation AUC 0.7235084587348393\n",
      "validation AUC 0.7237073527211224\n",
      "validation AUC 0.7239104106752534\n",
      "validation AUC 0.7241183217139563\n",
      "validation AUC 0.7243454934320523\n",
      "validation AUC 0.7245450898384711\n",
      "validation AUC 0.724724089902966\n",
      "validation AUC 0.7249107713573503\n",
      "validation AUC 0.7250971042623492\n",
      "validation AUC 0.7252800261726\n",
      "validation AUC 0.7254627059607586\n",
      "validation AUC 0.7256411818708344\n",
      "validation AUC 0.7258373938893279\n",
      "validation AUC 0.7260380572293612\n",
      "validation AUC 0.7262358071222421\n",
      "validation AUC 0.72643337076736\n",
      "validation AUC 0.7266281220712538\n",
      "validation AUC 0.7268136620929182\n",
      "validation AUC 0.7268136620929182\n",
      "train auc 0.7152384510005567\n",
      "Test AUC 0.7371904565378492\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.13690194e-04 ... 9.98578873e-01\n",
      " 9.98578873e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [11.42886925 10.42886925  7.77916622 ...  0.0601698   0.05998201\n",
      "  0.02857057]\n",
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.033s | Train Loss: 0.213999 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.211224 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.209965 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.206550 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.205370 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.202691 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.201067 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.198426 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.197606 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.194736 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.193122 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.190724 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.188748 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.186595 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.184839 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.182890 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.180296 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.178011 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.176517 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.017s | Train Loss: 0.174201 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.172087 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.170634 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.168163 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.166036 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.019s | Train Loss: 0.164155 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.162527 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.160420 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.158080 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.157083 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.154337 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.152582 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.018s | Train Loss: 0.150771 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.149345 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.148070 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.145846 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.144005 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.143039 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.019s | Train Loss: 0.141114 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.017s | Train Loss: 0.138872 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.137766 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.136016 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.134721 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.019s | Train Loss: 0.133849 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.132138 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.130075 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.128941 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.127120 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.126574 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.019s | Train Loss: 0.125253 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.123609 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.122434 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.120684 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.120139 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.019s | Train Loss: 0.118660 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.118192 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.116306 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.114948 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.113408 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.112801 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.111177 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.110233 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.110176 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.108966 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.106159 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.106100 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.105999 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.104296 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.103356 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.018s | Train Loss: 0.102202 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.100831 |\n",
      "INFO:root:Pretraining Time: 1.261s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.111163\n",
      "INFO:root:Test AUC: 49.77%\n",
      "INFO:root:Test AUC: 49.77%\n",
      "INFO:root:Test Time: 0.226s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.407s | Train Loss: 1.507159 || Validation Loss: 0.016166 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.386s | Train Loss: 1.444289 || Validation Loss: 0.014184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.416s | Train Loss: 1.400869 || Validation Loss: 0.016201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.413s | Train Loss: 1.360637 || Validation Loss: 0.013493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.412s | Train Loss: 1.303108 || Validation Loss: 0.014907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.412s | Train Loss: 1.244086 || Validation Loss: 0.015822 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.414s | Train Loss: 1.204957 || Validation Loss: 0.014992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.417s | Train Loss: 1.186843 || Validation Loss: 0.014749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.419s | Train Loss: 1.178856 || Validation Loss: 0.015677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.391s | Train Loss: 1.129198 || Validation Loss: 0.014328 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.447s | Train Loss: 1.094513 || Validation Loss: 0.014760 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.419s | Train Loss: 1.105692 || Validation Loss: 0.014950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.419s | Train Loss: 1.041168 || Validation Loss: 0.014522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.421s | Train Loss: 1.052756 || Validation Loss: 0.013347 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.419s | Train Loss: 1.003047 || Validation Loss: 0.014504 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.418s | Train Loss: 1.017871 || Validation Loss: 0.014567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.419s | Train Loss: 0.962780 || Validation Loss: 0.013481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.422s | Train Loss: 0.963537 || Validation Loss: 0.013714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.422s | Train Loss: 0.963148 || Validation Loss: 0.013386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.420s | Train Loss: 0.971289 || Validation Loss: 0.014351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.421s | Train Loss: 0.883105 || Validation Loss: 0.014200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.421s | Train Loss: 0.935830 || Validation Loss: 0.013900 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.423s | Train Loss: 0.900014 || Validation Loss: 0.013641 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.425s | Train Loss: 0.898031 || Validation Loss: 0.012773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.451s | Train Loss: 0.870290 || Validation Loss: 0.014520 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.426s | Train Loss: 0.881072 || Validation Loss: 0.013218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.423s | Train Loss: 0.815959 || Validation Loss: 0.012273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.449s | Train Loss: 0.842371 || Validation Loss: 0.012927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.422s | Train Loss: 0.817044 || Validation Loss: 0.013873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.424s | Train Loss: 0.788714 || Validation Loss: 0.011810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.423s | Train Loss: 0.777514 || Validation Loss: 0.013590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.455s | Train Loss: 0.776476 || Validation Loss: 0.011577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.427s | Train Loss: 0.761285 || Validation Loss: 0.014022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.429s | Train Loss: 0.779120 || Validation Loss: 0.013075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.458s | Train Loss: 0.780055 || Validation Loss: 0.013884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.427s | Train Loss: 0.747786 || Validation Loss: 0.013505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.429s | Train Loss: 0.756654 || Validation Loss: 0.012611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.453s | Train Loss: 0.709903 || Validation Loss: 0.013346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.426s | Train Loss: 0.716078 || Validation Loss: 0.014797 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.457s | Train Loss: 0.699094 || Validation Loss: 0.013837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.427s | Train Loss: 0.728529 || Validation Loss: 0.013134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.456s | Train Loss: 0.669474 || Validation Loss: 0.012970 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.427s | Train Loss: 0.667101 || Validation Loss: 0.013334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.457s | Train Loss: 0.650746 || Validation Loss: 0.012431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.427s | Train Loss: 0.664281 || Validation Loss: 0.013852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.457s | Train Loss: 0.654015 || Validation Loss: 0.012450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.431s | Train Loss: 0.668321 || Validation Loss: 0.013314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.461s | Train Loss: 0.660426 || Validation Loss: 0.013730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.458s | Train Loss: 0.653730 || Validation Loss: 0.013799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.432s | Train Loss: 0.664609 || Validation Loss: 0.013168 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.461s | Train Loss: 0.638777 || Validation Loss: 0.012880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.653546 || Validation Loss: 0.013708 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.460s | Train Loss: 0.646817 || Validation Loss: 0.013719 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.431s | Train Loss: 0.648105 || Validation Loss: 0.013006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.462s | Train Loss: 0.629781 || Validation Loss: 0.014001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.433s | Train Loss: 0.631449 || Validation Loss: 0.014099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.462s | Train Loss: 0.640589 || Validation Loss: 0.014281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.463s | Train Loss: 0.646617 || Validation Loss: 0.013291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.452s | Train Loss: 0.654729 || Validation Loss: 0.013553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.436s | Train Loss: 0.666304 || Validation Loss: 0.013314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.511s | Train Loss: 0.629109 || Validation Loss: 0.013568 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.464s | Train Loss: 0.639947 || Validation Loss: 0.013339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.465s | Train Loss: 0.633459 || Validation Loss: 0.014260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.468s | Train Loss: 0.627222 || Validation Loss: 0.013681 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.467s | Train Loss: 0.640293 || Validation Loss: 0.014988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.470s | Train Loss: 0.650033 || Validation Loss: 0.012873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.474s | Train Loss: 0.619539 || Validation Loss: 0.013657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.467s | Train Loss: 0.617354 || Validation Loss: 0.014592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.470s | Train Loss: 0.616454 || Validation Loss: 0.013966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.645s | Train Loss: 0.621766 || Validation Loss: 0.014085 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.936s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.813%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.012060\n",
      "INFO:root:Test AUC: 77.37%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 939\n",
      "len all outlier 16676 len chosen outlier 61\n",
      "final sizes 939 61 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 939, -1.0: 61})\n",
      "Resampled dataset shape Counter({1.0: 939, -1.0: 225})\n",
      "class weights 0.19329896907216496 0.8067010309278351\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5994220465840776\n",
      "validation AUC 0.6073107541799853\n",
      "validation AUC 0.6151831624359335\n",
      "validation AUC 0.6220915735552016\n",
      "validation AUC 0.6289962065055598\n",
      "validation AUC 0.6358121190035937\n",
      "validation AUC 0.6414541118504024\n",
      "validation AUC 0.6471329524868121\n",
      "validation AUC 0.6521923516877134\n",
      "validation AUC 0.6567450319207382\n",
      "validation AUC 0.6622070775001421\n",
      "validation AUC 0.6674926480025832\n",
      "validation AUC 0.6722847151462854\n",
      "validation AUC 0.6766458235908016\n",
      "validation AUC 0.6805252421061281\n",
      "validation AUC 0.6843931797771965\n",
      "validation AUC 0.6881872968169939\n",
      "validation AUC 0.6920522092922522\n",
      "validation AUC 0.6953569390488763\n",
      "validation AUC 0.6982897772540609\n",
      "validation AUC 0.7008318117777979\n",
      "validation AUC 0.7034642350016804\n",
      "validation AUC 0.7064040628193485\n",
      "validation AUC 0.7094524384940708\n",
      "validation AUC 0.7126420431869185\n",
      "validation AUC 0.7154227143089155\n",
      "validation AUC 0.7178095326075133\n",
      "validation AUC 0.7200582428683603\n",
      "validation AUC 0.7216940330260919\n",
      "validation AUC 0.7232688004345639\n",
      "validation AUC 0.7248338084602461\n",
      "validation AUC 0.7267460940651246\n",
      "validation AUC 0.7281692531336985\n",
      "validation AUC 0.7296796134049858\n",
      "validation AUC 0.7314868925209923\n",
      "validation AUC 0.7331999462329314\n",
      "validation AUC 0.7352157297198003\n",
      "validation AUC 0.7370811686908783\n",
      "validation AUC 0.7390712978787124\n",
      "validation AUC 0.7412894448816241\n",
      "validation AUC 0.7433614592288746\n",
      "validation AUC 0.7452461056656995\n",
      "validation AUC 0.7474515878207161\n",
      "validation AUC 0.7489506588168733\n",
      "validation AUC 0.7499357152542027\n",
      "validation AUC 0.7512225382994578\n",
      "validation AUC 0.7525042448525907\n",
      "validation AUC 0.753664007013133\n",
      "validation AUC 0.7550393482988556\n",
      "validation AUC 0.7551408480084153\n",
      "validation AUC 0.7552313511179016\n",
      "validation AUC 0.7552786660317924\n",
      "validation AUC 0.755383531504501\n",
      "validation AUC 0.7555621351273286\n",
      "validation AUC 0.7557048088959594\n",
      "validation AUC 0.7558568615698068\n",
      "validation AUC 0.7560448095089807\n",
      "validation AUC 0.7561665915999491\n",
      "validation AUC 0.7562787978952086\n",
      "validation AUC 0.7563473929463822\n",
      "validation AUC 0.7564340487092178\n",
      "validation AUC 0.7565905766507458\n",
      "validation AUC 0.7568007918403472\n",
      "validation AUC 0.7569933986342825\n",
      "validation AUC 0.7571157580933168\n",
      "validation AUC 0.7572601533234158\n",
      "validation AUC 0.7573984263234712\n",
      "validation AUC 0.7575767133251015\n",
      "validation AUC 0.7577778050349901\n",
      "validation AUC 0.7579475352822441\n",
      "validation AUC 0.7579475352822441\n",
      "train auc 0.8132430816671035\n",
      "Test AUC 0.7736883853202152\n",
      "test false positive rates [0.         0.         0.         ... 0.99562293 0.99562293 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [24.9875412  23.9875412  16.8449707  ...  0.12493408  0.12427473\n",
      "  0.06320872]\n",
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.035s | Train Loss: 0.222580 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.219880 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.019s | Train Loss: 0.218619 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.215697 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.019s | Train Loss: 0.213026 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.019s | Train Loss: 0.211654 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.021s | Train Loss: 0.208957 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.020s | Train Loss: 0.206886 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.019s | Train Loss: 0.204509 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.202714 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.019s | Train Loss: 0.200323 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.199169 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.019s | Train Loss: 0.196439 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.194478 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.018s | Train Loss: 0.192638 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.018s | Train Loss: 0.190807 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.188746 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.186192 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.017s | Train Loss: 0.184487 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.182738 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.180882 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.178793 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.019s | Train Loss: 0.176447 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.019s | Train Loss: 0.174572 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.173034 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.018s | Train Loss: 0.171242 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.169264 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.167443 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.165715 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.017s | Train Loss: 0.163912 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.162375 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.159902 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.158726 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.156653 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.018s | Train Loss: 0.154892 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.018s | Train Loss: 0.152438 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.151562 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.150624 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.148156 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.017s | Train Loss: 0.146880 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.018s | Train Loss: 0.145390 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.143794 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.141936 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.140905 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.019s | Train Loss: 0.139711 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.137736 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.135600 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.135287 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.133115 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.131746 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.130255 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.128884 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.127555 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.126361 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.125241 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.016s | Train Loss: 0.123853 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.018s | Train Loss: 0.122521 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.018s | Train Loss: 0.121343 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.017s | Train Loss: 0.120116 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.018s | Train Loss: 0.118981 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.018s | Train Loss: 0.118293 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.115852 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.115392 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.113844 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.112317 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.111911 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.111198 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.016s | Train Loss: 0.109797 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.018s | Train Loss: 0.108067 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.108116 |\n",
      "INFO:root:Pretraining Time: 1.270s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.110570\n",
      "INFO:root:Test AUC: 47.78%\n",
      "INFO:root:Test AUC: 47.78%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.408s | Train Loss: 1.714787 || Validation Loss: 0.017233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.375s | Train Loss: 1.575310 || Validation Loss: 0.017857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.399s | Train Loss: 1.426272 || Validation Loss: 0.015943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.405s | Train Loss: 1.396174 || Validation Loss: 0.015904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.407s | Train Loss: 1.351535 || Validation Loss: 0.014279 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.401s | Train Loss: 1.346183 || Validation Loss: 0.015962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.408s | Train Loss: 1.262583 || Validation Loss: 0.015578 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.410s | Train Loss: 1.269817 || Validation Loss: 0.014893 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.401s | Train Loss: 1.242322 || Validation Loss: 0.014613 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.376s | Train Loss: 1.164307 || Validation Loss: 0.013815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.436s | Train Loss: 1.124082 || Validation Loss: 0.014698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.407s | Train Loss: 1.116671 || Validation Loss: 0.014382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.405s | Train Loss: 1.060559 || Validation Loss: 0.013521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.412s | Train Loss: 1.002578 || Validation Loss: 0.013876 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.412s | Train Loss: 1.010737 || Validation Loss: 0.014246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.406s | Train Loss: 1.006862 || Validation Loss: 0.012630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.403s | Train Loss: 0.944455 || Validation Loss: 0.014194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.415s | Train Loss: 0.973356 || Validation Loss: 0.013629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.406s | Train Loss: 0.936983 || Validation Loss: 0.013779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.413s | Train Loss: 0.935176 || Validation Loss: 0.013036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.409s | Train Loss: 0.892195 || Validation Loss: 0.013131 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.409s | Train Loss: 0.905763 || Validation Loss: 0.014868 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.412s | Train Loss: 0.865377 || Validation Loss: 0.012775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.413s | Train Loss: 0.865560 || Validation Loss: 0.012908 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.441s | Train Loss: 0.853611 || Validation Loss: 0.012434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.412s | Train Loss: 0.855323 || Validation Loss: 0.012936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.412s | Train Loss: 0.841637 || Validation Loss: 0.012167 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.431s | Train Loss: 0.832474 || Validation Loss: 0.013548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.409s | Train Loss: 0.822736 || Validation Loss: 0.013214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.419s | Train Loss: 0.787077 || Validation Loss: 0.012761 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.410s | Train Loss: 0.769048 || Validation Loss: 0.013682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.440s | Train Loss: 0.790888 || Validation Loss: 0.013340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.419s | Train Loss: 0.766834 || Validation Loss: 0.013295 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.418s | Train Loss: 0.762541 || Validation Loss: 0.013422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.444s | Train Loss: 0.736891 || Validation Loss: 0.012803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.415s | Train Loss: 0.714974 || Validation Loss: 0.012751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.415s | Train Loss: 0.710772 || Validation Loss: 0.013896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.448s | Train Loss: 0.731898 || Validation Loss: 0.013541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.421s | Train Loss: 0.704383 || Validation Loss: 0.013224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.448s | Train Loss: 0.726168 || Validation Loss: 0.013681 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.421s | Train Loss: 0.696597 || Validation Loss: 0.014069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.444s | Train Loss: 0.692900 || Validation Loss: 0.013555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.419s | Train Loss: 0.703524 || Validation Loss: 0.013145 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.458s | Train Loss: 0.691479 || Validation Loss: 0.013356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.413s | Train Loss: 0.668196 || Validation Loss: 0.014249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.450s | Train Loss: 0.658977 || Validation Loss: 0.013507 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.423s | Train Loss: 0.658222 || Validation Loss: 0.013425 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.452s | Train Loss: 0.630067 || Validation Loss: 0.012792 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.452s | Train Loss: 0.640171 || Validation Loss: 0.014135 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.427s | Train Loss: 0.619851 || Validation Loss: 0.012156 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.457s | Train Loss: 0.628926 || Validation Loss: 0.013142 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.428s | Train Loss: 0.657103 || Validation Loss: 0.013024 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.455s | Train Loss: 0.632479 || Validation Loss: 0.012758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.430s | Train Loss: 0.614398 || Validation Loss: 0.012556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.457s | Train Loss: 0.641246 || Validation Loss: 0.013132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.427s | Train Loss: 0.649877 || Validation Loss: 0.013564 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.458s | Train Loss: 0.626187 || Validation Loss: 0.013204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.457s | Train Loss: 0.632250 || Validation Loss: 0.012896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.458s | Train Loss: 0.631175 || Validation Loss: 0.013274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.430s | Train Loss: 0.624184 || Validation Loss: 0.013400 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.461s | Train Loss: 0.622438 || Validation Loss: 0.013703 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.462s | Train Loss: 0.656811 || Validation Loss: 0.013626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.460s | Train Loss: 0.663048 || Validation Loss: 0.012956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.463s | Train Loss: 0.622018 || Validation Loss: 0.013195 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.463s | Train Loss: 0.634104 || Validation Loss: 0.013767 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.466s | Train Loss: 0.640993 || Validation Loss: 0.012321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.465s | Train Loss: 0.629388 || Validation Loss: 0.013667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.464s | Train Loss: 0.628667 || Validation Loss: 0.013411 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.463s | Train Loss: 0.616603 || Validation Loss: 0.013650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.461s | Train Loss: 0.656551 || Validation Loss: 0.012878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.124s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.810%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.010741\n",
      "INFO:root:Test AUC: 79.67%\n",
      "INFO:root:Test Time: 0.181s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 932\n",
      "len all outlier 16676 len chosen outlier 68\n",
      "final sizes 932 68 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 932, -1.0: 68})\n",
      "Resampled dataset shape Counter({1.0: 932, -1.0: 223})\n",
      "class weights 0.19307359307359306 0.8069264069264069\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5733366983869667\n",
      "validation AUC 0.5820248532207986\n",
      "validation AUC 0.590754951050895\n",
      "validation AUC 0.5999598449822618\n",
      "validation AUC 0.6089990076719178\n",
      "validation AUC 0.6175956110874251\n",
      "validation AUC 0.6260350134087747\n",
      "validation AUC 0.63427797489295\n",
      "validation AUC 0.6422478253179037\n",
      "validation AUC 0.6501979254979147\n",
      "validation AUC 0.6578731000333543\n",
      "validation AUC 0.6652065684583987\n",
      "validation AUC 0.6721198406442853\n",
      "validation AUC 0.6787253117947196\n",
      "validation AUC 0.685120136725015\n",
      "validation AUC 0.691163573321913\n",
      "validation AUC 0.696855618924731\n",
      "validation AUC 0.7022705491414191\n",
      "validation AUC 0.7075345095819685\n",
      "validation AUC 0.7122983601895514\n",
      "validation AUC 0.7168530066668185\n",
      "validation AUC 0.7208520015674611\n",
      "validation AUC 0.7246959664907282\n",
      "validation AUC 0.7285242972446186\n",
      "validation AUC 0.7323277772255383\n",
      "validation AUC 0.7359356331987595\n",
      "validation AUC 0.7394804389827849\n",
      "validation AUC 0.7426635649141567\n",
      "validation AUC 0.7455581079185525\n",
      "validation AUC 0.7485042654994857\n",
      "validation AUC 0.751565737052641\n",
      "validation AUC 0.7541836755219141\n",
      "validation AUC 0.7566152943896001\n",
      "validation AUC 0.7590153362793827\n",
      "validation AUC 0.7617108070317791\n",
      "validation AUC 0.7640770449525474\n",
      "validation AUC 0.7660840535090887\n",
      "validation AUC 0.7678240173621229\n",
      "validation AUC 0.7693430913680441\n",
      "validation AUC 0.7711476459453437\n",
      "validation AUC 0.7727378852215699\n",
      "validation AUC 0.7742889406291086\n",
      "validation AUC 0.7756085007097636\n",
      "validation AUC 0.7770506703535919\n",
      "validation AUC 0.7786877136927015\n",
      "validation AUC 0.7803714972383181\n",
      "validation AUC 0.7821355668732703\n",
      "validation AUC 0.7838161043864434\n",
      "validation AUC 0.7853857446600637\n",
      "validation AUC 0.7855282454843431\n",
      "validation AUC 0.7856762805278709\n",
      "validation AUC 0.7858168257506369\n",
      "validation AUC 0.7859263101678721\n",
      "validation AUC 0.7860603846112094\n",
      "validation AUC 0.7862213585529123\n",
      "validation AUC 0.786350595875772\n",
      "validation AUC 0.7864756239991842\n",
      "validation AUC 0.7866012321513447\n",
      "validation AUC 0.7867442837368666\n",
      "validation AUC 0.7868844378393303\n",
      "validation AUC 0.7870338697410816\n",
      "validation AUC 0.7871730074628948\n",
      "validation AUC 0.787311511942313\n",
      "validation AUC 0.787469085531997\n",
      "validation AUC 0.787636700536798\n",
      "validation AUC 0.7877798346034721\n",
      "validation AUC 0.7879029417142415\n",
      "validation AUC 0.7880306012524789\n",
      "validation AUC 0.7881656308807731\n",
      "validation AUC 0.7882605187947407\n",
      "validation AUC 0.7882605187947407\n",
      "train auc 0.8104003003398452\n",
      "Test AUC 0.7967371598564126\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.98294647e-01\n",
      " 9.98294647e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [17.7380085  16.7380085  15.48208046 ...  0.06026895  0.06015747\n",
      "  0.02300613]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.032s | Train Loss: 0.216103 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.015s | Train Loss: 0.213114 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.210608 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.015s | Train Loss: 0.208671 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.017s | Train Loss: 0.206238 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.015s | Train Loss: 0.204365 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.016s | Train Loss: 0.202280 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.201018 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.016s | Train Loss: 0.198069 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.016s | Train Loss: 0.196641 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.016s | Train Loss: 0.195668 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.016s | Train Loss: 0.193087 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.017s | Train Loss: 0.191594 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.015s | Train Loss: 0.189815 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.187738 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.185399 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.184521 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.016s | Train Loss: 0.182952 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.016s | Train Loss: 0.180090 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.178721 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.177692 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.175718 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.016s | Train Loss: 0.174705 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.172909 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.170459 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.015s | Train Loss: 0.169184 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.167853 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.016s | Train Loss: 0.166721 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.164692 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.163627 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.016s | Train Loss: 0.162251 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.160165 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.159044 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.016s | Train Loss: 0.157575 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.155688 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.154113 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.017s | Train Loss: 0.152824 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.015s | Train Loss: 0.151704 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.015s | Train Loss: 0.149710 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.149103 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.016s | Train Loss: 0.147335 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.146145 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.015s | Train Loss: 0.144320 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.143277 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.016s | Train Loss: 0.141880 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.140043 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.015s | Train Loss: 0.138310 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.016s | Train Loss: 0.137688 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.016s | Train Loss: 0.136949 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.016s | Train Loss: 0.136154 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.134673 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.016s | Train Loss: 0.132157 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.131938 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.130639 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.017s | Train Loss: 0.129669 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.015s | Train Loss: 0.127594 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.126886 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.015s | Train Loss: 0.126284 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.015s | Train Loss: 0.125062 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.123654 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.122788 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.015s | Train Loss: 0.122441 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.015s | Train Loss: 0.120134 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.120181 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.118553 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.117900 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.015s | Train Loss: 0.116668 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.015s | Train Loss: 0.115422 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.115339 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.113435 |\n",
      "INFO:root:Pretraining Time: 1.131s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.118590\n",
      "INFO:root:Test AUC: 48.64%\n",
      "INFO:root:Test AUC: 48.64%\n",
      "INFO:root:Test Time: 0.220s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.427s | Train Loss: 1.921285 || Validation Loss: 0.016476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.395s | Train Loss: 1.906338 || Validation Loss: 0.017071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.421s | Train Loss: 1.680363 || Validation Loss: 0.016754 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.446s | Train Loss: 1.660957 || Validation Loss: 0.017459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.419s | Train Loss: 1.658757 || Validation Loss: 0.015521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.420s | Train Loss: 1.622227 || Validation Loss: 0.015126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.425s | Train Loss: 1.498784 || Validation Loss: 0.015786 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.414s | Train Loss: 1.449757 || Validation Loss: 0.015129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.425s | Train Loss: 1.459603 || Validation Loss: 0.014500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.388s | Train Loss: 1.370113 || Validation Loss: 0.015238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.450s | Train Loss: 1.340687 || Validation Loss: 0.014166 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.423s | Train Loss: 1.271017 || Validation Loss: 0.014097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.419s | Train Loss: 1.262540 || Validation Loss: 0.013213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.413s | Train Loss: 1.161319 || Validation Loss: 0.012930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.416s | Train Loss: 1.161869 || Validation Loss: 0.014152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.417s | Train Loss: 1.155670 || Validation Loss: 0.013219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.421s | Train Loss: 1.154592 || Validation Loss: 0.013395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.426s | Train Loss: 1.128585 || Validation Loss: 0.013731 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.422s | Train Loss: 1.097317 || Validation Loss: 0.013215 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.419s | Train Loss: 1.116468 || Validation Loss: 0.013535 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.429s | Train Loss: 1.104561 || Validation Loss: 0.013089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.423s | Train Loss: 1.060751 || Validation Loss: 0.012863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.433s | Train Loss: 1.046394 || Validation Loss: 0.012322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.420s | Train Loss: 1.041890 || Validation Loss: 0.014421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.431s | Train Loss: 1.007916 || Validation Loss: 0.013071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.425s | Train Loss: 1.020391 || Validation Loss: 0.013276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.432s | Train Loss: 1.005972 || Validation Loss: 0.013039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.425s | Train Loss: 0.988420 || Validation Loss: 0.012429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.460s | Train Loss: 0.986118 || Validation Loss: 0.011190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.420s | Train Loss: 0.986596 || Validation Loss: 0.012158 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.432s | Train Loss: 0.962413 || Validation Loss: 0.011228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.490s | Train Loss: 0.927611 || Validation Loss: 0.012088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.448s | Train Loss: 0.904735 || Validation Loss: 0.012108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.431s | Train Loss: 0.904399 || Validation Loss: 0.011838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.461s | Train Loss: 0.871474 || Validation Loss: 0.011455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.435s | Train Loss: 0.891158 || Validation Loss: 0.012480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.432s | Train Loss: 0.889283 || Validation Loss: 0.011479 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.448s | Train Loss: 0.896061 || Validation Loss: 0.011207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.428s | Train Loss: 0.886507 || Validation Loss: 0.010669 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.426s | Train Loss: 0.876409 || Validation Loss: 0.011329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.468s | Train Loss: 0.849773 || Validation Loss: 0.010992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.432s | Train Loss: 0.840634 || Validation Loss: 0.011297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.461s | Train Loss: 0.840819 || Validation Loss: 0.010855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.426s | Train Loss: 0.849094 || Validation Loss: 0.010943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.429s | Train Loss: 0.876680 || Validation Loss: 0.011148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.458s | Train Loss: 0.840048 || Validation Loss: 0.012439 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.427s | Train Loss: 0.825760 || Validation Loss: 0.011257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.460s | Train Loss: 0.816345 || Validation Loss: 0.011860 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.425s | Train Loss: 0.834208 || Validation Loss: 0.010956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.460s | Train Loss: 0.811052 || Validation Loss: 0.012014 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.461s | Train Loss: 0.789721 || Validation Loss: 0.011710 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.429s | Train Loss: 0.805037 || Validation Loss: 0.011277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.434s | Train Loss: 0.788612 || Validation Loss: 0.012027 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.462s | Train Loss: 0.801876 || Validation Loss: 0.010250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.465s | Train Loss: 0.830941 || Validation Loss: 0.011815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.434s | Train Loss: 0.822845 || Validation Loss: 0.011112 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.430s | Train Loss: 0.756922 || Validation Loss: 0.011714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.464s | Train Loss: 0.799773 || Validation Loss: 0.010694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.466s | Train Loss: 0.803166 || Validation Loss: 0.010642 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.432s | Train Loss: 0.768628 || Validation Loss: 0.010989 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.463s | Train Loss: 0.803892 || Validation Loss: 0.011419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.433s | Train Loss: 0.779562 || Validation Loss: 0.011343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.463s | Train Loss: 0.815724 || Validation Loss: 0.011079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.435s | Train Loss: 0.804669 || Validation Loss: 0.011258 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.463s | Train Loss: 0.793583 || Validation Loss: 0.011271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.434s | Train Loss: 0.802847 || Validation Loss: 0.010625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.465s | Train Loss: 0.803181 || Validation Loss: 0.011764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.436s | Train Loss: 0.796582 || Validation Loss: 0.010619 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.472s | Train Loss: 0.790013 || Validation Loss: 0.011983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.470s | Train Loss: 0.782769 || Validation Loss: 0.010965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.724s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.687%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008880\n",
      "INFO:root:Test AUC: 70.13%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 923\n",
      "len all outlier 16676 len chosen outlier 77\n",
      "final sizes 923 77 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 923, -1.0: 77})\n",
      "Resampled dataset shape Counter({1.0: 923, -1.0: 221})\n",
      "class weights 0.19318181818181818 0.8068181818181818\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4818991439627096\n",
      "validation AUC 0.4869129177830003\n",
      "validation AUC 0.4924701652368869\n",
      "validation AUC 0.4975861864184725\n",
      "validation AUC 0.5026677890134253\n",
      "validation AUC 0.5080611224716334\n",
      "validation AUC 0.5134150911347555\n",
      "validation AUC 0.5190280692407456\n",
      "validation AUC 0.5240570754673808\n",
      "validation AUC 0.5292711614176371\n",
      "validation AUC 0.5346722587468867\n",
      "validation AUC 0.5402909679626177\n",
      "validation AUC 0.5460102350063633\n",
      "validation AUC 0.5518574835309085\n",
      "validation AUC 0.5572551592226807\n",
      "validation AUC 0.5624013791487221\n",
      "validation AUC 0.5676671834421269\n",
      "validation AUC 0.5729102974366135\n",
      "validation AUC 0.5780547852584577\n",
      "validation AUC 0.5828663365788691\n",
      "validation AUC 0.5879705336881762\n",
      "validation AUC 0.5932509876984949\n",
      "validation AUC 0.5982339587994405\n",
      "validation AUC 0.6030856332094023\n",
      "validation AUC 0.6077868886044678\n",
      "validation AUC 0.6123519250462373\n",
      "validation AUC 0.6165621515171317\n",
      "validation AUC 0.620691211212839\n",
      "validation AUC 0.6250117921440906\n",
      "validation AUC 0.6295378948213117\n",
      "validation AUC 0.6335509886776259\n",
      "validation AUC 0.637440076112543\n",
      "validation AUC 0.6410677515084472\n",
      "validation AUC 0.6446787497475013\n",
      "validation AUC 0.6482260379481415\n",
      "validation AUC 0.6518298337201256\n",
      "validation AUC 0.6552945370018963\n",
      "validation AUC 0.658585425889035\n",
      "validation AUC 0.6616400648099645\n",
      "validation AUC 0.6647301786084121\n",
      "validation AUC 0.667688652487855\n",
      "validation AUC 0.6705465605572363\n",
      "validation AUC 0.6733776329846279\n",
      "validation AUC 0.6759386727622544\n",
      "validation AUC 0.6783421655570202\n",
      "validation AUC 0.680875885448473\n",
      "validation AUC 0.6835516139379738\n",
      "validation AUC 0.6860848948168419\n",
      "validation AUC 0.6884836063654589\n",
      "validation AUC 0.688707239376055\n",
      "validation AUC 0.6889583412710356\n",
      "validation AUC 0.6892266710841088\n",
      "validation AUC 0.6894838632809448\n",
      "validation AUC 0.6897340951328033\n",
      "validation AUC 0.689990513071081\n",
      "validation AUC 0.6902517814332477\n",
      "validation AUC 0.6904883373779253\n",
      "validation AUC 0.6907192526760612\n",
      "validation AUC 0.690951567493103\n",
      "validation AUC 0.6911929126659759\n",
      "validation AUC 0.6914357957132363\n",
      "validation AUC 0.691665077352421\n",
      "validation AUC 0.6918801775547818\n",
      "validation AUC 0.6921178962176379\n",
      "validation AUC 0.6923534224782533\n",
      "validation AUC 0.6925930967426224\n",
      "validation AUC 0.6928413517075089\n",
      "validation AUC 0.6930840032754063\n",
      "validation AUC 0.6933391148186596\n",
      "validation AUC 0.6935932578735444\n",
      "validation AUC 0.6935932578735444\n",
      "train auc 0.6867860981462892\n",
      "Test AUC 0.7013403608146523\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-04 ... 9.99829465e-01\n",
      " 9.99829465e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [26.28668594 25.28668594 10.36103916 ...  0.06642528  0.06353095\n",
      "  0.03193267]\n",
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.033s | Train Loss: 0.210066 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.018s | Train Loss: 0.207266 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.018s | Train Loss: 0.204533 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.018s | Train Loss: 0.201807 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.198762 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.017s | Train Loss: 0.197366 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.195165 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.192914 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.190694 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.188759 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.186859 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.184324 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.182894 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.179963 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.178510 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.177129 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.018s | Train Loss: 0.175301 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.172816 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.170999 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.169318 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.018s | Train Loss: 0.167149 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.018s | Train Loss: 0.165875 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.018s | Train Loss: 0.164686 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.018s | Train Loss: 0.162126 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.018s | Train Loss: 0.160522 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.017s | Train Loss: 0.159104 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.018s | Train Loss: 0.157109 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.018s | Train Loss: 0.154738 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.017s | Train Loss: 0.154112 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.151653 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.019s | Train Loss: 0.151015 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.017s | Train Loss: 0.150088 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.018s | Train Loss: 0.147728 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.018s | Train Loss: 0.145401 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.144372 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.019s | Train Loss: 0.143698 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.141080 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.017s | Train Loss: 0.140867 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.139066 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.137988 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.135695 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.017s | Train Loss: 0.134675 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.017s | Train Loss: 0.133106 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.017s | Train Loss: 0.130874 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.018s | Train Loss: 0.129839 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.018s | Train Loss: 0.129406 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.128823 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.018s | Train Loss: 0.126130 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.126094 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.123405 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.122916 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.121671 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.018s | Train Loss: 0.120339 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.017s | Train Loss: 0.119409 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.018s | Train Loss: 0.116968 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.018s | Train Loss: 0.117159 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.116633 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.114762 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.016s | Train Loss: 0.113860 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.112637 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.016s | Train Loss: 0.111626 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.110069 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.109944 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.018s | Train Loss: 0.108350 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.108147 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.017s | Train Loss: 0.106316 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.105689 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.104151 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.017s | Train Loss: 0.103702 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.017s | Train Loss: 0.102772 |\n",
      "INFO:root:Pretraining Time: 1.264s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.105837\n",
      "INFO:root:Test AUC: 53.83%\n",
      "INFO:root:Test AUC: 53.83%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.412s | Train Loss: 1.610466 || Validation Loss: 0.011147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.384s | Train Loss: 1.568705 || Validation Loss: 0.010944 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.416s | Train Loss: 1.447498 || Validation Loss: 0.011451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.414s | Train Loss: 1.383343 || Validation Loss: 0.010599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.414s | Train Loss: 1.364831 || Validation Loss: 0.010826 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.414s | Train Loss: 1.333037 || Validation Loss: 0.010715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.416s | Train Loss: 1.322757 || Validation Loss: 0.010955 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.415s | Train Loss: 1.250521 || Validation Loss: 0.010742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.420s | Train Loss: 1.184453 || Validation Loss: 0.010614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.387s | Train Loss: 1.179311 || Validation Loss: 0.010184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.443s | Train Loss: 1.133645 || Validation Loss: 0.011017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.417s | Train Loss: 1.167774 || Validation Loss: 0.010893 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.420s | Train Loss: 1.082436 || Validation Loss: 0.010030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.423s | Train Loss: 1.075681 || Validation Loss: 0.009852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.420s | Train Loss: 1.062372 || Validation Loss: 0.010257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.420s | Train Loss: 1.055631 || Validation Loss: 0.010115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.419s | Train Loss: 1.018631 || Validation Loss: 0.010288 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.421s | Train Loss: 0.993918 || Validation Loss: 0.010287 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.421s | Train Loss: 0.955099 || Validation Loss: 0.009371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.419s | Train Loss: 0.962644 || Validation Loss: 0.010004 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.422s | Train Loss: 0.964241 || Validation Loss: 0.010017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.421s | Train Loss: 0.923252 || Validation Loss: 0.009445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.421s | Train Loss: 0.936214 || Validation Loss: 0.010301 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.418s | Train Loss: 0.879403 || Validation Loss: 0.009700 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.449s | Train Loss: 0.924357 || Validation Loss: 0.009223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.425s | Train Loss: 0.859511 || Validation Loss: 0.009132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.423s | Train Loss: 0.887149 || Validation Loss: 0.009853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.456s | Train Loss: 0.869666 || Validation Loss: 0.009900 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.424s | Train Loss: 0.841214 || Validation Loss: 0.010096 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.426s | Train Loss: 0.854396 || Validation Loss: 0.009384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.426s | Train Loss: 0.845298 || Validation Loss: 0.009306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.450s | Train Loss: 0.809938 || Validation Loss: 0.009729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.428s | Train Loss: 0.828115 || Validation Loss: 0.009778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.429s | Train Loss: 0.820664 || Validation Loss: 0.010580 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.456s | Train Loss: 0.798540 || Validation Loss: 0.009977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.425s | Train Loss: 0.814022 || Validation Loss: 0.009981 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.426s | Train Loss: 0.801156 || Validation Loss: 0.010549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.454s | Train Loss: 0.771935 || Validation Loss: 0.010688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.429s | Train Loss: 0.777487 || Validation Loss: 0.009699 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.461s | Train Loss: 0.795126 || Validation Loss: 0.010034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.431s | Train Loss: 0.759566 || Validation Loss: 0.010155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.457s | Train Loss: 0.766501 || Validation Loss: 0.010187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.428s | Train Loss: 0.750904 || Validation Loss: 0.010046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.453s | Train Loss: 0.767443 || Validation Loss: 0.010275 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.432s | Train Loss: 0.756175 || Validation Loss: 0.009515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.460s | Train Loss: 0.753816 || Validation Loss: 0.010041 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.433s | Train Loss: 0.736937 || Validation Loss: 0.009790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.462s | Train Loss: 0.741802 || Validation Loss: 0.010481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.459s | Train Loss: 0.716851 || Validation Loss: 0.009526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.436s | Train Loss: 0.740561 || Validation Loss: 0.009813 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.468s | Train Loss: 0.743098 || Validation Loss: 0.009163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.433s | Train Loss: 0.710844 || Validation Loss: 0.009933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.462s | Train Loss: 0.743562 || Validation Loss: 0.010165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.437s | Train Loss: 0.745705 || Validation Loss: 0.009073 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.467s | Train Loss: 0.705964 || Validation Loss: 0.009979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.439s | Train Loss: 0.729298 || Validation Loss: 0.009375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.465s | Train Loss: 0.724402 || Validation Loss: 0.009138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.465s | Train Loss: 0.704091 || Validation Loss: 0.009276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.464s | Train Loss: 0.716563 || Validation Loss: 0.010618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.439s | Train Loss: 0.707413 || Validation Loss: 0.010063 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.463s | Train Loss: 0.720862 || Validation Loss: 0.008820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.467s | Train Loss: 0.708826 || Validation Loss: 0.010022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.465s | Train Loss: 0.727546 || Validation Loss: 0.010140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.455s | Train Loss: 0.738692 || Validation Loss: 0.009669 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.468s | Train Loss: 0.713716 || Validation Loss: 0.009291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.469s | Train Loss: 0.722270 || Validation Loss: 0.009861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.473s | Train Loss: 0.731043 || Validation Loss: 0.009322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.469s | Train Loss: 0.709030 || Validation Loss: 0.009011 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.475s | Train Loss: 0.714801 || Validation Loss: 0.010110 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.475s | Train Loss: 0.725767 || Validation Loss: 0.010891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.785s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.756%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008317\n",
      "INFO:root:Test AUC: 74.73%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 948\n",
      "len all outlier 16676 len chosen outlier 52\n",
      "final sizes 948 52 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 948, -1.0: 52})\n",
      "Resampled dataset shape Counter({1.0: 948, -1.0: 227})\n",
      "class weights 0.19319148936170213 0.8068085106382978\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5301645972589013\n",
      "validation AUC 0.5352301532787163\n",
      "validation AUC 0.5404774551871918\n",
      "validation AUC 0.5458660526308508\n",
      "validation AUC 0.5516534703918206\n",
      "validation AUC 0.5576773669376802\n",
      "validation AUC 0.5645004978668778\n",
      "validation AUC 0.5714262140640263\n",
      "validation AUC 0.57794034399004\n",
      "validation AUC 0.5842510541091258\n",
      "validation AUC 0.5906199480294242\n",
      "validation AUC 0.5969344283353728\n",
      "validation AUC 0.6030028221325348\n",
      "validation AUC 0.6091981730477988\n",
      "validation AUC 0.6155345960009306\n",
      "validation AUC 0.6219769646637973\n",
      "validation AUC 0.6281202832753978\n",
      "validation AUC 0.633770109170989\n",
      "validation AUC 0.6391494846896889\n",
      "validation AUC 0.6440224073087453\n",
      "validation AUC 0.6489466305438243\n",
      "validation AUC 0.6536654570850032\n",
      "validation AUC 0.6584060453469668\n",
      "validation AUC 0.6629082790429972\n",
      "validation AUC 0.6669423443974438\n",
      "validation AUC 0.6710748071058524\n",
      "validation AUC 0.6748296391710334\n",
      "validation AUC 0.6786882192223443\n",
      "validation AUC 0.6820584922275083\n",
      "validation AUC 0.6852933498054403\n",
      "validation AUC 0.6885488462962132\n",
      "validation AUC 0.691363188353108\n",
      "validation AUC 0.693930190719838\n",
      "validation AUC 0.6964231383152516\n",
      "validation AUC 0.6986998362935376\n",
      "validation AUC 0.7010178067761406\n",
      "validation AUC 0.7035232436144156\n",
      "validation AUC 0.7059471757708475\n",
      "validation AUC 0.7082739424692366\n",
      "validation AUC 0.7104498378367333\n",
      "validation AUC 0.7125778063334031\n",
      "validation AUC 0.7150995212261787\n",
      "validation AUC 0.7174738582639623\n",
      "validation AUC 0.719698454760844\n",
      "validation AUC 0.7216240650628357\n",
      "validation AUC 0.7235101988210837\n",
      "validation AUC 0.7256303928039824\n",
      "validation AUC 0.7277072761998986\n",
      "validation AUC 0.7299590036745087\n",
      "validation AUC 0.7301512379729177\n",
      "validation AUC 0.7303220112076454\n",
      "validation AUC 0.730500662722755\n",
      "validation AUC 0.7306652977633452\n",
      "validation AUC 0.7308340382367721\n",
      "validation AUC 0.7309902256109619\n",
      "validation AUC 0.7311626085584998\n",
      "validation AUC 0.7313399856067729\n",
      "validation AUC 0.7315303494455032\n",
      "validation AUC 0.7317136944042446\n",
      "validation AUC 0.7318838317358952\n",
      "validation AUC 0.7320433156954926\n",
      "validation AUC 0.7322048483804853\n",
      "validation AUC 0.732379915956495\n",
      "validation AUC 0.7325565666384919\n",
      "validation AUC 0.7327046123247488\n",
      "validation AUC 0.7328873958795183\n",
      "validation AUC 0.7330585043602198\n",
      "validation AUC 0.7332317653329271\n",
      "validation AUC 0.7333993617129516\n",
      "validation AUC 0.733599503559248\n",
      "validation AUC 0.733599503559248\n",
      "train auc 0.7558597030074921\n",
      "Test AUC 0.7473100243326297\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.95016580e-01\n",
      " 9.95016580e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [11.80612946 10.80612946 10.42433643 ...  0.11363692  0.11358375\n",
      "  0.04915268]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.034s | Train Loss: 0.203132 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.200780 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.017s | Train Loss: 0.198120 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.019s | Train Loss: 0.196764 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.018s | Train Loss: 0.193881 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.018s | Train Loss: 0.192184 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.018s | Train Loss: 0.189412 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.018s | Train Loss: 0.187459 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.018s | Train Loss: 0.185885 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.018s | Train Loss: 0.183428 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.018s | Train Loss: 0.181274 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.018s | Train Loss: 0.178887 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.018s | Train Loss: 0.177465 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.018s | Train Loss: 0.175847 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.017s | Train Loss: 0.173557 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.017s | Train Loss: 0.171619 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.017s | Train Loss: 0.169173 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.018s | Train Loss: 0.167920 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.018s | Train Loss: 0.165573 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.018s | Train Loss: 0.164270 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.017s | Train Loss: 0.161577 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.019s | Train Loss: 0.160128 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.017s | Train Loss: 0.158450 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.017s | Train Loss: 0.156706 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.155729 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.019s | Train Loss: 0.153419 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.019s | Train Loss: 0.152189 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.017s | Train Loss: 0.150295 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.018s | Train Loss: 0.148407 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.018s | Train Loss: 0.147446 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.018s | Train Loss: 0.145640 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.019s | Train Loss: 0.143767 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.017s | Train Loss: 0.142976 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.019s | Train Loss: 0.141572 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.017s | Train Loss: 0.139798 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.017s | Train Loss: 0.137647 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.018s | Train Loss: 0.136944 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.018s | Train Loss: 0.135471 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.018s | Train Loss: 0.134250 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.018s | Train Loss: 0.133189 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.017s | Train Loss: 0.131903 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.018s | Train Loss: 0.129313 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.018s | Train Loss: 0.129549 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.018s | Train Loss: 0.127581 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.125403 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.017s | Train Loss: 0.124562 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.018s | Train Loss: 0.123689 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.017s | Train Loss: 0.122744 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.018s | Train Loss: 0.120894 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.017s | Train Loss: 0.120424 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.018s | Train Loss: 0.118625 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.018s | Train Loss: 0.117530 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.017s | Train Loss: 0.116296 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.018s | Train Loss: 0.115620 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.019s | Train Loss: 0.114277 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.017s | Train Loss: 0.112669 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.017s | Train Loss: 0.111794 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.017s | Train Loss: 0.110650 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.018s | Train Loss: 0.109344 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.017s | Train Loss: 0.108987 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.017s | Train Loss: 0.107310 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.017s | Train Loss: 0.105844 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.017s | Train Loss: 0.105409 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.017s | Train Loss: 0.104626 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.017s | Train Loss: 0.103522 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.103393 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.017s | Train Loss: 0.101755 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.017s | Train Loss: 0.101739 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.018s | Train Loss: 0.100228 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.016s | Train Loss: 0.098909 |\n",
      "INFO:root:Pretraining Time: 1.258s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.102649\n",
      "INFO:root:Test AUC: 55.46%\n",
      "INFO:root:Test AUC: 55.46%\n",
      "INFO:root:Test Time: 0.226s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.409s | Train Loss: 1.704834 || Validation Loss: 0.011057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.385s | Train Loss: 1.607322 || Validation Loss: 0.010837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.407s | Train Loss: 1.508427 || Validation Loss: 0.011107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.415s | Train Loss: 1.507808 || Validation Loss: 0.011632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.417s | Train Loss: 1.436110 || Validation Loss: 0.011791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.413s | Train Loss: 1.430595 || Validation Loss: 0.011366 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.414s | Train Loss: 1.379052 || Validation Loss: 0.011140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.414s | Train Loss: 1.320257 || Validation Loss: 0.010933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.415s | Train Loss: 1.291216 || Validation Loss: 0.011335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.386s | Train Loss: 1.241192 || Validation Loss: 0.010208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.446s | Train Loss: 1.201762 || Validation Loss: 0.011360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.416s | Train Loss: 1.219779 || Validation Loss: 0.010609 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.416s | Train Loss: 1.189591 || Validation Loss: 0.011073 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.417s | Train Loss: 1.211730 || Validation Loss: 0.009841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.416s | Train Loss: 1.134129 || Validation Loss: 0.010748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.417s | Train Loss: 1.172830 || Validation Loss: 0.010837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.419s | Train Loss: 1.074596 || Validation Loss: 0.010666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.418s | Train Loss: 1.079014 || Validation Loss: 0.010240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.418s | Train Loss: 1.065021 || Validation Loss: 0.010240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.419s | Train Loss: 1.065976 || Validation Loss: 0.009537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.420s | Train Loss: 1.004388 || Validation Loss: 0.010120 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.419s | Train Loss: 1.021066 || Validation Loss: 0.010451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.421s | Train Loss: 1.006991 || Validation Loss: 0.010438 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.419s | Train Loss: 0.980970 || Validation Loss: 0.009810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.451s | Train Loss: 0.932282 || Validation Loss: 0.009749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.424s | Train Loss: 0.948224 || Validation Loss: 0.010138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.418s | Train Loss: 0.933392 || Validation Loss: 0.009572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.448s | Train Loss: 0.936079 || Validation Loss: 0.009448 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.422s | Train Loss: 0.920406 || Validation Loss: 0.010330 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.422s | Train Loss: 0.898393 || Validation Loss: 0.010194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.423s | Train Loss: 0.937779 || Validation Loss: 0.010469 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.452s | Train Loss: 0.890024 || Validation Loss: 0.010073 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.423s | Train Loss: 0.902722 || Validation Loss: 0.009543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.423s | Train Loss: 0.910872 || Validation Loss: 0.010223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.453s | Train Loss: 0.883686 || Validation Loss: 0.010580 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.425s | Train Loss: 0.902089 || Validation Loss: 0.009928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.425s | Train Loss: 0.866653 || Validation Loss: 0.009455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.455s | Train Loss: 0.878446 || Validation Loss: 0.009688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.425s | Train Loss: 0.845267 || Validation Loss: 0.010508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.457s | Train Loss: 0.829222 || Validation Loss: 0.010620 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.423s | Train Loss: 0.853163 || Validation Loss: 0.010180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.454s | Train Loss: 0.818289 || Validation Loss: 0.010063 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.425s | Train Loss: 0.813445 || Validation Loss: 0.009806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.449s | Train Loss: 0.828383 || Validation Loss: 0.009890 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.426s | Train Loss: 0.796187 || Validation Loss: 0.010011 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.459s | Train Loss: 0.809563 || Validation Loss: 0.010175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.426s | Train Loss: 0.838545 || Validation Loss: 0.010457 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.458s | Train Loss: 0.771333 || Validation Loss: 0.009753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.456s | Train Loss: 0.786739 || Validation Loss: 0.010599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.429s | Train Loss: 0.800352 || Validation Loss: 0.010960 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.459s | Train Loss: 0.793649 || Validation Loss: 0.009904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.432s | Train Loss: 0.778750 || Validation Loss: 0.009757 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.459s | Train Loss: 0.756411 || Validation Loss: 0.009760 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.429s | Train Loss: 0.768762 || Validation Loss: 0.010058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.458s | Train Loss: 0.748495 || Validation Loss: 0.009548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.436s | Train Loss: 0.770028 || Validation Loss: 0.009839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.456s | Train Loss: 0.762520 || Validation Loss: 0.009424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.461s | Train Loss: 0.784318 || Validation Loss: 0.009481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.464s | Train Loss: 0.772834 || Validation Loss: 0.009696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.433s | Train Loss: 0.777844 || Validation Loss: 0.009493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.463s | Train Loss: 0.767638 || Validation Loss: 0.009762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.465s | Train Loss: 0.785009 || Validation Loss: 0.010124 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.464s | Train Loss: 0.774270 || Validation Loss: 0.009493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.461s | Train Loss: 0.781586 || Validation Loss: 0.008987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.468s | Train Loss: 0.776034 || Validation Loss: 0.009692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.466s | Train Loss: 0.776093 || Validation Loss: 0.009737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.468s | Train Loss: 0.777951 || Validation Loss: 0.010614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.467s | Train Loss: 0.768008 || Validation Loss: 0.010130 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.469s | Train Loss: 0.776790 || Validation Loss: 0.010150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.470s | Train Loss: 0.765111 || Validation Loss: 0.009643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.581s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.725%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008074\n",
      "INFO:root:Test AUC: 68.37%\n",
      "INFO:root:Test Time: 0.193s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 937\n",
      "len all outlier 16676 len chosen outlier 63\n",
      "final sizes 937 63 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 937, -1.0: 63})\n",
      "Resampled dataset shape Counter({1.0: 937, -1.0: 224})\n",
      "class weights 0.1929371231696813 0.8070628768303187\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5376365329138111\n",
      "validation AUC 0.5416909631308185\n",
      "validation AUC 0.5453430369836973\n",
      "validation AUC 0.5487370485966178\n",
      "validation AUC 0.5522840627469778\n",
      "validation AUC 0.5559988249362554\n",
      "validation AUC 0.5597080129960497\n",
      "validation AUC 0.5637848142300954\n",
      "validation AUC 0.5674533151356852\n",
      "validation AUC 0.570987728294526\n",
      "validation AUC 0.5743176068205421\n",
      "validation AUC 0.5779625979370558\n",
      "validation AUC 0.5813638040349993\n",
      "validation AUC 0.5844329011037149\n",
      "validation AUC 0.5874137859104313\n",
      "validation AUC 0.5911350321825491\n",
      "validation AUC 0.5944202963872404\n",
      "validation AUC 0.597034225208241\n",
      "validation AUC 0.5999231541729183\n",
      "validation AUC 0.6030894459671825\n",
      "validation AUC 0.6068198662719775\n",
      "validation AUC 0.6105102725751977\n",
      "validation AUC 0.613932141532123\n",
      "validation AUC 0.6176727655536572\n",
      "validation AUC 0.6208236732826838\n",
      "validation AUC 0.6237533319724831\n",
      "validation AUC 0.6271073668333815\n",
      "validation AUC 0.6305530222903452\n",
      "validation AUC 0.6341240810269299\n",
      "validation AUC 0.6375000000000001\n",
      "validation AUC 0.6404535888666835\n",
      "validation AUC 0.6429789488942523\n",
      "validation AUC 0.6455842943540534\n",
      "validation AUC 0.6482814759251885\n",
      "validation AUC 0.6508170636156374\n",
      "validation AUC 0.653723481245276\n",
      "validation AUC 0.6565882352502946\n",
      "validation AUC 0.6584456336393685\n",
      "validation AUC 0.6603501447304762\n",
      "validation AUC 0.6627766204992164\n",
      "validation AUC 0.6656593553954274\n",
      "validation AUC 0.668438270467086\n",
      "validation AUC 0.6708486145188539\n",
      "validation AUC 0.673581728604868\n",
      "validation AUC 0.6758798078263653\n",
      "validation AUC 0.6780840447820508\n",
      "validation AUC 0.6798097048068737\n",
      "validation AUC 0.6813370295887031\n",
      "validation AUC 0.6830678539979307\n",
      "validation AUC 0.6832969839782224\n",
      "validation AUC 0.6834851900035823\n",
      "validation AUC 0.683680694280576\n",
      "validation AUC 0.6838807722704966\n",
      "validation AUC 0.6841121452059932\n",
      "validation AUC 0.6843312843241324\n",
      "validation AUC 0.6845604728394354\n",
      "validation AUC 0.6847669151814916\n",
      "validation AUC 0.684947977274793\n",
      "validation AUC 0.6851547122919053\n",
      "validation AUC 0.6853648210542134\n",
      "validation AUC 0.685646629883975\n",
      "validation AUC 0.6859192992699301\n",
      "validation AUC 0.6861799237469729\n",
      "validation AUC 0.6864331435450889\n",
      "validation AUC 0.6866678982078284\n",
      "validation AUC 0.6868745693685647\n",
      "validation AUC 0.6871118330547422\n",
      "validation AUC 0.687334604004263\n",
      "validation AUC 0.6875197688697188\n",
      "validation AUC 0.6877608852239113\n",
      "validation AUC 0.6877608852239113\n",
      "train auc 0.7251631630754676\n",
      "Test AUC 0.6837448007371402\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99488394e-01\n",
      " 9.99488394e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [11.10733604 10.10733604  9.1756506  ...  0.07738125  0.07704231\n",
      "  0.03822992]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/preprocessing.py:185: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  solution_idx = solution_idx.append(\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.031s | Train Loss: 0.229715 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.016s | Train Loss: 0.227734 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.015s | Train Loss: 0.225145 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.015s | Train Loss: 0.223115 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.016s | Train Loss: 0.220319 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.015s | Train Loss: 0.219032 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.017s | Train Loss: 0.216931 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.016s | Train Loss: 0.215167 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.017s | Train Loss: 0.212829 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.015s | Train Loss: 0.210972 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.015s | Train Loss: 0.208858 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.015s | Train Loss: 0.206679 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.016s | Train Loss: 0.205237 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.016s | Train Loss: 0.203273 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.016s | Train Loss: 0.200829 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.016s | Train Loss: 0.199388 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.015s | Train Loss: 0.197670 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.017s | Train Loss: 0.195814 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.015s | Train Loss: 0.193581 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.016s | Train Loss: 0.191649 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.016s | Train Loss: 0.190579 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.016s | Train Loss: 0.188524 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.015s | Train Loss: 0.186572 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.016s | Train Loss: 0.184443 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.017s | Train Loss: 0.183073 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.015s | Train Loss: 0.180937 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.016s | Train Loss: 0.179065 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.016s | Train Loss: 0.178495 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.016s | Train Loss: 0.176778 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.016s | Train Loss: 0.175013 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.017s | Train Loss: 0.172894 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.016s | Train Loss: 0.171367 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.016s | Train Loss: 0.169076 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.015s | Train Loss: 0.167989 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.016s | Train Loss: 0.166888 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.016s | Train Loss: 0.165371 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.016s | Train Loss: 0.163830 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.015s | Train Loss: 0.161935 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.016s | Train Loss: 0.160137 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.016s | Train Loss: 0.158809 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.016s | Train Loss: 0.157367 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.016s | Train Loss: 0.155094 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.016s | Train Loss: 0.153890 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.016s | Train Loss: 0.152855 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.017s | Train Loss: 0.150998 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.016s | Train Loss: 0.149826 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.017s | Train Loss: 0.148293 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.015s | Train Loss: 0.146169 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.017s | Train Loss: 0.144825 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.015s | Train Loss: 0.144929 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.016s | Train Loss: 0.142599 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.017s | Train Loss: 0.142167 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.015s | Train Loss: 0.139897 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.016s | Train Loss: 0.138819 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.016s | Train Loss: 0.136827 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.016s | Train Loss: 0.136043 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.016s | Train Loss: 0.134308 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.015s | Train Loss: 0.132512 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.015s | Train Loss: 0.132764 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.015s | Train Loss: 0.130907 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.015s | Train Loss: 0.129236 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.015s | Train Loss: 0.127774 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.016s | Train Loss: 0.126997 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.015s | Train Loss: 0.125550 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.015s | Train Loss: 0.125301 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.016s | Train Loss: 0.123609 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.014s | Train Loss: 0.121890 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.015s | Train Loss: 0.121467 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.015s | Train Loss: 0.120026 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.015s | Train Loss: 0.119262 |\n",
      "INFO:root:Pretraining Time: 1.125s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.125866\n",
      "INFO:root:Test AUC: 53.06%\n",
      "INFO:root:Test AUC: 53.06%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.407s | Train Loss: 1.406425 || Validation Loss: 0.013989 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.385s | Train Loss: 1.388642 || Validation Loss: 0.013722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.415s | Train Loss: 1.336426 || Validation Loss: 0.014217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.412s | Train Loss: 1.295542 || Validation Loss: 0.012908 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.409s | Train Loss: 1.280592 || Validation Loss: 0.012343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.411s | Train Loss: 1.214726 || Validation Loss: 0.013589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.412s | Train Loss: 1.184986 || Validation Loss: 0.013497 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.416s | Train Loss: 1.140964 || Validation Loss: 0.012725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.417s | Train Loss: 1.168245 || Validation Loss: 0.012854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.386s | Train Loss: 1.121932 || Validation Loss: 0.012679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.444s | Train Loss: 1.066007 || Validation Loss: 0.012654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.467s | Train Loss: 1.032367 || Validation Loss: 0.012516 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.415s | Train Loss: 1.055643 || Validation Loss: 0.012335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.417s | Train Loss: 0.993337 || Validation Loss: 0.012056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.419s | Train Loss: 1.001305 || Validation Loss: 0.011501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.422s | Train Loss: 0.981350 || Validation Loss: 0.012437 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.421s | Train Loss: 0.958473 || Validation Loss: 0.011808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.423s | Train Loss: 0.962341 || Validation Loss: 0.011820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.418s | Train Loss: 0.933893 || Validation Loss: 0.011677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.418s | Train Loss: 0.942956 || Validation Loss: 0.011664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.418s | Train Loss: 0.960166 || Validation Loss: 0.011608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.417s | Train Loss: 0.921955 || Validation Loss: 0.011119 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.416s | Train Loss: 0.906731 || Validation Loss: 0.012564 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.416s | Train Loss: 0.922050 || Validation Loss: 0.010922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.424s | Train Loss: 0.888733 || Validation Loss: 0.010735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.425s | Train Loss: 0.869395 || Validation Loss: 0.010581 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.420s | Train Loss: 0.872888 || Validation Loss: 0.011253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.423s | Train Loss: 0.855211 || Validation Loss: 0.010089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.450s | Train Loss: 0.862840 || Validation Loss: 0.011297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.421s | Train Loss: 0.828767 || Validation Loss: 0.011302 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.420s | Train Loss: 0.846762 || Validation Loss: 0.011423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.450s | Train Loss: 0.812705 || Validation Loss: 0.011148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.429s | Train Loss: 0.821564 || Validation Loss: 0.010825 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.423s | Train Loss: 0.848075 || Validation Loss: 0.010194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.464s | Train Loss: 0.808573 || Validation Loss: 0.011266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.427s | Train Loss: 0.809449 || Validation Loss: 0.010803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.431s | Train Loss: 0.785420 || Validation Loss: 0.011150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.461s | Train Loss: 0.775208 || Validation Loss: 0.011026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.426s | Train Loss: 0.792327 || Validation Loss: 0.010446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.426s | Train Loss: 0.783216 || Validation Loss: 0.010304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.460s | Train Loss: 0.775802 || Validation Loss: 0.011887 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.428s | Train Loss: 0.748850 || Validation Loss: 0.011276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.454s | Train Loss: 0.756441 || Validation Loss: 0.010633 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.423s | Train Loss: 0.733948 || Validation Loss: 0.010755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.422s | Train Loss: 0.765717 || Validation Loss: 0.010405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.453s | Train Loss: 0.751318 || Validation Loss: 0.010869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.429s | Train Loss: 0.739614 || Validation Loss: 0.010929 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.460s | Train Loss: 0.730303 || Validation Loss: 0.009365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.427s | Train Loss: 0.711961 || Validation Loss: 0.010166 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.456s | Train Loss: 0.700849 || Validation Loss: 0.010938 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.461s | Train Loss: 0.692780 || Validation Loss: 0.010703 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.425s | Train Loss: 0.709651 || Validation Loss: 0.011470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.433s | Train Loss: 0.705582 || Validation Loss: 0.010971 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.452s | Train Loss: 0.705384 || Validation Loss: 0.010917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.461s | Train Loss: 0.676949 || Validation Loss: 0.010950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.434s | Train Loss: 0.699941 || Validation Loss: 0.009618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.432s | Train Loss: 0.719556 || Validation Loss: 0.010927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.466s | Train Loss: 0.718431 || Validation Loss: 0.010962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.465s | Train Loss: 0.725690 || Validation Loss: 0.010773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.432s | Train Loss: 0.727686 || Validation Loss: 0.011378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.464s | Train Loss: 0.726708 || Validation Loss: 0.011114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.432s | Train Loss: 0.690413 || Validation Loss: 0.011701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.461s | Train Loss: 0.715579 || Validation Loss: 0.011079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.438s | Train Loss: 0.710867 || Validation Loss: 0.011213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.461s | Train Loss: 0.724711 || Validation Loss: 0.009824 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.431s | Train Loss: 0.725394 || Validation Loss: 0.011218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.467s | Train Loss: 0.707552 || Validation Loss: 0.011002 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.436s | Train Loss: 0.710975 || Validation Loss: 0.010262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.494s | Train Loss: 0.686444 || Validation Loss: 0.010420 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.469s | Train Loss: 0.703761 || Validation Loss: 0.010365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.484s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.779%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.008671\n",
      "INFO:root:Test AUC: 68.60%\n",
      "INFO:root:Test Time: 0.184s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 2\n",
      "experiment_method type 2\n",
      "method num 2\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 2\n",
      "i index 0\n",
      "n cluster 200\n",
      "i index 1\n",
      "n cluster 200\n",
      "i index 2\n",
      "n cluster 200\n",
      "i index 3\n",
      "n cluster 200\n",
      "i index 4\n",
      "n cluster 200\n",
      "size of chosen indexes of normal 926\n",
      "len all outlier 16676 len chosen outlier 74\n",
      "final sizes 926 74 0 0\n",
      "len of selected to training 1000\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 926, -1.0: 74})\n",
      "Resampled dataset shape Counter({1.0: 926, -1.0: 222})\n",
      "class weights 0.19337979094076654 0.8066202090592335\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.48556267737438763\n",
      "validation AUC 0.4904223524561184\n",
      "validation AUC 0.4953797464178169\n",
      "validation AUC 0.5004665320219505\n",
      "validation AUC 0.5060516288377948\n",
      "validation AUC 0.5117614903695007\n",
      "validation AUC 0.5175284288585693\n",
      "validation AUC 0.5231023880581765\n",
      "validation AUC 0.528226045398052\n",
      "validation AUC 0.5332266887510395\n",
      "validation AUC 0.53795229737148\n",
      "validation AUC 0.5428672561106826\n",
      "validation AUC 0.5476948194173616\n",
      "validation AUC 0.5527747989960927\n",
      "validation AUC 0.5576232885693042\n",
      "validation AUC 0.562330921619917\n",
      "validation AUC 0.5668952343560925\n",
      "validation AUC 0.5715611585504857\n",
      "validation AUC 0.5760613940740857\n",
      "validation AUC 0.5803484583900147\n",
      "validation AUC 0.5846393886773706\n",
      "validation AUC 0.5889012669743551\n",
      "validation AUC 0.593449599652451\n",
      "validation AUC 0.5981700704953106\n",
      "validation AUC 0.6028150072274775\n",
      "validation AUC 0.6074506023039805\n",
      "validation AUC 0.6118192298367641\n",
      "validation AUC 0.6158025506577952\n",
      "validation AUC 0.619324397413987\n",
      "validation AUC 0.6229547201888105\n",
      "validation AUC 0.6268744560234975\n",
      "validation AUC 0.630764344323796\n",
      "validation AUC 0.6345089753329205\n",
      "validation AUC 0.6379510814396548\n",
      "validation AUC 0.641200727089982\n",
      "validation AUC 0.6443820065078161\n",
      "validation AUC 0.6474442762656706\n",
      "validation AUC 0.6502117637067178\n",
      "validation AUC 0.6530390020908706\n",
      "validation AUC 0.6557750988017778\n",
      "validation AUC 0.6579929823971385\n",
      "validation AUC 0.6605907928045784\n",
      "validation AUC 0.6635223804890675\n",
      "validation AUC 0.6661625888694506\n",
      "validation AUC 0.6686043396154527\n",
      "validation AUC 0.6706550552006441\n",
      "validation AUC 0.672584621937262\n",
      "validation AUC 0.674496960755787\n",
      "validation AUC 0.6765087851473476\n",
      "validation AUC 0.6767118404407964\n",
      "validation AUC 0.6769573229746939\n",
      "validation AUC 0.6772122589129133\n",
      "validation AUC 0.677463642840221\n",
      "validation AUC 0.6776988232121332\n",
      "validation AUC 0.6779325322267166\n",
      "validation AUC 0.6781732867281121\n",
      "validation AUC 0.6784402303256909\n",
      "validation AUC 0.6786909623858275\n",
      "validation AUC 0.6789217393284821\n",
      "validation AUC 0.6791438238219617\n",
      "validation AUC 0.6793553400852227\n",
      "validation AUC 0.6796111460665644\n",
      "validation AUC 0.6798720951468515\n",
      "validation AUC 0.6800891482900433\n",
      "validation AUC 0.6802974424670016\n",
      "validation AUC 0.6805043344643716\n",
      "validation AUC 0.6807289865163005\n",
      "validation AUC 0.6809489451245976\n",
      "validation AUC 0.6812034659963735\n",
      "validation AUC 0.6814472563363085\n",
      "validation AUC 0.6814472563363085\n",
      "train auc 0.7793875181836809\n",
      "Test AUC 0.6860026003608253\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.99715775e-01\n",
      " 9.99715775e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [13.06443214 12.06443214 11.86502361 ...  0.04722062  0.04682755\n",
      "  0.02352292]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "   python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1k_points_by_5_k_means\" --iteration_num $i --experiment_method 2 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.00 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 1000 --k_means_chosen_k 5 \n",
    "\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feddfceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"10k_points_by_20_k_means\" --iteration_num $i --experiment_method 2 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.00 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 10000 --k_means_chosen_k 20 \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58971205",
   "metadata": {},
   "source": [
    "# 80% labeled, unlabeled selection by method (6.3 Studying a restricted dataset example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a8d706",
   "metadata": {},
   "source": [
    "# 80% labeled 0 unlabeled "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d137c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {2..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.8 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.8 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1715ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#80% labeled 10% unlabeled random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cae32cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 012/020 | Train Time: 4.003s | Train Loss: 0.014798 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.80\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.20\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.80\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 013/020 | Train Time: 4.523s | Train Loss: 0.014625 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 20\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 014/020 | Train Time: 5.830s | Train Loss: 0.014467 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/020 | Train Time: 5.772s | Train Loss: 0.089790 |\n",
      "INFO:root:| Epoch: 002/020 | Train Time: 5.366s | Train Loss: 0.034885 |\n",
      "INFO:root:| Epoch: 015/020 | Train Time: 5.792s | Train Loss: 0.014242 |\n",
      "INFO:root:| Epoch: 003/020 | Train Time: 5.527s | Train Loss: 0.025900 |\n",
      "INFO:root:| Epoch: 016/020 | Train Time: 5.844s | Train Loss: 0.014007 |\n",
      "INFO:root:| Epoch: 004/020 | Train Time: 5.400s | Train Loss: 0.022067 |\n",
      "INFO:root:| Epoch: 017/020 | Train Time: 5.656s | Train Loss: 0.013880 |\n",
      "INFO:root:| Epoch: 005/020 | Train Time: 5.400s | Train Loss: 0.020489 |\n",
      "INFO:root:| Epoch: 018/020 | Train Time: 5.589s | Train Loss: 0.013634 |\n",
      "INFO:root:| Epoch: 006/020 | Train Time: 5.316s | Train Loss: 0.019529 |\n",
      "INFO:root:| Epoch: 019/020 | Train Time: 5.786s | Train Loss: 0.013476 |\n",
      "INFO:root:| Epoch: 007/020 | Train Time: 5.512s | Train Loss: 0.018595 |\n",
      "INFO:root:| Epoch: 020/020 | Train Time: 5.593s | Train Loss: 0.013422 |\n",
      "INFO:root:Pretraining Time: 90.711s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.013779\n",
      "INFO:root:Test AUC: 50.93%\n",
      "INFO:root:Test AUC: 50.93%\n",
      "INFO:root:Test Time: 0.328s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 1e-05\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 32\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 008/020 | Train Time: 5.193s | Train Loss: 0.017675 |\n",
      "INFO:root:| Epoch: 009/020 | Train Time: 5.646s | Train Loss: 0.016936 |\n",
      "INFO:root:| Epoch: 010/020 | Train Time: 5.484s | Train Loss: 0.016271 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 13.127s | Train Loss: 0.949138 || Validation Loss: 1.381581 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 011/020 | Train Time: 5.307s | Train Loss: 0.015770 |\n",
      "INFO:root:| Epoch: 012/020 | Train Time: 5.544s | Train Loss: 0.015305 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 13.064s | Train Loss: 0.824128 || Validation Loss: 1.465915 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 013/020 | Train Time: 5.404s | Train Loss: 0.014964 |\n",
      "INFO:root:| Epoch: 014/020 | Train Time: 5.520s | Train Loss: 0.014678 |\n",
      "INFO:root:| Epoch: 015/020 | Train Time: 5.355s | Train Loss: 0.014495 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 13.394s | Train Loss: 0.786625 || Validation Loss: 1.510101 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 016/020 | Train Time: 5.570s | Train Loss: 0.014352 |\n",
      "INFO:root:| Epoch: 017/020 | Train Time: 5.465s | Train Loss: 0.014254 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 13.826s | Train Loss: 0.767021 || Validation Loss: 1.554000 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 018/020 | Train Time: 5.300s | Train Loss: 0.014169 |\n",
      "INFO:root:| Epoch: 019/020 | Train Time: 5.571s | Train Loss: 0.014181 |\n",
      "INFO:root:| Epoch: 020/020 | Train Time: 5.548s | Train Loss: 0.014118 |\n",
      "INFO:root:Pretraining Time: 109.206s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.014268\n",
      "INFO:root:Test AUC: 52.57%\n",
      "INFO:root:Test AUC: 52.57%\n",
      "INFO:root:Test Time: 0.243s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 32\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 14.259s | Train Loss: 0.753346 || Validation Loss: 1.557701 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 13.986s | Train Loss: 0.739276 || Validation Loss: 1.577984 || Validation number of baches in epoch: 1761.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 12.670s | Train Loss: 0.755213 || Validation Loss: 0.014617 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 13.564s | Train Loss: 0.659728 || Validation Loss: 0.013714 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 15.041s | Train Loss: 0.729115 || Validation Loss: 1.576503 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 13.040s | Train Loss: 0.635599 || Validation Loss: 0.014893 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 14.874s | Train Loss: 0.723534 || Validation Loss: 1.573696 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 12.930s | Train Loss: 0.621725 || Validation Loss: 0.016806 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 14.639s | Train Loss: 0.716460 || Validation Loss: 1.585180 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 13.176s | Train Loss: 0.616146 || Validation Loss: 0.016224 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 14.680s | Train Loss: 0.707866 || Validation Loss: 1.594157 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 13.394s | Train Loss: 0.609824 || Validation Loss: 0.018743 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 15.697s | Train Loss: 0.705419 || Validation Loss: 1.597981 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 14.078s | Train Loss: 0.607999 || Validation Loss: 0.017413 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 15.433s | Train Loss: 0.701369 || Validation Loss: 1.601830 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 13.459s | Train Loss: 0.602868 || Validation Loss: 0.017667 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 15.196s | Train Loss: 0.694890 || Validation Loss: 1.595891 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 14.017s | Train Loss: 0.603525 || Validation Loss: 0.017583 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 15.970s | Train Loss: 0.691049 || Validation Loss: 1.600332 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 14.381s | Train Loss: 0.597191 || Validation Loss: 0.018600 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 15.763s | Train Loss: 0.691184 || Validation Loss: 1.604636 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 15.288s | Train Loss: 0.601299 || Validation Loss: 0.016096 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 15.801s | Train Loss: 0.595230 || Validation Loss: 0.011624 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 17.065s | Train Loss: 0.688721 || Validation Loss: 1.605705 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 15.138s | Train Loss: 0.593212 || Validation Loss: 0.012548 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 16.427s | Train Loss: 0.686385 || Validation Loss: 1.596772 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 16.138s | Train Loss: 0.592097 || Validation Loss: 0.016410 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 17.151s | Train Loss: 0.683188 || Validation Loss: 1.608289 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 15.096s | Train Loss: 0.592442 || Validation Loss: 0.020160 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 17.064s | Train Loss: 0.679096 || Validation Loss: 1.621467 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 15.467s | Train Loss: 0.589258 || Validation Loss: 0.020259 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 16.433s | Train Loss: 0.678389 || Validation Loss: 1.616901 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 15.734s | Train Loss: 0.589654 || Validation Loss: 0.016720 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 17.118s | Train Loss: 0.675994 || Validation Loss: 1.642292 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 16.334s | Train Loss: 0.590972 || Validation Loss: 0.016319 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 16.976s | Train Loss: 0.675955 || Validation Loss: 1.626956 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 16.104s | Train Loss: 0.587097 || Validation Loss: 0.019217 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 17.517s | Train Loss: 0.673985 || Validation Loss: 1.618065 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 15.841s | Train Loss: 0.586654 || Validation Loss: 0.020107 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 18.480s | Train Loss: 0.672721 || Validation Loss: 1.632013 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 16.632s | Train Loss: 0.587842 || Validation Loss: 0.017584 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 18.567s | Train Loss: 0.669324 || Validation Loss: 1.634671 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 16.856s | Train Loss: 0.584895 || Validation Loss: 0.015903 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 18.927s | Train Loss: 0.669863 || Validation Loss: 1.647980 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 17.690s | Train Loss: 0.584873 || Validation Loss: 0.019346 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 19.589s | Train Loss: 0.669009 || Validation Loss: 1.623042 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 18.591s | Train Loss: 0.584709 || Validation Loss: 0.016818 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 19.214s | Train Loss: 0.668359 || Validation Loss: 1.630745 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 19.106s | Train Loss: 0.586723 || Validation Loss: 0.015195 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 18.884s | Train Loss: 0.583003 || Validation Loss: 0.017407 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 21.120s | Train Loss: 0.667093 || Validation Loss: 1.633018 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 19.446s | Train Loss: 0.580682 || Validation Loss: 0.018765 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 20.140s | Train Loss: 0.664138 || Validation Loss: 1.654229 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 18.992s | Train Loss: 0.582246 || Validation Loss: 0.019391 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 19.817s | Train Loss: 0.660901 || Validation Loss: 1.656222 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 20.617s | Train Loss: 0.583019 || Validation Loss: 0.017407 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 20.726s | Train Loss: 0.662114 || Validation Loss: 1.642405 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 19.748s | Train Loss: 0.581880 || Validation Loss: 0.022619 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 19.564s | Train Loss: 0.661228 || Validation Loss: 1.643024 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 20.013s | Train Loss: 0.579221 || Validation Loss: 0.015484 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 21.586s | Train Loss: 0.664108 || Validation Loss: 1.640495 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 20.763s | Train Loss: 0.578636 || Validation Loss: 0.016078 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 20.229s | Train Loss: 0.662118 || Validation Loss: 1.643522 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 20.659s | Train Loss: 0.580244 || Validation Loss: 0.016033 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 21.260s | Train Loss: 0.658730 || Validation Loss: 1.643214 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 21.363s | Train Loss: 0.581224 || Validation Loss: 0.018233 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 20.688s | Train Loss: 0.662077 || Validation Loss: 1.652669 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 20.922s | Train Loss: 0.579697 || Validation Loss: 0.020712 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 21.585s | Train Loss: 0.661576 || Validation Loss: 1.656816 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 21.614s | Train Loss: 0.578740 || Validation Loss: 0.018157 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 21.179s | Train Loss: 0.661827 || Validation Loss: 1.643435 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 21.546s | Train Loss: 0.579727 || Validation Loss: 0.016622 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 22.714s | Train Loss: 0.657109 || Validation Loss: 1.668012 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 22.032s | Train Loss: 0.577886 || Validation Loss: 0.016220 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 22.651s | Train Loss: 0.658108 || Validation Loss: 1.650678 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 21.744s | Train Loss: 0.580186 || Validation Loss: 0.017859 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 22.243s | Train Loss: 0.658744 || Validation Loss: 1.657884 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 22.872s | Train Loss: 0.577784 || Validation Loss: 0.014329 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 22.339s | Train Loss: 0.658964 || Validation Loss: 1.660650 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 23.098s | Train Loss: 0.577147 || Validation Loss: 0.013170 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 23.027s | Train Loss: 0.657964 || Validation Loss: 1.660468 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 23.390s | Train Loss: 0.578646 || Validation Loss: 0.017199 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 22.786s | Train Loss: 0.659117 || Validation Loss: 1.654989 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 23.353s | Train Loss: 0.578288 || Validation Loss: 0.019085 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 22.927s | Train Loss: 0.657938 || Validation Loss: 1.665556 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 23.814s | Train Loss: 0.581491 || Validation Loss: 0.019135 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 24.863s | Train Loss: 0.657828 || Validation Loss: 1.662096 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 23.758s | Train Loss: 0.576641 || Validation Loss: 0.018812 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 23.622s | Train Loss: 0.655787 || Validation Loss: 1.664701 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 23.742s | Train Loss: 0.579196 || Validation Loss: 0.021793 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 23.837s | Train Loss: 0.656807 || Validation Loss: 1.659908 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 25.262s | Train Loss: 0.579788 || Validation Loss: 0.017244 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 24.639s | Train Loss: 0.656463 || Validation Loss: 1.661279 || Validation number of baches in epoch: 1761.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-06\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 24.810s | Train Loss: 0.576520 || Validation Loss: 0.020070 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 25.485s | Train Loss: 0.655044 || Validation Loss: 1.660313 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 24.987s | Train Loss: 0.578786 || Validation Loss: 0.018227 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 25.419s | Train Loss: 0.655384 || Validation Loss: 1.665438 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 25.644s | Train Loss: 0.578026 || Validation Loss: 0.017507 || Validation number of baches in epoch: 1761.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 26.474s | Train Loss: 0.654001 || Validation Loss: 1.667632 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 26.426s | Train Loss: 0.575462 || Validation Loss: 0.016272 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 26.744s | Train Loss: 0.655776 || Validation Loss: 1.666740 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 26.202s | Train Loss: 0.576541 || Validation Loss: 0.015302 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 25.333s | Train Loss: 0.654417 || Validation Loss: 1.668655 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 27.862s | Train Loss: 0.573610 || Validation Loss: 0.015798 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 28.561s | Train Loss: 0.655293 || Validation Loss: 1.669173 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 27.544s | Train Loss: 0.575041 || Validation Loss: 0.018521 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 26.390s | Train Loss: 0.656118 || Validation Loss: 1.670481 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 26.675s | Train Loss: 0.573899 || Validation Loss: 0.017075 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 26.660s | Train Loss: 0.656254 || Validation Loss: 1.672547 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 28.326s | Train Loss: 0.576683 || Validation Loss: 0.015756 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 26.533s | Train Loss: 0.652216 || Validation Loss: 1.672063 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 27.354s | Train Loss: 0.573597 || Validation Loss: 0.020125 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 28.314s | Train Loss: 0.659183 || Validation Loss: 1.669225 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 28.131s | Train Loss: 0.574633 || Validation Loss: 0.016019 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 30.250s | Train Loss: 0.655461 || Validation Loss: 1.668144 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 27.571s | Train Loss: 0.571357 || Validation Loss: 0.018805 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 27.465s | Train Loss: 0.652666 || Validation Loss: 1.675874 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 28.564s | Train Loss: 0.574738 || Validation Loss: 0.019465 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 28.996s | Train Loss: 0.654820 || Validation Loss: 1.672293 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 29.910s | Train Loss: 0.573092 || Validation Loss: 0.018516 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 27.616s | Train Loss: 0.654494 || Validation Loss: 1.670043 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 28.601s | Train Loss: 0.574109 || Validation Loss: 0.016811 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 28.714s | Train Loss: 0.657631 || Validation Loss: 1.671103 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 29.545s | Train Loss: 0.574806 || Validation Loss: 0.019119 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 31.095s | Train Loss: 0.656261 || Validation Loss: 1.671323 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 29.472s | Train Loss: 0.575407 || Validation Loss: 0.018273 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 30.843s | Train Loss: 0.655064 || Validation Loss: 1.671819 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 29.550s | Train Loss: 0.573963 || Validation Loss: 0.014580 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 29.163s | Train Loss: 0.655226 || Validation Loss: 1.673241 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 31.105s | Train Loss: 0.576121 || Validation Loss: 0.016627 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 29.156s | Train Loss: 0.653936 || Validation Loss: 1.672226 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 31.361s | Train Loss: 0.574336 || Validation Loss: 0.014995 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 31.413s | Train Loss: 0.653123 || Validation Loss: 1.675996 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 28.083s | Train Loss: 0.574491 || Validation Loss: 0.018861 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:Training Time: 1525.405s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.897%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 1.657184\n",
      "INFO:root:Test AUC: 88.92%\n",
      "INFO:root:Test Time: 0.411s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "len of unlabeled 52588\n",
      "total iso selected normal 24924 outlier 1370\n",
      "final sizes 197012 13341 24924 1370\n",
      "len of selected to training 236647\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 197012, 0.0: 26294, -1.0: 13341})\n",
      "Resampled dataset shape Counter({1.0: 197012, -1.0: 47282, 0.0: 26294})\n",
      "class weights 0.17473797803302438 0.8252620219669756\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8072255538103844\n",
      "validation AUC 0.8312668008785785\n",
      "validation AUC 0.841794830847653\n",
      "validation AUC 0.8471657108076704\n",
      "validation AUC 0.8517901656072541\n",
      "validation AUC 0.8554620748469096\n",
      "validation AUC 0.8586773684276623\n",
      "validation AUC 0.8607615953068117\n",
      "validation AUC 0.8627844296018576\n",
      "validation AUC 0.8645743371229254\n",
      "validation AUC 0.8657514389502183\n",
      "validation AUC 0.8671348181572625\n",
      "validation AUC 0.8682604943164632\n",
      "validation AUC 0.8700006869881777\n",
      "validation AUC 0.870083125569519\n",
      "validation AUC 0.8707969201218464\n",
      "validation AUC 0.8720877315299157\n",
      "validation AUC 0.8720364122891169\n",
      "validation AUC 0.8733043923821044\n",
      "validation AUC 0.8736261726425237\n",
      "validation AUC 0.8740525523074182\n",
      "validation AUC 0.8742341651619621\n",
      "validation AUC 0.8747892633166618\n",
      "validation AUC 0.875565969702704\n",
      "validation AUC 0.8753002021267152\n",
      "validation AUC 0.8758443223061175\n",
      "validation AUC 0.87632604948486\n",
      "validation AUC 0.8758191708760436\n",
      "validation AUC 0.8762300068347607\n",
      "validation AUC 0.8768167538057868\n",
      "validation AUC 0.8771334202347659\n",
      "validation AUC 0.8772074111497064\n",
      "validation AUC 0.8771279605146228\n",
      "validation AUC 0.8772343186301188\n",
      "validation AUC 0.8775053463750757\n",
      "validation AUC 0.8774748469735165\n",
      "validation AUC 0.8777883604939333\n",
      "validation AUC 0.8777018430865788\n",
      "validation AUC 0.877629368760567\n",
      "validation AUC 0.8779817495284739\n",
      "validation AUC 0.8777370412531348\n",
      "validation AUC 0.8776561139393573\n",
      "validation AUC 0.8781426037394721\n",
      "validation AUC 0.8776882097503153\n",
      "validation AUC 0.8779823721281392\n",
      "validation AUC 0.8782370925510898\n",
      "validation AUC 0.8785294829144815\n",
      "validation AUC 0.8787531372105364\n",
      "validation AUC 0.8786841803065659\n",
      "validation AUC 0.8785043740553248\n",
      "validation AUC 0.8783582440603459\n",
      "validation AUC 0.8782946643953664\n",
      "validation AUC 0.8783007041442574\n",
      "validation AUC 0.8781884047251163\n",
      "validation AUC 0.8781976505962164\n",
      "validation AUC 0.8781846318775712\n",
      "validation AUC 0.8782335378794957\n",
      "validation AUC 0.8782325241595277\n",
      "validation AUC 0.8782791925276121\n",
      "validation AUC 0.8781248011139957\n",
      "validation AUC 0.8782852721867382\n",
      "validation AUC 0.8782265402849654\n",
      "validation AUC 0.8782862672819298\n",
      "validation AUC 0.8782407164004246\n",
      "validation AUC 0.8782331547412401\n",
      "validation AUC 0.8782704069545553\n",
      "validation AUC 0.8782982270490074\n",
      "validation AUC 0.8783034366650113\n",
      "validation AUC 0.8783678597662898\n",
      "validation AUC 0.8782951991925148\n",
      "validation AUC 0.8782951991925148\n",
      "train auc 0.8969793126929261\n",
      "Test AUC 0.8892356679113247\n",
      "test false positive rates [0.         0.         0.         ... 0.97271435 0.97271435 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.06382979e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.81626663e+01 3.71626663e+01 2.84817963e+01 ... 1.80671811e-01\n",
      " 1.80638283e-01 2.96464432e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 069/070 | Train Time: 25.723s | Train Loss: 0.574559 || Validation Loss: 0.014661 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 26.208s | Train Loss: 0.575218 || Validation Loss: 0.016443 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:Training Time: 1524.193s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.915%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.016040\n",
      "INFO:root:Test AUC: 89.23%\n",
      "INFO:root:Test Time: 0.359s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "80_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 13341 0 197012 0\n",
      "n selected size 26294 n unlabeled outlier and normal 1659 24635\n",
      "final sizes 197012 13341 24635 1659\n",
      "len of selected to training 236647\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 197012, 0.0: 26294, -1.0: 13341})\n",
      "Resampled dataset shape Counter({1.0: 197012, -1.0: 47282, 0.0: 26294})\n",
      "class weights 0.17473797803302438 0.8252620219669756\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8622406340256995\n",
      "validation AUC 0.8737059532022163\n",
      "validation AUC 0.8775692479826175\n",
      "validation AUC 0.8793718203515803\n",
      "validation AUC 0.8813822132960468\n",
      "validation AUC 0.8814933606397387\n",
      "validation AUC 0.8818415987252564\n",
      "validation AUC 0.8838043415098584\n",
      "validation AUC 0.8819385007757486\n",
      "validation AUC 0.882410074790716\n",
      "validation AUC 0.8824999686039485\n",
      "validation AUC 0.8819909614492673\n",
      "validation AUC 0.8799179573281897\n",
      "validation AUC 0.8819769316713364\n",
      "validation AUC 0.8823614960527182\n",
      "validation AUC 0.8814421877364681\n",
      "validation AUC 0.8811967291487116\n",
      "validation AUC 0.8806510737556041\n",
      "validation AUC 0.8803663142291376\n",
      "validation AUC 0.8792082123343699\n",
      "validation AUC 0.8797621956631729\n",
      "validation AUC 0.8811836891446078\n",
      "validation AUC 0.8821933914823683\n",
      "validation AUC 0.877847260018693\n",
      "validation AUC 0.8814011919431135\n",
      "validation AUC 0.8812205688623965\n",
      "validation AUC 0.8791934349047038\n",
      "validation AUC 0.8811729612734494\n",
      "validation AUC 0.8814811268223812\n",
      "validation AUC 0.88036368015363\n",
      "validation AUC 0.8809197654214745\n",
      "validation AUC 0.8801667550722715\n",
      "validation AUC 0.8790468180048586\n",
      "validation AUC 0.8806742270132478\n",
      "validation AUC 0.8810868323257153\n",
      "validation AUC 0.8789218830053279\n",
      "validation AUC 0.8785427384338543\n",
      "validation AUC 0.879837051299871\n",
      "validation AUC 0.8787688724858416\n",
      "validation AUC 0.8798561683024188\n",
      "validation AUC 0.8773494516759425\n",
      "validation AUC 0.8801065784199931\n",
      "validation AUC 0.8797756906439552\n",
      "validation AUC 0.8790404483313584\n",
      "validation AUC 0.8792890970772296\n",
      "validation AUC 0.8799613317715483\n",
      "validation AUC 0.8788203300821213\n",
      "validation AUC 0.8775858426583154\n",
      "validation AUC 0.8808847907522345\n",
      "validation AUC 0.8794709201256778\n",
      "validation AUC 0.879712563294972\n",
      "validation AUC 0.8796799779184651\n",
      "validation AUC 0.8793778042261426\n",
      "validation AUC 0.8796270995178205\n",
      "validation AUC 0.8799876219736601\n",
      "validation AUC 0.8793847938386259\n",
      "validation AUC 0.8789724812012151\n",
      "validation AUC 0.8797017502819793\n",
      "validation AUC 0.8797714974086016\n",
      "validation AUC 0.8791963004595744\n",
      "validation AUC 0.8793179016241445\n",
      "validation AUC 0.879477803310868\n",
      "validation AUC 0.879198572682285\n",
      "validation AUC 0.8790838068106231\n",
      "validation AUC 0.8788215140857587\n",
      "validation AUC 0.8792679819022516\n",
      "validation AUC 0.8794560522328128\n",
      "validation AUC 0.8786482025600872\n",
      "validation AUC 0.878668929275445\n",
      "validation AUC 0.878523062688017\n",
      "validation AUC 0.878523062688017\n",
      "train auc 0.9150033257627235\n",
      "Test AUC 0.8923286411549805\n",
      "test false positive rates [0.         0.         0.         ... 0.98806253 0.98806253 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.57558791e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.44480896e+01 5.34480896e+01 2.93535709e+01 ... 4.86680605e-02\n",
      " 4.86074761e-02 2.31088442e-03]\n",
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.80\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.20\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.80\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 20\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/020 | Train Time: 3.739s | Train Loss: 0.092924 |\n",
      "INFO:root:| Epoch: 002/020 | Train Time: 3.782s | Train Loss: 0.035428 |\n",
      "INFO:root:| Epoch: 003/020 | Train Time: 3.696s | Train Loss: 0.025091 |\n",
      "INFO:root:| Epoch: 004/020 | Train Time: 3.754s | Train Loss: 0.021609 |\n",
      "INFO:root:| Epoch: 005/020 | Train Time: 3.678s | Train Loss: 0.020105 |\n",
      "INFO:root:| Epoch: 006/020 | Train Time: 3.709s | Train Loss: 0.019131 |\n",
      "INFO:root:| Epoch: 007/020 | Train Time: 3.739s | Train Loss: 0.018245 |\n",
      "INFO:root:| Epoch: 008/020 | Train Time: 3.741s | Train Loss: 0.017577 |\n",
      "INFO:root:| Epoch: 009/020 | Train Time: 3.739s | Train Loss: 0.017005 |\n",
      "INFO:root:| Epoch: 010/020 | Train Time: 3.693s | Train Loss: 0.016671 |\n",
      "INFO:root:| Epoch: 011/020 | Train Time: 3.690s | Train Loss: 0.016157 |\n",
      "INFO:root:| Epoch: 012/020 | Train Time: 3.742s | Train Loss: 0.015745 |\n",
      "INFO:root:| Epoch: 013/020 | Train Time: 3.690s | Train Loss: 0.015315 |\n",
      "INFO:root:| Epoch: 014/020 | Train Time: 3.677s | Train Loss: 0.015021 |\n",
      "INFO:root:| Epoch: 015/020 | Train Time: 3.747s | Train Loss: 0.014681 |\n",
      "INFO:root:| Epoch: 016/020 | Train Time: 3.708s | Train Loss: 0.014462 |\n",
      "INFO:root:| Epoch: 017/020 | Train Time: 3.717s | Train Loss: 0.014383 |\n",
      "INFO:root:| Epoch: 018/020 | Train Time: 3.689s | Train Loss: 0.014241 |\n",
      "INFO:root:| Epoch: 019/020 | Train Time: 3.720s | Train Loss: 0.014137 |\n",
      "INFO:root:| Epoch: 020/020 | Train Time: 3.691s | Train Loss: 0.014124 |\n",
      "INFO:root:Pretraining Time: 74.347s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.014178\n",
      "INFO:root:Test AUC: 50.18%\n",
      "INFO:root:Test AUC: 50.18%\n",
      "INFO:root:Test Time: 0.220s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 32\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 8.683s | Train Loss: 0.733619 || Validation Loss: 0.017605 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 8.945s | Train Loss: 0.646277 || Validation Loss: 0.014943 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 9.204s | Train Loss: 0.626609 || Validation Loss: 0.014509 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 9.260s | Train Loss: 0.617190 || Validation Loss: 0.018048 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 9.544s | Train Loss: 0.611493 || Validation Loss: 0.017382 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 9.612s | Train Loss: 0.606814 || Validation Loss: 0.016401 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 9.962s | Train Loss: 0.600615 || Validation Loss: 0.017133 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 10.069s | Train Loss: 0.600584 || Validation Loss: 0.019039 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 10.157s | Train Loss: 0.600043 || Validation Loss: 0.016120 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 10.493s | Train Loss: 0.597675 || Validation Loss: 0.016261 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 10.962s | Train Loss: 0.592776 || Validation Loss: 0.016230 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 10.968s | Train Loss: 0.592533 || Validation Loss: 0.016564 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 10.872s | Train Loss: 0.591413 || Validation Loss: 0.017361 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 11.717s | Train Loss: 0.589566 || Validation Loss: 0.017973 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 11.475s | Train Loss: 0.587216 || Validation Loss: 0.018003 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 11.997s | Train Loss: 0.588322 || Validation Loss: 0.019020 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 12.073s | Train Loss: 0.587993 || Validation Loss: 0.016239 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 12.466s | Train Loss: 0.588136 || Validation Loss: 0.018705 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 12.266s | Train Loss: 0.586376 || Validation Loss: 0.018671 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 12.512s | Train Loss: 0.584191 || Validation Loss: 0.019424 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 13.058s | Train Loss: 0.588144 || Validation Loss: 0.015775 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 12.874s | Train Loss: 0.583054 || Validation Loss: 0.019569 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 12.905s | Train Loss: 0.582146 || Validation Loss: 0.016511 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 14.028s | Train Loss: 0.582760 || Validation Loss: 0.016604 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 13.937s | Train Loss: 0.582113 || Validation Loss: 0.019257 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 13.921s | Train Loss: 0.582179 || Validation Loss: 0.018232 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 14.728s | Train Loss: 0.579785 || Validation Loss: 0.023979 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 14.201s | Train Loss: 0.580020 || Validation Loss: 0.018282 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 15.566s | Train Loss: 0.583027 || Validation Loss: 0.016843 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 15.868s | Train Loss: 0.577900 || Validation Loss: 0.016226 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 15.767s | Train Loss: 0.576543 || Validation Loss: 0.018360 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 16.730s | Train Loss: 0.575936 || Validation Loss: 0.016608 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 15.931s | Train Loss: 0.578334 || Validation Loss: 0.017674 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 16.999s | Train Loss: 0.577003 || Validation Loss: 0.019325 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 16.044s | Train Loss: 0.577584 || Validation Loss: 0.017247 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 17.063s | Train Loss: 0.576863 || Validation Loss: 0.020033 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 16.741s | Train Loss: 0.574260 || Validation Loss: 0.021067 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 17.988s | Train Loss: 0.575589 || Validation Loss: 0.016799 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 17.840s | Train Loss: 0.575695 || Validation Loss: 0.018693 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 18.057s | Train Loss: 0.575865 || Validation Loss: 0.019070 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 18.119s | Train Loss: 0.576830 || Validation Loss: 0.020819 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 18.646s | Train Loss: 0.575868 || Validation Loss: 0.018521 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 18.512s | Train Loss: 0.574483 || Validation Loss: 0.019445 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 19.052s | Train Loss: 0.572886 || Validation Loss: 0.017695 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 18.608s | Train Loss: 0.575895 || Validation Loss: 0.018782 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 18.525s | Train Loss: 0.573263 || Validation Loss: 0.019605 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 20.179s | Train Loss: 0.573552 || Validation Loss: 0.015193 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 19.470s | Train Loss: 0.575615 || Validation Loss: 0.018667 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 19.789s | Train Loss: 0.574294 || Validation Loss: 0.017755 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 20.197s | Train Loss: 0.571660 || Validation Loss: 0.017335 || Validation number of baches in epoch: 1761.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 20.977s | Train Loss: 0.572661 || Validation Loss: 0.017682 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 20.940s | Train Loss: 0.571469 || Validation Loss: 0.017881 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 21.938s | Train Loss: 0.573261 || Validation Loss: 0.015814 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 22.203s | Train Loss: 0.573379 || Validation Loss: 0.019506 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 21.627s | Train Loss: 0.570365 || Validation Loss: 0.018501 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 24.010s | Train Loss: 0.570548 || Validation Loss: 0.018590 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 22.181s | Train Loss: 0.571853 || Validation Loss: 0.017337 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 22.570s | Train Loss: 0.568285 || Validation Loss: 0.020001 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 22.153s | Train Loss: 0.572390 || Validation Loss: 0.017529 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 23.065s | Train Loss: 0.572124 || Validation Loss: 0.017405 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 24.739s | Train Loss: 0.569673 || Validation Loss: 0.018213 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 24.362s | Train Loss: 0.568679 || Validation Loss: 0.020318 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 24.231s | Train Loss: 0.571388 || Validation Loss: 0.017050 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 23.292s | Train Loss: 0.570888 || Validation Loss: 0.018683 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 23.806s | Train Loss: 0.571350 || Validation Loss: 0.019456 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 25.897s | Train Loss: 0.573439 || Validation Loss: 0.019595 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 25.922s | Train Loss: 0.572043 || Validation Loss: 0.020735 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 25.018s | Train Loss: 0.568627 || Validation Loss: 0.017909 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 770.106s | Train Loss: 0.568283 || Validation Loss: 0.016495 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 25.673s | Train Loss: 0.569425 || Validation Loss: 0.017426 || Validation number of baches in epoch: 1761.000000 |\n",
      "INFO:root:Training Time: 1955.751s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.917%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.017174\n",
      "INFO:root:Test AUC: 89.28%\n",
      "INFO:root:Test Time: 0.338s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "80_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 13341 0 197012 0\n",
      "n selected size 26294 n unlabeled outlier and normal 1620 24674\n",
      "final sizes 197012 13341 24674 1620\n",
      "len of selected to training 236647\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 197012, 0.0: 26294, -1.0: 13341})\n",
      "Resampled dataset shape Counter({1.0: 197012, -1.0: 47282, 0.0: 26294})\n",
      "class weights 0.17473797803302438 0.8252620219669756\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8689801397092364\n",
      "validation AUC 0.8765077421598737\n",
      "validation AUC 0.8786527390234613\n",
      "validation AUC 0.879524711140362\n",
      "validation AUC 0.8813734756152721\n",
      "validation AUC 0.8797189170043783\n",
      "validation AUC 0.8800350140473384\n",
      "validation AUC 0.8809234105562679\n",
      "validation AUC 0.8817411260390764\n",
      "validation AUC 0.8821310437133079\n",
      "validation AUC 0.881414833261424\n",
      "validation AUC 0.8815132173119744\n",
      "validation AUC 0.8815413806344472\n",
      "validation AUC 0.8808965562895017\n",
      "validation AUC 0.8819035739987906\n",
      "validation AUC 0.881904872411768\n",
      "validation AUC 0.880566336344729\n",
      "validation AUC 0.8822417467230506\n",
      "validation AUC 0.8803897202516027\n",
      "validation AUC 0.8819860312049081\n",
      "validation AUC 0.8817540649372516\n",
      "validation AUC 0.8799466102162115\n",
      "validation AUC 0.8816660442443799\n",
      "validation AUC 0.8798314585456115\n",
      "validation AUC 0.8805108744215411\n",
      "validation AUC 0.8808973651369303\n",
      "validation AUC 0.8801818863726875\n",
      "validation AUC 0.8795945434088194\n",
      "validation AUC 0.8797278409329161\n",
      "validation AUC 0.8782546663578858\n",
      "validation AUC 0.8796279669002605\n",
      "validation AUC 0.8784686357702558\n",
      "validation AUC 0.8802859988722963\n",
      "validation AUC 0.8800103894323659\n",
      "validation AUC 0.8790186786285267\n",
      "validation AUC 0.8804315089282921\n",
      "validation AUC 0.8797520797489509\n",
      "validation AUC 0.8803432035424114\n",
      "validation AUC 0.8797118183039192\n",
      "validation AUC 0.8791390505578599\n",
      "validation AUC 0.8801213319035182\n",
      "validation AUC 0.8794352270722086\n",
      "validation AUC 0.8791089316338738\n",
      "validation AUC 0.8797992297005369\n",
      "validation AUC 0.8790966579062814\n",
      "validation AUC 0.8803498658909679\n",
      "validation AUC 0.8781715998555144\n",
      "validation AUC 0.879531809840821\n",
      "validation AUC 0.8782726951413599\n",
      "validation AUC 0.8790878137982135\n",
      "validation AUC 0.878435494311142\n",
      "validation AUC 0.8788845057399433\n",
      "validation AUC 0.8788260638525445\n",
      "validation AUC 0.8788331944811915\n",
      "validation AUC 0.8788945471550602\n",
      "validation AUC 0.8785029825184657\n",
      "validation AUC 0.878777881556214\n",
      "validation AUC 0.8790561782852988\n",
      "validation AUC 0.8788124757478806\n",
      "validation AUC 0.8786515656625535\n",
      "validation AUC 0.8789835469790294\n",
      "validation AUC 0.878840822657434\n",
      "validation AUC 0.8786087393197551\n",
      "validation AUC 0.8794217932871198\n",
      "validation AUC 0.8787861509568985\n",
      "validation AUC 0.8786569136340387\n",
      "validation AUC 0.8792619288499488\n",
      "validation AUC 0.8790579662638253\n",
      "validation AUC 0.8789298969805086\n",
      "validation AUC 0.8787198281284355\n",
      "validation AUC 0.8787198281284355\n",
      "train auc 0.9166886203009527\n",
      "Test AUC 0.8927763519940078\n",
      "test false positive rates [0.         0.         0.         ... 0.99723354 0.99723354 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.65957447e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.07907104e+01 6.97907104e+01 3.46427422e+01 ... 2.04856712e-02\n",
      " 2.03607082e-02 3.04391212e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {4..5}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_baseline_10_unlabeled_random\" --index_baseline_name \"80_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.8 --ratio_unknown_normal 0.2 --ratio_known_outlier 0.8 --ratio_unknown_outlier 0.2 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 32 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 20 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 26294\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c240c58",
   "metadata": {},
   "source": [
    "# 80% labeled 5% unlabeled selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9c55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {2..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_baseline_5_unlabeled_random\" --index_baseline_name \"80_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.0 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 13147\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488c5481",
   "metadata": {},
   "source": [
    "# 80% labeled 2% unlabeled selected randomly - to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa3d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {2..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_baseline_2_unlabeled_random\" --index_baseline_name \"80_per_labeled_0_unlabeled\" --iteration_num 1 --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.0 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 5259 \n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac281b0",
   "metadata": {},
   "source": [
    "### 80% labeled 10% unlabeled selected by isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f032726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.80\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.20\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.80\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 20\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/020 | Train Time: 3.895s | Train Loss: 0.089600 |\n",
      "INFO:root:| Epoch: 002/020 | Train Time: 3.694s | Train Loss: 0.033772 |\n",
      "INFO:root:| Epoch: 003/020 | Train Time: 3.719s | Train Loss: 0.024951 |\n",
      "INFO:root:| Epoch: 004/020 | Train Time: 3.709s | Train Loss: 0.021728 |\n",
      "INFO:root:| Epoch: 005/020 | Train Time: 3.717s | Train Loss: 0.019924 |\n",
      "INFO:root:| Epoch: 006/020 | Train Time: 3.817s | Train Loss: 0.018763 |\n",
      "INFO:root:| Epoch: 007/020 | Train Time: 3.748s | Train Loss: 0.017822 |\n",
      "INFO:root:| Epoch: 008/020 | Train Time: 3.740s | Train Loss: 0.016984 |\n",
      "\n",
      "Aborted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "len of unlabeled 52588\n",
      "total iso selected normal 24946 outlier 1348\n",
      "final sizes 197012 13341 24946 1348\n",
      "len of selected to training 236647\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 197012, 0.0: 26294, -1.0: 13341})\n",
      "Resampled dataset shape Counter({1.0: 197012, -1.0: 47282, 0.0: 26294})\n",
      "class weights 0.17473797803302438 0.8252620219669756\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "Error while terminating subprocess (pid=11008): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..2}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_baseline_10_unlabeled_isolation\" --index_baseline_name \"80_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 4 --balanced_train 1 --balanced_batches 1 --minority_loss 0 --ratio_known_normal 0.8 --ratio_unknown_normal 0.2 --ratio_known_outlier 0.8 --ratio_unknown_outlier 0.2 --lr 0.00001 --n_epochs 70 --lr_milestone 50 --batch_size 32 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 20 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 26294\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc17d28",
   "metadata": {},
   "source": [
    "# 80% labeled 2% unlabeled selected by isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad971ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.80\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.80\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 3.475s | Train Loss: 0.098262 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 3.406s | Train Loss: 0.039696 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 3.427s | Train Loss: 0.028715 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 3.351s | Train Loss: 0.024485 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 3.363s | Train Loss: 0.022323 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 3.404s | Train Loss: 0.021127 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 3.335s | Train Loss: 0.020328 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.375s | Train Loss: 0.019459 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.378s | Train Loss: 0.018580 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.310s | Train Loss: 0.017834 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.260s | Train Loss: 0.017283 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.278s | Train Loss: 0.016990 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.293s | Train Loss: 0.016805 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.269s | Train Loss: 0.016539 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.235s | Train Loss: 0.016264 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 3.251s | Train Loss: 0.016014 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 3.281s | Train Loss: 0.015711 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 3.298s | Train Loss: 0.015550 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 3.241s | Train Loss: 0.015296 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 3.273s | Train Loss: 0.015150 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 3.256s | Train Loss: 0.015015 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 3.272s | Train Loss: 0.014943 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 3.284s | Train Loss: 0.014855 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 3.280s | Train Loss: 0.014723 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 3.251s | Train Loss: 0.014709 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 3.308s | Train Loss: 0.014671 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 3.287s | Train Loss: 0.014593 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 3.313s | Train Loss: 0.014517 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 3.314s | Train Loss: 0.014518 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 3.262s | Train Loss: 0.014408 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 3.277s | Train Loss: 0.014281 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 3.263s | Train Loss: 0.014246 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 3.293s | Train Loss: 0.014189 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 3.291s | Train Loss: 0.014143 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 3.274s | Train Loss: 0.014115 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 3.260s | Train Loss: 0.014025 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 3.291s | Train Loss: 0.013960 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 3.290s | Train Loss: 0.013992 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 3.274s | Train Loss: 0.013879 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 3.436s | Train Loss: 0.013897 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 3.394s | Train Loss: 0.013808 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 3.280s | Train Loss: 0.013856 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 3.239s | Train Loss: 0.013821 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 3.381s | Train Loss: 0.013814 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 3.282s | Train Loss: 0.013764 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 3.255s | Train Loss: 0.013753 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 3.351s | Train Loss: 0.013762 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 3.294s | Train Loss: 0.013726 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 3.257s | Train Loss: 0.013752 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 3.249s | Train Loss: 0.013791 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 3.264s | Train Loss: 0.013689 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 3.262s | Train Loss: 0.013779 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 3.239s | Train Loss: 0.013732 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 3.258s | Train Loss: 0.013672 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 3.289s | Train Loss: 0.013762 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 3.248s | Train Loss: 0.013710 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 3.258s | Train Loss: 0.013710 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 3.248s | Train Loss: 0.013747 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 3.279s | Train Loss: 0.013719 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 3.250s | Train Loss: 0.013709 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 3.240s | Train Loss: 0.013707 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 3.282s | Train Loss: 0.013758 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 3.258s | Train Loss: 0.013695 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 3.269s | Train Loss: 0.013706 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 3.248s | Train Loss: 0.013721 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 3.241s | Train Loss: 0.013715 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 3.350s | Train Loss: 0.013759 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 3.261s | Train Loss: 0.013691 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 3.252s | Train Loss: 0.013726 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 3.220s | Train Loss: 0.013646 |\n",
      "INFO:root:Pretraining Time: 230.489s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.013226\n",
      "INFO:root:Test AUC: 50.38%\n",
      "INFO:root:Test AUC: 50.38%\n",
      "INFO:root:Test Time: 0.227s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.691s | Train Loss: 0.870124 || Validation Loss: 0.616206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.925s | Train Loss: 0.745484 || Validation Loss: 0.632917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.916s | Train Loss: 0.711216 || Validation Loss: 0.671785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 3.120s | Train Loss: 0.691168 || Validation Loss: 0.675801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 111.890s | Train Loss: 0.681709 || Validation Loss: 0.695715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 3.427s | Train Loss: 0.675253 || Validation Loss: 0.701723 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 3.573s | Train Loss: 0.665618 || Validation Loss: 0.624179 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.700s | Train Loss: 0.663182 || Validation Loss: 0.674238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.701s | Train Loss: 0.658984 || Validation Loss: 0.673321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.965s | Train Loss: 0.657046 || Validation Loss: 0.669831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 4.108s | Train Loss: 0.654610 || Validation Loss: 0.701476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 4.079s | Train Loss: 0.649904 || Validation Loss: 0.659686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 4.591s | Train Loss: 0.651929 || Validation Loss: 0.654412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 4.419s | Train Loss: 0.647469 || Validation Loss: 0.657682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 4.779s | Train Loss: 0.645192 || Validation Loss: 0.688722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 5.049s | Train Loss: 0.644364 || Validation Loss: 0.689338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 4.972s | Train Loss: 0.643624 || Validation Loss: 0.624093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 5.071s | Train Loss: 0.643429 || Validation Loss: 0.669862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 5.482s | Train Loss: 0.642402 || Validation Loss: 0.761478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 5.242s | Train Loss: 0.642381 || Validation Loss: 0.651445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 5.961s | Train Loss: 0.638843 || Validation Loss: 0.707608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 6.091s | Train Loss: 0.639965 || Validation Loss: 0.738451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 6.483s | Train Loss: 0.637032 || Validation Loss: 0.703297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 6.108s | Train Loss: 0.635195 || Validation Loss: 0.697913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 6.694s | Train Loss: 0.639419 || Validation Loss: 0.724464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 6.977s | Train Loss: 0.638409 || Validation Loss: 0.698295 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 7.499s | Train Loss: 0.636744 || Validation Loss: 0.706517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 6.707s | Train Loss: 0.635155 || Validation Loss: 0.702483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 7.458s | Train Loss: 0.634328 || Validation Loss: 0.669486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 7.308s | Train Loss: 0.632902 || Validation Loss: 0.696706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 7.394s | Train Loss: 0.633337 || Validation Loss: 0.695046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 7.907s | Train Loss: 0.633081 || Validation Loss: 0.722244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 7.776s | Train Loss: 0.629632 || Validation Loss: 0.679647 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 8.351s | Train Loss: 0.632487 || Validation Loss: 0.687187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 7.938s | Train Loss: 0.634251 || Validation Loss: 0.689755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 8.528s | Train Loss: 0.633678 || Validation Loss: 0.723575 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 8.545s | Train Loss: 0.630209 || Validation Loss: 0.716691 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 8.737s | Train Loss: 0.629657 || Validation Loss: 0.706276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 9.115s | Train Loss: 0.630118 || Validation Loss: 0.699546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 8.898s | Train Loss: 0.629949 || Validation Loss: 0.714367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 8.915s | Train Loss: 0.628172 || Validation Loss: 0.675888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 9.510s | Train Loss: 0.629972 || Validation Loss: 0.731749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 9.582s | Train Loss: 0.630663 || Validation Loss: 0.737569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 9.505s | Train Loss: 0.627478 || Validation Loss: 0.695004 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 9.625s | Train Loss: 0.626647 || Validation Loss: 0.679393 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 10.241s | Train Loss: 0.626299 || Validation Loss: 0.738266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 10.625s | Train Loss: 0.627825 || Validation Loss: 0.719416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 11.124s | Train Loss: 0.627568 || Validation Loss: 0.731746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 11.387s | Train Loss: 0.625450 || Validation Loss: 0.720639 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 10.894s | Train Loss: 0.625896 || Validation Loss: 0.654313 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 11.413s | Train Loss: 0.620768 || Validation Loss: 0.713993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 10.789s | Train Loss: 0.628070 || Validation Loss: 0.699881 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 12.603s | Train Loss: 0.625307 || Validation Loss: 0.736232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 11.402s | Train Loss: 0.624855 || Validation Loss: 0.675036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 11.496s | Train Loss: 0.624757 || Validation Loss: 0.709074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 12.818s | Train Loss: 0.623743 || Validation Loss: 0.718219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 12.859s | Train Loss: 0.627430 || Validation Loss: 0.713175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 13.358s | Train Loss: 0.624109 || Validation Loss: 0.686286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 13.064s | Train Loss: 0.625253 || Validation Loss: 0.705995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 12.765s | Train Loss: 0.625653 || Validation Loss: 0.755380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 13.134s | Train Loss: 0.620745 || Validation Loss: 0.712104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 13.239s | Train Loss: 0.626288 || Validation Loss: 0.689474 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 14.269s | Train Loss: 0.625821 || Validation Loss: 0.765919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 13.807s | Train Loss: 0.624793 || Validation Loss: 0.650095 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 14.145s | Train Loss: 0.621785 || Validation Loss: 0.720562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 14.049s | Train Loss: 0.622449 || Validation Loss: 0.751469 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 14.243s | Train Loss: 0.624496 || Validation Loss: 0.771280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 15.163s | Train Loss: 0.623494 || Validation Loss: 0.644570 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 15.182s | Train Loss: 0.624767 || Validation Loss: 0.715244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 14.937s | Train Loss: 0.626336 || Validation Loss: 0.773389 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 726.033s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.907%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.644905\n",
      "INFO:root:Test AUC: 89.24%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "unlabeled is correctly sum 3335 49253 52588\n",
      "[-1.00000000e+05 -1.00000000e+05  2.79846177e-02 ...  1.30182768e-01\n",
      " -1.00000000e+05 -1.00000000e+05]\n",
      "summary chosen samples {0: 257682, 1: 5259}\n",
      "len indexes assigned as normal by isolation 5259\n",
      "len before subset 262941\n",
      "len after subset 215471\n",
      "yes smote\n",
      "class weights 7.495689978262499e-05 0.00019538882375928098 5.075832944186141e-06\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8430655354793475\n",
      "validation AUC 0.8656005356911378\n",
      "validation AUC 0.8734019516211324\n",
      "validation AUC 0.8762736659711288\n",
      "validation AUC 0.8812144679178117\n",
      "validation AUC 0.8828123593164217\n",
      "validation AUC 0.8833675718804616\n",
      "validation AUC 0.8854331606798832\n",
      "validation AUC 0.8841705764506625\n",
      "validation AUC 0.885927539403109\n",
      "validation AUC 0.8869512156444715\n",
      "validation AUC 0.8870023353340956\n",
      "validation AUC 0.8868192564435872\n",
      "validation AUC 0.884916259280726\n",
      "validation AUC 0.8851735020305262\n",
      "validation AUC 0.8869605732642294\n",
      "validation AUC 0.8865491359913055\n",
      "validation AUC 0.8847775286433095\n",
      "validation AUC 0.8843615628890582\n",
      "validation AUC 0.8859097554024091\n",
      "validation AUC 0.8851582297139469\n",
      "validation AUC 0.8852625524074599\n",
      "validation AUC 0.8862840522106756\n",
      "validation AUC 0.8858631136411479\n",
      "validation AUC 0.8864631480597348\n",
      "validation AUC 0.8868907729239599\n",
      "validation AUC 0.8872755794274341\n",
      "validation AUC 0.8872083918772136\n",
      "validation AUC 0.8863642212299886\n",
      "validation AUC 0.8866414137759063\n",
      "validation AUC 0.885915736616289\n",
      "validation AUC 0.8861649946584141\n",
      "validation AUC 0.8872062180997493\n",
      "validation AUC 0.8864034396875464\n",
      "validation AUC 0.8878088067733736\n",
      "validation AUC 0.8853662525013074\n",
      "validation AUC 0.8876387466015104\n",
      "validation AUC 0.8882053788141147\n",
      "validation AUC 0.8872026181965555\n",
      "validation AUC 0.8868582381004175\n",
      "validation AUC 0.8878987990318523\n",
      "validation AUC 0.8865367664791489\n",
      "validation AUC 0.8871833282496563\n",
      "validation AUC 0.8873861227962366\n",
      "validation AUC 0.8867269919623981\n",
      "validation AUC 0.8864802269796168\n",
      "validation AUC 0.8864298389776338\n",
      "validation AUC 0.8863844477370685\n",
      "validation AUC 0.8865412736750176\n",
      "validation AUC 0.886855862111096\n",
      "validation AUC 0.8871271878524739\n",
      "validation AUC 0.8870191854352972\n",
      "validation AUC 0.8869120104903253\n",
      "validation AUC 0.8868575143948235\n",
      "validation AUC 0.8864368525362583\n",
      "validation AUC 0.8866392639445829\n",
      "validation AUC 0.886805498055254\n",
      "validation AUC 0.8869625262050601\n",
      "validation AUC 0.8867368604331634\n",
      "validation AUC 0.8870508794832231\n",
      "validation AUC 0.8870281120245175\n",
      "validation AUC 0.8869973039838077\n",
      "validation AUC 0.8868092389746113\n",
      "validation AUC 0.8869695849952842\n",
      "validation AUC 0.8872675122386066\n",
      "validation AUC 0.8865597095428883\n",
      "validation AUC 0.8868881574732288\n",
      "validation AUC 0.8868542763444269\n",
      "validation AUC 0.8868596189945475\n",
      "validation AUC 0.8868238354778788\n",
      "validation AUC 0.8868238354778788\n",
      "train auc 0.9069178767234501\n",
      "Test AUC 0.8923779244112984\n",
      "test false positive rates [0.         0.         0.         ... 0.99780199 0.99780199 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.07166853e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.94567032e+01 4.84567032e+01 3.08249168e+01 ... 3.46106924e-02\n",
      " 3.45031917e-02 6.73597492e-03]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.80\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.80\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:56: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:57: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 3.270s | Train Loss: 0.105962 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 3.283s | Train Loss: 0.041082 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 3.253s | Train Loss: 0.029487 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 3.264s | Train Loss: 0.025009 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 3.293s | Train Loss: 0.022989 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 3.291s | Train Loss: 0.021616 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 3.272s | Train Loss: 0.020594 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.272s | Train Loss: 0.019640 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.280s | Train Loss: 0.018803 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.238s | Train Loss: 0.018192 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.311s | Train Loss: 0.017798 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.261s | Train Loss: 0.017561 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.301s | Train Loss: 0.017248 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.302s | Train Loss: 0.016916 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.308s | Train Loss: 0.016578 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 3.402s | Train Loss: 0.016430 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 3.251s | Train Loss: 0.016217 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 3.309s | Train Loss: 0.016108 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 3.274s | Train Loss: 0.015965 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 3.275s | Train Loss: 0.015729 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 3.392s | Train Loss: 0.015524 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 3.275s | Train Loss: 0.015312 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 3.291s | Train Loss: 0.015002 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 3.256s | Train Loss: 0.014803 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 3.314s | Train Loss: 0.014625 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 3.291s | Train Loss: 0.014510 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 3.270s | Train Loss: 0.014355 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 3.229s | Train Loss: 0.014291 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 3.253s | Train Loss: 0.014226 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 3.278s | Train Loss: 0.014168 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 3.248s | Train Loss: 0.014246 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 3.269s | Train Loss: 0.014235 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 3.277s | Train Loss: 0.014149 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 3.253s | Train Loss: 0.014161 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 3.279s | Train Loss: 0.014181 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 3.292s | Train Loss: 0.014136 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 3.246s | Train Loss: 0.014138 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 3.230s | Train Loss: 0.014155 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 3.261s | Train Loss: 0.014116 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 3.267s | Train Loss: 0.014116 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 3.272s | Train Loss: 0.014135 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 3.266s | Train Loss: 0.014113 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 3.290s | Train Loss: 0.014088 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 3.316s | Train Loss: 0.014044 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 3.287s | Train Loss: 0.014097 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 3.262s | Train Loss: 0.014066 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 3.253s | Train Loss: 0.014072 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 3.246s | Train Loss: 0.014160 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 3.267s | Train Loss: 0.014021 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 3.251s | Train Loss: 0.014071 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 3.270s | Train Loss: 0.014083 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 3.289s | Train Loss: 0.014086 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 3.258s | Train Loss: 0.014071 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 3.307s | Train Loss: 0.014056 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 3.325s | Train Loss: 0.014084 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 3.241s | Train Loss: 0.014040 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 3.251s | Train Loss: 0.014011 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 3.264s | Train Loss: 0.014022 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 3.272s | Train Loss: 0.014068 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 3.291s | Train Loss: 0.014028 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 3.241s | Train Loss: 0.014070 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 3.259s | Train Loss: 0.014052 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 3.319s | Train Loss: 0.014090 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 3.232s | Train Loss: 0.013996 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 3.258s | Train Loss: 0.014072 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 3.230s | Train Loss: 0.014060 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 3.269s | Train Loss: 0.014072 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 3.268s | Train Loss: 0.014030 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 3.264s | Train Loss: 0.014109 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 3.262s | Train Loss: 0.014029 |\n",
      "INFO:root:Pretraining Time: 229.272s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.013497\n",
      "INFO:root:Test AUC: 51.17%\n",
      "INFO:root:Test AUC: 51.17%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.834s | Train Loss: 0.831561 || Validation Loss: 0.613353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.997s | Train Loss: 0.723633 || Validation Loss: 0.573935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.993s | Train Loss: 0.696276 || Validation Loss: 0.729303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 3.295s | Train Loss: 0.680909 || Validation Loss: 0.726339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 3.403s | Train Loss: 0.672988 || Validation Loss: 0.662567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 3.567s | Train Loss: 0.663435 || Validation Loss: 0.721303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 3.658s | Train Loss: 0.658404 || Validation Loss: 0.757213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.818s | Train Loss: 0.656691 || Validation Loss: 0.728885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 4.022s | Train Loss: 0.649562 || Validation Loss: 0.738634 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 4.303s | Train Loss: 0.651892 || Validation Loss: 0.738744 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 4.350s | Train Loss: 0.648440 || Validation Loss: 0.737254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 4.251s | Train Loss: 0.645879 || Validation Loss: 0.806680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 4.866s | Train Loss: 0.645025 || Validation Loss: 0.733936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 4.648s | Train Loss: 0.645032 || Validation Loss: 0.763280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 5.066s | Train Loss: 0.642371 || Validation Loss: 0.766740 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 5.378s | Train Loss: 0.640807 || Validation Loss: 0.744163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 5.283s | Train Loss: 0.640331 || Validation Loss: 0.715502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 5.221s | Train Loss: 0.642941 || Validation Loss: 0.729354 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 5.570s | Train Loss: 0.641388 || Validation Loss: 0.745121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 5.366s | Train Loss: 0.637627 || Validation Loss: 0.649597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 6.120s | Train Loss: 0.638820 || Validation Loss: 0.689201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 6.372s | Train Loss: 0.633702 || Validation Loss: 0.758305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 6.518s | Train Loss: 0.635930 || Validation Loss: 0.727733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 6.190s | Train Loss: 0.631193 || Validation Loss: 0.801577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 6.539s | Train Loss: 0.631479 || Validation Loss: 0.794512 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 7.075s | Train Loss: 0.633286 || Validation Loss: 0.754333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 7.210s | Train Loss: 0.631298 || Validation Loss: 0.698968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 6.758s | Train Loss: 0.633719 || Validation Loss: 0.759294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 7.741s | Train Loss: 0.630034 || Validation Loss: 0.766882 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 7.636s | Train Loss: 0.632791 || Validation Loss: 0.732234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 7.643s | Train Loss: 0.628742 || Validation Loss: 0.742894 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 8.070s | Train Loss: 0.626759 || Validation Loss: 0.729519 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 7.923s | Train Loss: 0.627629 || Validation Loss: 0.712124 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 8.455s | Train Loss: 0.629898 || Validation Loss: 0.775395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 8.187s | Train Loss: 0.628134 || Validation Loss: 0.725408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 8.614s | Train Loss: 0.624089 || Validation Loss: 0.715326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 8.733s | Train Loss: 0.625158 || Validation Loss: 0.757363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 9.007s | Train Loss: 0.627318 || Validation Loss: 0.715510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 9.248s | Train Loss: 0.624698 || Validation Loss: 0.676589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 9.092s | Train Loss: 0.625796 || Validation Loss: 0.764638 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 8.957s | Train Loss: 0.624783 || Validation Loss: 0.729989 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 9.674s | Train Loss: 0.623076 || Validation Loss: 0.820187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 9.727s | Train Loss: 0.624874 || Validation Loss: 0.707318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 9.660s | Train Loss: 0.627852 || Validation Loss: 0.705161 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 9.783s | Train Loss: 0.622338 || Validation Loss: 0.764858 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 10.278s | Train Loss: 0.621865 || Validation Loss: 0.787296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 10.480s | Train Loss: 0.624188 || Validation Loss: 0.818369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 11.143s | Train Loss: 0.619218 || Validation Loss: 0.764267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 11.610s | Train Loss: 0.623784 || Validation Loss: 0.750306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 10.881s | Train Loss: 0.621049 || Validation Loss: 0.770802 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 11.545s | Train Loss: 0.622585 || Validation Loss: 0.759136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 11.166s | Train Loss: 0.620254 || Validation Loss: 0.752536 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 12.817s | Train Loss: 0.622208 || Validation Loss: 0.769861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 11.699s | Train Loss: 0.620924 || Validation Loss: 0.808998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 12.056s | Train Loss: 0.622008 || Validation Loss: 0.771081 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 13.130s | Train Loss: 0.620718 || Validation Loss: 0.788383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 13.032s | Train Loss: 0.618158 || Validation Loss: 0.766579 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 13.492s | Train Loss: 0.617725 || Validation Loss: 0.719104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 13.166s | Train Loss: 0.619367 || Validation Loss: 0.788977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 12.879s | Train Loss: 0.619950 || Validation Loss: 0.761773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 13.042s | Train Loss: 0.619635 || Validation Loss: 0.696262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 13.450s | Train Loss: 0.621565 || Validation Loss: 0.812204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 14.419s | Train Loss: 0.619478 || Validation Loss: 0.768638 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 13.966s | Train Loss: 0.620493 || Validation Loss: 0.867668 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 14.053s | Train Loss: 0.619024 || Validation Loss: 0.767860 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 14.424s | Train Loss: 0.620841 || Validation Loss: 0.795825 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 14.330s | Train Loss: 0.619755 || Validation Loss: 0.791033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 14.923s | Train Loss: 0.618623 || Validation Loss: 0.644643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 15.325s | Train Loss: 0.619967 || Validation Loss: 0.728664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 15.065s | Train Loss: 0.619606 || Validation Loss: 0.771187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 628.172s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.910%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.680769\n",
      "INFO:root:Test AUC: 89.15%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "unlabeled is correctly sum 3335 49253 52588\n",
      "[-1.00000000e+05 -6.59185070e-02  2.79846177e-02 ...  1.30182768e-01\n",
      " -1.00000000e+05  1.42203615e-01]\n",
      "summary chosen samples {0: 257682, 1: 5259}\n",
      "len indexes assigned as normal by isolation 5259\n",
      "len before subset 262941\n",
      "len after subset 215469\n",
      "yes smote\n",
      "class weights 7.495689978262499e-05 0.00019546520719311962 5.075832944186141e-06\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8573149660401149\n",
      "validation AUC 0.8731936095518922\n",
      "validation AUC 0.8781931407396655\n",
      "validation AUC 0.8806950841020399\n",
      "validation AUC 0.8820261143842234\n",
      "validation AUC 0.883041912345204\n",
      "validation AUC 0.8836702856912974\n",
      "validation AUC 0.8836467812235861\n",
      "validation AUC 0.884417415932549\n",
      "validation AUC 0.885649636455009\n",
      "validation AUC 0.8849728652973163\n",
      "validation AUC 0.8865461374023182\n",
      "validation AUC 0.8867365890435656\n",
      "validation AUC 0.8860859777145504\n",
      "validation AUC 0.8875890769837569\n",
      "validation AUC 0.8876217874123332\n",
      "validation AUC 0.8863698219662952\n",
      "validation AUC 0.8849922669928735\n",
      "validation AUC 0.8860668367658618\n",
      "validation AUC 0.8863863634283466\n",
      "validation AUC 0.8867971248879586\n",
      "validation AUC 0.8866055078678505\n",
      "validation AUC 0.8867984445863948\n",
      "validation AUC 0.888635656378624\n",
      "validation AUC 0.8869608632786035\n",
      "validation AUC 0.8875380823462025\n",
      "validation AUC 0.886171803344499\n",
      "validation AUC 0.886577065173733\n",
      "validation AUC 0.8869709233184966\n",
      "validation AUC 0.8854863131308078\n",
      "validation AUC 0.886365357341344\n",
      "validation AUC 0.885819566253438\n",
      "validation AUC 0.8854720891230667\n",
      "validation AUC 0.8872750393089206\n",
      "validation AUC 0.8868277786090932\n",
      "validation AUC 0.8869623266538854\n",
      "validation AUC 0.8860965033738516\n",
      "validation AUC 0.8852604345043245\n",
      "validation AUC 0.8868977066621143\n",
      "validation AUC 0.8856218110391922\n",
      "validation AUC 0.8867900953652403\n",
      "validation AUC 0.8868343717799092\n",
      "validation AUC 0.8864782900028798\n",
      "validation AUC 0.8859040828276796\n",
      "validation AUC 0.886459635959058\n",
      "validation AUC 0.8863217088477053\n",
      "validation AUC 0.8864816637480756\n",
      "validation AUC 0.8868907782453247\n",
      "validation AUC 0.8853875166744961\n",
      "validation AUC 0.8855195503744857\n",
      "validation AUC 0.8856496071875033\n",
      "validation AUC 0.8856987712756141\n",
      "validation AUC 0.8860481002408875\n",
      "validation AUC 0.8855816932710067\n",
      "validation AUC 0.8858726974189042\n",
      "validation AUC 0.8858987454989237\n",
      "validation AUC 0.8859659756200613\n",
      "validation AUC 0.8861013404943292\n",
      "validation AUC 0.8863864219633579\n",
      "validation AUC 0.8863883882076005\n",
      "validation AUC 0.8863751060814045\n",
      "validation AUC 0.8862066263548461\n",
      "validation AUC 0.8861711621200572\n",
      "validation AUC 0.8861010345158612\n",
      "validation AUC 0.8861953902933626\n",
      "validation AUC 0.8861138137730966\n",
      "validation AUC 0.8855759701433128\n",
      "validation AUC 0.8861805596500499\n",
      "validation AUC 0.8863139502980283\n",
      "validation AUC 0.8859947828276581\n",
      "validation AUC 0.8859947828276581\n",
      "train auc 0.909568027578799\n",
      "Test AUC 0.8914969739375094\n",
      "test false positive rates [0.         0.         0.         ... 0.98108953 0.98108953 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.84770437e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.70363007e+01 3.60363007e+01 2.44571152e+01 ... 9.87645984e-02\n",
      " 9.85800326e-02 6.23503514e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..2}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_baseline_2_unlabeled_isolation\" --index_baseline_name \"80_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 4 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.8 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.8 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 5259\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05dcb750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 80% labeled 5% unlabeled selected by isolation forest - to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bafe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {2..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"80_per_labeled_baseline_5_unlabeled_isolation\" --index_baseline_name \"80_per_labeled_0_unlabeled\" --iteration_num 1 --experiment_method 4 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.8 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.8 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 13147\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201d23e",
   "metadata": {},
   "source": [
    "# Drop unlabeled samples - by isolation forest little labeled 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4382ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"25_per_labeled_baseline_0_unlabeled_random\" --index_baseline_name \"25_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.0 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 0\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851ae14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 05/07 25% labeled 40% unlabeled selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da3409b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.541s | Train Loss: 0.110547 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.509s | Train Loss: 0.045337 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.490s | Train Loss: 0.031377 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.484s | Train Loss: 0.025481 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.520s | Train Loss: 0.022533 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.540s | Train Loss: 0.020836 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.535s | Train Loss: 0.019859 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 2.474s | Train Loss: 0.018937 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 2.482s | Train Loss: 0.018245 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 2.517s | Train Loss: 0.017502 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 2.524s | Train Loss: 0.016974 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 2.540s | Train Loss: 0.016441 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 2.542s | Train Loss: 0.015964 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 2.533s | Train Loss: 0.015565 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 2.540s | Train Loss: 0.015256 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 2.534s | Train Loss: 0.014906 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 2.533s | Train Loss: 0.014694 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 2.540s | Train Loss: 0.014490 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 2.547s | Train Loss: 0.014285 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 2.532s | Train Loss: 0.014149 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 2.546s | Train Loss: 0.014086 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 2.549s | Train Loss: 0.013892 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 2.553s | Train Loss: 0.013870 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 2.560s | Train Loss: 0.013712 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 2.551s | Train Loss: 0.013681 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 2.560s | Train Loss: 0.013586 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 2.560s | Train Loss: 0.013575 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 2.548s | Train Loss: 0.013569 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 2.552s | Train Loss: 0.013550 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 2.552s | Train Loss: 0.013488 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 2.554s | Train Loss: 0.013443 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 2.538s | Train Loss: 0.013385 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2.553s | Train Loss: 0.013428 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 2.559s | Train Loss: 0.013367 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 2.555s | Train Loss: 0.013410 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 2.557s | Train Loss: 0.013388 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 2.559s | Train Loss: 0.013342 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 2.551s | Train Loss: 0.013306 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2.561s | Train Loss: 0.013357 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 2.565s | Train Loss: 0.013303 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 2.563s | Train Loss: 0.013234 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 2.541s | Train Loss: 0.013254 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 2.547s | Train Loss: 0.013250 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 2.525s | Train Loss: 0.013182 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 2.548s | Train Loss: 0.013259 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 2.558s | Train Loss: 0.013298 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 2.515s | Train Loss: 0.013201 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 2.557s | Train Loss: 0.013173 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 2.545s | Train Loss: 0.013220 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 2.544s | Train Loss: 0.013227 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 2.513s | Train Loss: 0.013238 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 2.552s | Train Loss: 0.013210 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 2.515s | Train Loss: 0.013226 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 2.538s | Train Loss: 0.013303 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 2.547s | Train Loss: 0.013223 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 2.571s | Train Loss: 0.013215 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 2.544s | Train Loss: 0.013196 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 2.516s | Train Loss: 0.013237 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 2.509s | Train Loss: 0.013202 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 2.505s | Train Loss: 0.013191 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 2.541s | Train Loss: 0.013232 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 2.545s | Train Loss: 0.013249 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 2.559s | Train Loss: 0.013250 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 2.540s | Train Loss: 0.013232 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 2.545s | Train Loss: 0.013153 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 2.543s | Train Loss: 0.013231 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 2.529s | Train Loss: 0.013271 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 2.544s | Train Loss: 0.013256 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 2.543s | Train Loss: 0.013232 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.547s | Train Loss: 0.013255 |\n",
      "INFO:root:Pretraining Time: 177.751s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.013336\n",
      "INFO:root:Test AUC: 51.67%\n",
      "INFO:root:Test AUC: 51.67%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.124s | Train Loss: 0.568790 || Validation Loss: 0.020183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.244s | Train Loss: 0.433642 || Validation Loss: 0.021560 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.411s | Train Loss: 0.405281 || Validation Loss: 0.024277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.461s | Train Loss: 0.385342 || Validation Loss: 0.024534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.614s | Train Loss: 0.379722 || Validation Loss: 0.025372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.776s | Train Loss: 0.372260 || Validation Loss: 0.025527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.851s | Train Loss: 0.366228 || Validation Loss: 0.025163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.031s | Train Loss: 0.362070 || Validation Loss: 0.028094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.007s | Train Loss: 0.359584 || Validation Loss: 0.026897 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.211s | Train Loss: 0.357013 || Validation Loss: 0.025137 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.419s | Train Loss: 0.354782 || Validation Loss: 0.029375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.390s | Train Loss: 0.352486 || Validation Loss: 0.027901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.709s | Train Loss: 0.353259 || Validation Loss: 0.025774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.621s | Train Loss: 0.347707 || Validation Loss: 0.029952 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.759s | Train Loss: 0.350535 || Validation Loss: 0.029217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 4.068s | Train Loss: 0.350910 || Validation Loss: 0.027847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 4.084s | Train Loss: 0.343366 || Validation Loss: 0.027968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 4.307s | Train Loss: 0.345129 || Validation Loss: 0.029371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 4.498s | Train Loss: 0.343465 || Validation Loss: 0.029957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 4.540s | Train Loss: 0.342149 || Validation Loss: 0.027108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 4.590s | Train Loss: 0.344768 || Validation Loss: 0.027297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 4.711s | Train Loss: 0.340549 || Validation Loss: 0.029133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 4.951s | Train Loss: 0.336607 || Validation Loss: 0.030001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 5.185s | Train Loss: 0.342062 || Validation Loss: 0.031079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 5.301s | Train Loss: 0.337243 || Validation Loss: 0.028712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 5.268s | Train Loss: 0.337711 || Validation Loss: 0.028452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 5.465s | Train Loss: 0.338260 || Validation Loss: 0.026565 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 5.325s | Train Loss: 0.336506 || Validation Loss: 0.028420 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 5.733s | Train Loss: 0.333519 || Validation Loss: 0.028584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 5.850s | Train Loss: 0.334694 || Validation Loss: 0.028613 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 6.022s | Train Loss: 0.332208 || Validation Loss: 0.030093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 5.916s | Train Loss: 0.332705 || Validation Loss: 0.029022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 6.058s | Train Loss: 0.329822 || Validation Loss: 0.027846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 6.090s | Train Loss: 0.331601 || Validation Loss: 0.031501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 6.973s | Train Loss: 0.326721 || Validation Loss: 0.028854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 6.295s | Train Loss: 0.331639 || Validation Loss: 0.029380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 7.182s | Train Loss: 0.330584 || Validation Loss: 0.029527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 7.158s | Train Loss: 0.328327 || Validation Loss: 0.029360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 7.112s | Train Loss: 0.326943 || Validation Loss: 0.029567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 7.347s | Train Loss: 0.329560 || Validation Loss: 0.029405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 7.349s | Train Loss: 0.327090 || Validation Loss: 0.029499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 7.256s | Train Loss: 0.329292 || Validation Loss: 0.026774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 7.622s | Train Loss: 0.325160 || Validation Loss: 0.028914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 8.183s | Train Loss: 0.325450 || Validation Loss: 0.030096 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 7.902s | Train Loss: 0.328194 || Validation Loss: 0.028382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 7.976s | Train Loss: 0.327166 || Validation Loss: 0.031623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 8.009s | Train Loss: 0.323603 || Validation Loss: 0.030753 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 8.467s | Train Loss: 0.323986 || Validation Loss: 0.031220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 8.929s | Train Loss: 0.326759 || Validation Loss: 0.030602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 8.520s | Train Loss: 0.324217 || Validation Loss: 0.030296 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 8.558s | Train Loss: 0.322539 || Validation Loss: 0.031213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 9.461s | Train Loss: 0.323866 || Validation Loss: 0.030134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 9.370s | Train Loss: 0.321961 || Validation Loss: 0.029402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 9.758s | Train Loss: 0.322490 || Validation Loss: 0.029537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 9.289s | Train Loss: 0.322099 || Validation Loss: 0.029208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 10.286s | Train Loss: 0.321573 || Validation Loss: 0.028571 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 9.757s | Train Loss: 0.325769 || Validation Loss: 0.028589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 10.453s | Train Loss: 0.321375 || Validation Loss: 0.029907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 10.472s | Train Loss: 0.321381 || Validation Loss: 0.031742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 10.164s | Train Loss: 0.324062 || Validation Loss: 0.028861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 10.378s | Train Loss: 0.322190 || Validation Loss: 0.030654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 10.328s | Train Loss: 0.320992 || Validation Loss: 0.029599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 10.771s | Train Loss: 0.324332 || Validation Loss: 0.030396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 10.837s | Train Loss: 0.322623 || Validation Loss: 0.030831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 10.899s | Train Loss: 0.320933 || Validation Loss: 0.031015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 11.580s | Train Loss: 0.319258 || Validation Loss: 0.026729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 12.695s | Train Loss: 0.323879 || Validation Loss: 0.031445 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 11.657s | Train Loss: 0.322560 || Validation Loss: 0.029482 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 11.454s | Train Loss: 0.322993 || Validation Loss: 0.030653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 12.486s | Train Loss: 0.323236 || Validation Loss: 0.032459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 556.681s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.931%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.026386\n",
      "INFO:root:Test AUC: 87.96%\n",
      "INFO:root:Test Time: 0.184s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "25_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 4169 0 61566 0\n",
      "n selected size 105000 n unlabeled outlier and normal 6579 98421\n",
      "final sizes 61566 4169 98421 6579\n",
      "len of selected to training 170735\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 4169})\n",
      "Resampled dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 14775})\n",
      "class weights 0.08147633464026337 0.9185236653597366\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8290649292694852\n",
      "validation AUC 0.8630707136992999\n",
      "validation AUC 0.8712776468840324\n",
      "validation AUC 0.8749015467717091\n",
      "validation AUC 0.876201939296852\n",
      "validation AUC 0.8772983133615421\n",
      "validation AUC 0.8778103829615864\n",
      "validation AUC 0.8783672504700362\n",
      "validation AUC 0.8773034990314053\n",
      "validation AUC 0.876607065431287\n",
      "validation AUC 0.8781938431598006\n",
      "validation AUC 0.8774051397571287\n",
      "validation AUC 0.8779946884266491\n",
      "validation AUC 0.8760561472085292\n",
      "validation AUC 0.8779658546122288\n",
      "validation AUC 0.8765192416289079\n",
      "validation AUC 0.8763394273956199\n",
      "validation AUC 0.8765141251367856\n",
      "validation AUC 0.8742274017074769\n",
      "validation AUC 0.875580137836116\n",
      "validation AUC 0.8763665557126659\n",
      "validation AUC 0.8747939141893762\n",
      "validation AUC 0.873995246531375\n",
      "validation AUC 0.8715816990180806\n",
      "validation AUC 0.8746208075362443\n",
      "validation AUC 0.8737170721936773\n",
      "validation AUC 0.8734866012294906\n",
      "validation AUC 0.8729619199951979\n",
      "validation AUC 0.872479014134183\n",
      "validation AUC 0.8729329158971085\n",
      "validation AUC 0.8734098485262907\n",
      "validation AUC 0.872724129493919\n",
      "validation AUC 0.8711837647079859\n",
      "validation AUC 0.8729232044066009\n",
      "validation AUC 0.8733443292238918\n",
      "validation AUC 0.8724731154014554\n",
      "validation AUC 0.8743235773916926\n",
      "validation AUC 0.8717300826109936\n",
      "validation AUC 0.8717584933769231\n",
      "validation AUC 0.870866568803223\n",
      "validation AUC 0.8725243388576903\n",
      "validation AUC 0.8725964273847643\n",
      "validation AUC 0.8722993036675057\n",
      "validation AUC 0.8729148099538467\n",
      "validation AUC 0.8724609427797915\n",
      "validation AUC 0.870696058976046\n",
      "validation AUC 0.8711767617920908\n",
      "validation AUC 0.8715171269185914\n",
      "validation AUC 0.8702071240088692\n",
      "validation AUC 0.8712088975132838\n",
      "validation AUC 0.8710797293681648\n",
      "validation AUC 0.8717571869818987\n",
      "validation AUC 0.8716584197930926\n",
      "validation AUC 0.8719201857028555\n",
      "validation AUC 0.8716858035356424\n",
      "validation AUC 0.8718453965832157\n",
      "validation AUC 0.8718061754649755\n",
      "validation AUC 0.871700016900654\n",
      "validation AUC 0.8712843012505421\n",
      "validation AUC 0.8718188057240004\n",
      "validation AUC 0.8711954637281949\n",
      "validation AUC 0.8716918672306747\n",
      "validation AUC 0.8716798595713151\n",
      "validation AUC 0.8719808944916002\n",
      "validation AUC 0.8718715085196113\n",
      "validation AUC 0.8724854849136119\n",
      "validation AUC 0.8717515995490037\n",
      "validation AUC 0.8718130320433423\n",
      "validation AUC 0.8719057142516574\n",
      "validation AUC 0.8715759466228811\n",
      "validation AUC 0.8715759466228811\n",
      "train auc 0.9308914794780377\n",
      "Test AUC 0.8795910638191777\n",
      "test false positive rates [0.         0.         0.         ... 0.98927523 0.98927523 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.20380739e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.07835747e+02 1.06835747e+02 5.31221695e+01 ... 2.02886537e-02\n",
      " 2.02877838e-02 2.47570977e-04]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.601s | Train Loss: 0.113619 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.502s | Train Loss: 0.045401 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.569s | Train Loss: 0.031445 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.562s | Train Loss: 0.025455 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.593s | Train Loss: 0.022400 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.557s | Train Loss: 0.020813 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.584s | Train Loss: 0.019762 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 2.572s | Train Loss: 0.018912 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 2.566s | Train Loss: 0.018332 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 2.577s | Train Loss: 0.017867 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 2.610s | Train Loss: 0.017418 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 2.564s | Train Loss: 0.017024 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 2.612s | Train Loss: 0.016639 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 2.558s | Train Loss: 0.016365 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 2.588s | Train Loss: 0.016072 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 2.591s | Train Loss: 0.015810 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 2.587s | Train Loss: 0.015435 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 2.591s | Train Loss: 0.014921 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 2.558s | Train Loss: 0.014619 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 2.590s | Train Loss: 0.014340 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 2.595s | Train Loss: 0.014090 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 2.616s | Train Loss: 0.013951 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 2.556s | Train Loss: 0.013863 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 2.539s | Train Loss: 0.013679 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 2.568s | Train Loss: 0.013626 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 2.565s | Train Loss: 0.013538 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 2.544s | Train Loss: 0.013456 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 2.547s | Train Loss: 0.013468 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 2.580s | Train Loss: 0.013384 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 2.589s | Train Loss: 0.013380 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 2.534s | Train Loss: 0.013323 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 2.601s | Train Loss: 0.013306 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2.583s | Train Loss: 0.013304 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 2.582s | Train Loss: 0.013254 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 2.572s | Train Loss: 0.013272 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 2.558s | Train Loss: 0.013235 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 2.536s | Train Loss: 0.013209 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 2.581s | Train Loss: 0.013199 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2.574s | Train Loss: 0.013157 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 2.582s | Train Loss: 0.013159 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 2.602s | Train Loss: 0.013150 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 2.598s | Train Loss: 0.013163 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 2.600s | Train Loss: 0.013150 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 2.563s | Train Loss: 0.013116 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 2.586s | Train Loss: 0.013088 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 2.582s | Train Loss: 0.013132 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 2.602s | Train Loss: 0.013091 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 2.596s | Train Loss: 0.013093 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 2.563s | Train Loss: 0.013111 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 2.593s | Train Loss: 0.013054 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 2.598s | Train Loss: 0.013069 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 2.571s | Train Loss: 0.013039 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 2.586s | Train Loss: 0.013045 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 2.543s | Train Loss: 0.013068 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 2.580s | Train Loss: 0.013067 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 2.606s | Train Loss: 0.013141 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 2.646s | Train Loss: 0.013085 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 2.588s | Train Loss: 0.013089 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 2.600s | Train Loss: 0.013089 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 2.609s | Train Loss: 0.013117 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 2.564s | Train Loss: 0.013095 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 2.614s | Train Loss: 0.013106 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 2.655s | Train Loss: 0.013123 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 2.607s | Train Loss: 0.013091 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 2.608s | Train Loss: 0.013099 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 2.572s | Train Loss: 0.013049 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 2.598s | Train Loss: 0.013147 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 2.594s | Train Loss: 0.013081 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 2.608s | Train Loss: 0.013103 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.555s | Train Loss: 0.013109 |\n",
      "INFO:root:Pretraining Time: 180.739s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.013429\n",
      "INFO:root:Test AUC: 52.06%\n",
      "INFO:root:Test AUC: 52.06%\n",
      "INFO:root:Test Time: 0.238s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.199s | Train Loss: 0.577217 || Validation Loss: 0.020930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.325s | Train Loss: 0.445861 || Validation Loss: 0.022493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.442s | Train Loss: 0.417849 || Validation Loss: 0.020548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.620s | Train Loss: 0.404412 || Validation Loss: 0.024484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.813s | Train Loss: 0.398920 || Validation Loss: 0.024221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.829s | Train Loss: 0.392443 || Validation Loss: 0.025306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.936s | Train Loss: 0.384039 || Validation Loss: 0.026172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.098s | Train Loss: 0.380240 || Validation Loss: 0.026436 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.148s | Train Loss: 0.381813 || Validation Loss: 0.022196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.299s | Train Loss: 0.374645 || Validation Loss: 0.025913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.447s | Train Loss: 0.370992 || Validation Loss: 0.025556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.411s | Train Loss: 0.368780 || Validation Loss: 0.024729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.779s | Train Loss: 0.366966 || Validation Loss: 0.025709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.658s | Train Loss: 0.367948 || Validation Loss: 0.026701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.819s | Train Loss: 0.364673 || Validation Loss: 0.024025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 4.116s | Train Loss: 0.363876 || Validation Loss: 0.027687 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 4.137s | Train Loss: 0.362450 || Validation Loss: 0.025013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 4.367s | Train Loss: 0.363634 || Validation Loss: 0.026144 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 4.691s | Train Loss: 0.358792 || Validation Loss: 0.025339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 4.615s | Train Loss: 0.357416 || Validation Loss: 0.026433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 4.737s | Train Loss: 0.356205 || Validation Loss: 0.024876 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 4.692s | Train Loss: 0.355754 || Validation Loss: 0.026281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 5.131s | Train Loss: 0.356610 || Validation Loss: 0.027326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 5.339s | Train Loss: 0.356343 || Validation Loss: 0.025599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 5.353s | Train Loss: 0.353388 || Validation Loss: 0.025943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 5.569s | Train Loss: 0.349013 || Validation Loss: 0.027016 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 5.445s | Train Loss: 0.352653 || Validation Loss: 0.027088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 5.555s | Train Loss: 0.349809 || Validation Loss: 0.027097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 6.163s | Train Loss: 0.349211 || Validation Loss: 0.026818 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 6.383s | Train Loss: 0.351270 || Validation Loss: 0.024589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 6.224s | Train Loss: 0.350047 || Validation Loss: 0.028687 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 6.334s | Train Loss: 0.347019 || Validation Loss: 0.027461 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 6.194s | Train Loss: 0.346507 || Validation Loss: 0.026150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 6.235s | Train Loss: 0.345809 || Validation Loss: 0.028397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 7.104s | Train Loss: 0.347442 || Validation Loss: 0.029154 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 6.358s | Train Loss: 0.345835 || Validation Loss: 0.026862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 7.274s | Train Loss: 0.343771 || Validation Loss: 0.027243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 7.275s | Train Loss: 0.342964 || Validation Loss: 0.028256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 7.245s | Train Loss: 0.340574 || Validation Loss: 0.026731 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 7.450s | Train Loss: 0.343816 || Validation Loss: 0.027852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 7.394s | Train Loss: 0.345155 || Validation Loss: 0.029538 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 7.390s | Train Loss: 0.340399 || Validation Loss: 0.029190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 7.731s | Train Loss: 0.341723 || Validation Loss: 0.027961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 8.303s | Train Loss: 0.342722 || Validation Loss: 0.029308 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 8.038s | Train Loss: 0.340200 || Validation Loss: 0.027046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 8.038s | Train Loss: 0.338701 || Validation Loss: 0.028572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 8.096s | Train Loss: 0.337557 || Validation Loss: 0.026978 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 8.623s | Train Loss: 0.339125 || Validation Loss: 0.026779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 9.068s | Train Loss: 0.341963 || Validation Loss: 0.029421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 8.659s | Train Loss: 0.336979 || Validation Loss: 0.026647 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 95.640s | Train Loss: 0.338979 || Validation Loss: 0.027671 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 9.766s | Train Loss: 0.336968 || Validation Loss: 0.027381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 9.464s | Train Loss: 0.340445 || Validation Loss: 0.027991 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 10.017s | Train Loss: 0.336131 || Validation Loss: 0.028814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 9.483s | Train Loss: 0.339210 || Validation Loss: 0.028095 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 10.890s | Train Loss: 0.333855 || Validation Loss: 0.028799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 10.046s | Train Loss: 0.335992 || Validation Loss: 0.029226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 10.555s | Train Loss: 0.336832 || Validation Loss: 0.027755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 10.583s | Train Loss: 0.334218 || Validation Loss: 0.028325 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 10.345s | Train Loss: 0.334510 || Validation Loss: 0.030216 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 10.494s | Train Loss: 0.340782 || Validation Loss: 0.027046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 10.459s | Train Loss: 0.337002 || Validation Loss: 0.026549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 10.972s | Train Loss: 0.333947 || Validation Loss: 0.028907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 10.914s | Train Loss: 0.335476 || Validation Loss: 0.030030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 11.015s | Train Loss: 0.338680 || Validation Loss: 0.030878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 11.715s | Train Loss: 0.335113 || Validation Loss: 0.029271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 12.753s | Train Loss: 0.336111 || Validation Loss: 0.026545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 11.795s | Train Loss: 0.333078 || Validation Loss: 0.027964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 11.541s | Train Loss: 0.338778 || Validation Loss: 0.026912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 109.722s | Train Loss: 0.335612 || Validation Loss: 0.026585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 684.656s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.925%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.024536\n",
      "INFO:root:Test AUC: 87.30%\n",
      "INFO:root:Test Time: 0.193s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "25_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 4169 0 61566 0\n",
      "n selected size 105000 n unlabeled outlier and normal 6680 98320\n",
      "final sizes 61566 4169 98320 6680\n",
      "len of selected to training 170735\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 4169})\n",
      "Resampled dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 14775})\n",
      "class weights 0.08147633464026337 0.9185236653597366\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.837102023118988\n",
      "validation AUC 0.8610312368362742\n",
      "validation AUC 0.8699429235747738\n",
      "validation AUC 0.8737301121977811\n",
      "validation AUC 0.8761870873680806\n",
      "validation AUC 0.8763613061864269\n",
      "validation AUC 0.877944114176903\n",
      "validation AUC 0.8784305800308766\n",
      "validation AUC 0.8773324153269777\n",
      "validation AUC 0.8782551559234345\n",
      "validation AUC 0.8782418551724622\n",
      "validation AUC 0.8778316231886342\n",
      "validation AUC 0.8777841619372151\n",
      "validation AUC 0.8775115963178711\n",
      "validation AUC 0.8762149340693562\n",
      "validation AUC 0.8766538242625707\n",
      "validation AUC 0.8756129467099386\n",
      "validation AUC 0.8746422952067489\n",
      "validation AUC 0.8770717456183351\n",
      "validation AUC 0.8763106893657635\n",
      "validation AUC 0.8766744312472236\n",
      "validation AUC 0.8748490701340962\n",
      "validation AUC 0.8766906294812541\n",
      "validation AUC 0.8742266247882363\n",
      "validation AUC 0.8758486831644582\n",
      "validation AUC 0.8741776522692535\n",
      "validation AUC 0.8749268498606761\n",
      "validation AUC 0.875526974742462\n",
      "validation AUC 0.8737424630851612\n",
      "validation AUC 0.8749425665112048\n",
      "validation AUC 0.8729338178684187\n",
      "validation AUC 0.8748356735985601\n",
      "validation AUC 0.8741152300010877\n",
      "validation AUC 0.8715931293093742\n",
      "validation AUC 0.8746419466573635\n",
      "validation AUC 0.8737507058790223\n",
      "validation AUC 0.8736279313535444\n",
      "validation AUC 0.874128379093167\n",
      "validation AUC 0.8741064816775836\n",
      "validation AUC 0.8748285855408304\n",
      "validation AUC 0.8722494025703894\n",
      "validation AUC 0.8731069777351975\n",
      "validation AUC 0.8722489023621113\n",
      "validation AUC 0.8717456981555937\n",
      "validation AUC 0.8723774372648356\n",
      "validation AUC 0.8723796642559467\n",
      "validation AUC 0.8733921576494724\n",
      "validation AUC 0.8714409010304931\n",
      "validation AUC 0.8724600940221279\n",
      "validation AUC 0.8723349701141517\n",
      "validation AUC 0.8731071506795487\n",
      "validation AUC 0.8729262375844582\n",
      "validation AUC 0.8726671190536145\n",
      "validation AUC 0.8730035570129945\n",
      "validation AUC 0.8724956513807983\n",
      "validation AUC 0.8730729582509142\n",
      "validation AUC 0.8732784587114251\n",
      "validation AUC 0.8728250491747308\n",
      "validation AUC 0.8730577897009457\n",
      "validation AUC 0.8725902253342509\n",
      "validation AUC 0.8720456688029462\n",
      "validation AUC 0.87305484964697\n",
      "validation AUC 0.8723837484033244\n",
      "validation AUC 0.872725819027199\n",
      "validation AUC 0.8720535364405988\n",
      "validation AUC 0.8722434373206035\n",
      "validation AUC 0.8723399429294282\n",
      "validation AUC 0.8720550769756683\n",
      "validation AUC 0.8717987973928718\n",
      "validation AUC 0.872453325246278\n",
      "validation AUC 0.872453325246278\n",
      "train auc 0.9251400176065911\n",
      "Test AUC 0.8729645413057927\n",
      "test false positive rates [0.         0.         0.         ... 0.99492184 0.99492184 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.03919373e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.45740852e+01 6.35740852e+01 4.88229256e+01 ... 2.74547543e-02\n",
      " 2.73793135e-02 8.21703463e-04]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.590s | Train Loss: 0.123659 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.540s | Train Loss: 0.046711 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.490s | Train Loss: 0.032332 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.527s | Train Loss: 0.026353 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.562s | Train Loss: 0.023374 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.557s | Train Loss: 0.021753 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.565s | Train Loss: 0.020675 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 2.573s | Train Loss: 0.019791 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 2.564s | Train Loss: 0.019193 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 2.554s | Train Loss: 0.018553 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 2.575s | Train Loss: 0.018113 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 2.703s | Train Loss: 0.017627 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 2.546s | Train Loss: 0.017250 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 2.575s | Train Loss: 0.016894 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 2.559s | Train Loss: 0.016500 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 2.577s | Train Loss: 0.016199 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 2.575s | Train Loss: 0.015826 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 2.576s | Train Loss: 0.015462 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 2.572s | Train Loss: 0.015268 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 2.562s | Train Loss: 0.015223 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 2.579s | Train Loss: 0.015112 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 2.568s | Train Loss: 0.015086 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 2.579s | Train Loss: 0.014983 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 2.575s | Train Loss: 0.014905 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 2.568s | Train Loss: 0.014821 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 2.558s | Train Loss: 0.014709 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 2.569s | Train Loss: 0.014575 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 2.573s | Train Loss: 0.014484 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 2.565s | Train Loss: 0.014408 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 2.574s | Train Loss: 0.014327 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 2.579s | Train Loss: 0.014213 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 2.578s | Train Loss: 0.014022 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2.583s | Train Loss: 0.013959 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 2.581s | Train Loss: 0.013890 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 2.563s | Train Loss: 0.013820 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 2.576s | Train Loss: 0.013750 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 2.601s | Train Loss: 0.013771 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 2.569s | Train Loss: 0.013728 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2.460s | Train Loss: 0.013663 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 2.768s | Train Loss: 0.013696 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 2.569s | Train Loss: 0.013689 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 2.571s | Train Loss: 0.013659 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 2.606s | Train Loss: 0.013642 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 2.576s | Train Loss: 0.013596 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 2.575s | Train Loss: 0.013651 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 2.569s | Train Loss: 0.013640 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 2.572s | Train Loss: 0.013687 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 2.575s | Train Loss: 0.013653 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 2.578s | Train Loss: 0.013646 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 2.558s | Train Loss: 0.013600 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 2.563s | Train Loss: 0.013612 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 2.552s | Train Loss: 0.013621 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 2.569s | Train Loss: 0.013614 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 2.615s | Train Loss: 0.013661 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 2.579s | Train Loss: 0.013627 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 2.488s | Train Loss: 0.013615 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 2.537s | Train Loss: 0.013649 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 2.526s | Train Loss: 0.013647 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 2.523s | Train Loss: 0.013656 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 2.535s | Train Loss: 0.013579 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 2.542s | Train Loss: 0.013628 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 2.568s | Train Loss: 0.013598 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 2.579s | Train Loss: 0.013567 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 2.575s | Train Loss: 0.013564 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 2.577s | Train Loss: 0.013602 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 2.581s | Train Loss: 0.013616 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 2.569s | Train Loss: 0.013646 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 2.561s | Train Loss: 0.013564 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 2.567s | Train Loss: 0.013589 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.556s | Train Loss: 0.013655 |\n",
      "INFO:root:Pretraining Time: 179.828s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.014132\n",
      "INFO:root:Test AUC: 50.54%\n",
      "INFO:root:Test AUC: 50.54%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.153s | Train Loss: 0.551279 || Validation Loss: 0.021739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.266s | Train Loss: 0.447446 || Validation Loss: 0.021545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.372s | Train Loss: 0.423017 || Validation Loss: 0.023983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.479s | Train Loss: 0.410953 || Validation Loss: 0.025136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.644s | Train Loss: 0.397280 || Validation Loss: 0.025161 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.800s | Train Loss: 0.391610 || Validation Loss: 0.026508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.876s | Train Loss: 0.386466 || Validation Loss: 0.028670 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.070s | Train Loss: 0.378838 || Validation Loss: 0.027374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.039s | Train Loss: 0.377018 || Validation Loss: 0.025808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.246s | Train Loss: 0.370954 || Validation Loss: 0.027528 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.396s | Train Loss: 0.369963 || Validation Loss: 0.029824 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.414s | Train Loss: 0.365319 || Validation Loss: 0.027373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.720s | Train Loss: 0.363730 || Validation Loss: 0.026370 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.647s | Train Loss: 0.361776 || Validation Loss: 0.025727 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.794s | Train Loss: 0.360548 || Validation Loss: 0.026799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 4.087s | Train Loss: 0.356254 || Validation Loss: 0.027234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 4.097s | Train Loss: 0.359418 || Validation Loss: 0.028015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 4.326s | Train Loss: 0.354388 || Validation Loss: 0.027478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 4.544s | Train Loss: 0.356384 || Validation Loss: 0.025901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 4.568s | Train Loss: 0.355036 || Validation Loss: 0.028317 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 4.634s | Train Loss: 0.355139 || Validation Loss: 0.027939 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 4.686s | Train Loss: 0.354073 || Validation Loss: 0.029334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 4.898s | Train Loss: 0.354406 || Validation Loss: 0.026455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 5.214s | Train Loss: 0.348630 || Validation Loss: 0.027533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 5.354s | Train Loss: 0.349406 || Validation Loss: 0.027126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 5.229s | Train Loss: 0.349082 || Validation Loss: 0.027454 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 5.299s | Train Loss: 0.349440 || Validation Loss: 0.029828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 52.530s | Train Loss: 0.348162 || Validation Loss: 0.026435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 6.134s | Train Loss: 0.343932 || Validation Loss: 0.027663 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 5.849s | Train Loss: 0.346078 || Validation Loss: 0.027627 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 5.997s | Train Loss: 0.344514 || Validation Loss: 0.029202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 5.909s | Train Loss: 0.345062 || Validation Loss: 0.027483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 6.054s | Train Loss: 0.343098 || Validation Loss: 0.027231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 6.121s | Train Loss: 0.341638 || Validation Loss: 0.029038 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 6.977s | Train Loss: 0.346095 || Validation Loss: 0.027459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 6.306s | Train Loss: 0.342784 || Validation Loss: 0.031353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 7.217s | Train Loss: 0.345894 || Validation Loss: 0.027964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 7.205s | Train Loss: 0.343209 || Validation Loss: 0.026559 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 7.181s | Train Loss: 0.340994 || Validation Loss: 0.029495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 7.356s | Train Loss: 0.339252 || Validation Loss: 0.028667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 7.344s | Train Loss: 0.341318 || Validation Loss: 0.028231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 7.353s | Train Loss: 0.340140 || Validation Loss: 0.029660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 7.729s | Train Loss: 0.340355 || Validation Loss: 0.029329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 8.240s | Train Loss: 0.337225 || Validation Loss: 0.028068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 7.991s | Train Loss: 0.336967 || Validation Loss: 0.027463 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 8.013s | Train Loss: 0.339040 || Validation Loss: 0.028022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 8.051s | Train Loss: 0.342127 || Validation Loss: 0.028775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 8.545s | Train Loss: 0.336580 || Validation Loss: 0.031388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 8.960s | Train Loss: 0.339119 || Validation Loss: 0.028521 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 8.568s | Train Loss: 0.333992 || Validation Loss: 0.026648 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 8.556s | Train Loss: 0.334195 || Validation Loss: 0.028736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 9.520s | Train Loss: 0.337885 || Validation Loss: 0.030088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 9.436s | Train Loss: 0.336555 || Validation Loss: 0.030950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 9.756s | Train Loss: 0.335097 || Validation Loss: 0.027406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 9.311s | Train Loss: 0.336718 || Validation Loss: 0.028780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 10.373s | Train Loss: 0.336589 || Validation Loss: 0.030423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 9.832s | Train Loss: 0.336729 || Validation Loss: 0.028853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 927.853s | Train Loss: 0.338891 || Validation Loss: 0.029223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 966.103s | Train Loss: 0.334938 || Validation Loss: 0.027821 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 10.161s | Train Loss: 0.334509 || Validation Loss: 0.029741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 10.366s | Train Loss: 0.336048 || Validation Loss: 0.030581 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 10.370s | Train Loss: 0.337664 || Validation Loss: 0.028224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 10.776s | Train Loss: 0.332764 || Validation Loss: 0.029473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 10.768s | Train Loss: 0.335225 || Validation Loss: 0.028720 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 11.041s | Train Loss: 0.334291 || Validation Loss: 0.027108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 11.557s | Train Loss: 0.329797 || Validation Loss: 0.030391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 12.704s | Train Loss: 0.335324 || Validation Loss: 0.031165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 11.756s | Train Loss: 0.335482 || Validation Loss: 0.028080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 11.620s | Train Loss: 0.336132 || Validation Loss: 0.028776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 12.547s | Train Loss: 0.332136 || Validation Loss: 0.028146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 2412.974s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.926%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.025553\n",
      "INFO:root:Test AUC: 87.70%\n",
      "INFO:root:Test Time: 0.180s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "25_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 4169 0 61566 0\n",
      "n selected size 105000 n unlabeled outlier and normal 6682 98318\n",
      "final sizes 61566 4169 98318 6682\n",
      "len of selected to training 170735\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 4169})\n",
      "Resampled dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 14775})\n",
      "class weights 0.08147633464026337 0.9185236653597366\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8379982340519233\n",
      "validation AUC 0.8590370527685678\n",
      "validation AUC 0.8658671919350249\n",
      "validation AUC 0.8712072824791087\n",
      "validation AUC 0.8752516340314467\n",
      "validation AUC 0.8746867046555342\n",
      "validation AUC 0.8753737912786238\n",
      "validation AUC 0.8770623161601543\n",
      "validation AUC 0.8762744694971927\n",
      "validation AUC 0.8772035904098792\n",
      "validation AUC 0.876740839217521\n",
      "validation AUC 0.8759725831585619\n",
      "validation AUC 0.8757615963710847\n",
      "validation AUC 0.8781188065967043\n",
      "validation AUC 0.8752109495379247\n",
      "validation AUC 0.8758053566133813\n",
      "validation AUC 0.8769964429870054\n",
      "validation AUC 0.8764640910055529\n",
      "validation AUC 0.8763104499043539\n",
      "validation AUC 0.8747466790959555\n",
      "validation AUC 0.8763029999938273\n",
      "validation AUC 0.8762371800343248\n",
      "validation AUC 0.8761067214582753\n",
      "validation AUC 0.8755380910732405\n",
      "validation AUC 0.8761082832788036\n",
      "validation AUC 0.875614269069057\n",
      "validation AUC 0.8762955341192067\n",
      "validation AUC 0.8730059782339157\n",
      "validation AUC 0.8758771471440341\n",
      "validation AUC 0.8740385198688049\n",
      "validation AUC 0.8751832890844125\n",
      "validation AUC 0.8753814167941842\n",
      "validation AUC 0.8759751187888232\n",
      "validation AUC 0.8742315124616781\n",
      "validation AUC 0.8729421724109379\n",
      "validation AUC 0.8739877194610607\n",
      "validation AUC 0.8753119064682889\n",
      "validation AUC 0.8721951007046976\n",
      "validation AUC 0.8743676542551866\n",
      "validation AUC 0.8760012892602302\n",
      "validation AUC 0.8729729405414126\n",
      "validation AUC 0.8736260076802191\n",
      "validation AUC 0.8722529599026658\n",
      "validation AUC 0.8735450883484889\n",
      "validation AUC 0.8728804232954018\n",
      "validation AUC 0.8735671081554597\n",
      "validation AUC 0.8736134838484875\n",
      "validation AUC 0.8740602150725312\n",
      "validation AUC 0.8715188696655182\n",
      "validation AUC 0.8725661142309681\n",
      "validation AUC 0.8739549744436139\n",
      "validation AUC 0.8733432968791472\n",
      "validation AUC 0.8732034141662816\n",
      "validation AUC 0.8729970569660601\n",
      "validation AUC 0.873191928000659\n",
      "validation AUC 0.8726956522109314\n",
      "validation AUC 0.8733597691634583\n",
      "validation AUC 0.8724337585884165\n",
      "validation AUC 0.8734605052571891\n",
      "validation AUC 0.8730538731765546\n",
      "validation AUC 0.8728687455606516\n",
      "validation AUC 0.8723462620499643\n",
      "validation AUC 0.8731933887152586\n",
      "validation AUC 0.8726404536952941\n",
      "validation AUC 0.8730587635106788\n",
      "validation AUC 0.8732273842534007\n",
      "validation AUC 0.8729243032684035\n",
      "validation AUC 0.8731417927634975\n",
      "validation AUC 0.8730660910298184\n",
      "validation AUC 0.8729960326033628\n",
      "validation AUC 0.8729960326033628\n",
      "train auc 0.9262289693406629\n",
      "Test AUC 0.876980828306694\n",
      "test false positive rates [0.         0.         0.         ... 0.99281857 0.99281857 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.79171333e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [8.53284073e+01 8.43284073e+01 4.62948151e+01 ... 3.58114988e-02\n",
      " 3.57880145e-02 3.27863591e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"25_per_labeled_baseline_40_unlabeled_random\" --index_baseline_name \"25_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.0 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 105000\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8ac5e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 25% labeled 40% unlabeled selected by isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c502e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.605s | Train Loss: 0.112012 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.562s | Train Loss: 0.043755 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.565s | Train Loss: 0.029473 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.555s | Train Loss: 0.023178 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.563s | Train Loss: 0.020457 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.570s | Train Loss: 0.018967 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.597s | Train Loss: 0.017808 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 2.591s | Train Loss: 0.016826 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 2.599s | Train Loss: 0.016166 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 2.595s | Train Loss: 0.015615 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 2.591s | Train Loss: 0.015093 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 2.596s | Train Loss: 0.014483 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 2.593s | Train Loss: 0.014103 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 2.590s | Train Loss: 0.013733 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 2.592s | Train Loss: 0.013509 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 2.591s | Train Loss: 0.013383 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 2.569s | Train Loss: 0.013244 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 2.589s | Train Loss: 0.013150 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 2.585s | Train Loss: 0.013082 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 2.587s | Train Loss: 0.013005 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 2.587s | Train Loss: 0.012956 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 2.596s | Train Loss: 0.012869 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 2.593s | Train Loss: 0.012897 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 2.577s | Train Loss: 0.012808 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 2.700s | Train Loss: 0.012844 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 2.593s | Train Loss: 0.012765 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 2.596s | Train Loss: 0.012769 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 2.596s | Train Loss: 0.012773 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 2.582s | Train Loss: 0.012659 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 2.595s | Train Loss: 0.012639 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 2.596s | Train Loss: 0.012606 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 2.593s | Train Loss: 0.012560 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2.597s | Train Loss: 0.012506 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 2.598s | Train Loss: 0.012495 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 2.526s | Train Loss: 0.012521 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 2.581s | Train Loss: 0.012397 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 2.586s | Train Loss: 0.012373 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 2.592s | Train Loss: 0.012415 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2.582s | Train Loss: 0.012391 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 2.570s | Train Loss: 0.012293 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 2.592s | Train Loss: 0.012338 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 2.589s | Train Loss: 0.012347 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 2.579s | Train Loss: 0.012251 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 2.582s | Train Loss: 0.012216 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 2.587s | Train Loss: 0.012228 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 2.597s | Train Loss: 0.012161 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 2.585s | Train Loss: 0.012139 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 2.595s | Train Loss: 0.012200 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 2.594s | Train Loss: 0.012117 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 2.590s | Train Loss: 0.012112 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 2.592s | Train Loss: 0.012162 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 2.575s | Train Loss: 0.012143 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 2.577s | Train Loss: 0.012196 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 2.577s | Train Loss: 0.012139 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 2.587s | Train Loss: 0.012152 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 2.587s | Train Loss: 0.012110 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 2.632s | Train Loss: 0.012179 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 2.587s | Train Loss: 0.012127 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 933.268s | Train Loss: 0.012122 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 2.577s | Train Loss: 0.012157 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 594.564s | Train Loss: 0.012212 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 2.498s | Train Loss: 0.012175 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 2.598s | Train Loss: 0.012178 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 2.573s | Train Loss: 0.012164 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 2.530s | Train Loss: 0.012207 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 2.539s | Train Loss: 0.012146 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 2.587s | Train Loss: 0.012149 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 2.547s | Train Loss: 0.012171 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 2.551s | Train Loss: 0.012118 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.532s | Train Loss: 0.012140 |\n",
      "INFO:root:Pretraining Time: 1703.503s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.015902\n",
      "INFO:root:Test AUC: 50.11%\n",
      "INFO:root:Test AUC: 50.11%\n",
      "INFO:root:Test Time: 0.227s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.238s | Train Loss: 0.581520 || Validation Loss: 0.019437 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.267s | Train Loss: 0.452461 || Validation Loss: 0.019884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.404s | Train Loss: 0.423429 || Validation Loss: 0.024684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.544s | Train Loss: 0.402819 || Validation Loss: 0.025156 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.667s | Train Loss: 0.394423 || Validation Loss: 0.024180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.815s | Train Loss: 0.387577 || Validation Loss: 0.026899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.950s | Train Loss: 0.381137 || Validation Loss: 0.024012 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.308s | Train Loss: 0.372556 || Validation Loss: 0.023462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.233s | Train Loss: 0.368692 || Validation Loss: 0.024040 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.332s | Train Loss: 0.367688 || Validation Loss: 0.025267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.483s | Train Loss: 0.361707 || Validation Loss: 0.024020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.603s | Train Loss: 0.360597 || Validation Loss: 0.025616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.972s | Train Loss: 0.355468 || Validation Loss: 0.026237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.845s | Train Loss: 0.357549 || Validation Loss: 0.026126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.888s | Train Loss: 0.354191 || Validation Loss: 0.022156 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 4.179s | Train Loss: 0.354956 || Validation Loss: 0.028477 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 4.053s | Train Loss: 0.352446 || Validation Loss: 0.027403 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 4.374s | Train Loss: 0.348474 || Validation Loss: 0.026396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 4.628s | Train Loss: 0.349128 || Validation Loss: 0.024887 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 4.610s | Train Loss: 0.344183 || Validation Loss: 0.029382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 4.468s | Train Loss: 0.346091 || Validation Loss: 0.027471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 4.681s | Train Loss: 0.344329 || Validation Loss: 0.027873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 4.714s | Train Loss: 0.342663 || Validation Loss: 0.025869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 5.116s | Train Loss: 0.342409 || Validation Loss: 0.027814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 5.439s | Train Loss: 0.342470 || Validation Loss: 0.028783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 5.061s | Train Loss: 0.342592 || Validation Loss: 0.027411 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 5.330s | Train Loss: 0.340676 || Validation Loss: 0.028233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 5.796s | Train Loss: 0.336559 || Validation Loss: 0.027929 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 5.667s | Train Loss: 0.341118 || Validation Loss: 0.027548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 6.005s | Train Loss: 0.335834 || Validation Loss: 0.026653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 5.958s | Train Loss: 0.334010 || Validation Loss: 0.026189 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 6.151s | Train Loss: 0.335399 || Validation Loss: 0.026538 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 6.249s | Train Loss: 0.333837 || Validation Loss: 0.028731 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 5.985s | Train Loss: 0.334728 || Validation Loss: 0.025770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 6.979s | Train Loss: 0.331980 || Validation Loss: 0.028696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 6.391s | Train Loss: 0.333258 || Validation Loss: 0.028455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 7.315s | Train Loss: 0.331047 || Validation Loss: 0.027317 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 7.165s | Train Loss: 0.333448 || Validation Loss: 0.025332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 6.867s | Train Loss: 0.328092 || Validation Loss: 0.026114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1314.700s | Train Loss: 0.330126 || Validation Loss: 0.027993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 970.595s | Train Loss: 0.330481 || Validation Loss: 0.028922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1003.580s | Train Loss: 0.328929 || Validation Loss: 0.030022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 41.104s | Train Loss: 0.329077 || Validation Loss: 0.027365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 8.134s | Train Loss: 0.330464 || Validation Loss: 0.029857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 7.733s | Train Loss: 0.328144 || Validation Loss: 0.029863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 8.397s | Train Loss: 0.327114 || Validation Loss: 0.028851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 8.250s | Train Loss: 0.325436 || Validation Loss: 0.029434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 8.256s | Train Loss: 0.326034 || Validation Loss: 0.027336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 8.562s | Train Loss: 0.325060 || Validation Loss: 0.027190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 8.442s | Train Loss: 0.324297 || Validation Loss: 0.028702 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 8.647s | Train Loss: 0.324838 || Validation Loss: 0.030792 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 9.514s | Train Loss: 0.326727 || Validation Loss: 0.028834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 9.001s | Train Loss: 0.324990 || Validation Loss: 0.027298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 9.573s | Train Loss: 0.324029 || Validation Loss: 0.030092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 9.118s | Train Loss: 0.321857 || Validation Loss: 0.026269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 10.549s | Train Loss: 0.327811 || Validation Loss: 0.027238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 9.747s | Train Loss: 0.323433 || Validation Loss: 0.028783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 10.393s | Train Loss: 0.324827 || Validation Loss: 0.029577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 9.965s | Train Loss: 0.325470 || Validation Loss: 0.028987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 10.671s | Train Loss: 0.325088 || Validation Loss: 0.028782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 10.104s | Train Loss: 0.325232 || Validation Loss: 0.029954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 10.319s | Train Loss: 0.326119 || Validation Loss: 0.029606 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 10.656s | Train Loss: 0.325566 || Validation Loss: 0.028752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 11.138s | Train Loss: 0.326770 || Validation Loss: 0.027300 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 11.993s | Train Loss: 0.323173 || Validation Loss: 0.027256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 12.194s | Train Loss: 0.323124 || Validation Loss: 0.028179 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 12.106s | Train Loss: 0.324260 || Validation Loss: 0.028057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 11.747s | Train Loss: 0.321308 || Validation Loss: 0.030174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 11.836s | Train Loss: 0.322383 || Validation Loss: 0.028147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 13.418s | Train Loss: 0.327063 || Validation Loss: 0.029205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 3794.904s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.928%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.025116\n",
      "INFO:root:Test AUC: 86.68%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "25_per_labeled_0_unlabeled\n",
      "out size 4169 unlabled out size 0 normal size 61566 unlabled normal size 0\n",
      "final sizes 61566 4169 99462 5538\n",
      "len of selected to training 170735\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 4169})\n",
      "Resampled dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 14775})\n",
      "class weights 0.08147633464026337 0.9185236653597366\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8231546678052968\n",
      "validation AUC 0.8506250235470385\n",
      "validation AUC 0.8571637727666604\n",
      "validation AUC 0.8617542134033256\n",
      "validation AUC 0.8651792778397304\n",
      "validation AUC 0.8654794187707521\n",
      "validation AUC 0.8672363125454577\n",
      "validation AUC 0.8675720826735729\n",
      "validation AUC 0.8693456855333168\n",
      "validation AUC 0.8684010288965001\n",
      "validation AUC 0.8705081589291542\n",
      "validation AUC 0.8693248683547596\n",
      "validation AUC 0.8728610242605273\n",
      "validation AUC 0.8695838378873928\n",
      "validation AUC 0.870860313539063\n",
      "validation AUC 0.870350295325096\n",
      "validation AUC 0.8716327016376821\n",
      "validation AUC 0.871489940066534\n",
      "validation AUC 0.869622183641146\n",
      "validation AUC 0.8711114766297372\n",
      "validation AUC 0.8679492743042156\n",
      "validation AUC 0.869773350307777\n",
      "validation AUC 0.8700092091536838\n",
      "validation AUC 0.8702868540155974\n",
      "validation AUC 0.8697759231875912\n",
      "validation AUC 0.8693024281599807\n",
      "validation AUC 0.8668834395513196\n",
      "validation AUC 0.8701678283915238\n",
      "validation AUC 0.8687569217650839\n",
      "validation AUC 0.8701911678969307\n",
      "validation AUC 0.8693010392838039\n",
      "validation AUC 0.8679701979100659\n",
      "validation AUC 0.8674545922632041\n",
      "validation AUC 0.8656149193398188\n",
      "validation AUC 0.8683697818432058\n",
      "validation AUC 0.8655705737474093\n",
      "validation AUC 0.866603054186605\n",
      "validation AUC 0.8665135914039102\n",
      "validation AUC 0.8669039853402789\n",
      "validation AUC 0.8672556769914621\n",
      "validation AUC 0.8667591271514543\n",
      "validation AUC 0.8668921586073179\n",
      "validation AUC 0.8653192004628311\n",
      "validation AUC 0.8657564889252822\n",
      "validation AUC 0.8656880295689078\n",
      "validation AUC 0.8654279984240247\n",
      "validation AUC 0.8650740505142247\n",
      "validation AUC 0.8642531661587602\n",
      "validation AUC 0.8661348512561506\n",
      "validation AUC 0.8656810053675541\n",
      "validation AUC 0.8657981126396673\n",
      "validation AUC 0.8651426083158456\n",
      "validation AUC 0.8656759766779487\n",
      "validation AUC 0.8658738808904047\n",
      "validation AUC 0.8662917304077463\n",
      "validation AUC 0.8663244488183697\n",
      "validation AUC 0.8663668414699482\n",
      "validation AUC 0.8659752768333538\n",
      "validation AUC 0.8658650154968781\n",
      "validation AUC 0.8654087297625841\n",
      "validation AUC 0.8649304322033664\n",
      "validation AUC 0.8650928163067046\n",
      "validation AUC 0.8654788174565452\n",
      "validation AUC 0.8653161087499625\n",
      "validation AUC 0.8646788646889779\n",
      "validation AUC 0.8655334625502575\n",
      "validation AUC 0.8650891179582647\n",
      "validation AUC 0.8652293944521154\n",
      "validation AUC 0.8646400453337699\n",
      "validation AUC 0.8642206765668173\n",
      "validation AUC 0.8642206765668173\n",
      "train auc 0.9284006956205468\n",
      "Test AUC 0.8667936734101701\n",
      "test false positive rates [0.         0.         0.         ... 0.98749408 0.98749408 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.14781635e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [9.28608322e+01 9.18608322e+01 5.85114822e+01 ... 1.67480540e-02\n",
      " 1.67193972e-02 6.85851381e-04]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.579s | Train Loss: 0.112862 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.561s | Train Loss: 0.042721 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.554s | Train Loss: 0.029133 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.554s | Train Loss: 0.023160 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.533s | Train Loss: 0.020474 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.545s | Train Loss: 0.019027 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.563s | Train Loss: 0.018093 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 2.579s | Train Loss: 0.017133 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 2.590s | Train Loss: 0.016474 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 2.582s | Train Loss: 0.015778 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 2.576s | Train Loss: 0.015186 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 2.569s | Train Loss: 0.014856 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 2.591s | Train Loss: 0.014480 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 2.573s | Train Loss: 0.014206 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 2.582s | Train Loss: 0.013981 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 2.586s | Train Loss: 0.013826 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 2.598s | Train Loss: 0.013704 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 2.575s | Train Loss: 0.013352 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 2.584s | Train Loss: 0.013233 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 2.595s | Train Loss: 0.012982 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 2.583s | Train Loss: 0.012798 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 2.595s | Train Loss: 0.012534 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 2.587s | Train Loss: 0.012462 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 2.578s | Train Loss: 0.012227 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 2.591s | Train Loss: 0.012148 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 2.582s | Train Loss: 0.012036 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 2.583s | Train Loss: 0.012005 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 2.591s | Train Loss: 0.011913 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 2.587s | Train Loss: 0.011915 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 2.568s | Train Loss: 0.011817 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 2.583s | Train Loss: 0.011710 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 2.592s | Train Loss: 0.011720 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2.587s | Train Loss: 0.011702 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 2.583s | Train Loss: 0.011571 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 2.585s | Train Loss: 0.011592 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 2.590s | Train Loss: 0.011598 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 2.582s | Train Loss: 0.011562 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 2.588s | Train Loss: 0.011514 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2.586s | Train Loss: 0.011501 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 2.586s | Train Loss: 0.011493 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 2.572s | Train Loss: 0.011489 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 2.580s | Train Loss: 0.011505 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 2.593s | Train Loss: 0.011486 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 2.583s | Train Loss: 0.011466 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 2.590s | Train Loss: 0.011480 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 2.587s | Train Loss: 0.011443 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 2.588s | Train Loss: 0.011467 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 2.580s | Train Loss: 0.011485 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 2.584s | Train Loss: 0.011395 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 2.587s | Train Loss: 0.011436 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 2.576s | Train Loss: 0.011397 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 2.582s | Train Loss: 0.011407 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 2.709s | Train Loss: 0.011417 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 2.573s | Train Loss: 0.011421 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 2.582s | Train Loss: 0.011456 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 2.623s | Train Loss: 0.011426 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 2.588s | Train Loss: 0.011398 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 2.596s | Train Loss: 0.011458 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 2.579s | Train Loss: 0.011368 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 2.581s | Train Loss: 0.011391 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 2.573s | Train Loss: 0.011429 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 2.584s | Train Loss: 0.011413 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 2.593s | Train Loss: 0.011395 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 2.589s | Train Loss: 0.011416 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 2.576s | Train Loss: 0.011433 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 2.593s | Train Loss: 0.011406 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 2.582s | Train Loss: 0.011397 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 2.587s | Train Loss: 0.011439 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 2.590s | Train Loss: 0.011433 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.583s | Train Loss: 0.011401 |\n",
      "INFO:root:Pretraining Time: 180.870s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.014288\n",
      "INFO:root:Test AUC: 52.02%\n",
      "INFO:root:Test AUC: 52.02%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.177s | Train Loss: 0.621998 || Validation Loss: 0.018655 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.288s | Train Loss: 0.458097 || Validation Loss: 0.021819 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.407s | Train Loss: 0.426895 || Validation Loss: 0.024097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.498s | Train Loss: 0.414290 || Validation Loss: 0.024716 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.662s | Train Loss: 0.403457 || Validation Loss: 0.024696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.801s | Train Loss: 0.390761 || Validation Loss: 0.025503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.880s | Train Loss: 0.384619 || Validation Loss: 0.023553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.100s | Train Loss: 0.382870 || Validation Loss: 0.026116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 3.056s | Train Loss: 0.375774 || Validation Loss: 0.026447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.269s | Train Loss: 0.373558 || Validation Loss: 0.026224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 3.408s | Train Loss: 0.368357 || Validation Loss: 0.026840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.397s | Train Loss: 0.370584 || Validation Loss: 0.024590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 3.733s | Train Loss: 0.369507 || Validation Loss: 0.023419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 3.640s | Train Loss: 0.369224 || Validation Loss: 0.024968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.799s | Train Loss: 0.363548 || Validation Loss: 0.025875 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 4.085s | Train Loss: 0.362526 || Validation Loss: 0.026467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 4.082s | Train Loss: 0.360518 || Validation Loss: 0.026422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 4.327s | Train Loss: 0.359710 || Validation Loss: 0.024706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 4.528s | Train Loss: 0.357302 || Validation Loss: 0.026799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 4.562s | Train Loss: 0.356355 || Validation Loss: 0.025786 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 4.567s | Train Loss: 0.355139 || Validation Loss: 0.025435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 4.731s | Train Loss: 0.352157 || Validation Loss: 0.027959 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 4.642s | Train Loss: 0.356288 || Validation Loss: 0.023503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 5.085s | Train Loss: 0.353491 || Validation Loss: 0.027375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 5.497s | Train Loss: 0.352142 || Validation Loss: 0.025988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 5.130s | Train Loss: 0.352324 || Validation Loss: 0.025619 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 5.400s | Train Loss: 0.351303 || Validation Loss: 0.027189 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 5.804s | Train Loss: 0.348785 || Validation Loss: 0.028139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 12.705s | Train Loss: 0.347645 || Validation Loss: 0.027787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 5.932s | Train Loss: 0.345725 || Validation Loss: 0.027487 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 5.755s | Train Loss: 0.347813 || Validation Loss: 0.025841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 6.126s | Train Loss: 0.347656 || Validation Loss: 0.027512 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 6.163s | Train Loss: 0.347108 || Validation Loss: 0.027113 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 6.003s | Train Loss: 0.346941 || Validation Loss: 0.027152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 7.002s | Train Loss: 0.344002 || Validation Loss: 0.028277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 6.406s | Train Loss: 0.341948 || Validation Loss: 0.026854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 7.397s | Train Loss: 0.346472 || Validation Loss: 0.026196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 7.273s | Train Loss: 0.342961 || Validation Loss: 0.028105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 6.991s | Train Loss: 0.340327 || Validation Loss: 0.026007 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 7.911s | Train Loss: 0.342719 || Validation Loss: 0.025904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 8.052s | Train Loss: 0.339312 || Validation Loss: 0.027577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 7.366s | Train Loss: 0.341724 || Validation Loss: 0.027466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 7.828s | Train Loss: 0.344716 || Validation Loss: 0.028173 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 7.871s | Train Loss: 0.339607 || Validation Loss: 0.026427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 7.864s | Train Loss: 0.341068 || Validation Loss: 0.026585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 8.541s | Train Loss: 0.340997 || Validation Loss: 0.028434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 8.317s | Train Loss: 0.340057 || Validation Loss: 0.026634 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 8.325s | Train Loss: 0.338595 || Validation Loss: 0.028056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 8.639s | Train Loss: 0.338473 || Validation Loss: 0.027114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 8.551s | Train Loss: 0.336593 || Validation Loss: 0.028102 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 8.709s | Train Loss: 0.339408 || Validation Loss: 0.028988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 9.672s | Train Loss: 0.337725 || Validation Loss: 0.027542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 9.064s | Train Loss: 0.335622 || Validation Loss: 0.027821 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 9.668s | Train Loss: 0.331479 || Validation Loss: 0.027855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 9.201s | Train Loss: 0.337512 || Validation Loss: 0.029542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 10.531s | Train Loss: 0.337464 || Validation Loss: 0.028133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 9.730s | Train Loss: 0.337473 || Validation Loss: 0.026196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 10.505s | Train Loss: 0.337506 || Validation Loss: 0.026534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 10.022s | Train Loss: 0.336011 || Validation Loss: 0.028802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 10.532s | Train Loss: 0.337670 || Validation Loss: 0.028121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 10.389s | Train Loss: 0.334461 || Validation Loss: 0.029102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 10.386s | Train Loss: 0.337372 || Validation Loss: 0.025994 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 10.870s | Train Loss: 0.334963 || Validation Loss: 0.027406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 11.193s | Train Loss: 0.334926 || Validation Loss: 0.024731 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 12.140s | Train Loss: 0.335114 || Validation Loss: 0.027920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 12.351s | Train Loss: 0.335615 || Validation Loss: 0.027293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 12.339s | Train Loss: 0.337633 || Validation Loss: 0.027296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 11.832s | Train Loss: 0.337826 || Validation Loss: 0.027618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 11.757s | Train Loss: 0.336343 || Validation Loss: 0.029601 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 13.527s | Train Loss: 0.336298 || Validation Loss: 0.028204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 503.770s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.925%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.024361\n",
      "INFO:root:Test AUC: 87.13%\n",
      "INFO:root:Test Time: 0.193s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "25_per_labeled_0_unlabeled\n",
      "out size 4169 unlabled out size 0 normal size 61566 unlabled normal size 0\n",
      "final sizes 61566 4169 99443 5557\n",
      "len of selected to training 170735\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 4169})\n",
      "Resampled dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 14775})\n",
      "class weights 0.08147633464026337 0.9185236653597366\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8261725334357306\n",
      "validation AUC 0.8572383437103492\n",
      "validation AUC 0.864377313596321\n",
      "validation AUC 0.8700453731479256\n",
      "validation AUC 0.872683035255318\n",
      "validation AUC 0.8749415820587423\n",
      "validation AUC 0.8749520332189381\n",
      "validation AUC 0.8753247336178065\n",
      "validation AUC 0.8748829299774394\n",
      "validation AUC 0.8756673975738408\n",
      "validation AUC 0.8764367870834728\n",
      "validation AUC 0.8747994218018013\n",
      "validation AUC 0.8748423519112107\n",
      "validation AUC 0.8745088593271709\n",
      "validation AUC 0.8753517289007359\n",
      "validation AUC 0.8735810953624733\n",
      "validation AUC 0.8744577023879943\n",
      "validation AUC 0.8729025069587485\n",
      "validation AUC 0.8747810152014359\n",
      "validation AUC 0.8748664576931289\n",
      "validation AUC 0.8760588903720126\n",
      "validation AUC 0.8740356011002879\n",
      "validation AUC 0.8728202599465352\n",
      "validation AUC 0.8742117302885477\n",
      "validation AUC 0.8726070115790767\n",
      "validation AUC 0.8736481259324362\n",
      "validation AUC 0.8733772658104663\n",
      "validation AUC 0.8733869985864328\n",
      "validation AUC 0.8739694911264115\n",
      "validation AUC 0.8746121762827628\n",
      "validation AUC 0.8713391645202058\n",
      "validation AUC 0.8728052696622828\n",
      "validation AUC 0.8745453984776214\n",
      "validation AUC 0.8732203946409175\n",
      "validation AUC 0.8735151716363603\n",
      "validation AUC 0.8716750969499427\n",
      "validation AUC 0.8739817036583105\n",
      "validation AUC 0.8722648345279087\n",
      "validation AUC 0.8731515947172047\n",
      "validation AUC 0.8732172896246373\n",
      "validation AUC 0.8743084780194648\n",
      "validation AUC 0.8731457784656292\n",
      "validation AUC 0.8728345850602048\n",
      "validation AUC 0.8711479891733642\n",
      "validation AUC 0.8722316451765129\n",
      "validation AUC 0.873193519088693\n",
      "validation AUC 0.8690328504868943\n",
      "validation AUC 0.8709752922653113\n",
      "validation AUC 0.873028620640552\n",
      "validation AUC 0.8714345712672277\n",
      "validation AUC 0.8715377817955262\n",
      "validation AUC 0.8721527772308598\n",
      "validation AUC 0.8715024213273485\n",
      "validation AUC 0.8725021540884151\n",
      "validation AUC 0.8720366836787147\n",
      "validation AUC 0.8723893331755371\n",
      "validation AUC 0.8719071270739752\n",
      "validation AUC 0.8716027210691771\n",
      "validation AUC 0.8716094366313802\n",
      "validation AUC 0.8716838399520822\n",
      "validation AUC 0.8718693693310172\n",
      "validation AUC 0.8706110927464905\n",
      "validation AUC 0.8714423111921282\n",
      "validation AUC 0.8722670162874202\n",
      "validation AUC 0.8712047042779302\n",
      "validation AUC 0.8712190453556938\n",
      "validation AUC 0.8712408975396775\n",
      "validation AUC 0.8711798854331474\n",
      "validation AUC 0.8715953802466261\n",
      "validation AUC 0.8705990105880257\n",
      "validation AUC 0.8705990105880257\n",
      "train auc 0.9247589080123936\n",
      "Test AUC 0.8712748743715928\n",
      "test false positive rates [0.         0.         0.         ... 0.98885836 0.98885836 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.25979843e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [9.01176910e+01 8.91176910e+01 4.94517021e+01 ... 5.80116063e-02\n",
      " 5.79415001e-02 3.22262174e-03]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 1.00\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 1.00\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.651s | Train Loss: 0.109179 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.630s | Train Loss: 0.041673 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 2.630s | Train Loss: 0.028739 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.629s | Train Loss: 0.023026 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 2.624s | Train Loss: 0.020121 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.633s | Train Loss: 0.018434 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 2.671s | Train Loss: 0.017500 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 2.576s | Train Loss: 0.016688 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 2.612s | Train Loss: 0.016211 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 2.628s | Train Loss: 0.015690 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 2.615s | Train Loss: 0.015422 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 2.617s | Train Loss: 0.015000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 2.615s | Train Loss: 0.014689 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 2.610s | Train Loss: 0.014398 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 2.615s | Train Loss: 0.014172 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 2.621s | Train Loss: 0.014020 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 2.625s | Train Loss: 0.013816 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 2.650s | Train Loss: 0.013698 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 2.718s | Train Loss: 0.013523 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 2.745s | Train Loss: 0.013383 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 2.607s | Train Loss: 0.013251 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 2.667s | Train Loss: 0.013114 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 2.753s | Train Loss: 0.012944 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 2.644s | Train Loss: 0.012797 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 2.602s | Train Loss: 0.012693 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 2.617s | Train Loss: 0.012521 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 2.614s | Train Loss: 0.012322 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 2.618s | Train Loss: 0.012262 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 2.615s | Train Loss: 0.012202 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 2.601s | Train Loss: 0.012129 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 2.614s | Train Loss: 0.012097 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 2.625s | Train Loss: 0.012059 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2.612s | Train Loss: 0.012067 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 2.615s | Train Loss: 0.012054 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 2.626s | Train Loss: 0.011992 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 2.690s | Train Loss: 0.011935 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 2.595s | Train Loss: 0.011920 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 2.610s | Train Loss: 0.011881 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2.638s | Train Loss: 0.011913 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 2.754s | Train Loss: 0.011825 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 2.716s | Train Loss: 0.011794 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 2.579s | Train Loss: 0.011786 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 2.599s | Train Loss: 0.011697 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 2.608s | Train Loss: 0.011661 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 2.621s | Train Loss: 0.011647 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 2.616s | Train Loss: 0.011663 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 2.627s | Train Loss: 0.011606 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 2.620s | Train Loss: 0.011661 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 2.625s | Train Loss: 0.011619 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 2.621s | Train Loss: 0.011596 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 2.624s | Train Loss: 0.011590 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 2.623s | Train Loss: 0.011580 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 2.605s | Train Loss: 0.011601 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 2.629s | Train Loss: 0.011625 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 2.629s | Train Loss: 0.011555 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 2.629s | Train Loss: 0.011598 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 2.633s | Train Loss: 0.011595 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 2.622s | Train Loss: 0.011542 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 2.624s | Train Loss: 0.011550 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 2.628s | Train Loss: 0.011572 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 2.627s | Train Loss: 0.011517 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 2.624s | Train Loss: 0.011563 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 2.630s | Train Loss: 0.011508 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 2.576s | Train Loss: 0.011492 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 2.594s | Train Loss: 0.011497 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 2.613s | Train Loss: 0.011456 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 2.624s | Train Loss: 0.011490 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 2.662s | Train Loss: 0.011445 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 2.612s | Train Loss: 0.011460 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.627s | Train Loss: 0.011442 |\n",
      "INFO:root:Pretraining Time: 184.108s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.014293\n",
      "INFO:root:Test AUC: 52.24%\n",
      "INFO:root:Test AUC: 52.24%\n",
      "INFO:root:Test Time: 978.142s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 2.170s | Train Loss: 0.537612 || Validation Loss: 0.019257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 2.299s | Train Loss: 0.438703 || Validation Loss: 0.022346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 1024.699s | Train Loss: 0.415530 || Validation Loss: 0.023489 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 2.532s | Train Loss: 0.405453 || Validation Loss: 0.021692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 948.931s | Train Loss: 0.396787 || Validation Loss: 0.024046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 2.800s | Train Loss: 0.388244 || Validation Loss: 0.023810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 1000.127s | Train Loss: 0.385664 || Validation Loss: 0.023326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 3.187s | Train Loss: 0.381447 || Validation Loss: 0.024851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 906.323s | Train Loss: 0.376885 || Validation Loss: 0.025049 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 3.281s | Train Loss: 0.373731 || Validation Loss: 0.026750 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 926.805s | Train Loss: 0.370521 || Validation Loss: 0.028383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 3.386s | Train Loss: 0.367328 || Validation Loss: 0.027341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 1068.956s | Train Loss: 0.369656 || Validation Loss: 0.026570 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 314.797s | Train Loss: 0.368458 || Validation Loss: 0.026398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 3.783s | Train Loss: 0.366629 || Validation Loss: 0.023310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 979.732s | Train Loss: 0.361520 || Validation Loss: 0.024302 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 980.411s | Train Loss: 0.361030 || Validation Loss: 0.026551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 4.398s | Train Loss: 0.358475 || Validation Loss: 0.025040 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 974.859s | Train Loss: 0.356506 || Validation Loss: 0.027230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1012.886s | Train Loss: 0.353254 || Validation Loss: 0.025736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 986.782s | Train Loss: 0.354126 || Validation Loss: 0.026853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 964.970s | Train Loss: 0.355080 || Validation Loss: 0.025405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 4.629s | Train Loss: 0.352008 || Validation Loss: 0.025210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 960.403s | Train Loss: 0.347517 || Validation Loss: 0.028429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1063.734s | Train Loss: 0.351015 || Validation Loss: 0.029071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 944.412s | Train Loss: 0.348430 || Validation Loss: 0.029383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 904.550s | Train Loss: 0.349330 || Validation Loss: 0.029353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 921.100s | Train Loss: 0.348010 || Validation Loss: 0.026998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1018.993s | Train Loss: 0.349238 || Validation Loss: 0.026816 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 973.600s | Train Loss: 0.344453 || Validation Loss: 0.027406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 921.678s | Train Loss: 0.346038 || Validation Loss: 0.030857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 985.441s | Train Loss: 0.347949 || Validation Loss: 0.027878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 2060.631s | Train Loss: 0.342260 || Validation Loss: 0.027657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 6.058s | Train Loss: 0.343334 || Validation Loss: 0.029128 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1972.778s | Train Loss: 0.342613 || Validation Loss: 0.028693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1050.823s | Train Loss: 0.344061 || Validation Loss: 0.030082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1036.637s | Train Loss: 0.342103 || Validation Loss: 0.028277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 954.381s | Train Loss: 0.342098 || Validation Loss: 0.028257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 2030.294s | Train Loss: 0.340013 || Validation Loss: 0.027409 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 908.023s | Train Loss: 0.341557 || Validation Loss: 0.028966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 947.015s | Train Loss: 0.338125 || Validation Loss: 0.029919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1070.565s | Train Loss: 0.340146 || Validation Loss: 0.030280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 923.279s | Train Loss: 0.338092 || Validation Loss: 0.027247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 8.296s | Train Loss: 0.337066 || Validation Loss: 0.028887 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 8.162s | Train Loss: 0.334430 || Validation Loss: 0.030531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 8.686s | Train Loss: 0.336709 || Validation Loss: 0.027965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 8.669s | Train Loss: 0.336194 || Validation Loss: 0.029353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 8.651s | Train Loss: 0.337304 || Validation Loss: 0.028731 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 8.721s | Train Loss: 0.337072 || Validation Loss: 0.030473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 8.503s | Train Loss: 0.334057 || Validation Loss: 0.028003 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 8.731s | Train Loss: 0.333137 || Validation Loss: 0.030390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 9.592s | Train Loss: 0.334242 || Validation Loss: 0.028059 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 9.233s | Train Loss: 0.334130 || Validation Loss: 0.028284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 9.654s | Train Loss: 0.334356 || Validation Loss: 0.029183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 9.632s | Train Loss: 0.337989 || Validation Loss: 0.030917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 10.605s | Train Loss: 0.335264 || Validation Loss: 0.028172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 9.756s | Train Loss: 0.337785 || Validation Loss: 0.028539 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 10.659s | Train Loss: 0.331789 || Validation Loss: 0.029111 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 10.166s | Train Loss: 0.337004 || Validation Loss: 0.029282 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 11.096s | Train Loss: 0.337533 || Validation Loss: 0.028326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 11.415s | Train Loss: 0.335007 || Validation Loss: 0.027691 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 10.393s | Train Loss: 0.336268 || Validation Loss: 0.027275 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 10.881s | Train Loss: 0.335061 || Validation Loss: 0.029333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 11.242s | Train Loss: 0.334458 || Validation Loss: 0.029596 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 12.345s | Train Loss: 0.332089 || Validation Loss: 0.029536 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 12.312s | Train Loss: 0.334656 || Validation Loss: 0.030450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 12.294s | Train Loss: 0.334154 || Validation Loss: 0.028063 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 12.127s | Train Loss: 0.336112 || Validation Loss: 0.026796 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 12.001s | Train Loss: 0.333889 || Validation Loss: 0.028616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 14.225s | Train Loss: 0.333687 || Validation Loss: 0.028385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 34070.642s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.927%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.026068\n",
      "INFO:root:Test AUC: 87.27%\n",
      "INFO:root:Test Time: 0.195s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "25_per_labeled_0_unlabeled\n",
      "out size 4169 unlabled out size 0 normal size 61566 unlabled normal size 0\n",
      "final sizes 61566 4169 99480 5520\n",
      "len of selected to training 170735\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 4169})\n",
      "Resampled dataset shape Counter({0.0: 105000, 1.0: 61566, -1.0: 14775})\n",
      "class weights 0.08147633464026337 0.9185236653597366\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.8397409676752638\n",
      "validation AUC 0.8597012708270231\n",
      "validation AUC 0.8649523695291849\n",
      "validation AUC 0.866715851153342\n",
      "validation AUC 0.866574470476324\n",
      "validation AUC 0.867759945470912\n",
      "validation AUC 0.8680627976372289\n",
      "validation AUC 0.8660923335525027\n",
      "validation AUC 0.8677346450426274\n",
      "validation AUC 0.8674919961354122\n",
      "validation AUC 0.8669749989516912\n",
      "validation AUC 0.866948384146335\n",
      "validation AUC 0.8653196820463329\n",
      "validation AUC 0.8686654900774642\n",
      "validation AUC 0.8670305353739844\n",
      "validation AUC 0.8661329249221432\n",
      "validation AUC 0.8697951466174318\n",
      "validation AUC 0.8677381012689751\n",
      "validation AUC 0.8673877319769105\n",
      "validation AUC 0.8702115380808562\n",
      "validation AUC 0.8696569427951171\n",
      "validation AUC 0.8670052056781942\n",
      "validation AUC 0.8673406352389708\n",
      "validation AUC 0.8673789197970304\n",
      "validation AUC 0.8691413877012194\n",
      "validation AUC 0.8680299355497596\n",
      "validation AUC 0.8673275553246319\n",
      "validation AUC 0.8682748460369561\n",
      "validation AUC 0.8696437857209907\n",
      "validation AUC 0.8701576113713732\n",
      "validation AUC 0.8687051581903358\n",
      "validation AUC 0.8683421825853871\n",
      "validation AUC 0.8681189726232816\n",
      "validation AUC 0.8681047805437285\n",
      "validation AUC 0.8682210630043189\n",
      "validation AUC 0.8700586472920746\n",
      "validation AUC 0.8673512673255651\n",
      "validation AUC 0.8671589532066863\n",
      "validation AUC 0.8671337964552475\n",
      "validation AUC 0.8685333180219935\n",
      "validation AUC 0.8678356658293678\n",
      "validation AUC 0.8663662720839292\n",
      "validation AUC 0.8667219760440678\n",
      "validation AUC 0.8678146730457766\n",
      "validation AUC 0.8697185615372189\n",
      "validation AUC 0.8692992566266421\n",
      "validation AUC 0.8685557954663251\n",
      "validation AUC 0.8706627738400861\n",
      "validation AUC 0.8679789834831226\n",
      "validation AUC 0.8688171223635034\n",
      "validation AUC 0.8689305419299057\n",
      "validation AUC 0.868926298141588\n",
      "validation AUC 0.8690665320645214\n",
      "validation AUC 0.8683658573367676\n",
      "validation AUC 0.8680021473834958\n",
      "validation AUC 0.8684444964637402\n",
      "validation AUC 0.8687535347164765\n",
      "validation AUC 0.8684365968978998\n",
      "validation AUC 0.8682485318887035\n",
      "validation AUC 0.8681760229738211\n",
      "validation AUC 0.8687587336897512\n",
      "validation AUC 0.8688745691557102\n",
      "validation AUC 0.8693436102010985\n",
      "validation AUC 0.8690153165903335\n",
      "validation AUC 0.8689803605458699\n",
      "validation AUC 0.8692070746266052\n",
      "validation AUC 0.8686904100281756\n",
      "validation AUC 0.8683106056074839\n",
      "validation AUC 0.8689724769441233\n",
      "validation AUC 0.8689120714731645\n",
      "validation AUC 0.8689120714731645\n",
      "train auc 0.9266456911066603\n",
      "Test AUC 0.8727088020251199\n",
      "test false positive rates [0.         0.         0.         ... 0.99380388 0.99380388 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.39977604e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.50180054e+01 7.40180054e+01 6.89841461e+01 ... 2.86174268e-02\n",
      " 2.84962021e-02 1.93525269e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"25_per_labeled_baseline_40_unlabeled_isolation\" --index_baseline_name \"25_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 4 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 1.0 --ratio_unknown_normal 0.0 --ratio_known_outlier 1.0 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 105000\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0b5aa4",
   "metadata": {},
   "source": [
    "# Drop unlabeled samples - by isolation forest little labeled 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ebf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_baseline_0_unlabeled_random\" --index_baseline_name \"5_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 30 --lr_milestone 30 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 0\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdaf526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5% labeled 5% unlabeled selected randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f43f9007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.095s | Train Loss: 0.214363 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.085s | Train Loss: 0.202975 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.084s | Train Loss: 0.192445 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.083s | Train Loss: 0.182828 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.084s | Train Loss: 0.173336 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.081s | Train Loss: 0.164900 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.084s | Train Loss: 0.156244 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.084s | Train Loss: 0.148101 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.084s | Train Loss: 0.140474 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.082s | Train Loss: 0.133358 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.083s | Train Loss: 0.126821 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.083s | Train Loss: 0.120507 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.083s | Train Loss: 0.114479 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.084s | Train Loss: 0.109075 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.082s | Train Loss: 0.104175 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.083s | Train Loss: 0.099491 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.084s | Train Loss: 0.095236 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.083s | Train Loss: 0.090940 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.083s | Train Loss: 0.087465 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.083s | Train Loss: 0.084109 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.083s | Train Loss: 0.080817 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.083s | Train Loss: 0.078307 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.084s | Train Loss: 0.075297 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.083s | Train Loss: 0.073226 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.083s | Train Loss: 0.070182 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.084s | Train Loss: 0.067743 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.083s | Train Loss: 0.066115 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.083s | Train Loss: 0.064173 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.084s | Train Loss: 0.062253 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.082s | Train Loss: 0.060796 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.083s | Train Loss: 0.058981 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.083s | Train Loss: 0.056983 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.083s | Train Loss: 0.056519 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.083s | Train Loss: 0.054947 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.083s | Train Loss: 0.053951 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.084s | Train Loss: 0.052671 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.084s | Train Loss: 0.051192 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.083s | Train Loss: 0.050382 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.082s | Train Loss: 0.049184 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.084s | Train Loss: 0.048543 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.082s | Train Loss: 0.047052 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.082s | Train Loss: 0.046600 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.083s | Train Loss: 0.045598 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.083s | Train Loss: 0.044341 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.083s | Train Loss: 0.044056 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.083s | Train Loss: 0.042901 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.085s | Train Loss: 0.042440 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.083s | Train Loss: 0.041754 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.083s | Train Loss: 0.041041 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.084s | Train Loss: 0.040672 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.083s | Train Loss: 0.040080 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.085s | Train Loss: 0.040101 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.083s | Train Loss: 0.039202 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.083s | Train Loss: 0.038504 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.084s | Train Loss: 0.037982 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.083s | Train Loss: 0.037357 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.084s | Train Loss: 0.037222 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.083s | Train Loss: 0.036982 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.083s | Train Loss: 0.036445 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.083s | Train Loss: 0.035955 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.084s | Train Loss: 0.034988 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.084s | Train Loss: 0.035388 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.078s | Train Loss: 0.033912 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.078s | Train Loss: 0.034575 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.082s | Train Loss: 0.034558 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.082s | Train Loss: 0.034024 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.083s | Train Loss: 0.032940 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.082s | Train Loss: 0.032866 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.081s | Train Loss: 0.033020 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.080s | Train Loss: 0.032324 |\n",
      "INFO:root:Pretraining Time: 5.829s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.035733\n",
      "INFO:root:Test AUC: 52.49%\n",
      "INFO:root:Test AUC: 52.49%\n",
      "INFO:root:Test Time: 0.217s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.445s | Train Loss: 1.081073 || Validation Loss: 0.013544 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.452s | Train Loss: 0.848797 || Validation Loss: 0.014177 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.451s | Train Loss: 0.775352 || Validation Loss: 0.014169 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.450s | Train Loss: 0.729611 || Validation Loss: 0.014245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.454s | Train Loss: 0.685220 || Validation Loss: 0.013809 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.456s | Train Loss: 0.637963 || Validation Loss: 0.014958 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.459s | Train Loss: 0.608725 || Validation Loss: 0.014526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.483s | Train Loss: 0.626456 || Validation Loss: 0.015001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.459s | Train Loss: 0.599646 || Validation Loss: 0.014435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.463s | Train Loss: 0.572923 || Validation Loss: 0.015446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.491s | Train Loss: 0.558761 || Validation Loss: 0.014838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.496s | Train Loss: 0.555393 || Validation Loss: 0.014966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.470s | Train Loss: 0.550188 || Validation Loss: 0.015910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.501s | Train Loss: 0.529948 || Validation Loss: 0.015494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.504s | Train Loss: 0.515818 || Validation Loss: 0.014502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.506s | Train Loss: 0.523884 || Validation Loss: 0.015156 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.508s | Train Loss: 0.500274 || Validation Loss: 0.014175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.537s | Train Loss: 0.500725 || Validation Loss: 0.015005 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.482s | Train Loss: 0.488107 || Validation Loss: 0.015362 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.544s | Train Loss: 0.486548 || Validation Loss: 0.015923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.518s | Train Loss: 0.474969 || Validation Loss: 0.016607 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.492s | Train Loss: 0.463479 || Validation Loss: 0.016238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.554s | Train Loss: 0.469063 || Validation Loss: 0.016736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.526s | Train Loss: 0.465437 || Validation Loss: 0.017467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.529s | Train Loss: 0.447621 || Validation Loss: 0.016762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.561s | Train Loss: 0.444121 || Validation Loss: 0.016200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.532s | Train Loss: 0.438687 || Validation Loss: 0.018060 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.535s | Train Loss: 0.443599 || Validation Loss: 0.017174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.540s | Train Loss: 0.431926 || Validation Loss: 0.017513 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.572s | Train Loss: 0.439932 || Validation Loss: 0.017231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 15.174s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.824%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.015590\n",
      "INFO:root:Test AUC: 82.95%\n",
      "INFO:root:Test Time: 0.183s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "1_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 167 0 2462 0\n",
      "n selected size 2630 n unlabeled outlier and normal 175 2455\n",
      "final sizes 2462 167 2455 175\n",
      "len of selected to training 5259\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 590})\n",
      "class weights 0.10383667722632876 0.8961633227736713\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5321052430767449\n",
      "validation AUC 0.5781621890221098\n",
      "validation AUC 0.6207284128731899\n",
      "validation AUC 0.6572997974901464\n",
      "validation AUC 0.6845593234246686\n",
      "validation AUC 0.7055808131172916\n",
      "validation AUC 0.7212388035826831\n",
      "validation AUC 0.7350158938519719\n",
      "validation AUC 0.7465713196963076\n",
      "validation AUC 0.7557433036479232\n",
      "validation AUC 0.7633672094955282\n",
      "validation AUC 0.7704875120342661\n",
      "validation AUC 0.7772580279703698\n",
      "validation AUC 0.7824831770377582\n",
      "validation AUC 0.7863603951687969\n",
      "validation AUC 0.7906487685404326\n",
      "validation AUC 0.7946621151615684\n",
      "validation AUC 0.7971003857350816\n",
      "validation AUC 0.7999691813165611\n",
      "validation AUC 0.8028295212495926\n",
      "validation AUC 0.8060014936006332\n",
      "validation AUC 0.8081629254989791\n",
      "validation AUC 0.8105340431111422\n",
      "validation AUC 0.8113306088301022\n",
      "validation AUC 0.8131698799904555\n",
      "validation AUC 0.8160620629696237\n",
      "validation AUC 0.8170742450633168\n",
      "validation AUC 0.8186512953372287\n",
      "validation AUC 0.8206399850959221\n",
      "validation AUC 0.8207410005612975\n",
      "validation AUC 0.8207410005612975\n",
      "train auc 0.8239094708187409\n",
      "Test AUC 0.8294553750604072\n",
      "test false positive rates [0.         0.         0.         ... 0.99602084 0.99602084 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 8.39865622e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.60128822e+01 2.50128822e+01 2.39009914e+01 ... 5.64051345e-02\n",
      " 5.62184528e-02 1.66011527e-02]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.102s | Train Loss: 0.212418 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.086s | Train Loss: 0.201711 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.083s | Train Loss: 0.190834 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.082s | Train Loss: 0.180645 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.078s | Train Loss: 0.171058 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.083s | Train Loss: 0.161989 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.081s | Train Loss: 0.153460 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.080s | Train Loss: 0.145820 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.080s | Train Loss: 0.138470 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.081s | Train Loss: 0.131860 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.081s | Train Loss: 0.125602 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.080s | Train Loss: 0.119707 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.082s | Train Loss: 0.113821 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.081s | Train Loss: 0.108617 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.080s | Train Loss: 0.104137 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.080s | Train Loss: 0.099111 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.082s | Train Loss: 0.094777 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.082s | Train Loss: 0.091369 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.082s | Train Loss: 0.087810 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.080s | Train Loss: 0.083936 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.080s | Train Loss: 0.081326 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.079s | Train Loss: 0.077647 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.079s | Train Loss: 0.075027 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.080s | Train Loss: 0.073239 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.079s | Train Loss: 0.070560 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.080s | Train Loss: 0.068711 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.074s | Train Loss: 0.066353 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.080s | Train Loss: 0.064272 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.079s | Train Loss: 0.062639 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.080s | Train Loss: 0.060567 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.081s | Train Loss: 0.059151 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.080s | Train Loss: 0.057375 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.081s | Train Loss: 0.056104 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.080s | Train Loss: 0.055003 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.081s | Train Loss: 0.053615 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.080s | Train Loss: 0.052287 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.080s | Train Loss: 0.050928 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.080s | Train Loss: 0.049950 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.080s | Train Loss: 0.049207 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.080s | Train Loss: 0.047980 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.080s | Train Loss: 0.047585 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.081s | Train Loss: 0.046305 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.080s | Train Loss: 0.045293 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.079s | Train Loss: 0.044551 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.080s | Train Loss: 0.043943 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.077s | Train Loss: 0.043722 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.080s | Train Loss: 0.043006 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.082s | Train Loss: 0.041856 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.081s | Train Loss: 0.041377 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.081s | Train Loss: 0.040402 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.081s | Train Loss: 0.039704 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.080s | Train Loss: 0.039214 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.078s | Train Loss: 0.038760 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.081s | Train Loss: 0.038523 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.081s | Train Loss: 0.037419 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.080s | Train Loss: 0.037590 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.081s | Train Loss: 0.036712 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.081s | Train Loss: 0.036557 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.081s | Train Loss: 0.036076 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.079s | Train Loss: 0.035396 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.081s | Train Loss: 0.035116 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.081s | Train Loss: 0.034280 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.080s | Train Loss: 0.034204 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.082s | Train Loss: 0.033927 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.082s | Train Loss: 0.033467 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.080s | Train Loss: 0.033440 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.081s | Train Loss: 0.032814 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.079s | Train Loss: 0.032576 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.079s | Train Loss: 0.032091 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.078s | Train Loss: 0.031956 |\n",
      "INFO:root:Pretraining Time: 5.659s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.034444\n",
      "INFO:root:Test AUC: 53.31%\n",
      "INFO:root:Test AUC: 53.31%\n",
      "INFO:root:Test Time: 0.220s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.447s | Train Loss: 0.841665 || Validation Loss: 0.011803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.454s | Train Loss: 0.759561 || Validation Loss: 0.012917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.456s | Train Loss: 0.733690 || Validation Loss: 0.011807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.456s | Train Loss: 0.658099 || Validation Loss: 0.013558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.461s | Train Loss: 0.636134 || Validation Loss: 0.014560 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.461s | Train Loss: 0.615295 || Validation Loss: 0.014595 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.462s | Train Loss: 0.587359 || Validation Loss: 0.013847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.493s | Train Loss: 0.587720 || Validation Loss: 0.013768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.467s | Train Loss: 0.555345 || Validation Loss: 0.014771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.470s | Train Loss: 0.575052 || Validation Loss: 0.013558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.497s | Train Loss: 0.537187 || Validation Loss: 0.014451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.501s | Train Loss: 0.523423 || Validation Loss: 0.014972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.472s | Train Loss: 0.516580 || Validation Loss: 0.014660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.506s | Train Loss: 0.519346 || Validation Loss: 0.015313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.508s | Train Loss: 0.492408 || Validation Loss: 0.014969 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.509s | Train Loss: 0.494830 || Validation Loss: 0.017080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.514s | Train Loss: 0.478442 || Validation Loss: 0.016357 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.543s | Train Loss: 0.486301 || Validation Loss: 0.015973 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.494s | Train Loss: 0.459359 || Validation Loss: 0.016713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.552s | Train Loss: 0.470722 || Validation Loss: 0.016669 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.524s | Train Loss: 0.454910 || Validation Loss: 0.017020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.492s | Train Loss: 0.454641 || Validation Loss: 0.017192 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.562s | Train Loss: 0.451565 || Validation Loss: 0.018016 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.529s | Train Loss: 0.449281 || Validation Loss: 0.018651 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.533s | Train Loss: 0.432262 || Validation Loss: 0.016401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.567s | Train Loss: 0.438981 || Validation Loss: 0.017833 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.539s | Train Loss: 0.426687 || Validation Loss: 0.017958 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.540s | Train Loss: 0.424346 || Validation Loss: 0.017766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.545s | Train Loss: 0.423681 || Validation Loss: 0.017973 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.576s | Train Loss: 0.433454 || Validation Loss: 0.018562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 15.335s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.850%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.016075\n",
      "INFO:root:Test AUC: 80.54%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "1_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 167 0 2462 0\n",
      "n selected size 2630 n unlabeled outlier and normal 144 2486\n",
      "final sizes 2462 167 2486 144\n",
      "len of selected to training 5259\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 590})\n",
      "class weights 0.10383667722632876 0.8961633227736713\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6143500655485699\n",
      "validation AUC 0.6442147240883066\n",
      "validation AUC 0.670020668712483\n",
      "validation AUC 0.6912729619545841\n",
      "validation AUC 0.7072274748230274\n",
      "validation AUC 0.7200032838141329\n",
      "validation AUC 0.7297999081745314\n",
      "validation AUC 0.7390497436911497\n",
      "validation AUC 0.7455224095437185\n",
      "validation AUC 0.7507015181640526\n",
      "validation AUC 0.7577067568347076\n",
      "validation AUC 0.763268061829149\n",
      "validation AUC 0.7676570489031284\n",
      "validation AUC 0.7725537048085767\n",
      "validation AUC 0.7747454631641304\n",
      "validation AUC 0.7798388153322564\n",
      "validation AUC 0.7827272680347965\n",
      "validation AUC 0.7827511050877992\n",
      "validation AUC 0.7847279548101198\n",
      "validation AUC 0.786803044906145\n",
      "validation AUC 0.7881251352956965\n",
      "validation AUC 0.7894457463458722\n",
      "validation AUC 0.7891681946078398\n",
      "validation AUC 0.792338176768497\n",
      "validation AUC 0.7943398932278823\n",
      "validation AUC 0.7954960661279601\n",
      "validation AUC 0.7955499928374432\n",
      "validation AUC 0.7956366831891492\n",
      "validation AUC 0.7989773667461111\n",
      "validation AUC 0.7988799139343765\n",
      "validation AUC 0.7988799139343765\n",
      "train auc 0.8499750311791158\n",
      "Test AUC 0.8053843250546515\n",
      "test false positive rates [0.         0.         0.         ... 0.99747987 0.99747987 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.95968645e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.53488235e+01 3.43488235e+01 2.84956589e+01 ... 8.93342048e-02\n",
      " 8.91133919e-02 2.68294942e-02]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.100s | Train Loss: 0.206953 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.090s | Train Loss: 0.196398 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.087s | Train Loss: 0.186003 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.086s | Train Loss: 0.176237 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.086s | Train Loss: 0.167163 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.084s | Train Loss: 0.158274 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.082s | Train Loss: 0.149684 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.084s | Train Loss: 0.142208 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.085s | Train Loss: 0.135264 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.084s | Train Loss: 0.128529 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.083s | Train Loss: 0.121745 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.083s | Train Loss: 0.116423 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.081s | Train Loss: 0.110992 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.090s | Train Loss: 0.105986 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.086s | Train Loss: 0.101004 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.087s | Train Loss: 0.096685 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.086s | Train Loss: 0.092672 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.080s | Train Loss: 0.089134 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.081s | Train Loss: 0.085552 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.081s | Train Loss: 0.082752 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.081s | Train Loss: 0.079476 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.080s | Train Loss: 0.076861 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.083s | Train Loss: 0.073949 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.085s | Train Loss: 0.071668 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.081s | Train Loss: 0.069573 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.083s | Train Loss: 0.067699 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.081s | Train Loss: 0.065799 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.081s | Train Loss: 0.063598 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.082s | Train Loss: 0.062038 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.082s | Train Loss: 0.060087 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.081s | Train Loss: 0.059407 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.081s | Train Loss: 0.057476 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.081s | Train Loss: 0.056157 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.081s | Train Loss: 0.054903 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.080s | Train Loss: 0.053705 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.082s | Train Loss: 0.051901 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.081s | Train Loss: 0.051243 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.081s | Train Loss: 0.050209 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.081s | Train Loss: 0.048800 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.080s | Train Loss: 0.047937 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.081s | Train Loss: 0.047088 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.084s | Train Loss: 0.046492 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.082s | Train Loss: 0.045152 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.082s | Train Loss: 0.044353 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.082s | Train Loss: 0.043510 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.080s | Train Loss: 0.043420 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.080s | Train Loss: 0.042573 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.079s | Train Loss: 0.041470 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.081s | Train Loss: 0.040651 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.082s | Train Loss: 0.040697 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.078s | Train Loss: 0.039653 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.082s | Train Loss: 0.038820 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.081s | Train Loss: 0.039255 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.081s | Train Loss: 0.038436 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.080s | Train Loss: 0.038123 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.082s | Train Loss: 0.037107 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.084s | Train Loss: 0.037028 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.088s | Train Loss: 0.035696 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.083s | Train Loss: 0.035666 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.083s | Train Loss: 0.035320 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.081s | Train Loss: 0.035084 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.082s | Train Loss: 0.034910 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.082s | Train Loss: 0.033938 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.080s | Train Loss: 0.033946 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.083s | Train Loss: 0.033355 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.083s | Train Loss: 0.033162 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.082s | Train Loss: 0.032609 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.081s | Train Loss: 0.032485 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.080s | Train Loss: 0.032176 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.079s | Train Loss: 0.031890 |\n",
      "INFO:root:Pretraining Time: 5.788s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.035457\n",
      "INFO:root:Test AUC: 53.28%\n",
      "INFO:root:Test AUC: 53.28%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.455s | Train Loss: 1.280527 || Validation Loss: 0.013930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.458s | Train Loss: 0.972364 || Validation Loss: 0.014122 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.459s | Train Loss: 0.828751 || Validation Loss: 0.014823 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.460s | Train Loss: 0.814813 || Validation Loss: 0.013767 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.454s | Train Loss: 0.728763 || Validation Loss: 0.014656 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.471s | Train Loss: 0.671364 || Validation Loss: 0.013065 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.475s | Train Loss: 0.670902 || Validation Loss: 0.014074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.492s | Train Loss: 0.636962 || Validation Loss: 0.014629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.478s | Train Loss: 0.581802 || Validation Loss: 0.014440 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.473s | Train Loss: 0.571711 || Validation Loss: 0.013808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.514s | Train Loss: 0.567408 || Validation Loss: 0.015333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.506s | Train Loss: 0.553577 || Validation Loss: 0.015357 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.465s | Train Loss: 0.541335 || Validation Loss: 0.015109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.511s | Train Loss: 0.521089 || Validation Loss: 0.014879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.503s | Train Loss: 0.524744 || Validation Loss: 0.015736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.510s | Train Loss: 0.494748 || Validation Loss: 0.015764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.508s | Train Loss: 0.501556 || Validation Loss: 0.015338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.549s | Train Loss: 0.477722 || Validation Loss: 0.015321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.485s | Train Loss: 0.470894 || Validation Loss: 0.015771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.550s | Train Loss: 0.464544 || Validation Loss: 0.015741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.516s | Train Loss: 0.457979 || Validation Loss: 0.016265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.498s | Train Loss: 0.458494 || Validation Loss: 0.016676 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.560s | Train Loss: 0.445188 || Validation Loss: 0.017306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.529s | Train Loss: 0.451692 || Validation Loss: 0.016226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.532s | Train Loss: 0.423391 || Validation Loss: 0.016695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.558s | Train Loss: 0.429905 || Validation Loss: 0.016966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.529s | Train Loss: 0.415693 || Validation Loss: 0.016892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.531s | Train Loss: 0.424783 || Validation Loss: 0.016646 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.533s | Train Loss: 0.421003 || Validation Loss: 0.016364 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.575s | Train Loss: 0.412915 || Validation Loss: 0.017372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 15.353s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.816%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.014808\n",
      "INFO:root:Test AUC: 81.27%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 5\n",
      "experiment_method type 5\n",
      "method num 5\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 5\n",
      "1_per_labeled_0_unlabeled\n",
      "incidens loaded sizes 167 0 2462 0\n",
      "n selected size 2630 n unlabeled outlier and normal 165 2465\n",
      "final sizes 2462 167 2465 165\n",
      "len of selected to training 5259\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 590})\n",
      "class weights 0.10383667722632876 0.8961633227736713\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5303923223989223\n",
      "validation AUC 0.5635080527147155\n",
      "validation AUC 0.5924161507248868\n",
      "validation AUC 0.6198143594045435\n",
      "validation AUC 0.6429863855013671\n",
      "validation AUC 0.6638113093047466\n",
      "validation AUC 0.6826603571955386\n",
      "validation AUC 0.6986843803898772\n",
      "validation AUC 0.7126028939071013\n",
      "validation AUC 0.7245568101441388\n",
      "validation AUC 0.7354136631997621\n",
      "validation AUC 0.7451436960027823\n",
      "validation AUC 0.7527097054028666\n",
      "validation AUC 0.7605067125822391\n",
      "validation AUC 0.7662152996471083\n",
      "validation AUC 0.7714716398934619\n",
      "validation AUC 0.7765306000817787\n",
      "validation AUC 0.7809445150885166\n",
      "validation AUC 0.7837915409671644\n",
      "validation AUC 0.7873758738478979\n",
      "validation AUC 0.7912027891187884\n",
      "validation AUC 0.7942742196059082\n",
      "validation AUC 0.7971163099188322\n",
      "validation AUC 0.7991754066533661\n",
      "validation AUC 0.8009574944290634\n",
      "validation AUC 0.8024408381319627\n",
      "validation AUC 0.8041890048686231\n",
      "validation AUC 0.8058779634413733\n",
      "validation AUC 0.8069678268164\n",
      "validation AUC 0.8070866715140751\n",
      "validation AUC 0.8070866715140751\n",
      "train auc 0.8156461181090335\n",
      "Test AUC 0.8127477066483194\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99677878e-01\n",
      " 9.99677878e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.81908722e+01 2.71908722e+01 2.41794243e+01 ... 3.94153222e-02\n",
      " 3.93117927e-02 1.69128384e-02]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1_per_labeled_baseline_1_unlabeled_random\" --index_baseline_name \"1_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 5 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 30 --lr_milestone 30 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 2630\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "98a5825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5% labeled 1% unlabeled selected by isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "acb3981f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.107s | Train Loss: 0.220181 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.082s | Train Loss: 0.209326 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.083s | Train Loss: 0.198581 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.081s | Train Loss: 0.188169 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.081s | Train Loss: 0.177761 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.080s | Train Loss: 0.168456 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.080s | Train Loss: 0.159023 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.083s | Train Loss: 0.149742 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.082s | Train Loss: 0.140764 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.081s | Train Loss: 0.132175 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.080s | Train Loss: 0.124873 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.087s | Train Loss: 0.117412 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.080s | Train Loss: 0.110485 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.080s | Train Loss: 0.104243 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.081s | Train Loss: 0.099021 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.080s | Train Loss: 0.093848 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.081s | Train Loss: 0.089323 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.081s | Train Loss: 0.084715 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.081s | Train Loss: 0.081450 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.082s | Train Loss: 0.077328 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.081s | Train Loss: 0.073932 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.080s | Train Loss: 0.071031 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.081s | Train Loss: 0.068138 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.080s | Train Loss: 0.065952 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.081s | Train Loss: 0.063259 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.081s | Train Loss: 0.060299 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.082s | Train Loss: 0.058497 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.080s | Train Loss: 0.056321 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.082s | Train Loss: 0.055496 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.080s | Train Loss: 0.053674 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.081s | Train Loss: 0.052237 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.081s | Train Loss: 0.050276 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.080s | Train Loss: 0.049626 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.082s | Train Loss: 0.047886 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.080s | Train Loss: 0.046476 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.081s | Train Loss: 0.045740 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.081s | Train Loss: 0.044503 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.080s | Train Loss: 0.043615 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.081s | Train Loss: 0.042775 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.081s | Train Loss: 0.041480 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.080s | Train Loss: 0.040931 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.080s | Train Loss: 0.040669 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.080s | Train Loss: 0.039395 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.081s | Train Loss: 0.038493 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.081s | Train Loss: 0.038171 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.082s | Train Loss: 0.036869 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.082s | Train Loss: 0.035971 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.080s | Train Loss: 0.036008 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.082s | Train Loss: 0.035420 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.082s | Train Loss: 0.034369 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.081s | Train Loss: 0.033963 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.084s | Train Loss: 0.033494 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.080s | Train Loss: 0.033101 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.080s | Train Loss: 0.032447 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.083s | Train Loss: 0.032381 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.081s | Train Loss: 0.031678 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.081s | Train Loss: 0.030718 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.080s | Train Loss: 0.030663 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.081s | Train Loss: 0.030009 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.083s | Train Loss: 0.029642 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.081s | Train Loss: 0.029453 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.081s | Train Loss: 0.028308 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.082s | Train Loss: 0.028587 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.080s | Train Loss: 0.027837 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.080s | Train Loss: 0.027948 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.080s | Train Loss: 0.027703 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.082s | Train Loss: 0.026597 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.081s | Train Loss: 0.026784 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.081s | Train Loss: 0.026133 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.076s | Train Loss: 0.026565 |\n",
      "INFO:root:Pretraining Time: 5.715s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.039053\n",
      "INFO:root:Test AUC: 50.03%\n",
      "INFO:root:Test AUC: 50.03%\n",
      "INFO:root:Test Time: 0.218s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.456s | Train Loss: 1.073979 || Validation Loss: 0.011052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.456s | Train Loss: 0.948735 || Validation Loss: 0.012582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.456s | Train Loss: 0.858100 || Validation Loss: 0.013056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.457s | Train Loss: 0.819353 || Validation Loss: 0.011078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.461s | Train Loss: 0.795062 || Validation Loss: 0.010885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.465s | Train Loss: 0.747897 || Validation Loss: 0.011237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.464s | Train Loss: 0.695302 || Validation Loss: 0.011256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.494s | Train Loss: 0.695384 || Validation Loss: 0.012105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.466s | Train Loss: 0.669886 || Validation Loss: 0.011708 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.471s | Train Loss: 0.655964 || Validation Loss: 0.012198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.500s | Train Loss: 0.628265 || Validation Loss: 0.011837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.505s | Train Loss: 0.616019 || Validation Loss: 0.012228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.470s | Train Loss: 0.597123 || Validation Loss: 0.011113 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.507s | Train Loss: 0.586841 || Validation Loss: 0.012025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.504s | Train Loss: 0.562245 || Validation Loss: 0.012086 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.516s | Train Loss: 0.562726 || Validation Loss: 0.012266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.512s | Train Loss: 0.539780 || Validation Loss: 0.011529 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.545s | Train Loss: 0.552611 || Validation Loss: 0.011971 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.486s | Train Loss: 0.538875 || Validation Loss: 0.012551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.549s | Train Loss: 0.516778 || Validation Loss: 0.012771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.520s | Train Loss: 0.531880 || Validation Loss: 0.012783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.494s | Train Loss: 0.517624 || Validation Loss: 0.012961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.558s | Train Loss: 0.517335 || Validation Loss: 0.013151 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.531s | Train Loss: 0.508873 || Validation Loss: 0.011794 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.532s | Train Loss: 0.494230 || Validation Loss: 0.012890 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.562s | Train Loss: 0.479161 || Validation Loss: 0.012098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.537s | Train Loss: 0.501574 || Validation Loss: 0.013872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.536s | Train Loss: 0.476116 || Validation Loss: 0.013232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.544s | Train Loss: 0.480582 || Validation Loss: 0.013647 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.582s | Train Loss: 0.479090 || Validation Loss: 0.012091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 15.340s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.854%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.012828\n",
      "INFO:root:Test AUC: 79.54%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "1_per_labeled_0_unlabeled\n",
      "out size 167 unlabled out size 0 normal size 2462 unlabled normal size 0\n",
      "total iso selected normal 2573 outlier 57\n",
      "final sizes 2462 167 2573 57\n",
      "len of selected to training 5259\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 590})\n",
      "class weights 0.10383667722632876 0.8961633227736713\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5422407452422211\n",
      "validation AUC 0.5682082757650366\n",
      "validation AUC 0.5896700179202277\n",
      "validation AUC 0.6115040933001251\n",
      "validation AUC 0.6342903789939768\n",
      "validation AUC 0.6525550851705699\n",
      "validation AUC 0.6691600364236768\n",
      "validation AUC 0.6828612280687938\n",
      "validation AUC 0.6969450178510499\n",
      "validation AUC 0.7088730855592424\n",
      "validation AUC 0.7181115652836724\n",
      "validation AUC 0.7259207849949436\n",
      "validation AUC 0.7346320132659493\n",
      "validation AUC 0.7410861847156358\n",
      "validation AUC 0.747226164170911\n",
      "validation AUC 0.7518143937805593\n",
      "validation AUC 0.7573001593429434\n",
      "validation AUC 0.7613098342224623\n",
      "validation AUC 0.7652351787755028\n",
      "validation AUC 0.7686660834096241\n",
      "validation AUC 0.7713242460743761\n",
      "validation AUC 0.7748938680425019\n",
      "validation AUC 0.7776016896822783\n",
      "validation AUC 0.7803001297561558\n",
      "validation AUC 0.7833566417974464\n",
      "validation AUC 0.7866547704012076\n",
      "validation AUC 0.7882840684940517\n",
      "validation AUC 0.78949916486503\n",
      "validation AUC 0.7912900408744662\n",
      "validation AUC 0.7915969665454319\n",
      "validation AUC 0.7915969665454319\n",
      "train auc 0.8536431981845374\n",
      "Test AUC 0.7954456048756501\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99450497e-01\n",
      " 1.00000000e+00 1.00000000e+00] true positive rate [0.         0.         0.00307951 ... 0.99972004 0.99972004 1.        ] tresholds [19.80379105 18.80379105 16.27391052 ...  0.03290755  0.02176932\n",
      "  0.02174792]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.112s | Train Loss: 0.211968 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.095s | Train Loss: 0.201516 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.083s | Train Loss: 0.191549 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.083s | Train Loss: 0.181230 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.084s | Train Loss: 0.171456 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.088s | Train Loss: 0.161713 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.086s | Train Loss: 0.152317 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.086s | Train Loss: 0.143293 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.086s | Train Loss: 0.134355 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.084s | Train Loss: 0.126540 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.085s | Train Loss: 0.119273 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.084s | Train Loss: 0.112626 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.084s | Train Loss: 0.106455 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.085s | Train Loss: 0.101255 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.084s | Train Loss: 0.096077 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.081s | Train Loss: 0.091572 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.082s | Train Loss: 0.087209 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.083s | Train Loss: 0.082676 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.081s | Train Loss: 0.079376 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.081s | Train Loss: 0.075866 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.082s | Train Loss: 0.073260 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.081s | Train Loss: 0.070474 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.081s | Train Loss: 0.068038 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.080s | Train Loss: 0.065675 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.081s | Train Loss: 0.063364 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.081s | Train Loss: 0.061244 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.082s | Train Loss: 0.059571 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.082s | Train Loss: 0.057780 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.083s | Train Loss: 0.056776 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.082s | Train Loss: 0.054426 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.082s | Train Loss: 0.053252 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.081s | Train Loss: 0.051589 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.082s | Train Loss: 0.050435 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.081s | Train Loss: 0.049637 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.081s | Train Loss: 0.048317 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.080s | Train Loss: 0.047337 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.083s | Train Loss: 0.046457 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.083s | Train Loss: 0.045788 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.083s | Train Loss: 0.044364 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.084s | Train Loss: 0.043723 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.083s | Train Loss: 0.043367 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.084s | Train Loss: 0.041818 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.083s | Train Loss: 0.041648 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.082s | Train Loss: 0.040723 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.083s | Train Loss: 0.039329 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.081s | Train Loss: 0.039308 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.083s | Train Loss: 0.038633 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.084s | Train Loss: 0.038035 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.083s | Train Loss: 0.037749 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.083s | Train Loss: 0.036474 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.083s | Train Loss: 0.036370 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.083s | Train Loss: 0.035491 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.083s | Train Loss: 0.034866 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.083s | Train Loss: 0.034654 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.083s | Train Loss: 0.034558 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.083s | Train Loss: 0.033571 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.081s | Train Loss: 0.033974 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.081s | Train Loss: 0.033054 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.083s | Train Loss: 0.032223 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.080s | Train Loss: 0.032118 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.083s | Train Loss: 0.031360 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.083s | Train Loss: 0.031241 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.079s | Train Loss: 0.031269 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.082s | Train Loss: 0.030183 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.082s | Train Loss: 0.030399 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.083s | Train Loss: 0.030116 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.082s | Train Loss: 0.030287 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.084s | Train Loss: 0.028954 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.082s | Train Loss: 0.029164 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.077s | Train Loss: 0.028288 |\n",
      "INFO:root:Pretraining Time: 5.832s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.039195\n",
      "INFO:root:Test AUC: 52.13%\n",
      "INFO:root:Test AUC: 52.13%\n",
      "INFO:root:Test Time: 0.235s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.477s | Train Loss: 0.920090 || Validation Loss: 0.013507 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.473s | Train Loss: 0.813176 || Validation Loss: 0.014324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.468s | Train Loss: 0.736641 || Validation Loss: 0.013252 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.478s | Train Loss: 0.722780 || Validation Loss: 0.012630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.482s | Train Loss: 0.679597 || Validation Loss: 0.012983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.485s | Train Loss: 0.648015 || Validation Loss: 0.012555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.484s | Train Loss: 0.636817 || Validation Loss: 0.011954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.519s | Train Loss: 0.604832 || Validation Loss: 0.012774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.486s | Train Loss: 0.601979 || Validation Loss: 0.012907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.491s | Train Loss: 0.577369 || Validation Loss: 0.011925 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.530s | Train Loss: 0.573086 || Validation Loss: 0.011557 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.527s | Train Loss: 0.550924 || Validation Loss: 0.012687 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.501s | Train Loss: 0.542653 || Validation Loss: 0.013107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.534s | Train Loss: 0.537233 || Validation Loss: 0.013430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.520s | Train Loss: 0.510780 || Validation Loss: 0.012361 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.532s | Train Loss: 0.527822 || Validation Loss: 0.013430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.527s | Train Loss: 0.510727 || Validation Loss: 0.012858 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.583s | Train Loss: 0.483398 || Validation Loss: 0.012473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.505s | Train Loss: 0.474966 || Validation Loss: 0.012635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.572s | Train Loss: 0.482152 || Validation Loss: 0.011547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.540s | Train Loss: 0.473509 || Validation Loss: 0.013220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.507s | Train Loss: 0.460735 || Validation Loss: 0.013863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.581s | Train Loss: 0.460218 || Validation Loss: 0.012764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.560s | Train Loss: 0.445749 || Validation Loss: 0.012917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.550s | Train Loss: 0.449993 || Validation Loss: 0.013971 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.584s | Train Loss: 0.440974 || Validation Loss: 0.014687 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.560s | Train Loss: 0.446363 || Validation Loss: 0.013481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.564s | Train Loss: 0.437794 || Validation Loss: 0.014576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.570s | Train Loss: 0.417639 || Validation Loss: 0.014466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.603s | Train Loss: 0.424666 || Validation Loss: 0.014625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 16.001s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.900%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.013839\n",
      "INFO:root:Test AUC: 80.60%\n",
      "INFO:root:Test Time: 0.204s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "1_per_labeled_0_unlabeled\n",
      "out size 167 unlabled out size 0 normal size 2462 unlabled normal size 0\n",
      "total iso selected normal 2573 outlier 57\n",
      "final sizes 2462 167 2573 57\n",
      "len of selected to training 5259\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 590})\n",
      "class weights 0.10383667722632876 0.8961633227736713\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5722972948523459\n",
      "validation AUC 0.6078109837436566\n",
      "validation AUC 0.6362034069292257\n",
      "validation AUC 0.6601143018486633\n",
      "validation AUC 0.6794425013777012\n",
      "validation AUC 0.6993294654816911\n",
      "validation AUC 0.7139474803657608\n",
      "validation AUC 0.7260559609607661\n",
      "validation AUC 0.7352501403243861\n",
      "validation AUC 0.7440268267020971\n",
      "validation AUC 0.7516491999966796\n",
      "validation AUC 0.7571766770759655\n",
      "validation AUC 0.7614746528501335\n",
      "validation AUC 0.7684229183300364\n",
      "validation AUC 0.7719506292300858\n",
      "validation AUC 0.7750768272023054\n",
      "validation AUC 0.782043831867865\n",
      "validation AUC 0.7857341583506153\n",
      "validation AUC 0.787717079750015\n",
      "validation AUC 0.7911940514380137\n",
      "validation AUC 0.7921580858242464\n",
      "validation AUC 0.7947380483746104\n",
      "validation AUC 0.7957545939873262\n",
      "validation AUC 0.7995500254041948\n",
      "validation AUC 0.8000752866672357\n",
      "validation AUC 0.8009276309305811\n",
      "validation AUC 0.8047305255613879\n",
      "validation AUC 0.8059482852753795\n",
      "validation AUC 0.805418250748237\n",
      "validation AUC 0.8057647248013695\n",
      "validation AUC 0.8057647248013695\n",
      "train auc 0.8998925367042848\n",
      "Test AUC 0.8059934285455114\n",
      "test false positive rates [0.         0.         0.         ... 0.99636191 0.99636191 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [28.26597214 27.26597214 20.54869461 ...  0.0679158   0.06776871\n",
      "  0.03761104]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.097s | Train Loss: 0.221259 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.086s | Train Loss: 0.210515 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.085s | Train Loss: 0.201025 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.085s | Train Loss: 0.192078 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.082s | Train Loss: 0.182670 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.081s | Train Loss: 0.173373 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.080s | Train Loss: 0.163799 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.082s | Train Loss: 0.154103 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.081s | Train Loss: 0.144715 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.081s | Train Loss: 0.136015 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.081s | Train Loss: 0.127728 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.081s | Train Loss: 0.119861 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.077s | Train Loss: 0.112965 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.080s | Train Loss: 0.106541 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.080s | Train Loss: 0.100811 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.078s | Train Loss: 0.095447 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.079s | Train Loss: 0.090488 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.079s | Train Loss: 0.086050 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.079s | Train Loss: 0.081548 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.080s | Train Loss: 0.078139 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.079s | Train Loss: 0.075108 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.082s | Train Loss: 0.072177 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.078s | Train Loss: 0.069218 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.080s | Train Loss: 0.066843 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.080s | Train Loss: 0.064535 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.079s | Train Loss: 0.062260 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.080s | Train Loss: 0.059758 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.082s | Train Loss: 0.058041 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.080s | Train Loss: 0.056317 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.080s | Train Loss: 0.054553 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.080s | Train Loss: 0.052628 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.080s | Train Loss: 0.051722 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.080s | Train Loss: 0.049287 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.080s | Train Loss: 0.048494 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.079s | Train Loss: 0.047469 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.080s | Train Loss: 0.046749 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.081s | Train Loss: 0.045241 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.080s | Train Loss: 0.044509 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.080s | Train Loss: 0.042593 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.080s | Train Loss: 0.043174 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.079s | Train Loss: 0.041676 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.080s | Train Loss: 0.041361 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.082s | Train Loss: 0.039943 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.080s | Train Loss: 0.038772 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.081s | Train Loss: 0.038387 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.084s | Train Loss: 0.037498 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.082s | Train Loss: 0.036798 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.081s | Train Loss: 0.036456 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.079s | Train Loss: 0.035124 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.080s | Train Loss: 0.035943 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.080s | Train Loss: 0.034445 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.081s | Train Loss: 0.034275 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.081s | Train Loss: 0.033687 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.079s | Train Loss: 0.032903 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.081s | Train Loss: 0.032120 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.081s | Train Loss: 0.031868 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.082s | Train Loss: 0.031482 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.081s | Train Loss: 0.030621 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.079s | Train Loss: 0.030938 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.080s | Train Loss: 0.030123 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.082s | Train Loss: 0.030244 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.081s | Train Loss: 0.030030 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.081s | Train Loss: 0.029156 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.082s | Train Loss: 0.028654 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.082s | Train Loss: 0.028432 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.079s | Train Loss: 0.027952 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.082s | Train Loss: 0.027456 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.082s | Train Loss: 0.027238 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.080s | Train Loss: 0.027371 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.076s | Train Loss: 0.027022 |\n",
      "INFO:root:Pretraining Time: 5.664s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.039929\n",
      "INFO:root:Test AUC: 50.79%\n",
      "INFO:root:Test AUC: 50.79%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.460s | Train Loss: 1.046922 || Validation Loss: 0.012630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.454s | Train Loss: 0.899672 || Validation Loss: 0.012251 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.455s | Train Loss: 0.775945 || Validation Loss: 0.012426 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.509s | Train Loss: 0.727800 || Validation Loss: 0.012125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.525s | Train Loss: 0.681828 || Validation Loss: 0.011309 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.525s | Train Loss: 0.622155 || Validation Loss: 0.011873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.476s | Train Loss: 0.617208 || Validation Loss: 0.011965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.533s | Train Loss: 0.590417 || Validation Loss: 0.010990 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.502s | Train Loss: 0.575947 || Validation Loss: 0.012711 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.474s | Train Loss: 0.545960 || Validation Loss: 0.012550 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.518s | Train Loss: 0.548186 || Validation Loss: 0.010693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.528s | Train Loss: 0.503412 || Validation Loss: 0.012175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.523s | Train Loss: 0.521169 || Validation Loss: 0.012604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.523s | Train Loss: 0.503634 || Validation Loss: 0.013331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.533s | Train Loss: 0.488184 || Validation Loss: 0.013025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.525s | Train Loss: 0.469015 || Validation Loss: 0.012062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.522s | Train Loss: 0.472809 || Validation Loss: 0.013484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.557s | Train Loss: 0.458182 || Validation Loss: 0.012902 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.519s | Train Loss: 0.467284 || Validation Loss: 0.013947 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.563s | Train Loss: 0.430824 || Validation Loss: 0.012943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.526s | Train Loss: 0.441274 || Validation Loss: 0.013523 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.503s | Train Loss: 0.431323 || Validation Loss: 0.014222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.566s | Train Loss: 0.448417 || Validation Loss: 0.014260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.569s | Train Loss: 0.434191 || Validation Loss: 0.014385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.544s | Train Loss: 0.413106 || Validation Loss: 0.014633 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.572s | Train Loss: 0.404879 || Validation Loss: 0.015940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.544s | Train Loss: 0.411490 || Validation Loss: 0.014745 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.541s | Train Loss: 0.399293 || Validation Loss: 0.015478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.545s | Train Loss: 0.383813 || Validation Loss: 0.015950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.587s | Train Loss: 0.389945 || Validation Loss: 0.015978 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 15.945s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.913%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.014471\n",
      "INFO:root:Test AUC: 81.83%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 4\n",
      "experiment_method type 4\n",
      "method num 4\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 4\n",
      "1_per_labeled_0_unlabeled\n",
      "out size 167 unlabled out size 0 normal size 2462 unlabled normal size 0\n",
      "total iso selected normal 2573 outlier 57\n",
      "final sizes 2462 167 2573 57\n",
      "len of selected to training 5259\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 167})\n",
      "Resampled dataset shape Counter({0.0: 2630, 1.0: 2462, -1.0: 590})\n",
      "class weights 0.10383667722632876 0.8961633227736713\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6061187924418743\n",
      "validation AUC 0.6308784982385218\n",
      "validation AUC 0.6567070107702292\n",
      "validation AUC 0.6763248601598582\n",
      "validation AUC 0.6952927926585603\n",
      "validation AUC 0.7100617134623928\n",
      "validation AUC 0.7233912423747506\n",
      "validation AUC 0.7346580932741568\n",
      "validation AUC 0.7429163749671938\n",
      "validation AUC 0.7522118837567047\n",
      "validation AUC 0.758975402098278\n",
      "validation AUC 0.7659878325932733\n",
      "validation AUC 0.7714363140141545\n",
      "validation AUC 0.7774993811252898\n",
      "validation AUC 0.7798340473895194\n",
      "validation AUC 0.7836236412693541\n",
      "validation AUC 0.7870395556192227\n",
      "validation AUC 0.7901699627908898\n",
      "validation AUC 0.7922795991862994\n",
      "validation AUC 0.7958538746878221\n",
      "validation AUC 0.7983057732762342\n",
      "validation AUC 0.8017583278824822\n",
      "validation AUC 0.8025055618903446\n",
      "validation AUC 0.805200351508064\n",
      "validation AUC 0.8093617411547744\n",
      "validation AUC 0.8107017113295898\n",
      "validation AUC 0.8104910305205806\n",
      "validation AUC 0.8114789658161919\n",
      "validation AUC 0.8133628140483176\n",
      "validation AUC 0.8135453661237243\n",
      "validation AUC 0.8135453661237243\n",
      "train auc 0.9127761579335059\n",
      "Test AUC 0.818307378351439\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.91511132e-01\n",
      " 9.91511132e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [1.80258675e+01 1.70258675e+01 1.67997723e+01 ... 6.33123815e-02\n",
      " 6.32663667e-02 1.38446800e-02]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1_per_labeled_baseline_1_unlabeled_isolation\" --index_baseline_name \"1_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 4 --balanced_train 1 --balanced_batches 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 30 --lr_milestone 30 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 2630\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b301950",
   "metadata": {},
   "source": [
    "# Add anomalies - by isolation forest little labeled 5%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4c9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1% labeled 0.5% additional anomalies selected by isolation forest without smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "de55f935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.127s | Train Loss: 0.217584 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.081s | Train Loss: 0.210203 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.081s | Train Loss: 0.202526 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.078s | Train Loss: 0.195266 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.089s | Train Loss: 0.188452 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.099s | Train Loss: 0.181783 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.100s | Train Loss: 0.175494 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.092s | Train Loss: 0.169083 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.080s | Train Loss: 0.163370 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.078s | Train Loss: 0.157468 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.081s | Train Loss: 0.152209 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.078s | Train Loss: 0.146332 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.085s | Train Loss: 0.141150 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.078s | Train Loss: 0.136721 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.080s | Train Loss: 0.132073 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.091s | Train Loss: 0.127583 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.087s | Train Loss: 0.123309 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.080s | Train Loss: 0.119510 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.079s | Train Loss: 0.115697 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.089s | Train Loss: 0.112098 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.078s | Train Loss: 0.107662 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.078s | Train Loss: 0.105373 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.078s | Train Loss: 0.101583 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.079s | Train Loss: 0.098396 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.086s | Train Loss: 0.095957 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.075s | Train Loss: 0.093652 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.081s | Train Loss: 0.090744 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.078s | Train Loss: 0.088807 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.080s | Train Loss: 0.085985 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.087s | Train Loss: 0.083953 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.077s | Train Loss: 0.081141 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.078s | Train Loss: 0.079578 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.076s | Train Loss: 0.077537 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.080s | Train Loss: 0.076187 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.087s | Train Loss: 0.074106 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.078s | Train Loss: 0.072708 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.077s | Train Loss: 0.071093 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.078s | Train Loss: 0.069159 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.077s | Train Loss: 0.068122 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.088s | Train Loss: 0.067374 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.079s | Train Loss: 0.066065 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.076s | Train Loss: 0.064203 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.079s | Train Loss: 0.062421 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.080s | Train Loss: 0.062472 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.087s | Train Loss: 0.060354 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.078s | Train Loss: 0.060163 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.110s | Train Loss: 0.058392 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.092s | Train Loss: 0.057393 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.079s | Train Loss: 0.057007 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.088s | Train Loss: 0.055515 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.078s | Train Loss: 0.055109 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.071s | Train Loss: 0.054634 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.088s | Train Loss: 0.053398 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.076s | Train Loss: 0.052396 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.083s | Train Loss: 0.052244 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.078s | Train Loss: 0.051685 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.078s | Train Loss: 0.051042 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.078s | Train Loss: 0.049955 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.087s | Train Loss: 0.048915 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.077s | Train Loss: 0.047991 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.086s | Train Loss: 0.048029 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.110s | Train Loss: 0.047028 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.091s | Train Loss: 0.047001 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.083s | Train Loss: 0.046597 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.076s | Train Loss: 0.045720 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.111s | Train Loss: 0.045472 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.091s | Train Loss: 0.045467 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.078s | Train Loss: 0.043798 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.083s | Train Loss: 0.043738 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.074s | Train Loss: 0.043240 |\n",
      "INFO:root:Pretraining Time: 5.868s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.043693\n",
      "INFO:root:Test AUC: 59.50%\n",
      "INFO:root:Test AUC: 59.50%\n",
      "INFO:root:Test Time: 0.243s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (30,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.527s | Train Loss: 1.455228 || Validation Loss: 0.011332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.530s | Train Loss: 1.306962 || Validation Loss: 0.009094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.527s | Train Loss: 1.225986 || Validation Loss: 0.009427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.599s | Train Loss: 1.210587 || Validation Loss: 0.009034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.594s | Train Loss: 1.156791 || Validation Loss: 0.008986 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.586s | Train Loss: 1.127760 || Validation Loss: 0.009543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.571s | Train Loss: 1.132964 || Validation Loss: 0.008393 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.538s | Train Loss: 1.121415 || Validation Loss: 0.008869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.489s | Train Loss: 1.102616 || Validation Loss: 0.008576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.563s | Train Loss: 1.085367 || Validation Loss: 0.008090 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.525s | Train Loss: 1.071555 || Validation Loss: 0.007614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.540s | Train Loss: 1.072471 || Validation Loss: 0.008937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.502s | Train Loss: 1.068415 || Validation Loss: 0.008060 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.470s | Train Loss: 1.047198 || Validation Loss: 0.007765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.596s | Train Loss: 1.056694 || Validation Loss: 0.008017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.505s | Train Loss: 1.055526 || Validation Loss: 0.007996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.542s | Train Loss: 1.048236 || Validation Loss: 0.007172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.557s | Train Loss: 1.037080 || Validation Loss: 0.007471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.544s | Train Loss: 1.028951 || Validation Loss: 0.007679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.546s | Train Loss: 1.029442 || Validation Loss: 0.008238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 0.550s | Train Loss: 1.031424 || Validation Loss: 0.007543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.581s | Train Loss: 1.036163 || Validation Loss: 0.007618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.622s | Train Loss: 1.020871 || Validation Loss: 0.007409 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 0.605s | Train Loss: 1.017258 || Validation Loss: 0.006933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 0.560s | Train Loss: 1.030698 || Validation Loss: 0.007967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 0.606s | Train Loss: 1.017812 || Validation Loss: 0.007943 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 0.531s | Train Loss: 1.015720 || Validation Loss: 0.008000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 0.657s | Train Loss: 1.009770 || Validation Loss: 0.007791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 0.605s | Train Loss: 1.004860 || Validation Loss: 0.007365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 0.497s | Train Loss: 1.007556 || Validation Loss: 0.008079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 16.828s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.512%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.007600\n",
      "INFO:root:Test AUC: 48.01%\n",
      "INFO:root:Test Time: 0.265s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 7\n",
      "experiment_method type 7\n",
      "method num 7\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 7\n",
      "1_per_labeled_0_unlabeled\n",
      "out size 167 unlabled out size 0 normal size 2462 unlabled normal size 0\n",
      "len of unlabeled 260312\n",
      "total outlier 1399\n",
      "total iso selected normal 1167 outlier 65\n",
      "final sizes 2462 1399 0 0\n",
      "len of selected to training 3861\n",
      "class weights 0.3623413623413623 0.6376586376586376\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.47999954183050264\n",
      "validation AUC 0.47660768803095244\n",
      "validation AUC 0.4738858632200684\n",
      "validation AUC 0.47044785722480614\n",
      "validation AUC 0.4662868666804454\n",
      "validation AUC 0.4689087988338974\n",
      "validation AUC 0.4699168834768009\n",
      "validation AUC 0.4696562962493106\n",
      "validation AUC 0.4686014580964883\n",
      "validation AUC 0.4673880592408371\n",
      "validation AUC 0.46582499351325646\n",
      "validation AUC 0.46714268047355034\n",
      "validation AUC 0.47105282986979896\n",
      "validation AUC 0.4746345845493538\n",
      "validation AUC 0.47407022722014247\n",
      "validation AUC 0.47474903912118305\n",
      "validation AUC 0.47394759903219275\n",
      "validation AUC 0.47272932846719373\n",
      "validation AUC 0.4707559855241853\n",
      "validation AUC 0.4763913000583434\n",
      "validation AUC 0.48291508826121854\n",
      "validation AUC 0.4792569640167193\n",
      "validation AUC 0.4823387552008357\n",
      "validation AUC 0.48207174508619866\n",
      "validation AUC 0.4805300712445586\n",
      "validation AUC 0.4797901833806119\n",
      "validation AUC 0.4781299362308944\n",
      "validation AUC 0.48303733331091264\n",
      "validation AUC 0.48592079457340004\n",
      "validation AUC 0.4862231784596267\n",
      "validation AUC 0.4862231784596267\n",
      "train auc 0.5120069655203648\n",
      "Test AUC 0.48014778876497716\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 9.47418285e-05 ... 9.99791568e-01\n",
      " 9.99791568e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [8.22713423 7.22713423 6.16094637 ... 0.15766855 0.15683995 0.10594037]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..1}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1_per_labeled_baseline_0.5_outlier_isolation\" --index_baseline_name \"1_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 7 --balanced_train 0 --balanced_batches 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 30 --lr_milestone 30 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 1232\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e248aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1% labeled 0.5% additional anomalies selected by isolation forest without smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb26f341",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"1_per_labeled_baseline_0_outlier_isolation\" --index_baseline_name \"1_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 7 --balanced_train 0 --balanced_batches 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 30 --lr_milestone 30 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 --active_selection_n_samples 0\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac6db5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "074d681c",
   "metadata": {},
   "source": [
    "# Enhancements effect estimation loss function and smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d3eadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced model - with smote and loss 5% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85e9730b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.229s | Train Loss: 0.215726 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.209s | Train Loss: 0.188738 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.203s | Train Loss: 0.164385 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.211s | Train Loss: 0.142420 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.207s | Train Loss: 0.123945 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.210s | Train Loss: 0.108542 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.211s | Train Loss: 0.096052 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.213s | Train Loss: 0.085772 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.212s | Train Loss: 0.077636 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.209s | Train Loss: 0.070941 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.214s | Train Loss: 0.064996 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.212s | Train Loss: 0.060603 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.209s | Train Loss: 0.056879 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.213s | Train Loss: 0.053720 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.210s | Train Loss: 0.050916 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.214s | Train Loss: 0.048681 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.213s | Train Loss: 0.045993 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.212s | Train Loss: 0.043913 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.214s | Train Loss: 0.042541 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.211s | Train Loss: 0.040893 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.214s | Train Loss: 0.039435 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.210s | Train Loss: 0.038435 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.211s | Train Loss: 0.037064 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.213s | Train Loss: 0.036161 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.209s | Train Loss: 0.034881 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.212s | Train Loss: 0.034227 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.213s | Train Loss: 0.032993 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.211s | Train Loss: 0.032857 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.210s | Train Loss: 0.031923 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.212s | Train Loss: 0.031205 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.212s | Train Loss: 0.030307 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.214s | Train Loss: 0.029729 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.211s | Train Loss: 0.029454 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.212s | Train Loss: 0.028755 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.211s | Train Loss: 0.028123 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.213s | Train Loss: 0.027848 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.213s | Train Loss: 0.027313 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.211s | Train Loss: 0.026852 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.212s | Train Loss: 0.026933 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.211s | Train Loss: 0.026148 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.212s | Train Loss: 0.025813 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.213s | Train Loss: 0.025246 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.212s | Train Loss: 0.025262 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.213s | Train Loss: 0.025091 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.210s | Train Loss: 0.024635 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.211s | Train Loss: 0.024620 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.213s | Train Loss: 0.024082 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.213s | Train Loss: 0.023919 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.212s | Train Loss: 0.023663 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.211s | Train Loss: 0.023207 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.212s | Train Loss: 0.023077 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.213s | Train Loss: 0.022739 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.212s | Train Loss: 0.022821 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.209s | Train Loss: 0.022543 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.205s | Train Loss: 0.022492 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.207s | Train Loss: 0.022354 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.212s | Train Loss: 0.022110 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.213s | Train Loss: 0.022015 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.213s | Train Loss: 0.021778 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.211s | Train Loss: 0.021556 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.213s | Train Loss: 0.021469 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.212s | Train Loss: 0.021446 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.212s | Train Loss: 0.021131 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.211s | Train Loss: 0.020842 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.211s | Train Loss: 0.021088 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.213s | Train Loss: 0.020890 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.206s | Train Loss: 0.020651 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.209s | Train Loss: 0.020351 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.204s | Train Loss: 0.020722 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.210s | Train Loss: 0.020151 |\n",
      "INFO:root:Pretraining Time: 14.806s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.020233\n",
      "INFO:root:Test AUC: 63.21%\n",
      "INFO:root:Test AUC: 63.21%\n",
      "INFO:root:Test Time: 0.219s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 120\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/120 | Train Time: 0.524s | Train Loss: 1.040018 || Validation Loss: 0.005954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/120 | Train Time: 0.546s | Train Loss: 0.711654 || Validation Loss: 0.004370 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/120 | Train Time: 0.549s | Train Loss: 0.550818 || Validation Loss: 0.002674 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/120 | Train Time: 0.580s | Train Loss: 0.471490 || Validation Loss: 0.002196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/120 | Train Time: 0.587s | Train Loss: 0.437639 || Validation Loss: 0.001960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/120 | Train Time: 0.560s | Train Loss: 0.390714 || Validation Loss: 0.001801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/120 | Train Time: 0.639s | Train Loss: 0.385856 || Validation Loss: 0.001972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/120 | Train Time: 0.614s | Train Loss: 0.372177 || Validation Loss: 0.001835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/120 | Train Time: 0.610s | Train Loss: 0.372142 || Validation Loss: 0.001499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/120 | Train Time: 0.622s | Train Loss: 0.353482 || Validation Loss: 0.001686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/120 | Train Time: 0.621s | Train Loss: 0.346203 || Validation Loss: 0.001467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/120 | Train Time: 0.648s | Train Loss: 0.337620 || Validation Loss: 0.001605 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/120 | Train Time: 0.702s | Train Loss: 0.332565 || Validation Loss: 0.001616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/120 | Train Time: 0.634s | Train Loss: 0.336542 || Validation Loss: 0.001356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/120 | Train Time: 0.681s | Train Loss: 0.318898 || Validation Loss: 0.001448 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/120 | Train Time: 0.691s | Train Loss: 0.322572 || Validation Loss: 0.001385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/120 | Train Time: 0.727s | Train Loss: 0.318648 || Validation Loss: 0.001348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/120 | Train Time: 0.707s | Train Loss: 0.328867 || Validation Loss: 0.001470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/120 | Train Time: 0.704s | Train Loss: 0.337487 || Validation Loss: 0.001434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/120 | Train Time: 0.766s | Train Loss: 0.324405 || Validation Loss: 0.001523 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/120 | Train Time: 0.729s | Train Loss: 0.316382 || Validation Loss: 0.001436 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/120 | Train Time: 0.750s | Train Loss: 0.321867 || Validation Loss: 0.001433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/120 | Train Time: 0.764s | Train Loss: 0.310025 || Validation Loss: 0.001434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/120 | Train Time: 0.783s | Train Loss: 0.302006 || Validation Loss: 0.001475 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/120 | Train Time: 0.744s | Train Loss: 0.308368 || Validation Loss: 0.001428 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/120 | Train Time: 0.828s | Train Loss: 0.305113 || Validation Loss: 0.001380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/120 | Train Time: 0.759s | Train Loss: 0.308414 || Validation Loss: 0.001372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/120 | Train Time: 0.830s | Train Loss: 0.316698 || Validation Loss: 0.001341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/120 | Train Time: 0.801s | Train Loss: 0.311740 || Validation Loss: 0.001324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/120 | Train Time: 0.799s | Train Loss: 0.292239 || Validation Loss: 0.001348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/120 | Train Time: 0.865s | Train Loss: 0.293951 || Validation Loss: 0.001353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/120 | Train Time: 0.865s | Train Loss: 0.288620 || Validation Loss: 0.001375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/120 | Train Time: 0.840s | Train Loss: 0.293274 || Validation Loss: 0.001346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/120 | Train Time: 0.848s | Train Loss: 0.278562 || Validation Loss: 0.001239 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/120 | Train Time: 0.889s | Train Loss: 0.296975 || Validation Loss: 0.001344 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/120 | Train Time: 0.925s | Train Loss: 0.292764 || Validation Loss: 0.001457 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/120 | Train Time: 0.869s | Train Loss: 0.283703 || Validation Loss: 0.001408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/120 | Train Time: 0.945s | Train Loss: 0.297815 || Validation Loss: 0.001428 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/120 | Train Time: 0.890s | Train Loss: 0.286532 || Validation Loss: 0.001591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/120 | Train Time: 0.925s | Train Loss: 0.286381 || Validation Loss: 0.001409 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/120 | Train Time: 0.980s | Train Loss: 0.280970 || Validation Loss: 0.001407 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/120 | Train Time: 0.949s | Train Loss: 0.273220 || Validation Loss: 0.001359 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/120 | Train Time: 0.949s | Train Loss: 0.283972 || Validation Loss: 0.001316 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/120 | Train Time: 0.949s | Train Loss: 0.279720 || Validation Loss: 0.001144 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/120 | Train Time: 1.033s | Train Loss: 0.282841 || Validation Loss: 0.001391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/120 | Train Time: 0.969s | Train Loss: 0.280873 || Validation Loss: 0.001356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/120 | Train Time: 1.016s | Train Loss: 0.271749 || Validation Loss: 0.001289 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/120 | Train Time: 1.043s | Train Loss: 0.277945 || Validation Loss: 0.001263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/120 | Train Time: 0.946s | Train Loss: 0.278428 || Validation Loss: 0.001339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/120 | Train Time: 1.030s | Train Loss: 0.278382 || Validation Loss: 0.001347 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/120 | Train Time: 1.078s | Train Loss: 0.278165 || Validation Loss: 0.001198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/120 | Train Time: 1.012s | Train Loss: 0.268596 || Validation Loss: 0.001324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/120 | Train Time: 1.098s | Train Loss: 0.264784 || Validation Loss: 0.001202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/120 | Train Time: 1.027s | Train Loss: 0.275926 || Validation Loss: 0.001201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/120 | Train Time: 1.114s | Train Loss: 0.277591 || Validation Loss: 0.001376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/120 | Train Time: 1.078s | Train Loss: 0.267415 || Validation Loss: 0.001312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/120 | Train Time: 1.128s | Train Loss: 0.274668 || Validation Loss: 0.001380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/120 | Train Time: 1.133s | Train Loss: 0.273039 || Validation Loss: 0.001194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/120 | Train Time: 1.072s | Train Loss: 0.261694 || Validation Loss: 0.001346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/120 | Train Time: 1.144s | Train Loss: 0.269835 || Validation Loss: 0.001380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/120 | Train Time: 1.168s | Train Loss: 0.273875 || Validation Loss: 0.001305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/120 | Train Time: 1.137s | Train Loss: 0.273226 || Validation Loss: 0.001255 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/120 | Train Time: 1.144s | Train Loss: 0.277003 || Validation Loss: 0.001266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/120 | Train Time: 1.111s | Train Loss: 0.277602 || Validation Loss: 0.001172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/120 | Train Time: 1.158s | Train Loss: 0.272577 || Validation Loss: 0.001421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/120 | Train Time: 1.164s | Train Loss: 0.271688 || Validation Loss: 0.001223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/120 | Train Time: 1.254s | Train Loss: 0.268912 || Validation Loss: 0.001322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/120 | Train Time: 1.183s | Train Loss: 0.270716 || Validation Loss: 0.001388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/120 | Train Time: 1.266s | Train Loss: 0.275677 || Validation Loss: 0.001244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/120 | Train Time: 1.200s | Train Loss: 0.272091 || Validation Loss: 0.001224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 071/120 | Train Time: 1.256s | Train Loss: 0.278786 || Validation Loss: 0.001272 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 072/120 | Train Time: 1.292s | Train Loss: 0.270347 || Validation Loss: 0.001347 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 073/120 | Train Time: 1.354s | Train Loss: 0.281650 || Validation Loss: 0.001380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 074/120 | Train Time: 1.223s | Train Loss: 0.277383 || Validation Loss: 0.001402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 075/120 | Train Time: 1.324s | Train Loss: 0.271955 || Validation Loss: 0.001285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 076/120 | Train Time: 1.259s | Train Loss: 0.278648 || Validation Loss: 0.001211 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 077/120 | Train Time: 1.206s | Train Loss: 0.270797 || Validation Loss: 0.001305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 078/120 | Train Time: 1.314s | Train Loss: 0.270504 || Validation Loss: 0.001358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 079/120 | Train Time: 1.270s | Train Loss: 0.265830 || Validation Loss: 0.001281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 080/120 | Train Time: 1.373s | Train Loss: 0.283119 || Validation Loss: 0.001216 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 081/120 | Train Time: 1.248s | Train Loss: 0.267738 || Validation Loss: 0.001328 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 082/120 | Train Time: 1.389s | Train Loss: 0.269776 || Validation Loss: 0.001346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 083/120 | Train Time: 1.316s | Train Loss: 0.284264 || Validation Loss: 0.001180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 084/120 | Train Time: 1.267s | Train Loss: 0.273537 || Validation Loss: 0.001418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 085/120 | Train Time: 1.411s | Train Loss: 0.276508 || Validation Loss: 0.001217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 086/120 | Train Time: 1.282s | Train Loss: 0.273604 || Validation Loss: 0.001302 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 087/120 | Train Time: 1.367s | Train Loss: 0.275578 || Validation Loss: 0.001373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 088/120 | Train Time: 1.378s | Train Loss: 0.272313 || Validation Loss: 0.001241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 089/120 | Train Time: 1.426s | Train Loss: 0.264793 || Validation Loss: 0.001265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 090/120 | Train Time: 1.448s | Train Loss: 0.278410 || Validation Loss: 0.001382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 091/120 | Train Time: 1.318s | Train Loss: 0.276453 || Validation Loss: 0.001296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 092/120 | Train Time: 1.527s | Train Loss: 0.267975 || Validation Loss: 0.001432 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 093/120 | Train Time: 1.372s | Train Loss: 0.265770 || Validation Loss: 0.001318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 094/120 | Train Time: 1.479s | Train Loss: 0.275296 || Validation Loss: 0.001208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 095/120 | Train Time: 1.398s | Train Loss: 0.270467 || Validation Loss: 0.001467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 096/120 | Train Time: 1.462s | Train Loss: 0.274848 || Validation Loss: 0.001293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 097/120 | Train Time: 1.475s | Train Loss: 0.272151 || Validation Loss: 0.001227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 098/120 | Train Time: 1.516s | Train Loss: 0.270709 || Validation Loss: 0.001262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 099/120 | Train Time: 1.481s | Train Loss: 0.277046 || Validation Loss: 0.001245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 100/120 | Train Time: 1.584s | Train Loss: 0.270847 || Validation Loss: 0.001296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 101/120 | Train Time: 1.414s | Train Loss: 0.264315 || Validation Loss: 0.001240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 102/120 | Train Time: 1.582s | Train Loss: 0.272873 || Validation Loss: 0.001346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 103/120 | Train Time: 1.588s | Train Loss: 0.266508 || Validation Loss: 0.001200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 104/120 | Train Time: 1.512s | Train Loss: 0.276457 || Validation Loss: 0.001298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 105/120 | Train Time: 1.562s | Train Loss: 0.274920 || Validation Loss: 0.001367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 106/120 | Train Time: 1.602s | Train Loss: 0.265299 || Validation Loss: 0.001271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 107/120 | Train Time: 1.705s | Train Loss: 0.266049 || Validation Loss: 0.001372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 108/120 | Train Time: 1.544s | Train Loss: 0.270230 || Validation Loss: 0.001336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 109/120 | Train Time: 1.541s | Train Loss: 0.278329 || Validation Loss: 0.001383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 110/120 | Train Time: 1.591s | Train Loss: 0.267713 || Validation Loss: 0.001383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 111/120 | Train Time: 1.570s | Train Loss: 0.266662 || Validation Loss: 0.001271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 112/120 | Train Time: 1.686s | Train Loss: 0.262498 || Validation Loss: 0.001409 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 113/120 | Train Time: 1.627s | Train Loss: 0.271437 || Validation Loss: 0.001325 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 114/120 | Train Time: 1.693s | Train Loss: 0.265565 || Validation Loss: 0.001356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 115/120 | Train Time: 1.668s | Train Loss: 0.269171 || Validation Loss: 0.001312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 116/120 | Train Time: 1.582s | Train Loss: 0.266403 || Validation Loss: 0.001287 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 117/120 | Train Time: 1.516s | Train Loss: 0.271820 || Validation Loss: 0.001397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 118/120 | Train Time: 1.766s | Train Loss: 0.264381 || Validation Loss: 0.001369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 119/120 | Train Time: 1.619s | Train Loss: 0.260834 || Validation Loss: 0.001331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 120/120 | Train Time: 1.811s | Train Loss: 0.271788 || Validation Loss: 0.001358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 136.588s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.902%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001473\n",
      "INFO:root:Test AUC: 85.98%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({1.0: 12313, -1.0: 2955})\n",
      "class weights 0.19354204872936862 0.8064579512706314\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.646403667441954\n",
      "validation AUC 0.6694899051583819\n",
      "validation AUC 0.6984442032565474\n",
      "validation AUC 0.7204439300576857\n",
      "validation AUC 0.7441087517717484\n",
      "validation AUC 0.7621031751944374\n",
      "validation AUC 0.7758551991754865\n",
      "validation AUC 0.7853028564234086\n",
      "validation AUC 0.7929893840903541\n",
      "validation AUC 0.7999614813018954\n",
      "validation AUC 0.8044906597278909\n",
      "validation AUC 0.8069675447840728\n",
      "validation AUC 0.812370592393471\n",
      "validation AUC 0.8173052614673811\n",
      "validation AUC 0.8191111384037992\n",
      "validation AUC 0.820899670352102\n",
      "validation AUC 0.822915046754574\n",
      "validation AUC 0.8253072635776215\n",
      "validation AUC 0.8273104700191123\n",
      "validation AUC 0.8285705638496711\n",
      "validation AUC 0.8302395673049396\n",
      "validation AUC 0.8316718126036735\n",
      "validation AUC 0.8328422095114923\n",
      "validation AUC 0.8342581714343611\n",
      "validation AUC 0.8360278285022082\n",
      "validation AUC 0.8353664946233996\n",
      "validation AUC 0.8348802309812831\n",
      "validation AUC 0.8386637851122265\n",
      "validation AUC 0.8382582891429474\n",
      "validation AUC 0.8391531537706232\n",
      "validation AUC 0.8418203627553005\n",
      "validation AUC 0.842684688071182\n",
      "validation AUC 0.8434499136129661\n",
      "validation AUC 0.8441208711797656\n",
      "validation AUC 0.8454903190541253\n",
      "validation AUC 0.8458852840427387\n",
      "validation AUC 0.8453925522818757\n",
      "validation AUC 0.8449548487529808\n",
      "validation AUC 0.8461097498469043\n",
      "validation AUC 0.8471546343871269\n",
      "validation AUC 0.8481851086718449\n",
      "validation AUC 0.8473954713696745\n",
      "validation AUC 0.8489615090794187\n",
      "validation AUC 0.8492632943121211\n",
      "validation AUC 0.8479987385172931\n",
      "validation AUC 0.8497446702807957\n",
      "validation AUC 0.8503893376454834\n",
      "validation AUC 0.8510966933465701\n",
      "validation AUC 0.8492566319635643\n",
      "validation AUC 0.8493962698936557\n",
      "validation AUC 0.8494530515152798\n",
      "validation AUC 0.849820026125772\n",
      "validation AUC 0.8498627886121943\n",
      "validation AUC 0.850120909387164\n",
      "validation AUC 0.8501615140602161\n",
      "validation AUC 0.8501449938836235\n",
      "validation AUC 0.8501808678634913\n",
      "validation AUC 0.8503649817594261\n",
      "validation AUC 0.8508328920148241\n",
      "validation AUC 0.850803302566622\n",
      "validation AUC 0.850626867399894\n",
      "validation AUC 0.8509097910555522\n",
      "validation AUC 0.8508771923756339\n",
      "validation AUC 0.8513336696790557\n",
      "validation AUC 0.8515768853116074\n",
      "validation AUC 0.8517475707438182\n",
      "validation AUC 0.8520097597020716\n",
      "validation AUC 0.8521438474488207\n",
      "validation AUC 0.8521274203961097\n",
      "validation AUC 0.8524047965291079\n",
      "validation AUC 0.8525441843550603\n",
      "validation AUC 0.8525900438757159\n",
      "validation AUC 0.8523782455801278\n",
      "validation AUC 0.8523461071982525\n",
      "validation AUC 0.852437352638109\n",
      "validation AUC 0.8521095406108458\n",
      "validation AUC 0.8521652020852086\n",
      "validation AUC 0.8522083530312514\n",
      "validation AUC 0.852341671840807\n",
      "validation AUC 0.8526182178409181\n",
      "validation AUC 0.8526461656481221\n",
      "validation AUC 0.8526694838680702\n",
      "validation AUC 0.8527491872679752\n",
      "validation AUC 0.8527456272750163\n",
      "validation AUC 0.8527268827679951\n",
      "validation AUC 0.8528818967812768\n",
      "validation AUC 0.8529714872767235\n",
      "validation AUC 0.8530271700365449\n",
      "validation AUC 0.8530154178026893\n",
      "validation AUC 0.8529832740994495\n",
      "validation AUC 0.8528394243092283\n",
      "validation AUC 0.8529050393961911\n",
      "validation AUC 0.8528307425027827\n",
      "validation AUC 0.8531601828676324\n",
      "validation AUC 0.8532183746508918\n",
      "validation AUC 0.8531709027567436\n",
      "validation AUC 0.8532432972622856\n",
      "validation AUC 0.8534255540019536\n",
      "validation AUC 0.853366814118134\n",
      "validation AUC 0.8532803499244261\n",
      "validation AUC 0.8532581039594572\n",
      "validation AUC 0.8533330952909541\n",
      "validation AUC 0.8533101096562973\n",
      "validation AUC 0.8530942671235662\n",
      "validation AUC 0.852977119941218\n",
      "validation AUC 0.8533076937567409\n",
      "validation AUC 0.8533464944871726\n",
      "validation AUC 0.8531254849093548\n",
      "validation AUC 0.852855630525306\n",
      "validation AUC 0.8528934760707809\n",
      "validation AUC 0.8528620028594884\n",
      "validation AUC 0.8527336861327152\n",
      "validation AUC 0.8531244286184694\n",
      "validation AUC 0.8532428848565243\n",
      "validation AUC 0.8532609934604687\n",
      "validation AUC 0.8533496952880167\n",
      "validation AUC 0.853330376073612\n",
      "validation AUC 0.8536828845542708\n",
      "validation AUC 0.8536683838355674\n",
      "validation AUC 0.8537379553571564\n",
      "validation AUC 0.8537379553571564\n",
      "train auc 0.9020594703114967\n",
      "Test AUC 0.859775293177156\n",
      "test false positive rates [0.         0.         0.         ... 0.99791568 0.99791568 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 3.63941769e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.84604359e+00 5.84604359e+00 3.99730539e+00 ... 4.84292768e-03\n",
      " 4.84118052e-03 3.19200521e-03]\n",
      "Itration 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.235s | Train Loss: 0.201084 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.215s | Train Loss: 0.171229 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.211s | Train Loss: 0.146026 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.210s | Train Loss: 0.126182 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.214s | Train Loss: 0.110271 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.215s | Train Loss: 0.097306 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.210s | Train Loss: 0.087068 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.214s | Train Loss: 0.078796 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.211s | Train Loss: 0.072045 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.210s | Train Loss: 0.066415 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.212s | Train Loss: 0.061720 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.214s | Train Loss: 0.057715 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.209s | Train Loss: 0.054558 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.209s | Train Loss: 0.051400 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.209s | Train Loss: 0.049227 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.208s | Train Loss: 0.046634 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.211s | Train Loss: 0.044664 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.210s | Train Loss: 0.043509 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.210s | Train Loss: 0.041983 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.207s | Train Loss: 0.040469 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.207s | Train Loss: 0.039345 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.211s | Train Loss: 0.037832 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.212s | Train Loss: 0.036696 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.210s | Train Loss: 0.036088 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.210s | Train Loss: 0.035022 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.212s | Train Loss: 0.033963 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.212s | Train Loss: 0.033234 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.203s | Train Loss: 0.032606 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.207s | Train Loss: 0.031896 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.209s | Train Loss: 0.031441 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.212s | Train Loss: 0.030159 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.211s | Train Loss: 0.030224 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.207s | Train Loss: 0.029545 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.209s | Train Loss: 0.029096 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.208s | Train Loss: 0.028460 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.208s | Train Loss: 0.028243 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.210s | Train Loss: 0.027628 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.210s | Train Loss: 0.027545 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.205s | Train Loss: 0.027390 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.208s | Train Loss: 0.026989 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.213s | Train Loss: 0.026284 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.213s | Train Loss: 0.026044 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.210s | Train Loss: 0.025861 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.204s | Train Loss: 0.025631 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.203s | Train Loss: 0.025333 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.212s | Train Loss: 0.024880 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.211s | Train Loss: 0.024952 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.211s | Train Loss: 0.024620 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.213s | Train Loss: 0.024393 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.211s | Train Loss: 0.023817 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.211s | Train Loss: 0.023852 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.210s | Train Loss: 0.023662 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.209s | Train Loss: 0.023600 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.210s | Train Loss: 0.023213 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.211s | Train Loss: 0.023070 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.213s | Train Loss: 0.023321 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.209s | Train Loss: 0.022623 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.213s | Train Loss: 0.023055 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.215s | Train Loss: 0.022643 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.213s | Train Loss: 0.022508 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.210s | Train Loss: 0.022343 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.210s | Train Loss: 0.022354 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.208s | Train Loss: 0.021947 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.209s | Train Loss: 0.021871 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.211s | Train Loss: 0.021973 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.212s | Train Loss: 0.021905 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.210s | Train Loss: 0.021794 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.213s | Train Loss: 0.021814 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.212s | Train Loss: 0.021518 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.215s | Train Loss: 0.021742 |\n",
      "INFO:root:Pretraining Time: 14.760s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.020769\n",
      "INFO:root:Test AUC: 63.59%\n",
      "INFO:root:Test AUC: 63.59%\n",
      "INFO:root:Test Time: 0.218s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 120\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/120 | Train Time: 0.561s | Train Loss: 1.005516 || Validation Loss: 0.005068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/120 | Train Time: 0.559s | Train Loss: 0.672296 || Validation Loss: 0.003608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/120 | Train Time: 0.572s | Train Loss: 0.523004 || Validation Loss: 0.002552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/120 | Train Time: 0.649s | Train Loss: 0.471198 || Validation Loss: 0.002444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/120 | Train Time: 0.622s | Train Loss: 0.435355 || Validation Loss: 0.001944 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/120 | Train Time: 0.570s | Train Loss: 0.424300 || Validation Loss: 0.001883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/120 | Train Time: 0.640s | Train Loss: 0.393769 || Validation Loss: 0.001544 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/120 | Train Time: 0.608s | Train Loss: 0.381602 || Validation Loss: 0.001778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/120 | Train Time: 0.616s | Train Loss: 0.371170 || Validation Loss: 0.001573 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/120 | Train Time: 0.625s | Train Loss: 0.355718 || Validation Loss: 0.001563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/120 | Train Time: 0.632s | Train Loss: 0.350202 || Validation Loss: 0.001624 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/120 | Train Time: 0.670s | Train Loss: 0.331897 || Validation Loss: 0.001406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/120 | Train Time: 0.706s | Train Loss: 0.352424 || Validation Loss: 0.001534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/120 | Train Time: 0.672s | Train Loss: 0.337852 || Validation Loss: 0.001468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/120 | Train Time: 0.704s | Train Loss: 0.324664 || Validation Loss: 0.001509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/120 | Train Time: 0.709s | Train Loss: 0.317949 || Validation Loss: 0.001387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/120 | Train Time: 0.747s | Train Loss: 0.315351 || Validation Loss: 0.001337 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/120 | Train Time: 0.732s | Train Loss: 0.308840 || Validation Loss: 0.001488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/120 | Train Time: 0.746s | Train Loss: 0.307089 || Validation Loss: 0.001328 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/120 | Train Time: 0.750s | Train Loss: 0.301345 || Validation Loss: 0.001322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/120 | Train Time: 0.708s | Train Loss: 0.306119 || Validation Loss: 0.001260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/120 | Train Time: 0.756s | Train Loss: 0.291403 || Validation Loss: 0.001342 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/120 | Train Time: 0.776s | Train Loss: 0.293512 || Validation Loss: 0.001231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/120 | Train Time: 0.777s | Train Loss: 0.296658 || Validation Loss: 0.001260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/120 | Train Time: 0.760s | Train Loss: 0.292035 || Validation Loss: 0.001415 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/120 | Train Time: 0.828s | Train Loss: 0.295594 || Validation Loss: 0.001233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/120 | Train Time: 0.764s | Train Loss: 0.309427 || Validation Loss: 0.001340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/120 | Train Time: 0.846s | Train Loss: 0.294104 || Validation Loss: 0.001473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/120 | Train Time: 0.819s | Train Loss: 0.288233 || Validation Loss: 0.001316 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/120 | Train Time: 0.828s | Train Loss: 0.278331 || Validation Loss: 0.001311 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/120 | Train Time: 0.882s | Train Loss: 0.280585 || Validation Loss: 0.001234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/120 | Train Time: 0.884s | Train Loss: 0.275157 || Validation Loss: 0.001200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/120 | Train Time: 0.856s | Train Loss: 0.287278 || Validation Loss: 0.001260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/120 | Train Time: 0.859s | Train Loss: 0.279054 || Validation Loss: 0.001322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/120 | Train Time: 0.879s | Train Loss: 0.280779 || Validation Loss: 0.001239 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/120 | Train Time: 0.990s | Train Loss: 0.274381 || Validation Loss: 0.001244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/120 | Train Time: 0.895s | Train Loss: 0.282391 || Validation Loss: 0.001322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/120 | Train Time: 0.977s | Train Loss: 0.261309 || Validation Loss: 0.001214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/120 | Train Time: 0.882s | Train Loss: 0.270399 || Validation Loss: 0.001357 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/120 | Train Time: 0.936s | Train Loss: 0.284527 || Validation Loss: 0.001304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/120 | Train Time: 0.973s | Train Loss: 0.267743 || Validation Loss: 0.001353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/120 | Train Time: 0.951s | Train Loss: 0.280481 || Validation Loss: 0.001348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/120 | Train Time: 0.948s | Train Loss: 0.265069 || Validation Loss: 0.001134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/120 | Train Time: 0.954s | Train Loss: 0.264821 || Validation Loss: 0.001152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/120 | Train Time: 1.037s | Train Loss: 0.257139 || Validation Loss: 0.001233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/120 | Train Time: 0.972s | Train Loss: 0.254323 || Validation Loss: 0.001224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/120 | Train Time: 1.022s | Train Loss: 0.260910 || Validation Loss: 0.001207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/120 | Train Time: 1.050s | Train Loss: 0.254784 || Validation Loss: 0.001291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/120 | Train Time: 1.005s | Train Loss: 0.253483 || Validation Loss: 0.001221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/120 | Train Time: 1.036s | Train Loss: 0.257282 || Validation Loss: 0.001171 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/120 | Train Time: 1.081s | Train Loss: 0.261694 || Validation Loss: 0.001203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/120 | Train Time: 1.025s | Train Loss: 0.269170 || Validation Loss: 0.001234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/120 | Train Time: 1.123s | Train Loss: 0.257742 || Validation Loss: 0.001222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/120 | Train Time: 1.039s | Train Loss: 0.270613 || Validation Loss: 0.001277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/120 | Train Time: 1.126s | Train Loss: 0.252210 || Validation Loss: 0.001217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/120 | Train Time: 1.115s | Train Loss: 0.257817 || Validation Loss: 0.001256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/120 | Train Time: 1.140s | Train Loss: 0.251047 || Validation Loss: 0.001192 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/120 | Train Time: 1.154s | Train Loss: 0.262393 || Validation Loss: 0.001310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/120 | Train Time: 1.071s | Train Loss: 0.273716 || Validation Loss: 0.001246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/120 | Train Time: 1.158s | Train Loss: 0.261538 || Validation Loss: 0.001208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/120 | Train Time: 1.171s | Train Loss: 0.258277 || Validation Loss: 0.001291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/120 | Train Time: 1.138s | Train Loss: 0.259176 || Validation Loss: 0.001226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/120 | Train Time: 1.162s | Train Loss: 0.258585 || Validation Loss: 0.001275 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/120 | Train Time: 1.107s | Train Loss: 0.257504 || Validation Loss: 0.001336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/120 | Train Time: 1.168s | Train Loss: 0.259099 || Validation Loss: 0.001258 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/120 | Train Time: 1.169s | Train Loss: 0.256549 || Validation Loss: 0.001190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/120 | Train Time: 1.249s | Train Loss: 0.266209 || Validation Loss: 0.001245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/120 | Train Time: 1.181s | Train Loss: 0.264669 || Validation Loss: 0.001408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/120 | Train Time: 1.244s | Train Loss: 0.261584 || Validation Loss: 0.001341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/120 | Train Time: 1.210s | Train Loss: 0.253783 || Validation Loss: 0.001247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 071/120 | Train Time: 1.242s | Train Loss: 0.277228 || Validation Loss: 0.001361 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 072/120 | Train Time: 1.309s | Train Loss: 0.244782 || Validation Loss: 0.001294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 073/120 | Train Time: 1.261s | Train Loss: 0.255224 || Validation Loss: 0.001205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 074/120 | Train Time: 1.232s | Train Loss: 0.245463 || Validation Loss: 0.001213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 075/120 | Train Time: 1.335s | Train Loss: 0.265179 || Validation Loss: 0.001245 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 076/120 | Train Time: 1.286s | Train Loss: 0.269283 || Validation Loss: 0.001257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 077/120 | Train Time: 1.220s | Train Loss: 0.251537 || Validation Loss: 0.001225 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 078/120 | Train Time: 1.304s | Train Loss: 0.254277 || Validation Loss: 0.001252 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 079/120 | Train Time: 1.277s | Train Loss: 0.252349 || Validation Loss: 0.001207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 080/120 | Train Time: 1.380s | Train Loss: 0.252757 || Validation Loss: 0.001273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 081/120 | Train Time: 1.244s | Train Loss: 0.246363 || Validation Loss: 0.001148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 082/120 | Train Time: 1.396s | Train Loss: 0.257717 || Validation Loss: 0.001283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 083/120 | Train Time: 1.320s | Train Loss: 0.255831 || Validation Loss: 0.001201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 084/120 | Train Time: 1.276s | Train Loss: 0.252798 || Validation Loss: 0.001104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 085/120 | Train Time: 1.419s | Train Loss: 0.263735 || Validation Loss: 0.001336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 086/120 | Train Time: 1.276s | Train Loss: 0.259336 || Validation Loss: 0.001145 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 087/120 | Train Time: 1.374s | Train Loss: 0.247567 || Validation Loss: 0.001242 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 088/120 | Train Time: 1.380s | Train Loss: 0.262782 || Validation Loss: 0.001229 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 089/120 | Train Time: 1.425s | Train Loss: 0.270070 || Validation Loss: 0.001303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 090/120 | Train Time: 1.453s | Train Loss: 0.256634 || Validation Loss: 0.001102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 091/120 | Train Time: 1.493s | Train Loss: 0.258650 || Validation Loss: 0.001241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 092/120 | Train Time: 1.508s | Train Loss: 0.259047 || Validation Loss: 0.001196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 093/120 | Train Time: 1.374s | Train Loss: 0.261519 || Validation Loss: 0.001265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 094/120 | Train Time: 1.478s | Train Loss: 0.254210 || Validation Loss: 0.001299 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 095/120 | Train Time: 1.403s | Train Loss: 0.250664 || Validation Loss: 0.001203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 096/120 | Train Time: 1.447s | Train Loss: 0.258223 || Validation Loss: 0.001261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 097/120 | Train Time: 1.440s | Train Loss: 0.259845 || Validation Loss: 0.001141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 098/120 | Train Time: 1.509s | Train Loss: 0.253472 || Validation Loss: 0.001262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 099/120 | Train Time: 1.465s | Train Loss: 0.248389 || Validation Loss: 0.001226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 100/120 | Train Time: 1.577s | Train Loss: 0.267855 || Validation Loss: 0.001273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 101/120 | Train Time: 1.393s | Train Loss: 0.265875 || Validation Loss: 0.001357 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 102/120 | Train Time: 1.530s | Train Loss: 0.255181 || Validation Loss: 0.001336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 103/120 | Train Time: 1.550s | Train Loss: 0.258636 || Validation Loss: 0.001333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 104/120 | Train Time: 1.481s | Train Loss: 0.262440 || Validation Loss: 0.001296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 105/120 | Train Time: 1.518s | Train Loss: 0.248745 || Validation Loss: 0.001269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 106/120 | Train Time: 1.566s | Train Loss: 0.254395 || Validation Loss: 0.001218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 107/120 | Train Time: 1.638s | Train Loss: 0.250457 || Validation Loss: 0.001205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 108/120 | Train Time: 1.516s | Train Loss: 0.264965 || Validation Loss: 0.001220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 109/120 | Train Time: 1.517s | Train Loss: 0.260451 || Validation Loss: 0.001216 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 110/120 | Train Time: 1.564s | Train Loss: 0.254553 || Validation Loss: 0.001219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 111/120 | Train Time: 1.523s | Train Loss: 0.272255 || Validation Loss: 0.001162 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 112/120 | Train Time: 1.625s | Train Loss: 0.258257 || Validation Loss: 0.001293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 113/120 | Train Time: 1.627s | Train Loss: 0.253322 || Validation Loss: 0.001346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 114/120 | Train Time: 1.672s | Train Loss: 0.253102 || Validation Loss: 0.001260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 115/120 | Train Time: 1.666s | Train Loss: 0.252898 || Validation Loss: 0.001176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 116/120 | Train Time: 1.578s | Train Loss: 0.261051 || Validation Loss: 0.001208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 117/120 | Train Time: 1.518s | Train Loss: 0.256027 || Validation Loss: 0.001473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 118/120 | Train Time: 1.752s | Train Loss: 0.253004 || Validation Loss: 0.001382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 119/120 | Train Time: 1.620s | Train Loss: 0.256780 || Validation Loss: 0.001290 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 120/120 | Train Time: 1.802s | Train Loss: 0.251146 || Validation Loss: 0.001246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 137.002s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.912%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001393\n",
      "INFO:root:Test AUC: 87.26%\n",
      "INFO:root:Test Time: 0.203s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({1.0: 12313, -1.0: 2955})\n",
      "class weights 0.19354204872936862 0.8064579512706314\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5224620709090998\n",
      "validation AUC 0.5713594202947908\n",
      "validation AUC 0.6315277818870538\n",
      "validation AUC 0.6833171147007382\n",
      "validation AUC 0.7117984711932182\n",
      "validation AUC 0.7387237755167417\n",
      "validation AUC 0.757326093013623\n",
      "validation AUC 0.7703068384005851\n",
      "validation AUC 0.7837769284998028\n",
      "validation AUC 0.7906962377738986\n",
      "validation AUC 0.799953831840194\n",
      "validation AUC 0.8069363003914608\n",
      "validation AUC 0.8154959761968974\n",
      "validation AUC 0.8186220837059176\n",
      "validation AUC 0.8224236533169237\n",
      "validation AUC 0.8261022461905946\n",
      "validation AUC 0.830173454670381\n",
      "validation AUC 0.8334085410669934\n",
      "validation AUC 0.8351478849810444\n",
      "validation AUC 0.839101203948112\n",
      "validation AUC 0.840231030771749\n",
      "validation AUC 0.8429398155785293\n",
      "validation AUC 0.8445888426308572\n",
      "validation AUC 0.8471043102415197\n",
      "validation AUC 0.8477827948786336\n",
      "validation AUC 0.8498120760069672\n",
      "validation AUC 0.8499814523834712\n",
      "validation AUC 0.8511089777168919\n",
      "validation AUC 0.8533461352950581\n",
      "validation AUC 0.8534382374746251\n",
      "validation AUC 0.8545115833337414\n",
      "validation AUC 0.8542130574368946\n",
      "validation AUC 0.8556602291635049\n",
      "validation AUC 0.856504498255976\n",
      "validation AUC 0.8571570572044573\n",
      "validation AUC 0.8567496774720879\n",
      "validation AUC 0.8576545036944107\n",
      "validation AUC 0.8586628703696411\n",
      "validation AUC 0.856942869616136\n",
      "validation AUC 0.8596192712582664\n",
      "validation AUC 0.8604759072022117\n",
      "validation AUC 0.860189218681055\n",
      "validation AUC 0.8603972334863685\n",
      "validation AUC 0.8610642931535536\n",
      "validation AUC 0.8612502136527912\n",
      "validation AUC 0.8614714147997369\n",
      "validation AUC 0.8621379822406904\n",
      "validation AUC 0.8622174515005503\n",
      "validation AUC 0.8622086685881759\n",
      "validation AUC 0.862620233573852\n",
      "validation AUC 0.862635793244123\n",
      "validation AUC 0.862693295910659\n",
      "validation AUC 0.8628651094721781\n",
      "validation AUC 0.8627861271171847\n",
      "validation AUC 0.8629344681391806\n",
      "validation AUC 0.8631180259525082\n",
      "validation AUC 0.8631137608787318\n",
      "validation AUC 0.8633743348028106\n",
      "validation AUC 0.863253776625714\n",
      "validation AUC 0.8632164951448933\n",
      "validation AUC 0.8632687642492843\n",
      "validation AUC 0.8630324690514752\n",
      "validation AUC 0.8628046614303019\n",
      "validation AUC 0.8628356051658105\n",
      "validation AUC 0.8625480439408494\n",
      "validation AUC 0.862641250303584\n",
      "validation AUC 0.8627245908562353\n",
      "validation AUC 0.8629087313589935\n",
      "validation AUC 0.8632205686495419\n",
      "validation AUC 0.8633552311036744\n",
      "validation AUC 0.863135277816742\n",
      "validation AUC 0.8631573322125831\n",
      "validation AUC 0.8632156277624534\n",
      "validation AUC 0.8631847399012738\n",
      "validation AUC 0.8635913799639552\n",
      "validation AUC 0.8634237223882371\n",
      "validation AUC 0.8634774122969927\n",
      "validation AUC 0.8634327554047505\n",
      "validation AUC 0.8634533490859918\n",
      "validation AUC 0.863647773125959\n",
      "validation AUC 0.863526725383314\n",
      "validation AUC 0.8636438965118027\n",
      "validation AUC 0.8635591644222924\n",
      "validation AUC 0.8634015855112437\n",
      "validation AUC 0.8632978455071612\n",
      "validation AUC 0.863527401196626\n",
      "validation AUC 0.8634087799962666\n",
      "validation AUC 0.8635249187800111\n",
      "validation AUC 0.8634058851738904\n",
      "validation AUC 0.8634735516469305\n",
      "validation AUC 0.8634976361433899\n",
      "validation AUC 0.8635533428493523\n",
      "validation AUC 0.8633323731817695\n",
      "validation AUC 0.8631863762209073\n",
      "validation AUC 0.8631389868079113\n",
      "validation AUC 0.8631552781658234\n",
      "validation AUC 0.8633297497489912\n",
      "validation AUC 0.8634976787143072\n",
      "validation AUC 0.8636218634014207\n",
      "validation AUC 0.8636953195192124\n",
      "validation AUC 0.863743805133329\n",
      "validation AUC 0.8637411737185036\n",
      "validation AUC 0.8639312156146718\n",
      "validation AUC 0.8637296902135634\n",
      "validation AUC 0.8636301966584811\n",
      "validation AUC 0.8638695090700532\n",
      "validation AUC 0.8639278099412881\n",
      "validation AUC 0.863991559889937\n",
      "validation AUC 0.8638035347909757\n",
      "validation AUC 0.863802547677831\n",
      "validation AUC 0.8639279908676869\n",
      "validation AUC 0.864015702921408\n",
      "validation AUC 0.8641304528289758\n",
      "validation AUC 0.8640177223792971\n",
      "validation AUC 0.864084359168275\n",
      "validation AUC 0.8640278356328369\n",
      "validation AUC 0.8638607234969964\n",
      "validation AUC 0.8639116569388574\n",
      "validation AUC 0.8636299838038944\n",
      "validation AUC 0.8637325930179865\n",
      "validation AUC 0.8637325930179865\n",
      "train auc 0.9117715570668874\n",
      "Test AUC 0.8726142379038396\n",
      "test false positive rates [0.         0.         0.         ... 0.99814306 0.99814306 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 3.91937290e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.41017008e+00 4.41017008e+00 3.30541086e+00 ... 2.63633253e-03\n",
      " 2.63586314e-03 1.51359779e-03]\n",
      "Itration 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.231s | Train Loss: 0.206178 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.214s | Train Loss: 0.179659 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.213s | Train Loss: 0.156338 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.210s | Train Loss: 0.136337 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.211s | Train Loss: 0.119192 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.209s | Train Loss: 0.105228 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.212s | Train Loss: 0.093982 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.211s | Train Loss: 0.084261 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.208s | Train Loss: 0.076880 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.211s | Train Loss: 0.070581 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.211s | Train Loss: 0.065292 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.211s | Train Loss: 0.060970 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.211s | Train Loss: 0.057255 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.209s | Train Loss: 0.054058 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.210s | Train Loss: 0.050768 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.193s | Train Loss: 0.047946 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.209s | Train Loss: 0.045959 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.214s | Train Loss: 0.044308 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.211s | Train Loss: 0.042214 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.212s | Train Loss: 0.040622 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.212s | Train Loss: 0.039299 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.212s | Train Loss: 0.037967 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.212s | Train Loss: 0.036643 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.209s | Train Loss: 0.035225 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.210s | Train Loss: 0.034753 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.211s | Train Loss: 0.033459 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.211s | Train Loss: 0.032606 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.211s | Train Loss: 0.032031 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.212s | Train Loss: 0.031211 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.215s | Train Loss: 0.030758 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.212s | Train Loss: 0.030264 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.210s | Train Loss: 0.029444 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.216s | Train Loss: 0.028814 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.208s | Train Loss: 0.028355 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.212s | Train Loss: 0.027431 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.212s | Train Loss: 0.027232 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.211s | Train Loss: 0.027030 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.210s | Train Loss: 0.026643 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.208s | Train Loss: 0.026022 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.212s | Train Loss: 0.025729 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.212s | Train Loss: 0.025497 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.211s | Train Loss: 0.025370 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.212s | Train Loss: 0.024482 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.209s | Train Loss: 0.024558 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.211s | Train Loss: 0.024530 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.211s | Train Loss: 0.023763 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.211s | Train Loss: 0.023850 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.212s | Train Loss: 0.023530 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.210s | Train Loss: 0.023365 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.210s | Train Loss: 0.023186 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.212s | Train Loss: 0.022589 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.210s | Train Loss: 0.022635 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.207s | Train Loss: 0.022765 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.213s | Train Loss: 0.022688 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.212s | Train Loss: 0.021939 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.210s | Train Loss: 0.022130 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.213s | Train Loss: 0.021990 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.212s | Train Loss: 0.022089 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.212s | Train Loss: 0.021657 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.211s | Train Loss: 0.021493 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.209s | Train Loss: 0.021245 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.216s | Train Loss: 0.021154 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.224s | Train Loss: 0.021008 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.241s | Train Loss: 0.021142 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.232s | Train Loss: 0.020616 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.238s | Train Loss: 0.020580 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.254s | Train Loss: 0.020762 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.252s | Train Loss: 0.020456 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.238s | Train Loss: 0.020451 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.223s | Train Loss: 0.020368 |\n",
      "INFO:root:Pretraining Time: 15.001s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.019971\n",
      "INFO:root:Test AUC: 62.86%\n",
      "INFO:root:Test AUC: 62.86%\n",
      "INFO:root:Test Time: 0.228s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 120\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/120 | Train Time: 0.615s | Train Loss: 1.154562 || Validation Loss: 0.005798 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/120 | Train Time: 0.638s | Train Loss: 0.760226 || Validation Loss: 0.004239 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/120 | Train Time: 0.700s | Train Loss: 0.573672 || Validation Loss: 0.002913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/120 | Train Time: 0.688s | Train Loss: 0.493133 || Validation Loss: 0.002288 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/120 | Train Time: 0.656s | Train Loss: 0.441869 || Validation Loss: 0.002240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/120 | Train Time: 0.638s | Train Loss: 0.414773 || Validation Loss: 0.002059 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/120 | Train Time: 0.698s | Train Loss: 0.394781 || Validation Loss: 0.001774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/120 | Train Time: 0.638s | Train Loss: 0.372872 || Validation Loss: 0.001804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/120 | Train Time: 0.634s | Train Loss: 0.356034 || Validation Loss: 0.001632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/120 | Train Time: 0.667s | Train Loss: 0.352192 || Validation Loss: 0.001615 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/120 | Train Time: 0.696s | Train Loss: 0.348952 || Validation Loss: 0.001634 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/120 | Train Time: 0.694s | Train Loss: 0.335106 || Validation Loss: 0.001497 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/120 | Train Time: 0.719s | Train Loss: 0.324093 || Validation Loss: 0.001426 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/120 | Train Time: 0.651s | Train Loss: 0.322071 || Validation Loss: 0.001404 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/120 | Train Time: 0.668s | Train Loss: 0.329841 || Validation Loss: 0.001749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/120 | Train Time: 0.685s | Train Loss: 0.325326 || Validation Loss: 0.001458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/120 | Train Time: 0.716s | Train Loss: 0.321775 || Validation Loss: 0.001477 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/120 | Train Time: 0.689s | Train Loss: 0.316591 || Validation Loss: 0.001516 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/120 | Train Time: 0.692s | Train Loss: 0.316232 || Validation Loss: 0.001680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/120 | Train Time: 0.746s | Train Loss: 0.310422 || Validation Loss: 0.001599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/120 | Train Time: 0.714s | Train Loss: 0.302317 || Validation Loss: 0.001594 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/120 | Train Time: 0.761s | Train Loss: 0.301013 || Validation Loss: 0.001476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/120 | Train Time: 0.758s | Train Loss: 0.301371 || Validation Loss: 0.001453 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/120 | Train Time: 0.769s | Train Loss: 0.286276 || Validation Loss: 0.001410 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/120 | Train Time: 0.747s | Train Loss: 0.286782 || Validation Loss: 0.001439 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/120 | Train Time: 0.820s | Train Loss: 0.284235 || Validation Loss: 0.001329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/120 | Train Time: 0.754s | Train Loss: 0.280921 || Validation Loss: 0.001374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/120 | Train Time: 0.822s | Train Loss: 0.280963 || Validation Loss: 0.001285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/120 | Train Time: 0.794s | Train Loss: 0.291906 || Validation Loss: 0.001478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/120 | Train Time: 0.812s | Train Loss: 0.280368 || Validation Loss: 0.001294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/120 | Train Time: 0.847s | Train Loss: 0.286974 || Validation Loss: 0.001480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/120 | Train Time: 0.881s | Train Loss: 0.274781 || Validation Loss: 0.001332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/120 | Train Time: 0.843s | Train Loss: 0.283158 || Validation Loss: 0.001234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/120 | Train Time: 0.838s | Train Loss: 0.274164 || Validation Loss: 0.001367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/120 | Train Time: 0.877s | Train Loss: 0.273456 || Validation Loss: 0.001483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/120 | Train Time: 0.977s | Train Loss: 0.282516 || Validation Loss: 0.001281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/120 | Train Time: 0.852s | Train Loss: 0.281422 || Validation Loss: 0.001510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/120 | Train Time: 0.948s | Train Loss: 0.274362 || Validation Loss: 0.001361 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/120 | Train Time: 0.876s | Train Loss: 0.267605 || Validation Loss: 0.001249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/120 | Train Time: 0.913s | Train Loss: 0.274142 || Validation Loss: 0.001293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/120 | Train Time: 0.959s | Train Loss: 0.259764 || Validation Loss: 0.001289 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/120 | Train Time: 0.936s | Train Loss: 0.258028 || Validation Loss: 0.001281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/120 | Train Time: 0.948s | Train Loss: 0.271710 || Validation Loss: 0.001455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/120 | Train Time: 0.950s | Train Loss: 0.271168 || Validation Loss: 0.001381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/120 | Train Time: 1.048s | Train Loss: 0.272959 || Validation Loss: 0.001568 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/120 | Train Time: 0.982s | Train Loss: 0.263660 || Validation Loss: 0.001295 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/120 | Train Time: 1.018s | Train Loss: 0.260605 || Validation Loss: 0.001305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/120 | Train Time: 1.047s | Train Loss: 0.254212 || Validation Loss: 0.001275 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/120 | Train Time: 0.938s | Train Loss: 0.263538 || Validation Loss: 0.001423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/120 | Train Time: 1.061s | Train Loss: 0.261718 || Validation Loss: 0.001339 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/120 | Train Time: 1.077s | Train Loss: 0.260349 || Validation Loss: 0.001349 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/120 | Train Time: 1.019s | Train Loss: 0.256361 || Validation Loss: 0.001542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/120 | Train Time: 1.123s | Train Loss: 0.259129 || Validation Loss: 0.001386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/120 | Train Time: 1.027s | Train Loss: 0.253460 || Validation Loss: 0.001418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/120 | Train Time: 1.113s | Train Loss: 0.242304 || Validation Loss: 0.001455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/120 | Train Time: 1.066s | Train Loss: 0.254624 || Validation Loss: 0.001441 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/120 | Train Time: 1.152s | Train Loss: 0.243664 || Validation Loss: 0.001344 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/120 | Train Time: 1.173s | Train Loss: 0.252649 || Validation Loss: 0.001222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/120 | Train Time: 1.084s | Train Loss: 0.254855 || Validation Loss: 0.001194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/120 | Train Time: 1.176s | Train Loss: 0.254732 || Validation Loss: 0.001256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/120 | Train Time: 1.163s | Train Loss: 0.250933 || Validation Loss: 0.001360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/120 | Train Time: 1.130s | Train Loss: 0.252307 || Validation Loss: 0.001439 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/120 | Train Time: 1.146s | Train Loss: 0.252845 || Validation Loss: 0.001263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/120 | Train Time: 1.110s | Train Loss: 0.256615 || Validation Loss: 0.001289 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/120 | Train Time: 1.169s | Train Loss: 0.245576 || Validation Loss: 0.001314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/120 | Train Time: 1.152s | Train Loss: 0.248248 || Validation Loss: 0.001265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/120 | Train Time: 1.256s | Train Loss: 0.253894 || Validation Loss: 0.001223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/120 | Train Time: 1.190s | Train Loss: 0.249803 || Validation Loss: 0.001379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/120 | Train Time: 1.233s | Train Loss: 0.260921 || Validation Loss: 0.001336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/120 | Train Time: 1.192s | Train Loss: 0.252876 || Validation Loss: 0.001273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 071/120 | Train Time: 1.235s | Train Loss: 0.254158 || Validation Loss: 0.001353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 072/120 | Train Time: 1.286s | Train Loss: 0.256367 || Validation Loss: 0.001305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 073/120 | Train Time: 1.269s | Train Loss: 0.257583 || Validation Loss: 0.001426 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 074/120 | Train Time: 1.223s | Train Loss: 0.257629 || Validation Loss: 0.001246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 075/120 | Train Time: 1.321s | Train Loss: 0.264006 || Validation Loss: 0.001377 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 076/120 | Train Time: 1.258s | Train Loss: 0.249871 || Validation Loss: 0.001205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 077/120 | Train Time: 1.205s | Train Loss: 0.257494 || Validation Loss: 0.001266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 078/120 | Train Time: 1.312s | Train Loss: 0.256757 || Validation Loss: 0.001226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 079/120 | Train Time: 1.274s | Train Loss: 0.251261 || Validation Loss: 0.001358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 080/120 | Train Time: 1.392s | Train Loss: 0.242740 || Validation Loss: 0.001354 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 081/120 | Train Time: 1.267s | Train Loss: 0.241198 || Validation Loss: 0.001188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 082/120 | Train Time: 1.438s | Train Loss: 0.255317 || Validation Loss: 0.001248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 083/120 | Train Time: 1.327s | Train Loss: 0.266509 || Validation Loss: 0.001269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 084/120 | Train Time: 1.282s | Train Loss: 0.252558 || Validation Loss: 0.001272 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 085/120 | Train Time: 1.427s | Train Loss: 0.253184 || Validation Loss: 0.001356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 086/120 | Train Time: 1.271s | Train Loss: 0.245416 || Validation Loss: 0.001530 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 087/120 | Train Time: 1.390s | Train Loss: 0.246088 || Validation Loss: 0.001304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 088/120 | Train Time: 1.456s | Train Loss: 0.250336 || Validation Loss: 0.001384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 089/120 | Train Time: 1.445s | Train Loss: 0.255119 || Validation Loss: 0.001209 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 090/120 | Train Time: 1.446s | Train Loss: 0.256875 || Validation Loss: 0.001137 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 091/120 | Train Time: 1.331s | Train Loss: 0.260186 || Validation Loss: 0.001221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 092/120 | Train Time: 1.506s | Train Loss: 0.260794 || Validation Loss: 0.001108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 093/120 | Train Time: 1.401s | Train Loss: 0.250208 || Validation Loss: 0.001347 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 094/120 | Train Time: 1.531s | Train Loss: 0.241985 || Validation Loss: 0.001296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 095/120 | Train Time: 1.423s | Train Loss: 0.258407 || Validation Loss: 0.001423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 096/120 | Train Time: 1.468s | Train Loss: 0.252193 || Validation Loss: 0.001408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 097/120 | Train Time: 1.440s | Train Loss: 0.255280 || Validation Loss: 0.001268 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 098/120 | Train Time: 1.505s | Train Loss: 0.244669 || Validation Loss: 0.001254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 099/120 | Train Time: 1.476s | Train Loss: 0.250010 || Validation Loss: 0.001267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 100/120 | Train Time: 1.600s | Train Loss: 0.264722 || Validation Loss: 0.001332 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 101/120 | Train Time: 1.384s | Train Loss: 0.251104 || Validation Loss: 0.001321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 102/120 | Train Time: 1.556s | Train Loss: 0.255977 || Validation Loss: 0.001325 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 103/120 | Train Time: 1.564s | Train Loss: 0.253040 || Validation Loss: 0.001260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 104/120 | Train Time: 1.471s | Train Loss: 0.241633 || Validation Loss: 0.001314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 105/120 | Train Time: 1.570s | Train Loss: 0.259595 || Validation Loss: 0.001270 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 106/120 | Train Time: 1.570s | Train Loss: 0.249757 || Validation Loss: 0.001253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 107/120 | Train Time: 1.629s | Train Loss: 0.256411 || Validation Loss: 0.001359 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 108/120 | Train Time: 1.505s | Train Loss: 0.247476 || Validation Loss: 0.001217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 109/120 | Train Time: 1.530s | Train Loss: 0.256818 || Validation Loss: 0.001390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 110/120 | Train Time: 1.670s | Train Loss: 0.256578 || Validation Loss: 0.001305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 111/120 | Train Time: 1.563s | Train Loss: 0.265551 || Validation Loss: 0.001416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 112/120 | Train Time: 1.625s | Train Loss: 0.254351 || Validation Loss: 0.001434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 113/120 | Train Time: 1.655s | Train Loss: 0.233672 || Validation Loss: 0.001316 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 114/120 | Train Time: 1.674s | Train Loss: 0.239625 || Validation Loss: 0.001313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 115/120 | Train Time: 1.644s | Train Loss: 0.250570 || Validation Loss: 0.001233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 116/120 | Train Time: 1.576s | Train Loss: 0.253885 || Validation Loss: 0.001294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 117/120 | Train Time: 1.503s | Train Loss: 0.249010 || Validation Loss: 0.001243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 118/120 | Train Time: 1.782s | Train Loss: 0.261612 || Validation Loss: 0.001320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 119/120 | Train Time: 1.630s | Train Loss: 0.244542 || Validation Loss: 0.001346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 120/120 | Train Time: 1.803s | Train Loss: 0.259183 || Validation Loss: 0.001202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 137.365s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.914%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001445\n",
      "INFO:root:Test AUC: 86.52%\n",
      "INFO:root:Test Time: 0.196s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({1.0: 12313, -1.0: 2955})\n",
      "class weights 0.19354204872936862 0.8064579512706314\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5484429075042523\n",
      "validation AUC 0.6047461996409995\n",
      "validation AUC 0.6577238783255069\n",
      "validation AUC 0.6954276440211382\n",
      "validation AUC 0.7292022577273133\n",
      "validation AUC 0.7497928499164439\n",
      "validation AUC 0.7671352225809769\n",
      "validation AUC 0.7763156861271809\n",
      "validation AUC 0.7856281940161043\n",
      "validation AUC 0.7943371367609875\n",
      "validation AUC 0.80330688224863\n",
      "validation AUC 0.8090425523989457\n",
      "validation AUC 0.8136582881425308\n",
      "validation AUC 0.8175054352418656\n",
      "validation AUC 0.8205147999922519\n",
      "validation AUC 0.8229992067973836\n",
      "validation AUC 0.824806100114452\n",
      "validation AUC 0.826781587567419\n",
      "validation AUC 0.8307263045485109\n",
      "validation AUC 0.8330212548075869\n",
      "validation AUC 0.8349495896482855\n",
      "validation AUC 0.835321938837086\n",
      "validation AUC 0.8380436173104417\n",
      "validation AUC 0.8375903487899111\n",
      "validation AUC 0.8401062474311112\n",
      "validation AUC 0.8428101019935322\n",
      "validation AUC 0.8444086931515952\n",
      "validation AUC 0.8438198921338097\n",
      "validation AUC 0.8438796909691971\n",
      "validation AUC 0.8439825742335693\n",
      "validation AUC 0.8485919376429717\n",
      "validation AUC 0.8458546516070629\n",
      "validation AUC 0.8452220424546987\n",
      "validation AUC 0.8477740013235299\n",
      "validation AUC 0.8485256440820146\n",
      "validation AUC 0.8494702368624553\n",
      "validation AUC 0.8504855239724285\n",
      "validation AUC 0.852984862526801\n",
      "validation AUC 0.8526248402792397\n",
      "validation AUC 0.8524545592707432\n",
      "validation AUC 0.8540305293076288\n",
      "validation AUC 0.8548896556502356\n",
      "validation AUC 0.853304833523235\n",
      "validation AUC 0.8553998468085543\n",
      "validation AUC 0.8553348064289749\n",
      "validation AUC 0.8553695975111337\n",
      "validation AUC 0.8556997296533898\n",
      "validation AUC 0.8533499187853326\n",
      "validation AUC 0.8550899571374719\n",
      "validation AUC 0.8555914159363804\n",
      "validation AUC 0.8560889342647565\n",
      "validation AUC 0.8563096644709296\n",
      "validation AUC 0.8567728573365548\n",
      "validation AUC 0.8566363324047906\n",
      "validation AUC 0.8569664166547646\n",
      "validation AUC 0.8568336140175813\n",
      "validation AUC 0.8569518307942262\n",
      "validation AUC 0.8570290145279642\n",
      "validation AUC 0.8570995039849572\n",
      "validation AUC 0.8572551379372219\n",
      "validation AUC 0.85742216759191\n",
      "validation AUC 0.8573942676769881\n",
      "validation AUC 0.8571979811593892\n",
      "validation AUC 0.8572769475502884\n",
      "validation AUC 0.8574282738578596\n",
      "validation AUC 0.8573080296412783\n",
      "validation AUC 0.8572563192801769\n",
      "validation AUC 0.8571214306680462\n",
      "validation AUC 0.8572847353674711\n",
      "validation AUC 0.8574549179307214\n",
      "validation AUC 0.8574335686156981\n",
      "validation AUC 0.8575759603520019\n",
      "validation AUC 0.8575043826759355\n",
      "validation AUC 0.8574512408677399\n",
      "validation AUC 0.8576259519123176\n",
      "validation AUC 0.8576821109342763\n",
      "validation AUC 0.8579677671106887\n",
      "validation AUC 0.8579612138501075\n",
      "validation AUC 0.8579358229586235\n",
      "validation AUC 0.858117882807799\n",
      "validation AUC 0.8581704898188458\n",
      "validation AUC 0.8582769783077763\n",
      "validation AUC 0.8584587880528125\n",
      "validation AUC 0.8583742927640295\n",
      "validation AUC 0.858298862419948\n",
      "validation AUC 0.8582828078627632\n",
      "validation AUC 0.8585473754710207\n",
      "validation AUC 0.8582526942601418\n",
      "validation AUC 0.8583237291570128\n",
      "validation AUC 0.8579686185290343\n",
      "validation AUC 0.8578215466525104\n",
      "validation AUC 0.8579143086812956\n",
      "validation AUC 0.8577185489787982\n",
      "validation AUC 0.8578445455905789\n",
      "validation AUC 0.8577753359217869\n",
      "validation AUC 0.8580505223132271\n",
      "validation AUC 0.8583274062199941\n",
      "validation AUC 0.8583728107639713\n",
      "validation AUC 0.8588421710912394\n",
      "validation AUC 0.8587443644087549\n",
      "validation AUC 0.8589047396969333\n",
      "validation AUC 0.8588824112508123\n",
      "validation AUC 0.858635364235704\n",
      "validation AUC 0.8586577591988834\n",
      "validation AUC 0.8588207206702878\n",
      "validation AUC 0.8588291949435114\n",
      "validation AUC 0.858843887231343\n",
      "validation AUC 0.8591685330466324\n",
      "validation AUC 0.8592577856354231\n",
      "validation AUC 0.8591199197197642\n",
      "validation AUC 0.8589875773806243\n",
      "validation AUC 0.8588974707128054\n",
      "validation AUC 0.858918428907526\n",
      "validation AUC 0.8589081639950934\n",
      "validation AUC 0.85890337742758\n",
      "validation AUC 0.8591100672130927\n",
      "validation AUC 0.8588080691258041\n",
      "validation AUC 0.8589424601903388\n",
      "validation AUC 0.8586969803171235\n",
      "validation AUC 0.8587163580665396\n",
      "validation AUC 0.8587163580665396\n",
      "train auc 0.9142163614651164\n",
      "Test AUC 0.8651970852830293\n",
      "test false positive rates [0.        0.        0.        ... 0.9983136 0.9983136 1.       ] true positive rate [0.00000000e+00 2.79955207e-04 9.23852184e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.45430136e+00 5.45430136e+00 3.40809226e+00 ... 5.82837779e-03\n",
      " 5.80037571e-03 1.59202097e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_0_unlabeled_balanced_smote\" --index_baseline_name \"5_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 90 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 30 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6d86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#excluding balancing - exclude both smote and balanced batches 5% data - not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "30c22f6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.518s | Train Loss: 1.163861 || Validation Loss: 0.910002 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/005 | Train Time: 0.802s | Train Loss: 0.853007 || Validation Loss: 0.663282 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.571s | Train Loss: 0.698340 || Validation Loss: 0.520421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.598s | Train Loss: 0.617445 || Validation Loss: 0.433792 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.616s | Train Loss: 0.592190 || Validation Loss: 0.391552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 3.247s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.498%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.382747\n",
      "INFO:root:Test AUC: 47.54%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.47677165257959536\n",
      "validation AUC 0.47727069281826495\n",
      "validation AUC 0.48010939608258163\n",
      "validation AUC 0.4864099024849283\n",
      "validation AUC 0.48816343581785016\n",
      "validation AUC 0.48816343581785016\n",
      "train auc 0.4983068105004303\n",
      "Test AUC 0.47540543773536265\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-04 ... 9.99450497e-01\n",
      " 9.99450497e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [8.46837807 7.46837807 2.48442841 ... 0.04618941 0.04612263 0.02334787]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.231s | Train Loss: 0.213926 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.216s | Train Loss: 0.190083 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.214s | Train Loss: 0.169031 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.214s | Train Loss: 0.149567 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.206s | Train Loss: 0.132841 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.208s | Train Loss: 0.118454 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.213s | Train Loss: 0.106866 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.217s | Train Loss: 0.096307 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.210s | Train Loss: 0.087892 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.214s | Train Loss: 0.080727 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.224s | Train Loss: 0.074723 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.220s | Train Loss: 0.070009 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.217s | Train Loss: 0.065366 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.212s | Train Loss: 0.061751 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.214s | Train Loss: 0.058183 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.223s | Train Loss: 0.055490 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.291s | Train Loss: 0.214903 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.276s | Train Loss: 0.053033 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.307s | Train Loss: 0.190115 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.330s | Train Loss: 0.051117 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.294s | Train Loss: 0.167761 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.332s | Train Loss: 0.048784 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.254s | Train Loss: 0.148233 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.285s | Train Loss: 0.047330 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.263s | Train Loss: 0.131578 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.355s | Train Loss: 0.045354 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.264s | Train Loss: 0.117596 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.297s | Train Loss: 0.043977 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.270s | Train Loss: 0.105915 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.266s | Train Loss: 0.096135 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.299s | Train Loss: 0.042539 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.293s | Train Loss: 0.087891 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.302s | Train Loss: 0.041050 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.268s | Train Loss: 0.081365 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.274s | Train Loss: 0.040243 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.291s | Train Loss: 0.075108 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.302s | Train Loss: 0.038744 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.289s | Train Loss: 0.069893 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.303s | Train Loss: 0.037984 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.270s | Train Loss: 0.065335 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.273s | Train Loss: 0.037011 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.266s | Train Loss: 0.061447 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.270s | Train Loss: 0.036197 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.266s | Train Loss: 0.058578 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.306s | Train Loss: 0.035252 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.326s | Train Loss: 0.055542 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.266s | Train Loss: 0.034688 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.324s | Train Loss: 0.052821 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.290s | Train Loss: 0.033895 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.320s | Train Loss: 0.050398 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.302s | Train Loss: 0.033240 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.275s | Train Loss: 0.048663 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.287s | Train Loss: 0.032620 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.288s | Train Loss: 0.046646 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.346s | Train Loss: 0.031815 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.272s | Train Loss: 0.045169 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.335s | Train Loss: 0.031922 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.280s | Train Loss: 0.043541 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.299s | Train Loss: 0.030885 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.258s | Train Loss: 0.042182 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.272s | Train Loss: 0.030294 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.262s | Train Loss: 0.040629 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.274s | Train Loss: 0.030066 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.267s | Train Loss: 0.040060 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.278s | Train Loss: 0.029613 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.269s | Train Loss: 0.038819 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.269s | Train Loss: 0.029112 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.263s | Train Loss: 0.037646 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.272s | Train Loss: 0.028718 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.263s | Train Loss: 0.036527 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.286s | Train Loss: 0.027945 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.292s | Train Loss: 0.035813 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.306s | Train Loss: 0.027553 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.289s | Train Loss: 0.035046 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.282s | Train Loss: 0.027538 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.310s | Train Loss: 0.034266 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.271s | Train Loss: 0.027159 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.265s | Train Loss: 0.033573 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.272s | Train Loss: 0.026648 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.265s | Train Loss: 0.032602 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.269s | Train Loss: 0.026484 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.258s | Train Loss: 0.031992 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.272s | Train Loss: 0.026324 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.269s | Train Loss: 0.031696 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.282s | Train Loss: 0.026380 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.269s | Train Loss: 0.030775 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.266s | Train Loss: 0.025822 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.261s | Train Loss: 0.030458 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.277s | Train Loss: 0.025736 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.272s | Train Loss: 0.030227 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.281s | Train Loss: 0.025287 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.271s | Train Loss: 0.028931 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.279s | Train Loss: 0.024891 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.268s | Train Loss: 0.028866 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.276s | Train Loss: 0.025169 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.266s | Train Loss: 0.028631 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.273s | Train Loss: 0.028284 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.277s | Train Loss: 0.025046 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.272s | Train Loss: 0.027847 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.281s | Train Loss: 0.024420 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.285s | Train Loss: 0.027038 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.288s | Train Loss: 0.024605 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.298s | Train Loss: 0.026697 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.304s | Train Loss: 0.024356 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.287s | Train Loss: 0.026663 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.301s | Train Loss: 0.024207 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.269s | Train Loss: 0.025902 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.323s | Train Loss: 0.023864 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.286s | Train Loss: 0.025947 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.271s | Train Loss: 0.023821 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.320s | Train Loss: 0.025199 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.282s | Train Loss: 0.023615 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.288s | Train Loss: 0.025415 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.278s | Train Loss: 0.023592 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.311s | Train Loss: 0.024994 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.290s | Train Loss: 0.023478 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.276s | Train Loss: 0.022762 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.303s | Train Loss: 0.024570 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.274s | Train Loss: 0.022940 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.267s | Train Loss: 0.024504 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.278s | Train Loss: 0.022753 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.272s | Train Loss: 0.024639 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.273s | Train Loss: 0.022899 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.267s | Train Loss: 0.023708 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.303s | Train Loss: 0.022760 |\n",
      "INFO:root:Pretraining Time: 19.106s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.306s | Train Loss: 0.023919 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.235s | Train Loss: 0.023923 |\n",
      "INFO:root:Test Loss: 0.022535\n",
      "INFO:root:Test AUC: 62.40%\n",
      "INFO:root:Test AUC: 62.40%\n",
      "INFO:root:Test Time: 0.350s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.224s | Train Loss: 0.023390 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.257s | Train Loss: 0.023325 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.256s | Train Loss: 0.023106 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.257s | Train Loss: 0.023101 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.857s | Train Loss: 1.117077 || Validation Loss: 0.866428 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.267s | Train Loss: 0.022752 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.259s | Train Loss: 0.022643 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.247s | Train Loss: 0.022637 |\n",
      "INFO:root:| Epoch: 002/005 | Train Time: 0.813s | Train Loss: 0.815825 || Validation Loss: 0.602486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.234s | Train Loss: 0.022366 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.253s | Train Loss: 0.022185 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.240s | Train Loss: 0.021912 |\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.752s | Train Loss: 0.663800 || Validation Loss: 0.464375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.241s | Train Loss: 0.021738 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.276s | Train Loss: 0.021610 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.265s | Train Loss: 0.021720 |\n",
      "INFO:root:Pretraining Time: 19.207s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.752s | Train Loss: 0.595771 || Validation Loss: 0.385706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Test Loss: 0.021571\n",
      "INFO:root:Test AUC: 61.26%\n",
      "INFO:root:Test AUC: 61.26%\n",
      "INFO:root:Test Time: 0.272s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.759s | Train Loss: 0.543425 || Validation Loss: 0.339453 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.635s | Train Loss: 0.968554 || Validation Loss: 0.746659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 4.097s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.503%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.338803\n",
      "INFO:root:Test AUC: 51.86%\n",
      "INFO:root:Test Time: 0.270s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5116289538005505\n",
      "validation AUC 0.5165932229653922\n",
      "validation AUC 0.5167445945045629\n",
      "validation AUC 0.5254957968669083\n",
      "validation AUC 0.520835566532703\n",
      "validation AUC 0.520835566532703\n",
      "train auc 0.5034864694006156\n",
      "Test AUC 0.5186481969611532\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.32638560e-04 ... 9.99943155e-01\n",
      " 9.99943155e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [4.81484175 3.81484175 2.76560593 ... 0.04017575 0.03851767 0.03296726]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 002/005 | Train Time: 0.650s | Train Loss: 0.732621 || Validation Loss: 0.546782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.740s | Train Loss: 0.633924 || Validation Loss: 0.444133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.561s | Train Loss: 0.568045 || Validation Loss: 0.380451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.650s | Train Loss: 0.551354 || Validation Loss: 0.347725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 3.372s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.516%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.349951\n",
      "INFO:root:Test AUC: 50.77%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4976326525746997\n",
      "validation AUC 0.4983993202063072\n",
      "validation AUC 0.5000180660330269\n",
      "validation AUC 0.4985962692338065\n",
      "validation AUC 0.4963414660104218\n",
      "validation AUC 0.4963414660104218\n",
      "train auc 0.5158637763132197\n",
      "Test AUC 0.5076679267082307\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.99886310e-01\n",
      " 9.99886310e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [4.07300305 3.07300305 3.06056595 ... 0.03963082 0.03941334 0.03265321]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.228s | Train Loss: 0.199526 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.206s | Train Loss: 0.176731 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.212s | Train Loss: 0.156570 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.211s | Train Loss: 0.138604 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.213s | Train Loss: 0.123064 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.209s | Train Loss: 0.110140 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.210s | Train Loss: 0.099321 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.206s | Train Loss: 0.090231 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.207s | Train Loss: 0.082305 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.215s | Train Loss: 0.075899 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.216s | Train Loss: 0.070550 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.227s | Train Loss: 0.065767 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.269s | Train Loss: 0.061524 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.342s | Train Loss: 0.223018 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.270s | Train Loss: 0.058327 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.271s | Train Loss: 0.198322 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.271s | Train Loss: 0.054716 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.276s | Train Loss: 0.175941 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.265s | Train Loss: 0.052591 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.311s | Train Loss: 0.155959 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.274s | Train Loss: 0.050396 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.321s | Train Loss: 0.138874 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.269s | Train Loss: 0.047698 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.276s | Train Loss: 0.123805 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.270s | Train Loss: 0.046375 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.329s | Train Loss: 0.111211 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.297s | Train Loss: 0.044235 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.328s | Train Loss: 0.100774 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.298s | Train Loss: 0.043073 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.322s | Train Loss: 0.091787 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.268s | Train Loss: 0.041205 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.272s | Train Loss: 0.039741 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.314s | Train Loss: 0.083920 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.279s | Train Loss: 0.038596 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.276s | Train Loss: 0.078241 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.266s | Train Loss: 0.037650 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.266s | Train Loss: 0.072769 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.307s | Train Loss: 0.036073 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.359s | Train Loss: 0.067844 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.286s | Train Loss: 0.035314 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.323s | Train Loss: 0.063839 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.272s | Train Loss: 0.034456 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.320s | Train Loss: 0.060591 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.271s | Train Loss: 0.033330 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.275s | Train Loss: 0.057589 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.268s | Train Loss: 0.032898 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.326s | Train Loss: 0.054866 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.271s | Train Loss: 0.032075 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.312s | Train Loss: 0.052490 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.265s | Train Loss: 0.031436 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.273s | Train Loss: 0.050175 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.277s | Train Loss: 0.031042 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.274s | Train Loss: 0.048365 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.272s | Train Loss: 0.030307 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.270s | Train Loss: 0.046336 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.275s | Train Loss: 0.029607 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.279s | Train Loss: 0.044950 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.276s | Train Loss: 0.029315 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.269s | Train Loss: 0.043498 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.271s | Train Loss: 0.028726 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.310s | Train Loss: 0.041883 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.363s | Train Loss: 0.028284 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.276s | Train Loss: 0.040576 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.320s | Train Loss: 0.027959 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.298s | Train Loss: 0.039424 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.277s | Train Loss: 0.027296 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.274s | Train Loss: 0.038627 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.309s | Train Loss: 0.026739 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.266s | Train Loss: 0.037717 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.286s | Train Loss: 0.026755 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.317s | Train Loss: 0.036742 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.296s | Train Loss: 0.026183 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.299s | Train Loss: 0.036051 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.305s | Train Loss: 0.025799 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.314s | Train Loss: 0.034966 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.306s | Train Loss: 0.025647 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.256s | Train Loss: 0.034144 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.286s | Train Loss: 0.025524 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.265s | Train Loss: 0.033487 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.260s | Train Loss: 0.025028 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.261s | Train Loss: 0.032803 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.267s | Train Loss: 0.025112 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.269s | Train Loss: 0.032274 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.268s | Train Loss: 0.024937 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.267s | Train Loss: 0.031564 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.260s | Train Loss: 0.024472 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.259s | Train Loss: 0.030877 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.265s | Train Loss: 0.024183 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.270s | Train Loss: 0.030416 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.267s | Train Loss: 0.024246 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.268s | Train Loss: 0.029904 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.268s | Train Loss: 0.023930 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.274s | Train Loss: 0.029371 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.270s | Train Loss: 0.023934 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.266s | Train Loss: 0.028799 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.269s | Train Loss: 0.023670 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.270s | Train Loss: 0.028437 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.264s | Train Loss: 0.023363 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.268s | Train Loss: 0.027907 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.275s | Train Loss: 0.023340 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.270s | Train Loss: 0.027590 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.297s | Train Loss: 0.023181 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.322s | Train Loss: 0.027367 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.274s | Train Loss: 0.022803 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.279s | Train Loss: 0.026549 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.276s | Train Loss: 0.023053 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.365s | Train Loss: 0.026506 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.273s | Train Loss: 0.022796 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.274s | Train Loss: 0.026212 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.274s | Train Loss: 0.022641 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.268s | Train Loss: 0.022050 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.326s | Train Loss: 0.025729 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.295s | Train Loss: 0.022705 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.306s | Train Loss: 0.025398 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.286s | Train Loss: 0.021841 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.270s | Train Loss: 0.025310 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.289s | Train Loss: 0.025120 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.305s | Train Loss: 0.022069 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.275s | Train Loss: 0.024643 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.274s | Train Loss: 0.021741 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.300s | Train Loss: 0.024298 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.301s | Train Loss: 0.021846 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.270s | Train Loss: 0.021670 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.280s | Train Loss: 0.024141 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.269s | Train Loss: 0.021569 |\n",
      "INFO:root:Pretraining Time: 18.794s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.299s | Train Loss: 0.023832 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.236s | Train Loss: 0.023937 |\n",
      "INFO:root:Test Loss: 0.021615\n",
      "INFO:root:Test AUC: 63.02%\n",
      "INFO:root:Test AUC: 63.02%\n",
      "INFO:root:Test Time: 0.308s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.245s | Train Loss: 0.023435 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.258s | Train Loss: 0.023482 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.286s | Train Loss: 0.023081 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.725s | Train Loss: 1.454595 || Validation Loss: 1.149936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.279s | Train Loss: 0.022808 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.269s | Train Loss: 0.022851 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.236s | Train Loss: 0.022675 |\n",
      "INFO:root:| Epoch: 002/005 | Train Time: 0.733s | Train Loss: 1.047676 || Validation Loss: 0.819746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.253s | Train Loss: 0.022578 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.281s | Train Loss: 0.022668 |\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.719s | Train Loss: 0.807158 || Validation Loss: 0.617781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.228s | Train Loss: 0.022183 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.257s | Train Loss: 0.022102 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.249s | Train Loss: 0.021874 |\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.726s | Train Loss: 0.690465 || Validation Loss: 0.496486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.232s | Train Loss: 0.021427 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.237s | Train Loss: 0.022256 |\n",
      "INFO:root:Pretraining Time: 19.871s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.021475\n",
      "INFO:root:Test AUC: 61.23%\n",
      "INFO:root:Test AUC: 61.23%\n",
      "INFO:root:Test Time: 0.292s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.766s | Train Loss: 0.609837 || Validation Loss: 0.422449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 3.826s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.496%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.418505\n",
      "INFO:root:Test AUC: 55.99%\n",
      "INFO:root:Test Time: 0.228s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5969685143367142\n",
      "validation AUC 0.5877695138167105\n",
      "validation AUC 0.5837949732473713\n",
      "validation AUC 0.5785880738000908\n",
      "validation AUC 0.5749850815541706\n",
      "validation AUC 0.5749850815541706\n",
      "train auc 0.4962191375868483\n",
      "Test AUC 0.5598667991425493\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.13690194e-04 ... 9.99526291e-01\n",
      " 9.99526291e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [5.80859566 4.80859566 4.0242939  ... 0.04683663 0.04681273 0.02923681]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.652s | Train Loss: 1.055109 || Validation Loss: 0.811386 || Validation number of baches in epoch: 441.000000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 002/005 | Train Time: 0.791s | Train Loss: 0.768173 || Validation Loss: 0.572202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.591s | Train Loss: 0.631705 || Validation Loss: 0.444571 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.576s | Train Loss: 0.576819 || Validation Loss: 0.377843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.635s | Train Loss: 0.535478 || Validation Loss: 0.337672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 3.364s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.520%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.340802\n",
      "INFO:root:Test AUC: 48.79%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.49686720087491754\n",
      "validation AUC 0.5015533675404844\n",
      "validation AUC 0.49593055555259924\n",
      "validation AUC 0.4837460512813526\n",
      "validation AUC 0.4815222396857585\n",
      "validation AUC 0.4815222396857585\n",
      "train auc 0.52021810483915\n",
      "Test AUC 0.4878648687645315\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.41070583e-04 ... 9.99962103e-01\n",
      " 9.99962103e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99944009 1.         1.        ] tresholds [4.00766325 3.00766325 2.01643419 ... 0.02929929 0.02896387 0.02342932]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.214s | Train Loss: 0.205052 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.190s | Train Loss: 0.180924 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.189s | Train Loss: 0.160498 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.192s | Train Loss: 0.142593 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.192s | Train Loss: 0.127301 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.189s | Train Loss: 0.113714 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.189s | Train Loss: 0.102437 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.188s | Train Loss: 0.092585 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.188s | Train Loss: 0.084453 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.191s | Train Loss: 0.077866 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.185s | Train Loss: 0.072239 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.191s | Train Loss: 0.066959 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.193s | Train Loss: 0.062760 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.190s | Train Loss: 0.059000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.191s | Train Loss: 0.056033 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.190s | Train Loss: 0.053002 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.190s | Train Loss: 0.050861 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.192s | Train Loss: 0.048744 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.192s | Train Loss: 0.046540 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.189s | Train Loss: 0.044970 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.191s | Train Loss: 0.043100 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.191s | Train Loss: 0.041679 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.193s | Train Loss: 0.040560 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.189s | Train Loss: 0.039518 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.200s | Train Loss: 0.038062 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.195s | Train Loss: 0.037213 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.194s | Train Loss: 0.036234 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.193s | Train Loss: 0.035503 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.192s | Train Loss: 0.034229 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.190s | Train Loss: 0.033874 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.192s | Train Loss: 0.033101 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.191s | Train Loss: 0.032614 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.192s | Train Loss: 0.031800 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.191s | Train Loss: 0.030930 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.190s | Train Loss: 0.030682 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.192s | Train Loss: 0.029988 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.193s | Train Loss: 0.029575 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.192s | Train Loss: 0.028865 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.187s | Train Loss: 0.028714 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.193s | Train Loss: 0.027917 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.192s | Train Loss: 0.028160 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.180s | Train Loss: 0.027657 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.191s | Train Loss: 0.027512 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.193s | Train Loss: 0.027189 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.195s | Train Loss: 0.026656 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.194s | Train Loss: 0.026210 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.194s | Train Loss: 0.026313 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.192s | Train Loss: 0.026092 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.193s | Train Loss: 0.025486 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.194s | Train Loss: 0.025572 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.193s | Train Loss: 0.025385 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.192s | Train Loss: 0.024894 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.190s | Train Loss: 0.024737 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.193s | Train Loss: 0.024841 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.194s | Train Loss: 0.024495 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.191s | Train Loss: 0.024177 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.193s | Train Loss: 0.024587 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.193s | Train Loss: 0.024350 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.192s | Train Loss: 0.023845 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.192s | Train Loss: 0.023828 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.194s | Train Loss: 0.023872 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.192s | Train Loss: 0.023384 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.193s | Train Loss: 0.023457 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.193s | Train Loss: 0.023376 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.189s | Train Loss: 0.023352 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.192s | Train Loss: 0.023090 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.193s | Train Loss: 0.023087 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.193s | Train Loss: 0.023227 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.194s | Train Loss: 0.023012 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.193s | Train Loss: 0.022871 |\n",
      "INFO:root:Pretraining Time: 13.457s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.022645\n",
      "INFO:root:Test AUC: 60.98%\n",
      "INFO:root:Test AUC: 60.98%\n",
      "INFO:root:Test Time: 0.233s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.527s | Train Loss: 0.865042 || Validation Loss: 0.611739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/005 | Train Time: 0.540s | Train Loss: 0.671249 || Validation Loss: 0.462041 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.541s | Train Loss: 0.583834 || Validation Loss: 0.385786 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.546s | Train Loss: 0.548461 || Validation Loss: 0.342093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.592s | Train Loss: 0.531081 || Validation Loss: 0.320895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 2.864s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.509%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.321611\n",
      "INFO:root:Test AUC: 52.43%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5692541846679564\n",
      "validation AUC 0.5576598782727191\n",
      "validation AUC 0.5462787936381169\n",
      "validation AUC 0.5395500450932441\n",
      "validation AUC 0.5278740690538593\n",
      "validation AUC 0.5278740690538593\n",
      "train auc 0.5085345878284528\n",
      "Test AUC 0.524272119113713\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 9.47418285e-05 ... 9.99924207e-01\n",
      " 9.99924207e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99944009 1.         1.        ] tresholds [3.66073322 2.66073322 2.13404655 ... 0.05380284 0.05161139 0.03303859]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.203s | Train Loss: 0.215578 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.187s | Train Loss: 0.192125 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.193s | Train Loss: 0.171000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.190s | Train Loss: 0.151760 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.190s | Train Loss: 0.134868 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.188s | Train Loss: 0.120366 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.189s | Train Loss: 0.108227 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.187s | Train Loss: 0.097833 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.189s | Train Loss: 0.089171 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.189s | Train Loss: 0.082197 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.189s | Train Loss: 0.075533 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.188s | Train Loss: 0.070357 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.189s | Train Loss: 0.065642 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.187s | Train Loss: 0.062407 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.189s | Train Loss: 0.058505 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.191s | Train Loss: 0.055441 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.189s | Train Loss: 0.052657 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.190s | Train Loss: 0.050761 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.189s | Train Loss: 0.048404 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.190s | Train Loss: 0.046716 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.190s | Train Loss: 0.044797 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.190s | Train Loss: 0.043303 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.190s | Train Loss: 0.041699 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.188s | Train Loss: 0.040839 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.189s | Train Loss: 0.039043 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.188s | Train Loss: 0.038281 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.187s | Train Loss: 0.036933 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.191s | Train Loss: 0.036184 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.189s | Train Loss: 0.034992 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.190s | Train Loss: 0.034439 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.190s | Train Loss: 0.033784 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.188s | Train Loss: 0.032827 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.190s | Train Loss: 0.031583 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.188s | Train Loss: 0.031520 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.187s | Train Loss: 0.030865 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.188s | Train Loss: 0.030242 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.188s | Train Loss: 0.029769 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.189s | Train Loss: 0.028782 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.188s | Train Loss: 0.028630 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.190s | Train Loss: 0.027923 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.190s | Train Loss: 0.027386 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.188s | Train Loss: 0.027291 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.189s | Train Loss: 0.026661 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.190s | Train Loss: 0.026163 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.188s | Train Loss: 0.026396 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.189s | Train Loss: 0.025840 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.189s | Train Loss: 0.025658 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.188s | Train Loss: 0.025187 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.190s | Train Loss: 0.025201 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.190s | Train Loss: 0.024758 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.189s | Train Loss: 0.024507 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.189s | Train Loss: 0.024166 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.190s | Train Loss: 0.023994 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.188s | Train Loss: 0.023837 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.189s | Train Loss: 0.023825 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.189s | Train Loss: 0.023490 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.189s | Train Loss: 0.023252 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.191s | Train Loss: 0.023072 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.190s | Train Loss: 0.022828 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.189s | Train Loss: 0.022785 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.191s | Train Loss: 0.022774 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.190s | Train Loss: 0.022643 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.188s | Train Loss: 0.022556 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.191s | Train Loss: 0.022036 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.189s | Train Loss: 0.021819 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.190s | Train Loss: 0.021722 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.191s | Train Loss: 0.021838 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.188s | Train Loss: 0.021749 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.189s | Train Loss: 0.021528 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.190s | Train Loss: 0.021164 |\n",
      "INFO:root:Pretraining Time: 13.265s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.021490\n",
      "INFO:root:Test AUC: 62.87%\n",
      "INFO:root:Test AUC: 62.87%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.516s | Train Loss: 1.333486 || Validation Loss: 1.074250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/005 | Train Time: 0.525s | Train Loss: 0.951406 || Validation Loss: 0.738738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.531s | Train Loss: 0.730824 || Validation Loss: 0.552336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.531s | Train Loss: 0.637725 || Validation Loss: 0.453214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.578s | Train Loss: 0.591255 || Validation Loss: 0.394696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 2.800s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.512%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.391287\n",
      "INFO:root:Test AUC: 49.59%\n",
      "INFO:root:Test Time: 0.178s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5133250163951245\n",
      "validation AUC 0.5146672853994738\n",
      "validation AUC 0.5171525516049981\n",
      "validation AUC 0.4987298434688656\n",
      "validation AUC 0.4911154229537703\n",
      "validation AUC 0.4911154229537703\n",
      "train auc 0.5119627784534464\n",
      "Test AUC 0.49594878954848043\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99962103e-01\n",
      " 9.99962103e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.01873255 6.01873255 5.66125584 ... 0.02359962 0.02148409 0.01344412]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets = torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.205s | Train Loss: 0.194306 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.189s | Train Loss: 0.171317 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.191s | Train Loss: 0.151809 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.192s | Train Loss: 0.135138 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.193s | Train Loss: 0.120843 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.194s | Train Loss: 0.108417 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.191s | Train Loss: 0.098190 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.191s | Train Loss: 0.089301 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.315s | Train Loss: 0.081910 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.192s | Train Loss: 0.075738 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.192s | Train Loss: 0.070741 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.192s | Train Loss: 0.065832 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.192s | Train Loss: 0.061871 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.193s | Train Loss: 0.058642 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.191s | Train Loss: 0.055706 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.192s | Train Loss: 0.053227 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.192s | Train Loss: 0.050584 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.190s | Train Loss: 0.048627 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.193s | Train Loss: 0.047242 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.192s | Train Loss: 0.045052 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.191s | Train Loss: 0.044176 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.191s | Train Loss: 0.042674 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.193s | Train Loss: 0.041524 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.190s | Train Loss: 0.040471 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.193s | Train Loss: 0.039517 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.190s | Train Loss: 0.038345 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.191s | Train Loss: 0.037650 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.191s | Train Loss: 0.036538 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.193s | Train Loss: 0.036202 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.193s | Train Loss: 0.035189 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.193s | Train Loss: 0.034475 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.193s | Train Loss: 0.033670 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.192s | Train Loss: 0.032923 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.192s | Train Loss: 0.032162 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.191s | Train Loss: 0.031646 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.191s | Train Loss: 0.031490 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.193s | Train Loss: 0.030867 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.192s | Train Loss: 0.030212 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.194s | Train Loss: 0.029552 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.193s | Train Loss: 0.028706 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.192s | Train Loss: 0.028359 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.192s | Train Loss: 0.028257 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.192s | Train Loss: 0.027728 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.192s | Train Loss: 0.027415 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.192s | Train Loss: 0.027138 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.192s | Train Loss: 0.026880 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.192s | Train Loss: 0.026422 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.192s | Train Loss: 0.026522 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.192s | Train Loss: 0.025795 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.194s | Train Loss: 0.025175 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.192s | Train Loss: 0.025127 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.191s | Train Loss: 0.024818 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.192s | Train Loss: 0.024854 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.193s | Train Loss: 0.024618 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.192s | Train Loss: 0.024525 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.192s | Train Loss: 0.024227 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.194s | Train Loss: 0.023896 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.192s | Train Loss: 0.023573 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.192s | Train Loss: 0.023426 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.193s | Train Loss: 0.023125 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.194s | Train Loss: 0.023461 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.191s | Train Loss: 0.022957 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.193s | Train Loss: 0.022470 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.193s | Train Loss: 0.022502 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.192s | Train Loss: 0.022553 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.193s | Train Loss: 0.022473 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.192s | Train Loss: 0.022575 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.194s | Train Loss: 0.022229 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.192s | Train Loss: 0.021925 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.192s | Train Loss: 0.022198 |\n",
      "INFO:root:Pretraining Time: 13.589s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.021842\n",
      "INFO:root:Test AUC: 62.33%\n",
      "INFO:root:Test AUC: 62.33%\n",
      "INFO:root:Test Time: 0.233s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 5\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/005 | Train Time: 0.528s | Train Loss: 1.519821 || Validation Loss: 1.198883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/005 | Train Time: 0.536s | Train Loss: 1.042445 || Validation Loss: 0.838362 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/005 | Train Time: 0.539s | Train Loss: 0.814993 || Validation Loss: 0.632512 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/005 | Train Time: 0.541s | Train Loss: 0.680582 || Validation Loss: 0.505250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/005 | Train Time: 0.591s | Train Loss: 0.621231 || Validation Loss: 0.430502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 2.853s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.487%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.419840\n",
      "INFO:root:Test AUC: 52.08%\n",
      "INFO:root:Test Time: 0.189s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "len of selected to training 13147\n",
      "class weights 0.06343652544306685 0.9365634745569331\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4768390822519079\n",
      "validation AUC 0.49047073430362387\n",
      "validation AUC 0.503276675522127\n",
      "validation AUC 0.5121590787908923\n",
      "validation AUC 0.5151498506931502\n",
      "validation AUC 0.5151498506931502\n",
      "train auc 0.486760584152151\n",
      "Test AUC 0.5207552451484598\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 7.57934628e-05 ... 9.99810516e-01\n",
      " 9.99810516e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.82324648 4.82324648 3.3422873  ... 0.04888732 0.04811909 0.01813199]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {6..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_0_unlabeled_baseline_exclude_balance\" --index_baseline_name \"5_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 0 --balanced_batches 0 --minority_loss 0 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 5 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2a1c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#excluding loss function include balancing 5% data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b2f61243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 066/070 | Train Time: 0.204s | Train Loss: 0.020282 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.399s | Train Loss: 0.020129 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.216s | Train Loss: 0.020046 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.214s | Train Loss: 0.020059 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.215s | Train Loss: 0.019770 |\n",
      "INFO:root:Pretraining Time: 16.123s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.00\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:Test Loss: 0.021264\n",
      "INFO:root:Test AUC: 51.82%\n",
      "INFO:root:Test AUC: 51.82%\n",
      "INFO:root:Test Time: 0.242s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.596s | Train Loss: 1.341447 || Validation Loss: 1.420087 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.608s | Train Loss: 1.022737 || Validation Loss: 1.334218 || Validation number of baches in epoch: 441.000000 |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 003/070 | Train Time: 0.634s | Train Loss: 0.898619 || Validation Loss: 1.329374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.661s | Train Loss: 0.835572 || Validation Loss: 1.363336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.658s | Train Loss: 0.798114 || Validation Loss: 1.372960 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.290s | Train Loss: 0.201633 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.685s | Train Loss: 0.757282 || Validation Loss: 1.420748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.285s | Train Loss: 0.174541 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.283s | Train Loss: 0.151105 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.262s | Train Loss: 0.130834 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.877s | Train Loss: 0.757198 || Validation Loss: 1.459185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.274s | Train Loss: 0.114220 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.293s | Train Loss: 0.100449 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.265s | Train Loss: 0.089435 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.839s | Train Loss: 0.729684 || Validation Loss: 1.490163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.313s | Train Loss: 0.080740 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.282s | Train Loss: 0.073335 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.259s | Train Loss: 0.067599 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.832s | Train Loss: 0.718017 || Validation Loss: 1.506556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.281s | Train Loss: 0.062786 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.284s | Train Loss: 0.058335 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.262s | Train Loss: 0.054804 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.857s | Train Loss: 0.687600 || Validation Loss: 1.565800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.282s | Train Loss: 0.051969 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.302s | Train Loss: 0.049042 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.286s | Train Loss: 0.046930 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.892s | Train Loss: 0.678416 || Validation Loss: 1.589935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.298s | Train Loss: 0.045121 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.295s | Train Loss: 0.043090 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.308s | Train Loss: 0.041361 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.958s | Train Loss: 0.669687 || Validation Loss: 1.619946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.248s | Train Loss: 0.039745 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.334s | Train Loss: 0.038638 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.285s | Train Loss: 0.037546 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.246s | Train Loss: 0.036452 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 1.005s | Train Loss: 0.653885 || Validation Loss: 1.649198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.277s | Train Loss: 0.034968 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.289s | Train Loss: 0.034148 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.265s | Train Loss: 0.033360 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.972s | Train Loss: 0.662231 || Validation Loss: 1.621018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.264s | Train Loss: 0.032464 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.287s | Train Loss: 0.031533 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.287s | Train Loss: 0.031039 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.246s | Train Loss: 0.030086 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.988s | Train Loss: 0.649337 || Validation Loss: 1.653653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.266s | Train Loss: 0.029206 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.279s | Train Loss: 0.028806 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.267s | Train Loss: 0.028267 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.932s | Train Loss: 0.637726 || Validation Loss: 1.677596 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.245s | Train Loss: 0.027741 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.296s | Train Loss: 0.027327 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.285s | Train Loss: 0.026572 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.240s | Train Loss: 0.026190 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.997s | Train Loss: 0.635696 || Validation Loss: 1.685523 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.264s | Train Loss: 0.026074 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.288s | Train Loss: 0.025555 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.284s | Train Loss: 0.025105 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.248s | Train Loss: 0.024575 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.000s | Train Loss: 0.627659 || Validation Loss: 1.698818 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.297s | Train Loss: 0.024517 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.281s | Train Loss: 0.024266 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.270s | Train Loss: 0.023879 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.989s | Train Loss: 0.631152 || Validation Loss: 1.692804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.243s | Train Loss: 0.023546 |\n",
      "\n",
      "Aborted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels normal [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels outlier [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "len of selected to training 13147\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({1.0: 12313, -1.0: 2955})\n",
      "class weights 0.19354204872936862 0.8064579512706314\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Aborted!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 0 12313 0\n",
      "final sizes 12313 834 0 0\n",
      "labels normal [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "labels outlier [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "len of selected to training 13147\n",
      "apply smote\n",
      "Original dataset shape Counter({1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({1.0: 12313, -1.0: 2955})\n",
      "class weights 0.19354204872936862 0.8064579512706314\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6263866092753941\n",
      "validation AUC 0.7225756820446301\n",
      "validation AUC 0.7654237945459417\n",
      "validation AUC 0.7875023653465922\n",
      "validation AUC 0.8019983161073664\n",
      "validation AUC 0.8118879526006894\n",
      "validation AUC 0.8197241170419258\n",
      "validation AUC 0.8258353691100742\n",
      "validation AUC 0.8300501799366248\n",
      "validation AUC 0.8331495263027605\n",
      "validation AUC 0.8359524114615384\n",
      "validation AUC 0.8389990177825107\n",
      "validation AUC 0.8417814928471281\n",
      "validation AUC 0.8439534956363747\n",
      "validation AUC 0.8447075303482748\n",
      "validation AUC 0.8456564493981855\n",
      "validation AUC 0.8475258660893483\n",
      "validation AUC 0.8480155779757657\n",
      "validation AUC 0.8489707602718833\n",
      "Error while terminating subprocess (pid=43635): \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test dta --experiment_name \"5_per_labeled_0_unlabeled_balanced_exclude_loss\" --index_baseline_name \"5_per_labeled_0_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 0 --ratio_known_normal 0.05 --ratio_unknown_normal 0.0 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.0 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdcbd80",
   "metadata": {},
   "source": [
    "# Enhanced model - evaluation of loss function when unlabeled data exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1392383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basline balanced with loss minority"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32617380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.423s | Train Loss: 0.182770 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.400s | Train Loss: 0.139580 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.413s | Train Loss: 0.108866 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.410s | Train Loss: 0.087372 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.408s | Train Loss: 0.072645 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.399s | Train Loss: 0.062117 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.054765 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.398s | Train Loss: 0.049027 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.401s | Train Loss: 0.045048 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.399s | Train Loss: 0.041632 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.396s | Train Loss: 0.038921 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.406s | Train Loss: 0.036357 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.396s | Train Loss: 0.034437 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.397s | Train Loss: 0.032868 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.409s | Train Loss: 0.031405 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.408s | Train Loss: 0.029997 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.414s | Train Loss: 0.028645 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.412s | Train Loss: 0.027638 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.412s | Train Loss: 0.026874 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.413s | Train Loss: 0.025963 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.408s | Train Loss: 0.025069 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.408s | Train Loss: 0.024541 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.397s | Train Loss: 0.024086 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.406s | Train Loss: 0.023728 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.401s | Train Loss: 0.023272 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.401s | Train Loss: 0.022621 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.398s | Train Loss: 0.022334 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.405s | Train Loss: 0.022058 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.397s | Train Loss: 0.021834 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.400s | Train Loss: 0.021369 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.398s | Train Loss: 0.021148 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.394s | Train Loss: 0.021004 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.401s | Train Loss: 0.020563 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.420s | Train Loss: 0.020618 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.414s | Train Loss: 0.020287 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.394s | Train Loss: 0.020005 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.394s | Train Loss: 0.019849 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.403s | Train Loss: 0.019852 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.382s | Train Loss: 0.019580 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.404s | Train Loss: 0.019496 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.405s | Train Loss: 0.019503 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.401s | Train Loss: 0.019299 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.403s | Train Loss: 0.019208 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.406s | Train Loss: 0.018712 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.402s | Train Loss: 0.018699 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.403s | Train Loss: 0.018725 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.401s | Train Loss: 0.018586 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.383s | Train Loss: 0.018383 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.403s | Train Loss: 0.018441 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.401s | Train Loss: 0.018296 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.413s | Train Loss: 0.018118 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.398s | Train Loss: 0.018161 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.398s | Train Loss: 0.017829 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.387s | Train Loss: 0.017810 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.401s | Train Loss: 0.017758 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.407s | Train Loss: 0.017700 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.400s | Train Loss: 0.017743 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.405s | Train Loss: 0.017581 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.410s | Train Loss: 0.017348 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.402s | Train Loss: 0.017314 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.403s | Train Loss: 0.017389 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.396s | Train Loss: 0.017249 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.396s | Train Loss: 0.017095 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.398s | Train Loss: 0.016907 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.388s | Train Loss: 0.016895 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.397s | Train Loss: 0.016863 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.388s | Train Loss: 0.016786 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.398s | Train Loss: 0.016678 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.397s | Train Loss: 0.016537 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.403s | Train Loss: 0.016567 |\n",
      "INFO:root:Pretraining Time: 28.149s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018236\n",
      "INFO:root:Test AUC: 52.07%\n",
      "INFO:root:Test AUC: 52.07%\n",
      "INFO:root:Test Time: 0.235s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.662s | Train Loss: 0.903698 || Validation Loss: 0.013709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.664s | Train Loss: 0.696992 || Validation Loss: 0.015236 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.727s | Train Loss: 0.625603 || Validation Loss: 0.015462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.736s | Train Loss: 0.594045 || Validation Loss: 0.015249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.753s | Train Loss: 0.567213 || Validation Loss: 0.016704 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.797s | Train Loss: 0.536000 || Validation Loss: 0.017182 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.823s | Train Loss: 0.515811 || Validation Loss: 0.019067 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.822s | Train Loss: 0.503661 || Validation Loss: 0.017430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.830s | Train Loss: 0.494391 || Validation Loss: 0.021233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.856s | Train Loss: 0.483711 || Validation Loss: 0.019563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.892s | Train Loss: 0.468742 || Validation Loss: 0.022312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.878s | Train Loss: 0.457461 || Validation Loss: 0.021736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.960s | Train Loss: 0.451151 || Validation Loss: 0.020585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.907s | Train Loss: 0.440298 || Validation Loss: 0.020431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.981s | Train Loss: 0.443741 || Validation Loss: 0.021172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.963s | Train Loss: 0.442108 || Validation Loss: 0.022664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 1.018s | Train Loss: 0.423957 || Validation Loss: 0.021238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.027s | Train Loss: 0.420659 || Validation Loss: 0.024404 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.061s | Train Loss: 0.414743 || Validation Loss: 0.021962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1.027s | Train Loss: 0.415332 || Validation Loss: 0.022412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.129s | Train Loss: 0.405346 || Validation Loss: 0.022574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.060s | Train Loss: 0.406488 || Validation Loss: 0.023036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.139s | Train Loss: 0.399440 || Validation Loss: 0.024235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.126s | Train Loss: 0.405172 || Validation Loss: 0.022480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.174s | Train Loss: 0.399624 || Validation Loss: 0.024899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.200s | Train Loss: 0.393243 || Validation Loss: 0.023554 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.225s | Train Loss: 0.392621 || Validation Loss: 0.024792 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.211s | Train Loss: 0.391129 || Validation Loss: 0.023874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.275s | Train Loss: 0.396843 || Validation Loss: 0.023846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.255s | Train Loss: 0.396556 || Validation Loss: 0.022765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.250s | Train Loss: 0.385114 || Validation Loss: 0.025091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.238s | Train Loss: 0.384973 || Validation Loss: 0.022138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.339s | Train Loss: 0.381402 || Validation Loss: 0.025020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.308s | Train Loss: 0.383672 || Validation Loss: 0.023622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.335s | Train Loss: 0.376727 || Validation Loss: 0.023588 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.429s | Train Loss: 0.372840 || Validation Loss: 0.025176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.328s | Train Loss: 0.375676 || Validation Loss: 0.025920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.471s | Train Loss: 0.372046 || Validation Loss: 0.022994 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.361s | Train Loss: 0.369963 || Validation Loss: 0.024701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.492s | Train Loss: 0.371854 || Validation Loss: 0.024711 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.499s | Train Loss: 0.364976 || Validation Loss: 0.026147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.408s | Train Loss: 0.369609 || Validation Loss: 0.024220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.513s | Train Loss: 0.365489 || Validation Loss: 0.024806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.461s | Train Loss: 0.363684 || Validation Loss: 0.026980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.583s | Train Loss: 0.364213 || Validation Loss: 0.024917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.652s | Train Loss: 0.361800 || Validation Loss: 0.024391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.531s | Train Loss: 0.363728 || Validation Loss: 0.023977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.621s | Train Loss: 0.355675 || Validation Loss: 0.027021 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.517s | Train Loss: 0.362931 || Validation Loss: 0.026108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.709s | Train Loss: 0.365073 || Validation Loss: 0.024371 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.604s | Train Loss: 0.364162 || Validation Loss: 0.025333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.650s | Train Loss: 0.360059 || Validation Loss: 0.023401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.763s | Train Loss: 0.357975 || Validation Loss: 0.022133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.748s | Train Loss: 0.355030 || Validation Loss: 0.024706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.655s | Train Loss: 0.359283 || Validation Loss: 0.026945 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.791s | Train Loss: 0.354664 || Validation Loss: 0.024914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.829s | Train Loss: 0.357998 || Validation Loss: 0.025030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.642s | Train Loss: 0.353214 || Validation Loss: 0.025162 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.893s | Train Loss: 0.346295 || Validation Loss: 0.024531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.741s | Train Loss: 0.358121 || Validation Loss: 0.028556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.789s | Train Loss: 0.359941 || Validation Loss: 0.024968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.948s | Train Loss: 0.355362 || Validation Loss: 0.026132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.890s | Train Loss: 0.355706 || Validation Loss: 0.025968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.984s | Train Loss: 0.357630 || Validation Loss: 0.024564 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.806s | Train Loss: 0.357668 || Validation Loss: 0.026692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.975s | Train Loss: 0.361464 || Validation Loss: 0.027050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.896s | Train Loss: 0.353853 || Validation Loss: 0.026511 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.901s | Train Loss: 0.349714 || Validation Loss: 0.028214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.925s | Train Loss: 0.354140 || Validation Loss: 0.025514 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.123s | Train Loss: 0.346904 || Validation Loss: 0.024937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 96.455s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.920%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022473\n",
      "INFO:root:Test AUC: 85.90%\n",
      "INFO:root:Test Time: 0.200s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6041239618283614\n",
      "validation AUC 0.7020984721723491\n",
      "validation AUC 0.7548414760145873\n",
      "validation AUC 0.7851949152019256\n",
      "validation AUC 0.8034048725181955\n",
      "validation AUC 0.815073015508798\n",
      "validation AUC 0.8247516306257734\n",
      "validation AUC 0.8329658487587279\n",
      "validation AUC 0.8367900873831934\n",
      "validation AUC 0.8408819587219616\n",
      "validation AUC 0.8450832905318235\n",
      "validation AUC 0.8467654111509835\n",
      "validation AUC 0.8474393087717587\n",
      "validation AUC 0.8491475519700438\n",
      "validation AUC 0.8500591336648047\n",
      "validation AUC 0.8507819000379518\n",
      "validation AUC 0.8519744737329991\n",
      "validation AUC 0.8508041486636031\n",
      "validation AUC 0.853998925722902\n",
      "validation AUC 0.8549519182774615\n",
      "validation AUC 0.854990194853474\n",
      "validation AUC 0.8572220736378957\n",
      "validation AUC 0.8564576542828578\n",
      "validation AUC 0.8551617130792545\n",
      "validation AUC 0.8555054040586686\n",
      "validation AUC 0.8550381190636187\n",
      "validation AUC 0.8568918111222056\n",
      "validation AUC 0.8581288022480851\n",
      "validation AUC 0.8537434177379815\n",
      "validation AUC 0.8552206551749314\n",
      "validation AUC 0.8562644541567627\n",
      "validation AUC 0.8558022031726829\n",
      "validation AUC 0.8545015206331658\n",
      "validation AUC 0.8549024189433772\n",
      "validation AUC 0.8565499294067763\n",
      "validation AUC 0.856337567046534\n",
      "validation AUC 0.8538475541837314\n",
      "validation AUC 0.8560463580260973\n",
      "validation AUC 0.8567343758880027\n",
      "validation AUC 0.8555234833951073\n",
      "validation AUC 0.8568552613290257\n",
      "validation AUC 0.8546976368670954\n",
      "validation AUC 0.8569592674013413\n",
      "validation AUC 0.856425840504227\n",
      "validation AUC 0.856667885436553\n",
      "validation AUC 0.854061616719983\n",
      "validation AUC 0.8541007926066237\n",
      "validation AUC 0.8553324889746645\n",
      "validation AUC 0.855253988203173\n",
      "validation AUC 0.8556133106912814\n",
      "validation AUC 0.8553283409709107\n",
      "validation AUC 0.8548836664543086\n",
      "validation AUC 0.8549256094505734\n",
      "validation AUC 0.8551226276558132\n",
      "validation AUC 0.8551139564920968\n",
      "validation AUC 0.8552585592554175\n",
      "validation AUC 0.8553104026506355\n",
      "validation AUC 0.8553549185267143\n",
      "validation AUC 0.8552169993974088\n",
      "validation AUC 0.855195591547374\n",
      "validation AUC 0.8548965707636137\n",
      "validation AUC 0.8549416613470758\n",
      "validation AUC 0.8551069189873315\n",
      "validation AUC 0.8552784584985706\n",
      "validation AUC 0.8553352401201947\n",
      "validation AUC 0.8549325937416921\n",
      "validation AUC 0.8546838093010217\n",
      "validation AUC 0.8546784772936307\n",
      "validation AUC 0.8546752552073279\n",
      "validation AUC 0.854780094073213\n",
      "validation AUC 0.854780094073213\n",
      "train auc 0.9204082219560874\n",
      "Test AUC 0.8590196607860601\n",
      "test false positive rates [0.         0.         0.         ... 0.99927996 0.99927996 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 8.39865622e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.46149216e+01 6.36149216e+01 5.23763657e+01 ... 1.59522593e-02\n",
      " 1.59328282e-02 5.95889147e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.420s | Train Loss: 0.189759 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.402s | Train Loss: 0.145630 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.399s | Train Loss: 0.113118 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.398s | Train Loss: 0.090498 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.395s | Train Loss: 0.074645 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.399s | Train Loss: 0.063945 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.055959 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.397s | Train Loss: 0.050310 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.399s | Train Loss: 0.045734 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.398s | Train Loss: 0.042762 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.394s | Train Loss: 0.039661 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.394s | Train Loss: 0.037331 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.398s | Train Loss: 0.035304 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.397s | Train Loss: 0.033644 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.392s | Train Loss: 0.032178 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.395s | Train Loss: 0.031073 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.379s | Train Loss: 0.029539 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.395s | Train Loss: 0.028931 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.399s | Train Loss: 0.028070 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.396s | Train Loss: 0.027011 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.397s | Train Loss: 0.026319 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.398s | Train Loss: 0.025692 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.396s | Train Loss: 0.024947 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.396s | Train Loss: 0.024430 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.398s | Train Loss: 0.023963 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.397s | Train Loss: 0.023477 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.396s | Train Loss: 0.023304 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.396s | Train Loss: 0.022741 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.401s | Train Loss: 0.022537 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.431s | Train Loss: 0.022053 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.438s | Train Loss: 0.021754 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.430s | Train Loss: 0.021562 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.400s | Train Loss: 0.021133 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.399s | Train Loss: 0.020951 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.397s | Train Loss: 0.020682 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.394s | Train Loss: 0.020381 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.398s | Train Loss: 0.020387 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.400s | Train Loss: 0.020168 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.396s | Train Loss: 0.019958 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.398s | Train Loss: 0.019572 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.395s | Train Loss: 0.019388 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.400s | Train Loss: 0.019332 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.397s | Train Loss: 0.019060 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.398s | Train Loss: 0.019181 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.398s | Train Loss: 0.018993 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.398s | Train Loss: 0.018926 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.397s | Train Loss: 0.018483 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.407s | Train Loss: 0.018437 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.396s | Train Loss: 0.018434 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.398s | Train Loss: 0.018207 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.397s | Train Loss: 0.018224 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.396s | Train Loss: 0.018066 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.400s | Train Loss: 0.017912 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.399s | Train Loss: 0.017831 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.397s | Train Loss: 0.017993 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.399s | Train Loss: 0.017658 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.394s | Train Loss: 0.017580 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.397s | Train Loss: 0.017417 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.395s | Train Loss: 0.017400 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.394s | Train Loss: 0.017367 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.397s | Train Loss: 0.017356 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.398s | Train Loss: 0.017184 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.397s | Train Loss: 0.017176 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.395s | Train Loss: 0.016857 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.390s | Train Loss: 0.017009 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.399s | Train Loss: 0.016705 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.396s | Train Loss: 0.016546 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.397s | Train Loss: 0.016542 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.396s | Train Loss: 0.016587 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.399s | Train Loss: 0.016565 |\n",
      "INFO:root:Pretraining Time: 27.918s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017756\n",
      "INFO:root:Test AUC: 50.37%\n",
      "INFO:root:Test AUC: 50.37%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.664s | Train Loss: 0.800227 || Validation Loss: 0.016549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.672s | Train Loss: 0.614180 || Validation Loss: 0.018182 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.713s | Train Loss: 0.563795 || Validation Loss: 0.018402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.725s | Train Loss: 0.535290 || Validation Loss: 0.018657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.735s | Train Loss: 0.506039 || Validation Loss: 0.019911 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.788s | Train Loss: 0.494511 || Validation Loss: 0.023718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.798s | Train Loss: 0.482161 || Validation Loss: 0.021205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.804s | Train Loss: 0.466271 || Validation Loss: 0.021791 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.820s | Train Loss: 0.460148 || Validation Loss: 0.021321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.829s | Train Loss: 0.456266 || Validation Loss: 0.023588 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.880s | Train Loss: 0.438720 || Validation Loss: 0.021253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.854s | Train Loss: 0.434521 || Validation Loss: 0.020738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.938s | Train Loss: 0.431355 || Validation Loss: 0.023156 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.891s | Train Loss: 0.428764 || Validation Loss: 0.022776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.959s | Train Loss: 0.413212 || Validation Loss: 0.023609 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.942s | Train Loss: 0.409081 || Validation Loss: 0.023419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.992s | Train Loss: 0.406617 || Validation Loss: 0.022500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.005s | Train Loss: 0.401422 || Validation Loss: 0.022683 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.034s | Train Loss: 0.398340 || Validation Loss: 0.024170 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1.001s | Train Loss: 0.397200 || Validation Loss: 0.022422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.095s | Train Loss: 0.382218 || Validation Loss: 0.023912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.034s | Train Loss: 0.384315 || Validation Loss: 0.026114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.123s | Train Loss: 0.387287 || Validation Loss: 0.022703 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.093s | Train Loss: 0.378842 || Validation Loss: 0.022277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.160s | Train Loss: 0.377332 || Validation Loss: 0.024358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.167s | Train Loss: 0.381784 || Validation Loss: 0.024094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.180s | Train Loss: 0.374929 || Validation Loss: 0.024752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.191s | Train Loss: 0.371258 || Validation Loss: 0.022625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.256s | Train Loss: 0.367817 || Validation Loss: 0.023726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.242s | Train Loss: 0.361905 || Validation Loss: 0.025781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.214s | Train Loss: 0.369472 || Validation Loss: 0.021417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.225s | Train Loss: 0.361879 || Validation Loss: 0.023746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.311s | Train Loss: 0.355750 || Validation Loss: 0.023934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.296s | Train Loss: 0.359694 || Validation Loss: 0.024880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.309s | Train Loss: 0.354201 || Validation Loss: 0.025674 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.408s | Train Loss: 0.353120 || Validation Loss: 0.025022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.295s | Train Loss: 0.356568 || Validation Loss: 0.023906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.425s | Train Loss: 0.347954 || Validation Loss: 0.024391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.337s | Train Loss: 0.357525 || Validation Loss: 0.022276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.469s | Train Loss: 0.350870 || Validation Loss: 0.026028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.466s | Train Loss: 0.343775 || Validation Loss: 0.025398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.385s | Train Loss: 0.352847 || Validation Loss: 0.023862 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.490s | Train Loss: 0.342883 || Validation Loss: 0.022328 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.451s | Train Loss: 0.343865 || Validation Loss: 0.024143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.557s | Train Loss: 0.341059 || Validation Loss: 0.024051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.617s | Train Loss: 0.341871 || Validation Loss: 0.026693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.508s | Train Loss: 0.345179 || Validation Loss: 0.024548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.586s | Train Loss: 0.336557 || Validation Loss: 0.025190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.495s | Train Loss: 0.346022 || Validation Loss: 0.022976 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.687s | Train Loss: 0.336172 || Validation Loss: 0.023722 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.582s | Train Loss: 0.325920 || Validation Loss: 0.022372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.617s | Train Loss: 0.336486 || Validation Loss: 0.025725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.731s | Train Loss: 0.326754 || Validation Loss: 0.026744 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.715s | Train Loss: 0.336531 || Validation Loss: 0.024470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.618s | Train Loss: 0.337255 || Validation Loss: 0.024840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.771s | Train Loss: 0.338428 || Validation Loss: 0.022888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.807s | Train Loss: 0.339656 || Validation Loss: 0.023925 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.608s | Train Loss: 0.331337 || Validation Loss: 0.023914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.863s | Train Loss: 0.347127 || Validation Loss: 0.025454 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.709s | Train Loss: 0.338187 || Validation Loss: 0.024436 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.760s | Train Loss: 0.326241 || Validation Loss: 0.026712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.927s | Train Loss: 0.326731 || Validation Loss: 0.024761 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.864s | Train Loss: 0.332120 || Validation Loss: 0.023821 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.967s | Train Loss: 0.338909 || Validation Loss: 0.026381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.778s | Train Loss: 0.324339 || Validation Loss: 0.024676 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.939s | Train Loss: 0.328210 || Validation Loss: 0.025695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.875s | Train Loss: 0.342753 || Validation Loss: 0.023265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.889s | Train Loss: 0.329384 || Validation Loss: 0.025592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.892s | Train Loss: 0.330510 || Validation Loss: 0.026019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.086s | Train Loss: 0.335413 || Validation Loss: 0.024870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 94.781s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.932%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021625\n",
      "INFO:root:Test AUC: 86.62%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.711085216759404\n",
      "validation AUC 0.7772574426202569\n",
      "validation AUC 0.799497700425475\n",
      "validation AUC 0.8134636139984244\n",
      "validation AUC 0.8226809306555941\n",
      "validation AUC 0.8286675563633624\n",
      "validation AUC 0.8338704115734998\n",
      "validation AUC 0.8379373736042858\n",
      "validation AUC 0.8422361129942829\n",
      "validation AUC 0.8443422027001456\n",
      "validation AUC 0.8472799631676423\n",
      "validation AUC 0.8486303392710539\n",
      "validation AUC 0.851365342441523\n",
      "validation AUC 0.8523150916243211\n",
      "validation AUC 0.8540965594610351\n",
      "validation AUC 0.855890157348496\n",
      "validation AUC 0.8567378241323035\n",
      "validation AUC 0.859060788715642\n",
      "validation AUC 0.857699942827258\n",
      "validation AUC 0.8600412075836683\n",
      "validation AUC 0.8605622623079439\n",
      "validation AUC 0.8597746524457098\n",
      "validation AUC 0.8603969647574533\n",
      "validation AUC 0.8612522171465863\n",
      "validation AUC 0.8605944326180072\n",
      "validation AUC 0.8615893202765492\n",
      "validation AUC 0.8608634648512158\n",
      "validation AUC 0.8616896466252012\n",
      "validation AUC 0.8604809678200052\n",
      "validation AUC 0.861870825788525\n",
      "validation AUC 0.8609522677846926\n",
      "validation AUC 0.8620646644783796\n",
      "validation AUC 0.8633165155150772\n",
      "validation AUC 0.8623696451905613\n",
      "validation AUC 0.8630262058052682\n",
      "validation AUC 0.8626593269793401\n",
      "validation AUC 0.860924934595107\n",
      "validation AUC 0.8617356045911032\n",
      "validation AUC 0.8606263847521192\n",
      "validation AUC 0.8595445752625083\n",
      "validation AUC 0.862010123151278\n",
      "validation AUC 0.8599221979236886\n",
      "validation AUC 0.8609055754704672\n",
      "validation AUC 0.8601982197693806\n",
      "validation AUC 0.8601234013822352\n",
      "validation AUC 0.8609690008158716\n",
      "validation AUC 0.8584623081355364\n",
      "validation AUC 0.8586936039112457\n",
      "validation AUC 0.8568203824443497\n",
      "validation AUC 0.8569089618805108\n",
      "validation AUC 0.8575694576443851\n",
      "validation AUC 0.8579467104707215\n",
      "validation AUC 0.8582014601611779\n",
      "validation AUC 0.8582124008869224\n",
      "validation AUC 0.8583115086430669\n",
      "validation AUC 0.8580874233164745\n",
      "validation AUC 0.8582825284911185\n",
      "validation AUC 0.8584730519707886\n",
      "validation AUC 0.858242894967117\n",
      "validation AUC 0.8584112230347828\n",
      "validation AUC 0.8586154197609516\n",
      "validation AUC 0.8588637385822139\n",
      "validation AUC 0.8586645865097446\n",
      "validation AUC 0.858553963320472\n",
      "validation AUC 0.8585562062756769\n",
      "validation AUC 0.8586933458250596\n",
      "validation AUC 0.8585931338857479\n",
      "validation AUC 0.8585640818953764\n",
      "validation AUC 0.858640211998911\n",
      "validation AUC 0.8588029952045991\n",
      "validation AUC 0.8588029952045991\n",
      "train auc 0.9324333443586621\n",
      "Test AUC 0.8661846441850214\n",
      "test false positive rates [0.         0.         0.         ... 0.99810516 0.99810516 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 6.71892497e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.24717903e+01 5.14717903e+01 3.79896088e+01 ... 1.67606715e-02\n",
      " 1.67499967e-02 4.86701960e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.421s | Train Loss: 0.199409 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.405s | Train Loss: 0.152759 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.401s | Train Loss: 0.118398 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.401s | Train Loss: 0.094850 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.403s | Train Loss: 0.078923 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.397s | Train Loss: 0.067880 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.402s | Train Loss: 0.059859 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.402s | Train Loss: 0.054133 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.403s | Train Loss: 0.049570 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.402s | Train Loss: 0.045795 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.401s | Train Loss: 0.042525 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.403s | Train Loss: 0.039789 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.399s | Train Loss: 0.037700 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.397s | Train Loss: 0.035906 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.399s | Train Loss: 0.034245 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.403s | Train Loss: 0.033018 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.404s | Train Loss: 0.031851 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.400s | Train Loss: 0.030561 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.400s | Train Loss: 0.029391 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.401s | Train Loss: 0.028467 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.401s | Train Loss: 0.027557 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.400s | Train Loss: 0.026971 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.400s | Train Loss: 0.026266 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.402s | Train Loss: 0.025781 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.402s | Train Loss: 0.025228 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.403s | Train Loss: 0.024797 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.400s | Train Loss: 0.024371 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.399s | Train Loss: 0.023965 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.403s | Train Loss: 0.023762 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.401s | Train Loss: 0.023333 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.403s | Train Loss: 0.023068 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.401s | Train Loss: 0.022684 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.401s | Train Loss: 0.022753 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.400s | Train Loss: 0.022243 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.399s | Train Loss: 0.022265 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.402s | Train Loss: 0.021841 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.398s | Train Loss: 0.021660 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.401s | Train Loss: 0.021555 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.403s | Train Loss: 0.021410 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.400s | Train Loss: 0.021028 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.404s | Train Loss: 0.021044 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.403s | Train Loss: 0.020719 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.401s | Train Loss: 0.020335 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.401s | Train Loss: 0.020523 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.404s | Train Loss: 0.020306 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.401s | Train Loss: 0.020070 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.399s | Train Loss: 0.019974 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.402s | Train Loss: 0.019841 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.401s | Train Loss: 0.019639 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.401s | Train Loss: 0.019538 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.400s | Train Loss: 0.019311 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.400s | Train Loss: 0.019378 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.401s | Train Loss: 0.019299 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.400s | Train Loss: 0.018999 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.400s | Train Loss: 0.019050 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.399s | Train Loss: 0.018931 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.403s | Train Loss: 0.018712 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.401s | Train Loss: 0.018525 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.402s | Train Loss: 0.018709 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.403s | Train Loss: 0.018598 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.404s | Train Loss: 0.018443 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.402s | Train Loss: 0.018395 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.400s | Train Loss: 0.018075 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.401s | Train Loss: 0.018100 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.400s | Train Loss: 0.018074 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.402s | Train Loss: 0.017858 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.401s | Train Loss: 0.017916 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.399s | Train Loss: 0.017788 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.404s | Train Loss: 0.017875 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.402s | Train Loss: 0.017553 |\n",
      "INFO:root:Pretraining Time: 28.110s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018937\n",
      "INFO:root:Test AUC: 51.28%\n",
      "INFO:root:Test AUC: 51.28%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.682s | Train Loss: 0.823146 || Validation Loss: 0.014093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.675s | Train Loss: 0.649237 || Validation Loss: 0.014758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.730s | Train Loss: 0.586977 || Validation Loss: 0.016477 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.743s | Train Loss: 0.552140 || Validation Loss: 0.016213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.744s | Train Loss: 0.525412 || Validation Loss: 0.016968 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.785s | Train Loss: 0.500970 || Validation Loss: 0.019123 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.802s | Train Loss: 0.486502 || Validation Loss: 0.019029 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.775s | Train Loss: 0.483394 || Validation Loss: 0.017999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.826s | Train Loss: 0.466828 || Validation Loss: 0.020666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.837s | Train Loss: 0.452254 || Validation Loss: 0.019251 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.883s | Train Loss: 0.452084 || Validation Loss: 0.020802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.860s | Train Loss: 0.444696 || Validation Loss: 0.023126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.944s | Train Loss: 0.437989 || Validation Loss: 0.020768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.887s | Train Loss: 0.433675 || Validation Loss: 0.020371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.969s | Train Loss: 0.427263 || Validation Loss: 0.021048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.949s | Train Loss: 0.421812 || Validation Loss: 0.021896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 1.000s | Train Loss: 0.415580 || Validation Loss: 0.022294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.011s | Train Loss: 0.421588 || Validation Loss: 0.022234 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.028s | Train Loss: 0.414865 || Validation Loss: 0.020910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1.003s | Train Loss: 0.404228 || Validation Loss: 0.021366 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.100s | Train Loss: 0.399678 || Validation Loss: 0.021978 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.032s | Train Loss: 0.398340 || Validation Loss: 0.023047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.122s | Train Loss: 0.398607 || Validation Loss: 0.022914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.105s | Train Loss: 0.402888 || Validation Loss: 0.021235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.126s | Train Loss: 0.388783 || Validation Loss: 0.023763 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.170s | Train Loss: 0.390081 || Validation Loss: 0.022720 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.182s | Train Loss: 0.382925 || Validation Loss: 0.020780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.195s | Train Loss: 0.385760 || Validation Loss: 0.022540 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.251s | Train Loss: 0.378389 || Validation Loss: 0.022688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.233s | Train Loss: 0.375128 || Validation Loss: 0.022129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.212s | Train Loss: 0.372482 || Validation Loss: 0.023976 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.227s | Train Loss: 0.373115 || Validation Loss: 0.023296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.309s | Train Loss: 0.370432 || Validation Loss: 0.022108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.301s | Train Loss: 0.367568 || Validation Loss: 0.022917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.303s | Train Loss: 0.363817 || Validation Loss: 0.023567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.400s | Train Loss: 0.359189 || Validation Loss: 0.022527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.292s | Train Loss: 0.360809 || Validation Loss: 0.023276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.442s | Train Loss: 0.363985 || Validation Loss: 0.021831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.335s | Train Loss: 0.356635 || Validation Loss: 0.021469 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.477s | Train Loss: 0.349833 || Validation Loss: 0.024972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.454s | Train Loss: 0.356694 || Validation Loss: 0.026299 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.382s | Train Loss: 0.344815 || Validation Loss: 0.023601 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.492s | Train Loss: 0.342642 || Validation Loss: 0.024697 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.440s | Train Loss: 0.347961 || Validation Loss: 0.025782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.548s | Train Loss: 0.344106 || Validation Loss: 0.024602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.620s | Train Loss: 0.335016 || Validation Loss: 0.025863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.505s | Train Loss: 0.340487 || Validation Loss: 0.026290 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.572s | Train Loss: 0.336469 || Validation Loss: 0.025351 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.486s | Train Loss: 0.345433 || Validation Loss: 0.024399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.686s | Train Loss: 0.333941 || Validation Loss: 0.025319 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.564s | Train Loss: 0.334209 || Validation Loss: 0.024816 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.611s | Train Loss: 0.332637 || Validation Loss: 0.025700 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.732s | Train Loss: 0.331191 || Validation Loss: 0.023794 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.707s | Train Loss: 0.342043 || Validation Loss: 0.024399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.611s | Train Loss: 0.336106 || Validation Loss: 0.025300 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.756s | Train Loss: 0.337350 || Validation Loss: 0.025816 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.789s | Train Loss: 0.339909 || Validation Loss: 0.026427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.616s | Train Loss: 0.337617 || Validation Loss: 0.024440 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.874s | Train Loss: 0.333390 || Validation Loss: 0.024538 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.705s | Train Loss: 0.333391 || Validation Loss: 0.025738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.757s | Train Loss: 0.331730 || Validation Loss: 0.026050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.893s | Train Loss: 0.340707 || Validation Loss: 0.024557 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.871s | Train Loss: 0.337548 || Validation Loss: 0.023500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.962s | Train Loss: 0.333339 || Validation Loss: 0.022961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.783s | Train Loss: 0.338889 || Validation Loss: 0.025190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.943s | Train Loss: 0.334778 || Validation Loss: 0.025190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.870s | Train Loss: 0.340173 || Validation Loss: 0.025165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.887s | Train Loss: 0.342633 || Validation Loss: 0.026602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.880s | Train Loss: 0.335506 || Validation Loss: 0.026399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.078s | Train Loss: 0.332394 || Validation Loss: 0.024561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 94.695s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.929%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021316\n",
      "INFO:root:Test AUC: 85.35%\n",
      "INFO:root:Test Time: 0.193s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6975959936935444\n",
      "validation AUC 0.758271292429635\n",
      "validation AUC 0.7910972478327678\n",
      "validation AUC 0.8097604470627452\n",
      "validation AUC 0.8224558928047274\n",
      "validation AUC 0.8297942755100369\n",
      "validation AUC 0.836139095576605\n",
      "validation AUC 0.8397384240629555\n",
      "validation AUC 0.8414691207594311\n",
      "validation AUC 0.8444400971851472\n",
      "validation AUC 0.8462658893288121\n",
      "validation AUC 0.8462897104177207\n",
      "validation AUC 0.8480472533989152\n",
      "validation AUC 0.8490310885830546\n",
      "validation AUC 0.8501631237730263\n",
      "validation AUC 0.8496317429406244\n",
      "validation AUC 0.8504288700635563\n",
      "validation AUC 0.850180753454151\n",
      "validation AUC 0.8508876861067471\n",
      "validation AUC 0.851999944444953\n",
      "validation AUC 0.851960933520617\n",
      "validation AUC 0.8529649659443306\n",
      "validation AUC 0.8536344840819889\n",
      "validation AUC 0.8525571658241529\n",
      "validation AUC 0.8538401814329925\n",
      "validation AUC 0.8535972504934503\n",
      "validation AUC 0.8541846360282356\n",
      "validation AUC 0.853371092495322\n",
      "validation AUC 0.8549097411411518\n",
      "validation AUC 0.8555451360279163\n",
      "validation AUC 0.8560856057511605\n",
      "validation AUC 0.8579035941135489\n",
      "validation AUC 0.8550971782293181\n",
      "validation AUC 0.8555130854485582\n",
      "validation AUC 0.8549101828144187\n",
      "validation AUC 0.8550933415253967\n",
      "validation AUC 0.8553785267610364\n",
      "validation AUC 0.8540906208180724\n",
      "validation AUC 0.8550801844512703\n",
      "validation AUC 0.8555947950029407\n",
      "validation AUC 0.8557088371690084\n",
      "validation AUC 0.8549944918554386\n",
      "validation AUC 0.8551126314722961\n",
      "validation AUC 0.8540119204954062\n",
      "validation AUC 0.8524439644337014\n",
      "validation AUC 0.8545343374890355\n",
      "validation AUC 0.8540716607957825\n",
      "validation AUC 0.8516155503472402\n",
      "validation AUC 0.8523799643809136\n",
      "validation AUC 0.8527772202170137\n",
      "validation AUC 0.8532364992189301\n",
      "validation AUC 0.8536647865930559\n",
      "validation AUC 0.853495178737189\n",
      "validation AUC 0.8534815666863842\n",
      "validation AUC 0.8537513359285982\n",
      "validation AUC 0.8531253598572852\n",
      "validation AUC 0.8536489289263636\n",
      "validation AUC 0.853828647375088\n",
      "validation AUC 0.8538153918557152\n",
      "validation AUC 0.8535841519543351\n",
      "validation AUC 0.8538669904681588\n",
      "validation AUC 0.8533956798607419\n",
      "validation AUC 0.8536417583874818\n",
      "validation AUC 0.8535584790305238\n",
      "validation AUC 0.8531404139979135\n",
      "validation AUC 0.852806463776513\n",
      "validation AUC 0.8529929323763108\n",
      "validation AUC 0.8530760866811989\n",
      "validation AUC 0.8529338146755999\n",
      "validation AUC 0.8531735927065802\n",
      "validation AUC 0.8531735927065802\n",
      "train auc 0.9294394723215292\n",
      "Test AUC 0.8534979468183244\n",
      "test false positive rates [0.         0.         0.         ... 0.99873046 0.99876836 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.51959686e-03 ... 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [8.41976852e+01 8.31976852e+01 5.10821800e+01 ... 1.70434620e-02\n",
      " 1.70239508e-02 9.57714673e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.423s | Train Loss: 0.191097 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.407s | Train Loss: 0.145099 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.402s | Train Loss: 0.113292 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.405s | Train Loss: 0.091451 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.400s | Train Loss: 0.076191 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.404s | Train Loss: 0.065361 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.403s | Train Loss: 0.058045 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.402s | Train Loss: 0.051832 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.401s | Train Loss: 0.047530 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.402s | Train Loss: 0.044083 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.404s | Train Loss: 0.041201 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.401s | Train Loss: 0.038478 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.406s | Train Loss: 0.036334 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.402s | Train Loss: 0.034697 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.403s | Train Loss: 0.033138 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.403s | Train Loss: 0.031970 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.403s | Train Loss: 0.030475 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.405s | Train Loss: 0.029316 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.404s | Train Loss: 0.028060 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.401s | Train Loss: 0.027297 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.405s | Train Loss: 0.026584 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.402s | Train Loss: 0.025711 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.401s | Train Loss: 0.025256 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.399s | Train Loss: 0.024575 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.404s | Train Loss: 0.024266 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.404s | Train Loss: 0.023884 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.402s | Train Loss: 0.023190 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.400s | Train Loss: 0.023040 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.402s | Train Loss: 0.022573 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.402s | Train Loss: 0.022197 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.400s | Train Loss: 0.022014 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.404s | Train Loss: 0.021566 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.402s | Train Loss: 0.021256 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.402s | Train Loss: 0.021134 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.403s | Train Loss: 0.020748 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.401s | Train Loss: 0.020407 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.405s | Train Loss: 0.020213 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.403s | Train Loss: 0.020075 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.403s | Train Loss: 0.019807 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.405s | Train Loss: 0.019700 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.402s | Train Loss: 0.019379 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.404s | Train Loss: 0.019302 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.403s | Train Loss: 0.019138 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.404s | Train Loss: 0.018924 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.403s | Train Loss: 0.018813 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.400s | Train Loss: 0.018603 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.404s | Train Loss: 0.018410 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.403s | Train Loss: 0.018603 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.405s | Train Loss: 0.018270 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.405s | Train Loss: 0.018194 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.411s | Train Loss: 0.017930 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.440s | Train Loss: 0.017907 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.407s | Train Loss: 0.017850 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.400s | Train Loss: 0.017654 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.406s | Train Loss: 0.017639 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.386s | Train Loss: 0.017456 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.403s | Train Loss: 0.017212 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.400s | Train Loss: 0.017113 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.402s | Train Loss: 0.017171 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.405s | Train Loss: 0.017016 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.404s | Train Loss: 0.016845 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.403s | Train Loss: 0.017002 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.403s | Train Loss: 0.016629 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.404s | Train Loss: 0.016714 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.401s | Train Loss: 0.016528 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.402s | Train Loss: 0.016586 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.403s | Train Loss: 0.016253 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.403s | Train Loss: 0.016253 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.401s | Train Loss: 0.016244 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.405s | Train Loss: 0.015932 |\n",
      "INFO:root:Pretraining Time: 28.262s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017261\n",
      "INFO:root:Test AUC: 51.27%\n",
      "INFO:root:Test AUC: 51.27%\n",
      "INFO:root:Test Time: 0.232s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.691s | Train Loss: 0.989946 || Validation Loss: 0.014772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.708s | Train Loss: 0.693628 || Validation Loss: 0.014693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.726s | Train Loss: 0.615882 || Validation Loss: 0.015679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.737s | Train Loss: 0.573282 || Validation Loss: 0.016470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.748s | Train Loss: 0.543284 || Validation Loss: 0.016546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.789s | Train Loss: 0.524076 || Validation Loss: 0.018555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.809s | Train Loss: 0.502714 || Validation Loss: 0.018440 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.813s | Train Loss: 0.485480 || Validation Loss: 0.019919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.831s | Train Loss: 0.473932 || Validation Loss: 0.018827 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.840s | Train Loss: 0.459681 || Validation Loss: 0.021765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.892s | Train Loss: 0.455731 || Validation Loss: 0.020676 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.862s | Train Loss: 0.440115 || Validation Loss: 0.020596 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.950s | Train Loss: 0.432524 || Validation Loss: 0.022780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.897s | Train Loss: 0.422633 || Validation Loss: 0.021174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.976s | Train Loss: 0.416495 || Validation Loss: 0.022102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.958s | Train Loss: 0.415174 || Validation Loss: 0.021959 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 1.007s | Train Loss: 0.411605 || Validation Loss: 0.023145 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.018s | Train Loss: 0.398556 || Validation Loss: 0.025013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.043s | Train Loss: 0.397296 || Validation Loss: 0.022551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1.010s | Train Loss: 0.398061 || Validation Loss: 0.022739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.100s | Train Loss: 0.393022 || Validation Loss: 0.022519 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.042s | Train Loss: 0.391350 || Validation Loss: 0.023590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.130s | Train Loss: 0.388432 || Validation Loss: 0.024453 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.106s | Train Loss: 0.391508 || Validation Loss: 0.023999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.159s | Train Loss: 0.377655 || Validation Loss: 0.025581 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.171s | Train Loss: 0.368804 || Validation Loss: 0.027228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.181s | Train Loss: 0.372902 || Validation Loss: 0.025419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.200s | Train Loss: 0.378751 || Validation Loss: 0.024790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.257s | Train Loss: 0.369306 || Validation Loss: 0.024193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.236s | Train Loss: 0.370452 || Validation Loss: 0.026125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.218s | Train Loss: 0.363086 || Validation Loss: 0.026810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.222s | Train Loss: 0.365595 || Validation Loss: 0.025728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.315s | Train Loss: 0.357650 || Validation Loss: 0.023153 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.295s | Train Loss: 0.358399 || Validation Loss: 0.024139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.303s | Train Loss: 0.355953 || Validation Loss: 0.025197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.411s | Train Loss: 0.348841 || Validation Loss: 0.025413 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.293s | Train Loss: 0.358580 || Validation Loss: 0.023995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.442s | Train Loss: 0.360891 || Validation Loss: 0.023889 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.340s | Train Loss: 0.347069 || Validation Loss: 0.027495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.474s | Train Loss: 0.345083 || Validation Loss: 0.026895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.468s | Train Loss: 0.346559 || Validation Loss: 0.025874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.377s | Train Loss: 0.344201 || Validation Loss: 0.024486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.481s | Train Loss: 0.348894 || Validation Loss: 0.025649 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.440s | Train Loss: 0.349198 || Validation Loss: 0.025400 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.541s | Train Loss: 0.343170 || Validation Loss: 0.027473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.611s | Train Loss: 0.334657 || Validation Loss: 0.027629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.509s | Train Loss: 0.346423 || Validation Loss: 0.028069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.571s | Train Loss: 0.347616 || Validation Loss: 0.026720 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.491s | Train Loss: 0.341605 || Validation Loss: 0.026504 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.678s | Train Loss: 0.333106 || Validation Loss: 0.024966 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.565s | Train Loss: 0.332612 || Validation Loss: 0.024246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.615s | Train Loss: 0.337815 || Validation Loss: 0.027372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.722s | Train Loss: 0.340294 || Validation Loss: 0.025109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.705s | Train Loss: 0.345868 || Validation Loss: 0.025244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.607s | Train Loss: 0.335965 || Validation Loss: 0.024696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.759s | Train Loss: 0.334491 || Validation Loss: 0.025646 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.794s | Train Loss: 0.333274 || Validation Loss: 0.025938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.606s | Train Loss: 0.332430 || Validation Loss: 0.026705 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.864s | Train Loss: 0.333032 || Validation Loss: 0.025031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.701s | Train Loss: 0.337201 || Validation Loss: 0.026750 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.749s | Train Loss: 0.333904 || Validation Loss: 0.026009 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.902s | Train Loss: 0.331622 || Validation Loss: 0.027257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.876s | Train Loss: 0.336143 || Validation Loss: 0.026273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.954s | Train Loss: 0.336910 || Validation Loss: 0.025178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.775s | Train Loss: 0.338054 || Validation Loss: 0.026258 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.923s | Train Loss: 0.337820 || Validation Loss: 0.025614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.863s | Train Loss: 0.332884 || Validation Loss: 0.028762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.879s | Train Loss: 0.333092 || Validation Loss: 0.027105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.884s | Train Loss: 0.329677 || Validation Loss: 0.027828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.096s | Train Loss: 0.337067 || Validation Loss: 0.024881 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 94.888s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.928%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.023737\n",
      "INFO:root:Test AUC: 86.75%\n",
      "INFO:root:Test Time: 0.199s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6024659523124842\n",
      "validation AUC 0.7137148276420628\n",
      "validation AUC 0.7633131444305641\n",
      "validation AUC 0.7927836122402456\n",
      "validation AUC 0.8123895737012197\n",
      "validation AUC 0.8257226812313128\n",
      "validation AUC 0.8341998758844906\n",
      "validation AUC 0.8416371641340874\n",
      "validation AUC 0.8461183651362918\n",
      "validation AUC 0.8498185680718546\n",
      "validation AUC 0.852889296138839\n",
      "validation AUC 0.8563715971735464\n",
      "validation AUC 0.8587947337859614\n",
      "validation AUC 0.8607916290889633\n",
      "validation AUC 0.8637101421804784\n",
      "validation AUC 0.8649829860007665\n",
      "validation AUC 0.8651559330129588\n",
      "validation AUC 0.8670876868304527\n",
      "validation AUC 0.8683347885491894\n",
      "validation AUC 0.8683570744243932\n",
      "validation AUC 0.8683700425900742\n",
      "validation AUC 0.8686897342148635\n",
      "validation AUC 0.8691250617544369\n",
      "validation AUC 0.869027582335879\n",
      "validation AUC 0.869982604991057\n",
      "validation AUC 0.868891243651878\n",
      "validation AUC 0.8705195546315774\n",
      "validation AUC 0.869236025511048\n",
      "validation AUC 0.8688605180923206\n",
      "validation AUC 0.8689704468435048\n",
      "validation AUC 0.8697263014621193\n",
      "validation AUC 0.8691913766008528\n",
      "validation AUC 0.8694969985374761\n",
      "validation AUC 0.8692208276935738\n",
      "validation AUC 0.8675259278171783\n",
      "validation AUC 0.8678964517778999\n",
      "validation AUC 0.8678318690356815\n",
      "validation AUC 0.866447563911186\n",
      "validation AUC 0.8665202617345136\n",
      "validation AUC 0.8656624364655666\n",
      "validation AUC 0.865855165650889\n",
      "validation AUC 0.8655741975967439\n",
      "validation AUC 0.8658965844927344\n",
      "validation AUC 0.8641765757571823\n",
      "validation AUC 0.8669417962968835\n",
      "validation AUC 0.8654829175680172\n",
      "validation AUC 0.8632391322301648\n",
      "validation AUC 0.8645112283987184\n",
      "validation AUC 0.863888658000789\n",
      "validation AUC 0.8642394396986148\n",
      "validation AUC 0.8645921078202136\n",
      "validation AUC 0.8644831049864806\n",
      "validation AUC 0.8644373838213061\n",
      "validation AUC 0.8644434714624794\n",
      "validation AUC 0.8642999675609611\n",
      "validation AUC 0.8644410768483812\n",
      "validation AUC 0.8647706023550658\n",
      "validation AUC 0.8647391690540083\n",
      "validation AUC 0.8643908245411972\n",
      "validation AUC 0.8642667489420595\n",
      "validation AUC 0.8645742173922205\n",
      "validation AUC 0.8644512991898968\n",
      "validation AUC 0.8644159121148954\n",
      "validation AUC 0.8641451397954426\n",
      "validation AUC 0.864502027759218\n",
      "validation AUC 0.864473340282326\n",
      "validation AUC 0.8647686813424228\n",
      "validation AUC 0.8647438518549107\n",
      "validation AUC 0.8647335230861021\n",
      "validation AUC 0.8649155377036779\n",
      "validation AUC 0.8649155377036779\n",
      "train auc 0.9275684155685091\n",
      "Test AUC 0.8675049638670792\n",
      "test false positive rates [0.         0.         0.         ... 0.99785883 0.99785883 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 4.75923852e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [8.42512589e+01 8.32512589e+01 6.37035141e+01 ... 2.18902398e-02\n",
      " 2.18691304e-02 3.35809169e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.420s | Train Loss: 0.199359 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.404s | Train Loss: 0.153736 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.404s | Train Loss: 0.118902 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.401s | Train Loss: 0.095091 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.402s | Train Loss: 0.078539 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.402s | Train Loss: 0.067190 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.399s | Train Loss: 0.059075 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.399s | Train Loss: 0.052787 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.401s | Train Loss: 0.048328 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.399s | Train Loss: 0.044414 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.401s | Train Loss: 0.041476 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.401s | Train Loss: 0.038626 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.400s | Train Loss: 0.036490 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.402s | Train Loss: 0.034385 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.402s | Train Loss: 0.032869 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.402s | Train Loss: 0.031165 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.399s | Train Loss: 0.029979 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.400s | Train Loss: 0.028691 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.400s | Train Loss: 0.027804 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.400s | Train Loss: 0.027051 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.401s | Train Loss: 0.025869 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.400s | Train Loss: 0.025317 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.400s | Train Loss: 0.024794 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.401s | Train Loss: 0.024264 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.400s | Train Loss: 0.023678 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.403s | Train Loss: 0.023102 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.401s | Train Loss: 0.022768 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.401s | Train Loss: 0.022336 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.402s | Train Loss: 0.022145 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.400s | Train Loss: 0.021688 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.399s | Train Loss: 0.021321 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.398s | Train Loss: 0.021079 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.403s | Train Loss: 0.020828 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.403s | Train Loss: 0.020625 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.403s | Train Loss: 0.020343 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.401s | Train Loss: 0.020258 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.405s | Train Loss: 0.020081 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.382s | Train Loss: 0.019984 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.401s | Train Loss: 0.019743 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.401s | Train Loss: 0.019423 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.402s | Train Loss: 0.019187 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.399s | Train Loss: 0.019293 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.402s | Train Loss: 0.019098 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.399s | Train Loss: 0.019058 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.403s | Train Loss: 0.018980 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.401s | Train Loss: 0.018757 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.402s | Train Loss: 0.018633 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.402s | Train Loss: 0.018478 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.401s | Train Loss: 0.018294 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.401s | Train Loss: 0.018130 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.401s | Train Loss: 0.018261 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.400s | Train Loss: 0.018129 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.400s | Train Loss: 0.017917 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.405s | Train Loss: 0.017813 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.402s | Train Loss: 0.017590 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.401s | Train Loss: 0.017541 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.400s | Train Loss: 0.017555 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.402s | Train Loss: 0.017448 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.401s | Train Loss: 0.017148 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.401s | Train Loss: 0.017153 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.401s | Train Loss: 0.016865 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.401s | Train Loss: 0.016883 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.402s | Train Loss: 0.016886 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.403s | Train Loss: 0.016720 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.403s | Train Loss: 0.016653 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.401s | Train Loss: 0.016520 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.403s | Train Loss: 0.016556 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.400s | Train Loss: 0.016242 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.400s | Train Loss: 0.016242 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.399s | Train Loss: 0.016184 |\n",
      "INFO:root:Pretraining Time: 28.085s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017350\n",
      "INFO:root:Test AUC: 52.26%\n",
      "INFO:root:Test AUC: 52.26%\n",
      "INFO:root:Test Time: 0.227s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.669s | Train Loss: 0.787067 || Validation Loss: 0.016341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.684s | Train Loss: 0.634229 || Validation Loss: 0.016177 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.721s | Train Loss: 0.575224 || Validation Loss: 0.014488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.733s | Train Loss: 0.531965 || Validation Loss: 0.016829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.744s | Train Loss: 0.509642 || Validation Loss: 0.018898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.786s | Train Loss: 0.490680 || Validation Loss: 0.016877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.798s | Train Loss: 0.479043 || Validation Loss: 0.018698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.816s | Train Loss: 0.466828 || Validation Loss: 0.019868 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.829s | Train Loss: 0.454139 || Validation Loss: 0.019861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.838s | Train Loss: 0.444385 || Validation Loss: 0.019692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.887s | Train Loss: 0.439674 || Validation Loss: 0.018879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.866s | Train Loss: 0.433527 || Validation Loss: 0.020904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.942s | Train Loss: 0.430918 || Validation Loss: 0.020514 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.891s | Train Loss: 0.418462 || Validation Loss: 0.020122 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.971s | Train Loss: 0.413224 || Validation Loss: 0.023422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.954s | Train Loss: 0.406550 || Validation Loss: 0.023079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 1.001s | Train Loss: 0.411115 || Validation Loss: 0.021435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.023s | Train Loss: 0.399995 || Validation Loss: 0.023220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.042s | Train Loss: 0.405580 || Validation Loss: 0.021684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.997s | Train Loss: 0.397316 || Validation Loss: 0.022758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.108s | Train Loss: 0.389033 || Validation Loss: 0.023447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.044s | Train Loss: 0.392218 || Validation Loss: 0.023020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.128s | Train Loss: 0.384369 || Validation Loss: 0.023187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.110s | Train Loss: 0.393022 || Validation Loss: 0.022635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.166s | Train Loss: 0.385206 || Validation Loss: 0.023783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.180s | Train Loss: 0.379042 || Validation Loss: 0.023434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.186s | Train Loss: 0.379665 || Validation Loss: 0.023180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.194s | Train Loss: 0.380589 || Validation Loss: 0.022065 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.249s | Train Loss: 0.381253 || Validation Loss: 0.023511 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.241s | Train Loss: 0.376078 || Validation Loss: 0.022490 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.221s | Train Loss: 0.367978 || Validation Loss: 0.023856 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.234s | Train Loss: 0.370211 || Validation Loss: 0.023024 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.319s | Train Loss: 0.368556 || Validation Loss: 0.024530 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.294s | Train Loss: 0.369761 || Validation Loss: 0.024076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.372s | Train Loss: 0.366677 || Validation Loss: 0.023311 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.415s | Train Loss: 0.365849 || Validation Loss: 0.023556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.304s | Train Loss: 0.365447 || Validation Loss: 0.022204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.469s | Train Loss: 0.363502 || Validation Loss: 0.022455 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.335s | Train Loss: 0.360984 || Validation Loss: 0.023701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.525s | Train Loss: 0.352276 || Validation Loss: 0.024761 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.512s | Train Loss: 0.358496 || Validation Loss: 0.023553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.428s | Train Loss: 0.360435 || Validation Loss: 0.026158 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.531s | Train Loss: 0.364138 || Validation Loss: 0.026372 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.463s | Train Loss: 0.359221 || Validation Loss: 0.023038 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.580s | Train Loss: 0.353048 || Validation Loss: 0.022413 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.731s | Train Loss: 0.352844 || Validation Loss: 0.025887 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.566s | Train Loss: 0.339442 || Validation Loss: 0.026138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.627s | Train Loss: 0.351017 || Validation Loss: 0.025940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.517s | Train Loss: 0.344434 || Validation Loss: 0.026626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.764s | Train Loss: 0.350467 || Validation Loss: 0.027355 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.583s | Train Loss: 0.353431 || Validation Loss: 0.025240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.637s | Train Loss: 0.348525 || Validation Loss: 0.025082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.730s | Train Loss: 0.344710 || Validation Loss: 0.026598 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.707s | Train Loss: 0.346574 || Validation Loss: 0.024962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.621s | Train Loss: 0.348361 || Validation Loss: 0.024117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.771s | Train Loss: 0.347218 || Validation Loss: 0.026982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.809s | Train Loss: 0.343903 || Validation Loss: 0.025041 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.603s | Train Loss: 0.347759 || Validation Loss: 0.026172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.874s | Train Loss: 0.342240 || Validation Loss: 0.024225 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.718s | Train Loss: 0.349835 || Validation Loss: 0.028359 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.754s | Train Loss: 0.347793 || Validation Loss: 0.027110 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.915s | Train Loss: 0.342094 || Validation Loss: 0.025626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.885s | Train Loss: 0.353964 || Validation Loss: 0.024524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.973s | Train Loss: 0.358241 || Validation Loss: 0.025003 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.801s | Train Loss: 0.346535 || Validation Loss: 0.026052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.940s | Train Loss: 0.341082 || Validation Loss: 0.025458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.876s | Train Loss: 0.352412 || Validation Loss: 0.024954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.893s | Train Loss: 0.347884 || Validation Loss: 0.025584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.895s | Train Loss: 0.348669 || Validation Loss: 0.027285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.101s | Train Loss: 0.343220 || Validation Loss: 0.026545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 95.773s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.926%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022866\n",
      "INFO:root:Test AUC: 86.49%\n",
      "INFO:root:Test Time: 0.196s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.665475534174549\n",
      "validation AUC 0.7517541426291756\n",
      "validation AUC 0.7894433357676802\n",
      "validation AUC 0.8103532417647094\n",
      "validation AUC 0.8248050730910723\n",
      "validation AUC 0.8313196606544468\n",
      "validation AUC 0.8385787603476597\n",
      "validation AUC 0.8432735529453435\n",
      "validation AUC 0.8457174535226689\n",
      "validation AUC 0.8490733801287048\n",
      "validation AUC 0.8527072921639925\n",
      "validation AUC 0.8530267257025959\n",
      "validation AUC 0.8550000659849218\n",
      "validation AUC 0.8580366867651062\n",
      "validation AUC 0.8587778730420305\n",
      "validation AUC 0.8586079884752011\n",
      "validation AUC 0.8602441192002713\n",
      "validation AUC 0.8614810438090923\n",
      "validation AUC 0.8610925150110375\n",
      "validation AUC 0.8618135971722695\n",
      "validation AUC 0.8624340363636518\n",
      "validation AUC 0.8620906167738354\n",
      "validation AUC 0.8625441353985054\n",
      "validation AUC 0.8623993490481037\n",
      "validation AUC 0.86353280504244\n",
      "validation AUC 0.8624745372700933\n",
      "validation AUC 0.8628520774501214\n",
      "validation AUC 0.8622080406671458\n",
      "validation AUC 0.8634762309540377\n",
      "validation AUC 0.8635252726507613\n",
      "validation AUC 0.8641040961098055\n",
      "validation AUC 0.8647134775054208\n",
      "validation AUC 0.8635833473639982\n",
      "validation AUC 0.8625651281820962\n",
      "validation AUC 0.8629811764174998\n",
      "validation AUC 0.8627218583354814\n",
      "validation AUC 0.8636317611396916\n",
      "validation AUC 0.8619629998065153\n",
      "validation AUC 0.8609571421547227\n",
      "validation AUC 0.8618737365749951\n",
      "validation AUC 0.8621313890698745\n",
      "validation AUC 0.8614293680368612\n",
      "validation AUC 0.8603503150141452\n",
      "validation AUC 0.8602965053746847\n",
      "validation AUC 0.8605231502776796\n",
      "validation AUC 0.8605334417969355\n",
      "validation AUC 0.8582285405859419\n",
      "validation AUC 0.8578614302806509\n",
      "validation AUC 0.8599958988242552\n",
      "validation AUC 0.8597446665558404\n",
      "validation AUC 0.8595724964628889\n",
      "validation AUC 0.8595930316091189\n",
      "validation AUC 0.8595716556872722\n",
      "validation AUC 0.8596364938549944\n",
      "validation AUC 0.8594659361355357\n",
      "validation AUC 0.8594344176926435\n",
      "validation AUC 0.859557711051176\n",
      "validation AUC 0.8595625588143828\n",
      "validation AUC 0.8596002739864237\n",
      "validation AUC 0.8595625109221008\n",
      "validation AUC 0.8594169795806467\n",
      "validation AUC 0.8595714534754152\n",
      "validation AUC 0.8592597731651241\n",
      "validation AUC 0.8591434827224868\n",
      "validation AUC 0.8590189920569055\n",
      "validation AUC 0.8588964357073786\n",
      "validation AUC 0.8586710280216677\n",
      "validation AUC 0.8589824396030432\n",
      "validation AUC 0.8589478427506942\n",
      "validation AUC 0.8588535508295687\n",
      "validation AUC 0.8588535508295687\n",
      "train auc 0.9259378800355774\n",
      "Test AUC 0.8648896597198166\n",
      "test false positive rates [0.         0.         0.         ... 0.99418285 0.99418285 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.31578947e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [8.63328094e+01 8.53328094e+01 4.83906898e+01 ... 2.64247376e-02\n",
      " 2.64143907e-02 4.13918355e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.426s | Train Loss: 0.193569 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.405s | Train Loss: 0.149418 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.402s | Train Loss: 0.116087 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.403s | Train Loss: 0.092674 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.399s | Train Loss: 0.076530 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.403s | Train Loss: 0.065251 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.057287 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.398s | Train Loss: 0.050710 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.403s | Train Loss: 0.045841 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.401s | Train Loss: 0.041909 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.396s | Train Loss: 0.038955 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.404s | Train Loss: 0.036404 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.395s | Train Loss: 0.034242 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.394s | Train Loss: 0.032494 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.404s | Train Loss: 0.030974 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.399s | Train Loss: 0.029670 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.384s | Train Loss: 0.028654 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.403s | Train Loss: 0.027895 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.404s | Train Loss: 0.027065 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.404s | Train Loss: 0.026186 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.403s | Train Loss: 0.025657 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.401s | Train Loss: 0.025010 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.404s | Train Loss: 0.024636 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.402s | Train Loss: 0.024130 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.404s | Train Loss: 0.023783 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.406s | Train Loss: 0.023264 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.400s | Train Loss: 0.023133 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.402s | Train Loss: 0.022800 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.405s | Train Loss: 0.022594 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.403s | Train Loss: 0.022163 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.403s | Train Loss: 0.021959 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.405s | Train Loss: 0.021715 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.402s | Train Loss: 0.021724 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.405s | Train Loss: 0.021327 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.405s | Train Loss: 0.021288 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.406s | Train Loss: 0.021110 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.404s | Train Loss: 0.020723 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.408s | Train Loss: 0.020652 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.403s | Train Loss: 0.020546 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.403s | Train Loss: 0.020550 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.401s | Train Loss: 0.020314 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.397s | Train Loss: 0.020178 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.397s | Train Loss: 0.020009 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.405s | Train Loss: 0.019876 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.403s | Train Loss: 0.019839 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.400s | Train Loss: 0.019741 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.402s | Train Loss: 0.019754 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.409s | Train Loss: 0.019709 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.400s | Train Loss: 0.019309 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.401s | Train Loss: 0.019154 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.396s | Train Loss: 0.019472 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.390s | Train Loss: 0.019302 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.395s | Train Loss: 0.019044 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.405s | Train Loss: 0.019076 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.399s | Train Loss: 0.018809 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.392s | Train Loss: 0.018682 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.391s | Train Loss: 0.018488 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.402s | Train Loss: 0.018443 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.382s | Train Loss: 0.018316 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.401s | Train Loss: 0.018164 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.390s | Train Loss: 0.018256 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.398s | Train Loss: 0.018119 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.400s | Train Loss: 0.018127 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.399s | Train Loss: 0.018064 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.395s | Train Loss: 0.017985 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.396s | Train Loss: 0.017756 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.382s | Train Loss: 0.017573 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.392s | Train Loss: 0.017441 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.397s | Train Loss: 0.017348 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.401s | Train Loss: 0.017385 |\n",
      "INFO:root:Pretraining Time: 28.025s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018307\n",
      "INFO:root:Test AUC: 51.53%\n",
      "INFO:root:Test AUC: 51.53%\n",
      "INFO:root:Test Time: 0.231s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.696s | Train Loss: 0.676535 || Validation Loss: 0.014827 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.697s | Train Loss: 0.577720 || Validation Loss: 0.015391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.724s | Train Loss: 0.530477 || Validation Loss: 0.014970 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.766s | Train Loss: 0.507262 || Validation Loss: 0.016191 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.759s | Train Loss: 0.490769 || Validation Loss: 0.017375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.795s | Train Loss: 0.476715 || Validation Loss: 0.018368 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.808s | Train Loss: 0.453552 || Validation Loss: 0.017662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.808s | Train Loss: 0.456456 || Validation Loss: 0.016823 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.834s | Train Loss: 0.441309 || Validation Loss: 0.020018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.846s | Train Loss: 0.438197 || Validation Loss: 0.020392 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.896s | Train Loss: 0.437870 || Validation Loss: 0.019179 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.863s | Train Loss: 0.422237 || Validation Loss: 0.018423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.965s | Train Loss: 0.420884 || Validation Loss: 0.020182 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.895s | Train Loss: 0.417980 || Validation Loss: 0.020657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.978s | Train Loss: 0.408846 || Validation Loss: 0.020680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.957s | Train Loss: 0.411500 || Validation Loss: 0.019679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 1.020s | Train Loss: 0.401874 || Validation Loss: 0.020105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.030s | Train Loss: 0.405874 || Validation Loss: 0.021291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.056s | Train Loss: 0.392471 || Validation Loss: 0.023117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1.028s | Train Loss: 0.387772 || Validation Loss: 0.021880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.115s | Train Loss: 0.394819 || Validation Loss: 0.020281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.056s | Train Loss: 0.388201 || Validation Loss: 0.022080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.139s | Train Loss: 0.388990 || Validation Loss: 0.024167 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.128s | Train Loss: 0.388949 || Validation Loss: 0.023426 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.163s | Train Loss: 0.375509 || Validation Loss: 0.022950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.186s | Train Loss: 0.383725 || Validation Loss: 0.020878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.204s | Train Loss: 0.368222 || Validation Loss: 0.022077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.214s | Train Loss: 0.375480 || Validation Loss: 0.022884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.286s | Train Loss: 0.375776 || Validation Loss: 0.022490 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.267s | Train Loss: 0.368929 || Validation Loss: 0.024334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.241s | Train Loss: 0.369302 || Validation Loss: 0.022638 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.249s | Train Loss: 0.366554 || Validation Loss: 0.022814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.330s | Train Loss: 0.362220 || Validation Loss: 0.024163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.318s | Train Loss: 0.360301 || Validation Loss: 0.024745 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.341s | Train Loss: 0.363318 || Validation Loss: 0.024576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.433s | Train Loss: 0.357783 || Validation Loss: 0.024103 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.328s | Train Loss: 0.368322 || Validation Loss: 0.025336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.478s | Train Loss: 0.355080 || Validation Loss: 0.024801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.363s | Train Loss: 0.353636 || Validation Loss: 0.025807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.502s | Train Loss: 0.358609 || Validation Loss: 0.025123 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.498s | Train Loss: 0.348952 || Validation Loss: 0.024975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.409s | Train Loss: 0.358572 || Validation Loss: 0.025134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.517s | Train Loss: 0.348131 || Validation Loss: 0.028004 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.461s | Train Loss: 0.365606 || Validation Loss: 0.023585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.578s | Train Loss: 0.354752 || Validation Loss: 0.022172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.648s | Train Loss: 0.348694 || Validation Loss: 0.024941 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.538s | Train Loss: 0.342842 || Validation Loss: 0.025193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.604s | Train Loss: 0.358490 || Validation Loss: 0.025680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.502s | Train Loss: 0.348426 || Validation Loss: 0.024897 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.715s | Train Loss: 0.346791 || Validation Loss: 0.025429 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.601s | Train Loss: 0.344574 || Validation Loss: 0.025993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.652s | Train Loss: 0.345046 || Validation Loss: 0.025688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.762s | Train Loss: 0.349392 || Validation Loss: 0.025604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.733s | Train Loss: 0.338988 || Validation Loss: 0.025960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.648s | Train Loss: 0.352491 || Validation Loss: 0.023868 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.814s | Train Loss: 0.349207 || Validation Loss: 0.024561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.830s | Train Loss: 0.338883 || Validation Loss: 0.026034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.636s | Train Loss: 0.341640 || Validation Loss: 0.027017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.900s | Train Loss: 0.345538 || Validation Loss: 0.025385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.726s | Train Loss: 0.350329 || Validation Loss: 0.026327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.783s | Train Loss: 0.340981 || Validation Loss: 0.023578 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.932s | Train Loss: 0.344809 || Validation Loss: 0.025702 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.915s | Train Loss: 0.338397 || Validation Loss: 0.026441 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.994s | Train Loss: 0.346809 || Validation Loss: 0.024260 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.812s | Train Loss: 0.346611 || Validation Loss: 0.025506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.971s | Train Loss: 0.340492 || Validation Loss: 0.026924 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.896s | Train Loss: 0.340843 || Validation Loss: 0.025391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.914s | Train Loss: 0.343362 || Validation Loss: 0.022808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.932s | Train Loss: 0.345317 || Validation Loss: 0.026227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.112s | Train Loss: 0.342303 || Validation Loss: 0.025212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 96.477s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.931%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022294\n",
      "INFO:root:Test AUC: 85.93%\n",
      "INFO:root:Test Time: 0.206s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7589953518943952\n",
      "validation AUC 0.802942578963198\n",
      "validation AUC 0.8238793099594872\n",
      "validation AUC 0.833124531852944\n",
      "validation AUC 0.8381066674996375\n",
      "validation AUC 0.8432777807695673\n",
      "validation AUC 0.8461193096785193\n",
      "validation AUC 0.8480278623460874\n",
      "validation AUC 0.8495294689725061\n",
      "validation AUC 0.8511087488982114\n",
      "validation AUC 0.8517258755400919\n",
      "validation AUC 0.8533114586222391\n",
      "validation AUC 0.8539250359138902\n",
      "validation AUC 0.8545758654188562\n",
      "validation AUC 0.854012875680363\n",
      "validation AUC 0.8556985057395176\n",
      "validation AUC 0.8568034738081367\n",
      "validation AUC 0.8578161773955666\n",
      "validation AUC 0.8571167957594259\n",
      "validation AUC 0.8579428844095298\n",
      "validation AUC 0.8579700792436341\n",
      "validation AUC 0.8583240936704921\n",
      "validation AUC 0.8585385579697759\n",
      "validation AUC 0.8585679984197676\n",
      "validation AUC 0.8596958164282448\n",
      "validation AUC 0.8580713927054306\n",
      "validation AUC 0.8583486996606884\n",
      "validation AUC 0.8588696532790355\n",
      "validation AUC 0.8579826483069652\n",
      "validation AUC 0.8589595417709033\n",
      "validation AUC 0.8577696207761402\n",
      "validation AUC 0.857503720166035\n",
      "validation AUC 0.8578898410465805\n",
      "validation AUC 0.8584542329646622\n",
      "validation AUC 0.8580984757908772\n",
      "validation AUC 0.8577655073612566\n",
      "validation AUC 0.8558395032782798\n",
      "validation AUC 0.857308630955485\n",
      "validation AUC 0.8556722980185579\n",
      "validation AUC 0.8579813552153523\n",
      "validation AUC 0.8568958367345721\n",
      "validation AUC 0.8565808199286384\n",
      "validation AUC 0.8566843657029107\n",
      "validation AUC 0.8568061850434319\n",
      "validation AUC 0.8565940222343644\n",
      "validation AUC 0.8573676714964082\n",
      "validation AUC 0.8579514358425412\n",
      "validation AUC 0.8559821158512172\n",
      "validation AUC 0.8562863489116639\n",
      "validation AUC 0.8564499116972748\n",
      "validation AUC 0.8562497964578016\n",
      "validation AUC 0.8563121176200389\n",
      "validation AUC 0.8564701940786834\n",
      "validation AUC 0.8566807711210818\n",
      "validation AUC 0.8563328975490433\n",
      "validation AUC 0.85624282014873\n",
      "validation AUC 0.8563003494220892\n",
      "validation AUC 0.8565366898514978\n",
      "validation AUC 0.8563293295740375\n",
      "validation AUC 0.8563096777743413\n",
      "validation AUC 0.8563526956862677\n",
      "validation AUC 0.8565116129205288\n",
      "validation AUC 0.8566025630246467\n",
      "validation AUC 0.8563341161415509\n",
      "validation AUC 0.8565417212017855\n",
      "validation AUC 0.8565992158862744\n",
      "validation AUC 0.8564625712238053\n",
      "validation AUC 0.8562373444644928\n",
      "validation AUC 0.8562597500704017\n",
      "validation AUC 0.8562442436137769\n",
      "validation AUC 0.8562442436137769\n",
      "train auc 0.9311406704551766\n",
      "Test AUC 0.8593171347439928\n",
      "test false positive rates [0.         0.         0.         ... 0.99861677 0.99861677 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 8.39865622e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.12599335e+01 7.02599335e+01 6.12841530e+01 ... 1.96908955e-02\n",
      " 1.96752343e-02 6.57476764e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.409s | Train Loss: 0.197729 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.392s | Train Loss: 0.152684 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.395s | Train Loss: 0.117934 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.392s | Train Loss: 0.093702 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.393s | Train Loss: 0.077733 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.393s | Train Loss: 0.067132 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.060272 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.395s | Train Loss: 0.054736 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.392s | Train Loss: 0.050552 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.395s | Train Loss: 0.046874 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.393s | Train Loss: 0.043809 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.394s | Train Loss: 0.041054 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.395s | Train Loss: 0.038239 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.398s | Train Loss: 0.035774 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.396s | Train Loss: 0.033943 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.394s | Train Loss: 0.031762 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.394s | Train Loss: 0.030366 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.397s | Train Loss: 0.029228 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.396s | Train Loss: 0.028204 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.393s | Train Loss: 0.027253 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.393s | Train Loss: 0.026615 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.395s | Train Loss: 0.025758 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.395s | Train Loss: 0.024877 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.391s | Train Loss: 0.024495 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.390s | Train Loss: 0.023927 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.395s | Train Loss: 0.023523 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.395s | Train Loss: 0.022940 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.392s | Train Loss: 0.022802 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.394s | Train Loss: 0.022211 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.395s | Train Loss: 0.021979 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.392s | Train Loss: 0.021699 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.394s | Train Loss: 0.021583 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.395s | Train Loss: 0.021238 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.397s | Train Loss: 0.021067 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.395s | Train Loss: 0.021031 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.396s | Train Loss: 0.020612 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.392s | Train Loss: 0.020531 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.391s | Train Loss: 0.020211 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.392s | Train Loss: 0.020142 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.390s | Train Loss: 0.019855 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.395s | Train Loss: 0.019811 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.397s | Train Loss: 0.019656 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.397s | Train Loss: 0.019510 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.386s | Train Loss: 0.019441 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.394s | Train Loss: 0.019314 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.384s | Train Loss: 0.019274 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.387s | Train Loss: 0.019036 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.396s | Train Loss: 0.018998 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.394s | Train Loss: 0.018634 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.393s | Train Loss: 0.018589 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.391s | Train Loss: 0.018435 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.396s | Train Loss: 0.018294 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.394s | Train Loss: 0.018351 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.391s | Train Loss: 0.018195 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.395s | Train Loss: 0.018201 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.388s | Train Loss: 0.018030 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.392s | Train Loss: 0.017949 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.386s | Train Loss: 0.017962 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.388s | Train Loss: 0.017792 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.397s | Train Loss: 0.017744 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.386s | Train Loss: 0.017606 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.391s | Train Loss: 0.017503 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.396s | Train Loss: 0.017407 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.395s | Train Loss: 0.017392 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.394s | Train Loss: 0.017231 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.394s | Train Loss: 0.017108 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.393s | Train Loss: 0.017245 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.399s | Train Loss: 0.017075 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.400s | Train Loss: 0.017163 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.394s | Train Loss: 0.017159 |\n",
      "INFO:root:Pretraining Time: 27.560s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018592\n",
      "INFO:root:Test AUC: 51.01%\n",
      "INFO:root:Test AUC: 51.01%\n",
      "INFO:root:Test Time: 0.217s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.656s | Train Loss: 0.816310 || Validation Loss: 0.014443 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.667s | Train Loss: 0.659735 || Validation Loss: 0.014048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.704s | Train Loss: 0.591872 || Validation Loss: 0.015643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.716s | Train Loss: 0.552375 || Validation Loss: 0.015920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.732s | Train Loss: 0.533186 || Validation Loss: 0.016584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.778s | Train Loss: 0.497828 || Validation Loss: 0.017671 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.788s | Train Loss: 0.488343 || Validation Loss: 0.018508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.796s | Train Loss: 0.477368 || Validation Loss: 0.018294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.811s | Train Loss: 0.458209 || Validation Loss: 0.019402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.822s | Train Loss: 0.446061 || Validation Loss: 0.019094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.879s | Train Loss: 0.441057 || Validation Loss: 0.019544 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.851s | Train Loss: 0.434663 || Validation Loss: 0.020966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.929s | Train Loss: 0.421319 || Validation Loss: 0.022074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.877s | Train Loss: 0.423263 || Validation Loss: 0.019522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.961s | Train Loss: 0.411258 || Validation Loss: 0.022278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.933s | Train Loss: 0.411502 || Validation Loss: 0.021128 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.990s | Train Loss: 0.403050 || Validation Loss: 0.022188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 1.011s | Train Loss: 0.405844 || Validation Loss: 0.020849 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 1.024s | Train Loss: 0.397415 || Validation Loss: 0.022184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 1.008s | Train Loss: 0.396123 || Validation Loss: 0.023778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.109s | Train Loss: 0.391224 || Validation Loss: 0.022747 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 1.035s | Train Loss: 0.385155 || Validation Loss: 0.023608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.117s | Train Loss: 0.382220 || Validation Loss: 0.023510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.093s | Train Loss: 0.382782 || Validation Loss: 0.022290 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.143s | Train Loss: 0.379219 || Validation Loss: 0.020688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.161s | Train Loss: 0.379757 || Validation Loss: 0.023999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.182s | Train Loss: 0.374047 || Validation Loss: 0.022397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.191s | Train Loss: 0.370676 || Validation Loss: 0.023640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.249s | Train Loss: 0.375441 || Validation Loss: 0.023899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.230s | Train Loss: 0.368699 || Validation Loss: 0.023715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.207s | Train Loss: 0.365979 || Validation Loss: 0.021768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.200s | Train Loss: 0.359997 || Validation Loss: 0.021591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.307s | Train Loss: 0.369461 || Validation Loss: 0.022348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.293s | Train Loss: 0.361217 || Validation Loss: 0.024055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.292s | Train Loss: 0.365822 || Validation Loss: 0.023497 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.406s | Train Loss: 0.361181 || Validation Loss: 0.022360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.299s | Train Loss: 0.355471 || Validation Loss: 0.023526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.431s | Train Loss: 0.355776 || Validation Loss: 0.022408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.327s | Train Loss: 0.351212 || Validation Loss: 0.025877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.469s | Train Loss: 0.347948 || Validation Loss: 0.023710 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.459s | Train Loss: 0.355875 || Validation Loss: 0.023248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.385s | Train Loss: 0.348137 || Validation Loss: 0.024101 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.475s | Train Loss: 0.344138 || Validation Loss: 0.024468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.448s | Train Loss: 0.345784 || Validation Loss: 0.024576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.541s | Train Loss: 0.353507 || Validation Loss: 0.023733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.609s | Train Loss: 0.342025 || Validation Loss: 0.024019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.489s | Train Loss: 0.343761 || Validation Loss: 0.024309 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.574s | Train Loss: 0.350463 || Validation Loss: 0.023957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.477s | Train Loss: 0.350529 || Validation Loss: 0.024159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.675s | Train Loss: 0.340310 || Validation Loss: 0.024668 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.563s | Train Loss: 0.339908 || Validation Loss: 0.025286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.608s | Train Loss: 0.336408 || Validation Loss: 0.023949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.705s | Train Loss: 0.337996 || Validation Loss: 0.024300 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.704s | Train Loss: 0.339467 || Validation Loss: 0.025375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.607s | Train Loss: 0.342861 || Validation Loss: 0.023068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.747s | Train Loss: 0.338739 || Validation Loss: 0.023482 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.794s | Train Loss: 0.343609 || Validation Loss: 0.025476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.609s | Train Loss: 0.344783 || Validation Loss: 0.026037 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.856s | Train Loss: 0.341312 || Validation Loss: 0.025288 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.692s | Train Loss: 0.337774 || Validation Loss: 0.024323 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.742s | Train Loss: 0.335332 || Validation Loss: 0.026704 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.898s | Train Loss: 0.337012 || Validation Loss: 0.026413 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.863s | Train Loss: 0.341859 || Validation Loss: 0.025221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.955s | Train Loss: 0.344722 || Validation Loss: 0.024481 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.780s | Train Loss: 0.346804 || Validation Loss: 0.024770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.944s | Train Loss: 0.341323 || Validation Loss: 0.021221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.863s | Train Loss: 0.344233 || Validation Loss: 0.022960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.882s | Train Loss: 0.344049 || Validation Loss: 0.024541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.886s | Train Loss: 0.339409 || Validation Loss: 0.025390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 2.082s | Train Loss: 0.337852 || Validation Loss: 0.024818 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 94.256s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.929%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021386\n",
      "INFO:root:Test AUC: 85.90%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6379366206311862\n",
      "validation AUC 0.7238632633843496\n",
      "validation AUC 0.7646768904626883\n",
      "validation AUC 0.7882358037165688\n",
      "validation AUC 0.8051822721716255\n",
      "validation AUC 0.8191732201046265\n",
      "validation AUC 0.8308330591056744\n",
      "validation AUC 0.8368481966853007\n",
      "validation AUC 0.842702801996491\n",
      "validation AUC 0.8471450798768764\n",
      "validation AUC 0.8500892472674261\n",
      "validation AUC 0.8530542211938034\n",
      "validation AUC 0.8559955363328943\n",
      "validation AUC 0.8565910502522008\n",
      "validation AUC 0.8579463752247478\n",
      "validation AUC 0.8584653625988523\n",
      "validation AUC 0.8599741477461998\n",
      "validation AUC 0.8598806726545499\n",
      "validation AUC 0.8603765227351048\n",
      "validation AUC 0.8619637022266505\n",
      "validation AUC 0.8614591863037439\n",
      "validation AUC 0.8619686856846565\n",
      "validation AUC 0.862928103787045\n",
      "validation AUC 0.8618723237526774\n",
      "validation AUC 0.8625831010912416\n",
      "validation AUC 0.8637902500040974\n",
      "validation AUC 0.863678083619073\n",
      "validation AUC 0.8611307144272626\n",
      "validation AUC 0.8616931693686074\n",
      "validation AUC 0.863135245888554\n",
      "validation AUC 0.8612627720733932\n",
      "validation AUC 0.8630582617059912\n",
      "validation AUC 0.8624727413095197\n",
      "validation AUC 0.8616658202149279\n",
      "validation AUC 0.8620323265453297\n",
      "validation AUC 0.8626894751708318\n",
      "validation AUC 0.8621626786940859\n",
      "validation AUC 0.8633418851211027\n",
      "validation AUC 0.8619676932501469\n",
      "validation AUC 0.8623941420927821\n",
      "validation AUC 0.8610286160641782\n",
      "validation AUC 0.8615722626421258\n",
      "validation AUC 0.8615766022150075\n",
      "validation AUC 0.8616808876589679\n",
      "validation AUC 0.8596011839397809\n",
      "validation AUC 0.8615759503478364\n",
      "validation AUC 0.8607597301684977\n",
      "validation AUC 0.8596710774039318\n",
      "validation AUC 0.8578329024446988\n",
      "validation AUC 0.8584013731887938\n",
      "validation AUC 0.8585949963633795\n",
      "validation AUC 0.8582917850049478\n",
      "validation AUC 0.8584236883315032\n",
      "validation AUC 0.8586732949230137\n",
      "validation AUC 0.8588944934092768\n",
      "validation AUC 0.8590851499230636\n",
      "validation AUC 0.8590407191888197\n",
      "validation AUC 0.8589973926377429\n",
      "validation AUC 0.8585823155513902\n",
      "validation AUC 0.8587277750544215\n",
      "validation AUC 0.8590228527069675\n",
      "validation AUC 0.8589026909715386\n",
      "validation AUC 0.8588341730801526\n",
      "validation AUC 0.8586202781668878\n",
      "validation AUC 0.8585314087163527\n",
      "validation AUC 0.859147005465893\n",
      "validation AUC 0.8588905795455681\n",
      "validation AUC 0.8588483039640122\n",
      "validation AUC 0.8588338511375908\n",
      "validation AUC 0.8588186799269398\n",
      "validation AUC 0.8588186799269398\n",
      "train auc 0.9285555620599402\n",
      "Test AUC 0.8590120485506781\n",
      "test false positive rates [0.         0.         0.         ... 0.99348176 0.99348176 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 6.15901456e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.16299820e+01 6.06299820e+01 4.34470520e+01 ... 1.79578084e-02\n",
      " 1.78949945e-02 2.98341061e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {4..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d30aeac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# balanced without loss minority"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b39d63e",
   "metadata": {},
   "source": [
    "###### %%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test dta --experiment_name \"5_per_labeled_5_unlabeled_balanced_exclude_loss\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 0 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51f4f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclude balance exclude loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "078a7c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Itration 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.423s | Train Loss: 0.197379 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.389s | Train Loss: 0.155137 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.380s | Train Loss: 0.122613 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.379s | Train Loss: 0.099537 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.379s | Train Loss: 0.083070 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.380s | Train Loss: 0.071510 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.378s | Train Loss: 0.062804 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.380s | Train Loss: 0.056283 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.380s | Train Loss: 0.051424 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.383s | Train Loss: 0.047023 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.380s | Train Loss: 0.043510 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.381s | Train Loss: 0.040869 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.380s | Train Loss: 0.038300 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.380s | Train Loss: 0.036422 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.382s | Train Loss: 0.035014 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.381s | Train Loss: 0.033367 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.379s | Train Loss: 0.032151 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.381s | Train Loss: 0.030934 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.380s | Train Loss: 0.029830 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.379s | Train Loss: 0.028985 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.381s | Train Loss: 0.028106 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.381s | Train Loss: 0.027649 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.380s | Train Loss: 0.026956 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.379s | Train Loss: 0.026136 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.379s | Train Loss: 0.025889 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.380s | Train Loss: 0.025072 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.379s | Train Loss: 0.025056 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.380s | Train Loss: 0.024632 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.383s | Train Loss: 0.024358 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.380s | Train Loss: 0.024077 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.377s | Train Loss: 0.023597 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.381s | Train Loss: 0.023505 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.383s | Train Loss: 0.023354 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.381s | Train Loss: 0.022897 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.381s | Train Loss: 0.022579 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.382s | Train Loss: 0.022687 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.380s | Train Loss: 0.022439 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.381s | Train Loss: 0.022333 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.377s | Train Loss: 0.021898 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.382s | Train Loss: 0.021767 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.382s | Train Loss: 0.021604 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.393s | Train Loss: 0.021558 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.394s | Train Loss: 0.021220 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.394s | Train Loss: 0.021256 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.400s | Train Loss: 0.021089 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.386s | Train Loss: 0.020952 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.395s | Train Loss: 0.020899 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.395s | Train Loss: 0.020527 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.394s | Train Loss: 0.020344 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.394s | Train Loss: 0.020410 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.387s | Train Loss: 0.020216 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.390s | Train Loss: 0.019976 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.397s | Train Loss: 0.019776 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.395s | Train Loss: 0.019701 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.395s | Train Loss: 0.019766 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.394s | Train Loss: 0.019787 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.394s | Train Loss: 0.019427 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.395s | Train Loss: 0.019231 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.389s | Train Loss: 0.019262 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.396s | Train Loss: 0.018991 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.395s | Train Loss: 0.018990 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.395s | Train Loss: 0.018965 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.389s | Train Loss: 0.018774 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.386s | Train Loss: 0.018603 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.394s | Train Loss: 0.018553 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.395s | Train Loss: 0.018319 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.394s | Train Loss: 0.018362 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.397s | Train Loss: 0.018220 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.392s | Train Loss: 0.018180 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.393s | Train Loss: 0.017868 |\n",
      "INFO:root:Pretraining Time: 27.056s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017687\n",
      "INFO:root:Test AUC: 61.85%\n",
      "INFO:root:Test AUC: 61.85%\n",
      "INFO:root:Test Time: 0.239s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.663s | Train Loss: 1.010227 || Validation Loss: 0.606303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.680s | Train Loss: 0.544896 || Validation Loss: 0.350672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.722s | Train Loss: 0.438448 || Validation Loss: 0.269283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.758s | Train Loss: 0.392764 || Validation Loss: 0.232381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.736s | Train Loss: 0.376905 || Validation Loss: 0.218410 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.744s | Train Loss: 0.382493 || Validation Loss: 0.212560 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.788s | Train Loss: 0.367038 || Validation Loss: 0.206352 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.799s | Train Loss: 0.370645 || Validation Loss: 0.203591 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.779s | Train Loss: 0.353902 || Validation Loss: 0.195667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.852s | Train Loss: 0.365654 || Validation Loss: 0.197353 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.870s | Train Loss: 0.357770 || Validation Loss: 0.193032 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.839s | Train Loss: 0.345384 || Validation Loss: 0.185194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.888s | Train Loss: 0.351950 || Validation Loss: 0.189855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.897s | Train Loss: 0.347993 || Validation Loss: 0.182689 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.907s | Train Loss: 0.348354 || Validation Loss: 0.186250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.908s | Train Loss: 0.353287 || Validation Loss: 0.188828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.957s | Train Loss: 0.351347 || Validation Loss: 0.185992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.933s | Train Loss: 0.332380 || Validation Loss: 0.179778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.983s | Train Loss: 0.340971 || Validation Loss: 0.179036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.969s | Train Loss: 0.333662 || Validation Loss: 0.178508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.056s | Train Loss: 0.339092 || Validation Loss: 0.177608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.997s | Train Loss: 0.335395 || Validation Loss: 0.176561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.043s | Train Loss: 0.335206 || Validation Loss: 0.180233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.059s | Train Loss: 0.336240 || Validation Loss: 0.176688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.057s | Train Loss: 0.338920 || Validation Loss: 0.178466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.107s | Train Loss: 0.330969 || Validation Loss: 0.177841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.060s | Train Loss: 0.347814 || Validation Loss: 0.182714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.176s | Train Loss: 0.335240 || Validation Loss: 0.178153 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.131s | Train Loss: 0.334748 || Validation Loss: 0.180131 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.122s | Train Loss: 0.335837 || Validation Loss: 0.176657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.218s | Train Loss: 0.331111 || Validation Loss: 0.174200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.164s | Train Loss: 0.324575 || Validation Loss: 0.176709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.219s | Train Loss: 0.331763 || Validation Loss: 0.174532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.193s | Train Loss: 0.329166 || Validation Loss: 0.174374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.230s | Train Loss: 0.322218 || Validation Loss: 0.168280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.294s | Train Loss: 0.335371 || Validation Loss: 0.173264 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.263s | Train Loss: 0.332903 || Validation Loss: 0.172008 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.255s | Train Loss: 0.327898 || Validation Loss: 0.172412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.295s | Train Loss: 0.323741 || Validation Loss: 0.170318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.270s | Train Loss: 0.335195 || Validation Loss: 0.177029 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.320s | Train Loss: 0.337554 || Validation Loss: 0.176408 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.344s | Train Loss: 0.326900 || Validation Loss: 0.170208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.313s | Train Loss: 0.337991 || Validation Loss: 0.179149 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.366s | Train Loss: 0.321776 || Validation Loss: 0.172191 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.413s | Train Loss: 0.338541 || Validation Loss: 0.178461 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.438s | Train Loss: 0.324550 || Validation Loss: 0.176713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.434s | Train Loss: 0.322099 || Validation Loss: 0.169119 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.517s | Train Loss: 0.323268 || Validation Loss: 0.173032 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.392s | Train Loss: 0.327979 || Validation Loss: 0.174050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.523s | Train Loss: 0.323657 || Validation Loss: 0.172847 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.594s | Train Loss: 0.322982 || Validation Loss: 0.172149 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.479s | Train Loss: 0.324123 || Validation Loss: 0.171896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.566s | Train Loss: 0.322019 || Validation Loss: 0.171535 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.640s | Train Loss: 0.329125 || Validation Loss: 0.171487 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.571s | Train Loss: 0.311588 || Validation Loss: 0.170480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.488s | Train Loss: 0.326939 || Validation Loss: 0.170459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.672s | Train Loss: 0.312066 || Validation Loss: 0.169687 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.641s | Train Loss: 0.319321 || Validation Loss: 0.169390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.698s | Train Loss: 0.319959 || Validation Loss: 0.169051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.680s | Train Loss: 0.316852 || Validation Loss: 0.168584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.738s | Train Loss: 0.336898 || Validation Loss: 0.169427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.612s | Train Loss: 0.328669 || Validation Loss: 0.169800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.755s | Train Loss: 0.317503 || Validation Loss: 0.168856 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.705s | Train Loss: 0.327689 || Validation Loss: 0.169605 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.597s | Train Loss: 0.330252 || Validation Loss: 0.169992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.846s | Train Loss: 0.317514 || Validation Loss: 0.169713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.719s | Train Loss: 0.321689 || Validation Loss: 0.169972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.820s | Train Loss: 0.327431 || Validation Loss: 0.170235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.810s | Train Loss: 0.326305 || Validation Loss: 0.170161 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 1.797s | Train Loss: 0.327653 || Validation Loss: 0.170517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 88.904s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.585%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.166757\n",
      "INFO:root:Test AUC: 49.12%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6088350351412279\n",
      "validation AUC 0.600969576587528\n",
      "validation AUC 0.5754650872714447\n",
      "validation AUC 0.5661414364449198\n",
      "validation AUC 0.5416525934309243\n",
      "validation AUC 0.5439139020226083\n",
      "validation AUC 0.538897294575635\n",
      "validation AUC 0.531495164582359\n",
      "validation AUC 0.5334674618772115\n",
      "validation AUC 0.5348480420464437\n",
      "validation AUC 0.5198605568318553\n",
      "validation AUC 0.5265998416787585\n",
      "validation AUC 0.5196146219819615\n",
      "validation AUC 0.5211591986280245\n",
      "validation AUC 0.5200147008020148\n",
      "validation AUC 0.5162430399210906\n",
      "validation AUC 0.5224533837812892\n",
      "validation AUC 0.5275031592941997\n",
      "validation AUC 0.5188878246750828\n",
      "validation AUC 0.5150746917386666\n",
      "validation AUC 0.5145841470586795\n",
      "validation AUC 0.5051362237425242\n",
      "validation AUC 0.49901257821607853\n",
      "validation AUC 0.49452432906637933\n",
      "validation AUC 0.49077108010718506\n",
      "validation AUC 0.4976811594742747\n",
      "validation AUC 0.4945432332143405\n",
      "validation AUC 0.5030553493231118\n",
      "validation AUC 0.4810334217886043\n",
      "validation AUC 0.49474670623491535\n",
      "validation AUC 0.4947363987515652\n",
      "validation AUC 0.4783697249046039\n",
      "validation AUC 0.48555232199875564\n",
      "validation AUC 0.48226749680664904\n",
      "validation AUC 0.5037302713065845\n",
      "validation AUC 0.4963619133541349\n",
      "validation AUC 0.4911039474308771\n",
      "validation AUC 0.5029417834191812\n",
      "validation AUC 0.500335392311224\n",
      "validation AUC 0.4856925186721365\n",
      "validation AUC 0.49164239503554985\n",
      "validation AUC 0.4855749218344744\n",
      "validation AUC 0.5033921624387006\n",
      "validation AUC 0.4921792462308242\n",
      "validation AUC 0.4798166305629812\n",
      "validation AUC 0.4965665544142529\n",
      "validation AUC 0.49900178116717964\n",
      "validation AUC 0.4899558097914387\n",
      "validation AUC 0.4865755352813927\n",
      "validation AUC 0.48790173642514556\n",
      "validation AUC 0.4865069428909017\n",
      "validation AUC 0.48678583827351096\n",
      "validation AUC 0.4839114073668547\n",
      "validation AUC 0.48650664223379825\n",
      "validation AUC 0.4864556183287381\n",
      "validation AUC 0.48460589600818893\n",
      "validation AUC 0.4842149592926246\n",
      "validation AUC 0.4859390787823781\n",
      "validation AUC 0.487637416260345\n",
      "validation AUC 0.48804368116681773\n",
      "validation AUC 0.4866512396757543\n",
      "validation AUC 0.48689134763134356\n",
      "validation AUC 0.4856116472326882\n",
      "validation AUC 0.4833743699238172\n",
      "validation AUC 0.48265996341455375\n",
      "validation AUC 0.4835208378254521\n",
      "validation AUC 0.4816523310876465\n",
      "validation AUC 0.48283703714504106\n",
      "validation AUC 0.4822595466878442\n",
      "validation AUC 0.48067450904557496\n",
      "validation AUC 0.48067450904557496\n",
      "train auc 0.5849430145526204\n",
      "Test AUC 0.4911723107722944\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.03173851e-04 ... 9.99962103e-01\n",
      " 9.99962103e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.02512896 1.02512896 0.66181809 ... 0.01032387 0.00997367 0.00885713]\n",
      "Itration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.409s | Train Loss: 0.201325 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.390s | Train Loss: 0.157106 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.386s | Train Loss: 0.124311 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.383s | Train Loss: 0.100164 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.384s | Train Loss: 0.083183 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.385s | Train Loss: 0.071049 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.385s | Train Loss: 0.062608 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.385s | Train Loss: 0.055845 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.386s | Train Loss: 0.050645 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.382s | Train Loss: 0.046671 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.382s | Train Loss: 0.043195 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.380s | Train Loss: 0.040713 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.381s | Train Loss: 0.038100 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.383s | Train Loss: 0.036306 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.385s | Train Loss: 0.034195 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.383s | Train Loss: 0.032631 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.385s | Train Loss: 0.031200 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.382s | Train Loss: 0.029955 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.382s | Train Loss: 0.028854 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.382s | Train Loss: 0.027873 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.384s | Train Loss: 0.027025 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.383s | Train Loss: 0.026413 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.385s | Train Loss: 0.025813 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.384s | Train Loss: 0.025058 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.385s | Train Loss: 0.024554 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.384s | Train Loss: 0.024048 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.382s | Train Loss: 0.023575 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.387s | Train Loss: 0.023268 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.386s | Train Loss: 0.023071 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.388s | Train Loss: 0.022338 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.385s | Train Loss: 0.022066 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.384s | Train Loss: 0.022055 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.385s | Train Loss: 0.021676 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.388s | Train Loss: 0.021487 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.386s | Train Loss: 0.021278 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.386s | Train Loss: 0.021034 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.383s | Train Loss: 0.020916 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.386s | Train Loss: 0.020782 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.387s | Train Loss: 0.020822 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.383s | Train Loss: 0.020501 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.385s | Train Loss: 0.020276 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.384s | Train Loss: 0.020423 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.384s | Train Loss: 0.020036 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.384s | Train Loss: 0.019782 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.384s | Train Loss: 0.019925 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.386s | Train Loss: 0.019672 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.385s | Train Loss: 0.019612 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.385s | Train Loss: 0.019486 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.385s | Train Loss: 0.019402 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.383s | Train Loss: 0.019205 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.366s | Train Loss: 0.018847 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.385s | Train Loss: 0.019077 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.385s | Train Loss: 0.019138 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.381s | Train Loss: 0.019017 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.385s | Train Loss: 0.018965 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.382s | Train Loss: 0.018697 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.386s | Train Loss: 0.018401 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.385s | Train Loss: 0.018494 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.387s | Train Loss: 0.018253 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.383s | Train Loss: 0.018496 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.381s | Train Loss: 0.018094 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.384s | Train Loss: 0.018095 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.385s | Train Loss: 0.018041 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.382s | Train Loss: 0.017967 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.384s | Train Loss: 0.018076 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.384s | Train Loss: 0.017948 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.386s | Train Loss: 0.017775 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.386s | Train Loss: 0.017434 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.384s | Train Loss: 0.017723 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.382s | Train Loss: 0.017439 |\n",
      "INFO:root:Pretraining Time: 26.920s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017263\n",
      "INFO:root:Test AUC: 63.82%\n",
      "INFO:root:Test AUC: 63.82%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.640s | Train Loss: 0.785942 || Validation Loss: 0.483910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.660s | Train Loss: 0.490753 || Validation Loss: 0.320714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.696s | Train Loss: 0.413130 || Validation Loss: 0.257280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.730s | Train Loss: 0.402079 || Validation Loss: 0.239847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.716s | Train Loss: 0.382413 || Validation Loss: 0.224873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.720s | Train Loss: 0.369232 || Validation Loss: 0.212213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.760s | Train Loss: 0.364378 || Validation Loss: 0.209864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.773s | Train Loss: 0.362636 || Validation Loss: 0.204266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.755s | Train Loss: 0.360456 || Validation Loss: 0.201094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.826s | Train Loss: 0.359531 || Validation Loss: 0.197522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.845s | Train Loss: 0.343075 || Validation Loss: 0.191919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.818s | Train Loss: 0.353621 || Validation Loss: 0.194494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.876s | Train Loss: 0.353411 || Validation Loss: 0.194834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.873s | Train Loss: 0.336276 || Validation Loss: 0.186903 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.891s | Train Loss: 0.346309 || Validation Loss: 0.188441 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.900s | Train Loss: 0.331741 || Validation Loss: 0.182388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.950s | Train Loss: 0.336934 || Validation Loss: 0.182796 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.926s | Train Loss: 0.333151 || Validation Loss: 0.180546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.973s | Train Loss: 0.336876 || Validation Loss: 0.180154 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.948s | Train Loss: 0.340271 || Validation Loss: 0.180368 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.033s | Train Loss: 0.351198 || Validation Loss: 0.186034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.980s | Train Loss: 0.343208 || Validation Loss: 0.184923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.023s | Train Loss: 0.344752 || Validation Loss: 0.184284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.036s | Train Loss: 0.328089 || Validation Loss: 0.178497 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.047s | Train Loss: 0.339787 || Validation Loss: 0.184117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.102s | Train Loss: 0.339640 || Validation Loss: 0.181090 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.036s | Train Loss: 0.334714 || Validation Loss: 0.181105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.167s | Train Loss: 0.333176 || Validation Loss: 0.176026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.104s | Train Loss: 0.321716 || Validation Loss: 0.171898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.107s | Train Loss: 0.321879 || Validation Loss: 0.170857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.209s | Train Loss: 0.329652 || Validation Loss: 0.175077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.140s | Train Loss: 0.323431 || Validation Loss: 0.176051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.193s | Train Loss: 0.319354 || Validation Loss: 0.170406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.171s | Train Loss: 0.327917 || Validation Loss: 0.173002 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.215s | Train Loss: 0.325765 || Validation Loss: 0.174376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.272s | Train Loss: 0.339163 || Validation Loss: 0.176264 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.252s | Train Loss: 0.328980 || Validation Loss: 0.175951 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.253s | Train Loss: 0.332197 || Validation Loss: 0.178127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.272s | Train Loss: 0.336567 || Validation Loss: 0.176870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.252s | Train Loss: 0.326781 || Validation Loss: 0.174658 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.303s | Train Loss: 0.330548 || Validation Loss: 0.176581 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.327s | Train Loss: 0.326169 || Validation Loss: 0.173799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.296s | Train Loss: 0.328720 || Validation Loss: 0.173885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.345s | Train Loss: 0.326049 || Validation Loss: 0.175261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.396s | Train Loss: 0.331008 || Validation Loss: 0.175210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.413s | Train Loss: 0.315272 || Validation Loss: 0.170183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.425s | Train Loss: 0.334351 || Validation Loss: 0.177946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.482s | Train Loss: 0.333514 || Validation Loss: 0.177011 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.369s | Train Loss: 0.334478 || Validation Loss: 0.174759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.495s | Train Loss: 0.325226 || Validation Loss: 0.174780 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.574s | Train Loss: 0.328906 || Validation Loss: 0.174556 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.450s | Train Loss: 0.325683 || Validation Loss: 0.174134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.545s | Train Loss: 0.318104 || Validation Loss: 0.173013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.614s | Train Loss: 0.317853 || Validation Loss: 0.172460 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.549s | Train Loss: 0.321634 || Validation Loss: 0.171864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.459s | Train Loss: 0.330024 || Validation Loss: 0.172316 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.643s | Train Loss: 0.320972 || Validation Loss: 0.171915 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.613s | Train Loss: 0.315937 || Validation Loss: 0.171143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.685s | Train Loss: 0.334463 || Validation Loss: 0.172132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.652s | Train Loss: 0.323554 || Validation Loss: 0.172114 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.726s | Train Loss: 0.323094 || Validation Loss: 0.172210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.580s | Train Loss: 0.336826 || Validation Loss: 0.172970 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.736s | Train Loss: 0.316552 || Validation Loss: 0.172194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.684s | Train Loss: 0.319569 || Validation Loss: 0.171720 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.575s | Train Loss: 0.329934 || Validation Loss: 0.171993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.824s | Train Loss: 0.314374 || Validation Loss: 0.170854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.701s | Train Loss: 0.325206 || Validation Loss: 0.170974 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.813s | Train Loss: 0.308996 || Validation Loss: 0.169970 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.790s | Train Loss: 0.325930 || Validation Loss: 0.170193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 1.780s | Train Loss: 0.324211 || Validation Loss: 0.170617 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 87.521s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.586%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.163389\n",
      "INFO:root:Test AUC: 49.16%\n",
      "INFO:root:Test Time: 0.180s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6248716007920745\n",
      "validation AUC 0.6147796167468029\n",
      "validation AUC 0.5981973318464731\n",
      "validation AUC 0.5760556895711683\n",
      "validation AUC 0.5769704481206324\n",
      "validation AUC 0.5538113396365253\n",
      "validation AUC 0.5556793621301466\n",
      "validation AUC 0.5433742943604389\n",
      "validation AUC 0.5239093942946031\n",
      "validation AUC 0.5386954764994701\n",
      "validation AUC 0.5383530759509864\n",
      "validation AUC 0.5273712133967271\n",
      "validation AUC 0.529686062898956\n",
      "validation AUC 0.5241313271291898\n",
      "validation AUC 0.5245726012937727\n",
      "validation AUC 0.5201497703405439\n",
      "validation AUC 0.524947472809423\n",
      "validation AUC 0.5215158178628846\n",
      "validation AUC 0.5155345880188836\n",
      "validation AUC 0.5057912092332914\n",
      "validation AUC 0.5121621438969375\n",
      "validation AUC 0.5115005678960367\n",
      "validation AUC 0.506273050821374\n",
      "validation AUC 0.5129833634983758\n",
      "validation AUC 0.5117197921560105\n",
      "validation AUC 0.5135892354539965\n",
      "validation AUC 0.5129107428348357\n",
      "validation AUC 0.5130754470531667\n",
      "validation AUC 0.5098166885658559\n",
      "validation AUC 0.5099604372501486\n",
      "validation AUC 0.5039789706054202\n",
      "validation AUC 0.5155931017447052\n",
      "validation AUC 0.5170030345614121\n",
      "validation AUC 0.5091616764682656\n",
      "validation AUC 0.4945916017584343\n",
      "validation AUC 0.5125059945172916\n",
      "validation AUC 0.5063533315893447\n",
      "validation AUC 0.4979804995399148\n",
      "validation AUC 0.5115776052922462\n",
      "validation AUC 0.504602967129079\n",
      "validation AUC 0.5105639571625888\n",
      "validation AUC 0.5120320125848146\n",
      "validation AUC 0.5053370068132625\n",
      "validation AUC 0.5024970211000623\n",
      "validation AUC 0.5080992340853415\n",
      "validation AUC 0.5081436222486683\n",
      "validation AUC 0.5200426619126304\n",
      "validation AUC 0.5127976452109952\n",
      "validation AUC 0.5150736168230048\n",
      "validation AUC 0.5105107089271\n",
      "validation AUC 0.5065634563157468\n",
      "validation AUC 0.5040629523825133\n",
      "validation AUC 0.5039473696813759\n",
      "validation AUC 0.5036828313406242\n",
      "validation AUC 0.5031078685316418\n",
      "validation AUC 0.5038881907849715\n",
      "validation AUC 0.50300104745742\n",
      "validation AUC 0.5026989296394264\n",
      "validation AUC 0.5032463437435545\n",
      "validation AUC 0.5007961240456931\n",
      "validation AUC 0.501619246034998\n",
      "validation AUC 0.5014394397837568\n",
      "validation AUC 0.5015600405317704\n",
      "validation AUC 0.5001268480301266\n",
      "validation AUC 0.5005529509840587\n",
      "validation AUC 0.5010293248699512\n",
      "validation AUC 0.5000149689987937\n",
      "validation AUC 0.4979636201712075\n",
      "validation AUC 0.4996543906686252\n",
      "validation AUC 0.5005943219336222\n",
      "validation AUC 0.5005943219336222\n",
      "train auc 0.5860476912722082\n",
      "Test AUC 0.4916237004163654\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 7.57934628e-05 ... 9.99981052e-01\n",
      " 9.99981052e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.2393862  1.2393862  0.85054821 ... 0.01324501 0.013002   0.01115321]\n",
      "Itration 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.407s | Train Loss: 0.197560 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.385s | Train Loss: 0.154284 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.385s | Train Loss: 0.121493 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.383s | Train Loss: 0.098057 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.383s | Train Loss: 0.081725 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.383s | Train Loss: 0.070266 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.384s | Train Loss: 0.062206 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.384s | Train Loss: 0.056202 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.383s | Train Loss: 0.051133 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.384s | Train Loss: 0.047265 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.384s | Train Loss: 0.044441 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.379s | Train Loss: 0.041677 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.386s | Train Loss: 0.039577 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.383s | Train Loss: 0.037530 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.384s | Train Loss: 0.036080 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.384s | Train Loss: 0.034428 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.383s | Train Loss: 0.033212 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.386s | Train Loss: 0.031914 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.385s | Train Loss: 0.030923 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.384s | Train Loss: 0.029949 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.384s | Train Loss: 0.029120 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.383s | Train Loss: 0.028362 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.387s | Train Loss: 0.027563 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.379s | Train Loss: 0.026732 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.383s | Train Loss: 0.026667 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.384s | Train Loss: 0.025743 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.383s | Train Loss: 0.025444 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.384s | Train Loss: 0.024887 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.382s | Train Loss: 0.024596 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.387s | Train Loss: 0.024196 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.386s | Train Loss: 0.023865 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.385s | Train Loss: 0.023717 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.385s | Train Loss: 0.023317 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.384s | Train Loss: 0.023010 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.386s | Train Loss: 0.022686 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.384s | Train Loss: 0.022591 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.384s | Train Loss: 0.022412 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.386s | Train Loss: 0.022310 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.384s | Train Loss: 0.021985 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.382s | Train Loss: 0.021839 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.387s | Train Loss: 0.021435 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.387s | Train Loss: 0.021225 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.383s | Train Loss: 0.021425 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.386s | Train Loss: 0.021197 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.387s | Train Loss: 0.020849 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.385s | Train Loss: 0.020797 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.385s | Train Loss: 0.020740 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.385s | Train Loss: 0.020571 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.385s | Train Loss: 0.020268 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.385s | Train Loss: 0.020259 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.384s | Train Loss: 0.019948 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.385s | Train Loss: 0.019873 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.384s | Train Loss: 0.019667 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.379s | Train Loss: 0.019464 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.376s | Train Loss: 0.019405 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.384s | Train Loss: 0.019186 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.382s | Train Loss: 0.018856 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.383s | Train Loss: 0.019103 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.384s | Train Loss: 0.019040 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.383s | Train Loss: 0.018749 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.384s | Train Loss: 0.018413 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.385s | Train Loss: 0.018405 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.384s | Train Loss: 0.018472 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.383s | Train Loss: 0.018387 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.383s | Train Loss: 0.018271 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.386s | Train Loss: 0.018030 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.382s | Train Loss: 0.017843 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.384s | Train Loss: 0.017691 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.384s | Train Loss: 0.017753 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.385s | Train Loss: 0.017615 |\n",
      "INFO:root:Pretraining Time: 26.910s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017417\n",
      "INFO:root:Test AUC: 63.68%\n",
      "INFO:root:Test AUC: 63.68%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.639s | Train Loss: 1.019103 || Validation Loss: 0.633604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.653s | Train Loss: 0.544388 || Validation Loss: 0.355150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.698s | Train Loss: 0.429623 || Validation Loss: 0.268427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.731s | Train Loss: 0.399833 || Validation Loss: 0.234333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.711s | Train Loss: 0.388367 || Validation Loss: 0.222026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.715s | Train Loss: 0.367727 || Validation Loss: 0.210077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.762s | Train Loss: 0.366384 || Validation Loss: 0.204991 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.771s | Train Loss: 0.359516 || Validation Loss: 0.197843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.750s | Train Loss: 0.354250 || Validation Loss: 0.195553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.820s | Train Loss: 0.362165 || Validation Loss: 0.194508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.839s | Train Loss: 0.356558 || Validation Loss: 0.193536 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.816s | Train Loss: 0.352517 || Validation Loss: 0.187769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.867s | Train Loss: 0.347118 || Validation Loss: 0.188949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.872s | Train Loss: 0.352172 || Validation Loss: 0.188671 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.887s | Train Loss: 0.345211 || Validation Loss: 0.187022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.892s | Train Loss: 0.348638 || Validation Loss: 0.189077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.943s | Train Loss: 0.343644 || Validation Loss: 0.185950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.921s | Train Loss: 0.343197 || Validation Loss: 0.182661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.969s | Train Loss: 0.341760 || Validation Loss: 0.182437 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.941s | Train Loss: 0.342780 || Validation Loss: 0.181746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.030s | Train Loss: 0.348538 || Validation Loss: 0.182697 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.971s | Train Loss: 0.346646 || Validation Loss: 0.185744 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.024s | Train Loss: 0.349169 || Validation Loss: 0.185549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.034s | Train Loss: 0.329176 || Validation Loss: 0.175112 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.050s | Train Loss: 0.338733 || Validation Loss: 0.178084 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.097s | Train Loss: 0.332067 || Validation Loss: 0.177625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.037s | Train Loss: 0.331151 || Validation Loss: 0.173492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.149s | Train Loss: 0.329063 || Validation Loss: 0.173116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.104s | Train Loss: 0.329035 || Validation Loss: 0.177557 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.111s | Train Loss: 0.322880 || Validation Loss: 0.173333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.205s | Train Loss: 0.324435 || Validation Loss: 0.175315 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.144s | Train Loss: 0.322726 || Validation Loss: 0.172836 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.197s | Train Loss: 0.326976 || Validation Loss: 0.170148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.166s | Train Loss: 0.331789 || Validation Loss: 0.173999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.215s | Train Loss: 0.320132 || Validation Loss: 0.172201 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.267s | Train Loss: 0.330944 || Validation Loss: 0.171091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.249s | Train Loss: 0.322378 || Validation Loss: 0.172437 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.263s | Train Loss: 0.338627 || Validation Loss: 0.176629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.279s | Train Loss: 0.326834 || Validation Loss: 0.171400 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.247s | Train Loss: 0.324377 || Validation Loss: 0.173257 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.295s | Train Loss: 0.324248 || Validation Loss: 0.171551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.323s | Train Loss: 0.326961 || Validation Loss: 0.173567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.285s | Train Loss: 0.334611 || Validation Loss: 0.173873 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.337s | Train Loss: 0.319406 || Validation Loss: 0.166622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.393s | Train Loss: 0.330529 || Validation Loss: 0.174508 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.408s | Train Loss: 0.313253 || Validation Loss: 0.164821 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.411s | Train Loss: 0.319347 || Validation Loss: 0.170168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.490s | Train Loss: 0.320471 || Validation Loss: 0.168343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.361s | Train Loss: 0.318729 || Validation Loss: 0.170540 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.495s | Train Loss: 0.310472 || Validation Loss: 0.169073 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.568s | Train Loss: 0.307051 || Validation Loss: 0.168227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.450s | Train Loss: 0.321516 || Validation Loss: 0.168590 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.539s | Train Loss: 0.316767 || Validation Loss: 0.168373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.610s | Train Loss: 0.324676 || Validation Loss: 0.168678 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.552s | Train Loss: 0.310412 || Validation Loss: 0.167364 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.452s | Train Loss: 0.327828 || Validation Loss: 0.167758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.645s | Train Loss: 0.317412 || Validation Loss: 0.167311 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.602s | Train Loss: 0.318463 || Validation Loss: 0.167023 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.677s | Train Loss: 0.327457 || Validation Loss: 0.167878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.638s | Train Loss: 0.313172 || Validation Loss: 0.167421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.720s | Train Loss: 0.318810 || Validation Loss: 0.167285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.576s | Train Loss: 0.319602 || Validation Loss: 0.167670 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.739s | Train Loss: 0.316101 || Validation Loss: 0.167212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.671s | Train Loss: 0.312415 || Validation Loss: 0.166236 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.574s | Train Loss: 0.318132 || Validation Loss: 0.166200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.816s | Train Loss: 0.320182 || Validation Loss: 0.166551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.696s | Train Loss: 0.322992 || Validation Loss: 0.166834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.793s | Train Loss: 0.321827 || Validation Loss: 0.167172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.777s | Train Loss: 0.325703 || Validation Loss: 0.168017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 1.762s | Train Loss: 0.314869 || Validation Loss: 0.167987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 87.230s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.581%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.164385\n",
      "INFO:root:Test AUC: 49.44%\n",
      "INFO:root:Test Time: 0.182s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.523848882396351\n",
      "validation AUC 0.53060601510033\n",
      "validation AUC 0.5245206567926262\n",
      "validation AUC 0.5226059606095559\n",
      "validation AUC 0.5094826585239853\n",
      "validation AUC 0.49965814489139415\n",
      "validation AUC 0.4956168478023722\n",
      "validation AUC 0.4940345027706217\n",
      "validation AUC 0.4893629671971925\n",
      "validation AUC 0.4973694711819368\n",
      "validation AUC 0.4977167061902158\n",
      "validation AUC 0.49913522768948687\n",
      "validation AUC 0.49456558294592023\n",
      "validation AUC 0.4932605528514745\n",
      "validation AUC 0.49510079250019634\n",
      "validation AUC 0.4889265727240044\n",
      "validation AUC 0.4887888265390504\n",
      "validation AUC 0.492233106423249\n",
      "validation AUC 0.4884966011379632\n",
      "validation AUC 0.4939334899659284\n",
      "validation AUC 0.4889062637357725\n",
      "validation AUC 0.482518883394639\n",
      "validation AUC 0.49667430140592583\n",
      "validation AUC 0.5062081514579582\n",
      "validation AUC 0.4991589982254313\n",
      "validation AUC 0.4846419003614909\n",
      "validation AUC 0.4895893220858132\n",
      "validation AUC 0.49675789472339865\n",
      "validation AUC 0.48177600226306994\n",
      "validation AUC 0.47606371418907834\n",
      "validation AUC 0.4668677681323905\n",
      "validation AUC 0.47508962508431696\n",
      "validation AUC 0.4757339226141608\n",
      "validation AUC 0.4750490922496878\n",
      "validation AUC 0.48787995075822\n",
      "validation AUC 0.4963697330995055\n",
      "validation AUC 0.49020882205633276\n",
      "validation AUC 0.49053893557381234\n",
      "validation AUC 0.4985590303239029\n",
      "validation AUC 0.48931354502289565\n",
      "validation AUC 0.4945266225745486\n",
      "validation AUC 0.48603261506972156\n",
      "validation AUC 0.5012032856021114\n",
      "validation AUC 0.5127476855788677\n",
      "validation AUC 0.5094275265254065\n",
      "validation AUC 0.5109638470741966\n",
      "validation AUC 0.49244246221245747\n",
      "validation AUC 0.4988058139314604\n",
      "validation AUC 0.4970273207375922\n",
      "validation AUC 0.4972717602840162\n",
      "validation AUC 0.4969955628332903\n",
      "validation AUC 0.49546196948388943\n",
      "validation AUC 0.4946468215595302\n",
      "validation AUC 0.4941796988661023\n",
      "validation AUC 0.495419377281136\n",
      "validation AUC 0.497225206325272\n",
      "validation AUC 0.4971021231606437\n",
      "validation AUC 0.49509581170287287\n",
      "validation AUC 0.49390116001492534\n",
      "validation AUC 0.4931082208059824\n",
      "validation AUC 0.493864389385112\n",
      "validation AUC 0.49193272337013516\n",
      "validation AUC 0.4926721589180857\n",
      "validation AUC 0.494301635276646\n",
      "validation AUC 0.4950157730569942\n",
      "validation AUC 0.4943750621269324\n",
      "validation AUC 0.493550311800041\n",
      "validation AUC 0.4944772083822988\n",
      "validation AUC 0.4928331408418356\n",
      "validation AUC 0.49090990652916544\n",
      "validation AUC 0.49090990652916544\n",
      "train auc 0.5808210012859781\n",
      "Test AUC 0.4944060864993955\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.32638560e-04 ... 9.99677878e-01\n",
      " 9.99677878e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.42544258 1.42544258 0.90577883 ... 0.01486631 0.01472623 0.0068951 ]\n",
      "Itration 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.409s | Train Loss: 0.189209 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.389s | Train Loss: 0.146796 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.386s | Train Loss: 0.114759 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.384s | Train Loss: 0.092574 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.387s | Train Loss: 0.077171 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.384s | Train Loss: 0.066329 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.382s | Train Loss: 0.058639 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.381s | Train Loss: 0.052606 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.384s | Train Loss: 0.047649 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.383s | Train Loss: 0.044161 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.384s | Train Loss: 0.041441 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.385s | Train Loss: 0.038690 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.384s | Train Loss: 0.036806 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.382s | Train Loss: 0.034872 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.381s | Train Loss: 0.033562 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.385s | Train Loss: 0.032243 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.383s | Train Loss: 0.030914 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.387s | Train Loss: 0.029584 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.383s | Train Loss: 0.028987 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.384s | Train Loss: 0.027933 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.385s | Train Loss: 0.027400 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.385s | Train Loss: 0.026491 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.382s | Train Loss: 0.026099 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.386s | Train Loss: 0.025472 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.385s | Train Loss: 0.025248 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.384s | Train Loss: 0.025045 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.386s | Train Loss: 0.024212 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.384s | Train Loss: 0.024105 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.387s | Train Loss: 0.023743 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.388s | Train Loss: 0.023235 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.385s | Train Loss: 0.022843 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.386s | Train Loss: 0.022804 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.384s | Train Loss: 0.022701 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.380s | Train Loss: 0.022495 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.385s | Train Loss: 0.022276 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.385s | Train Loss: 0.021991 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.385s | Train Loss: 0.021795 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.380s | Train Loss: 0.021847 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.385s | Train Loss: 0.021375 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.383s | Train Loss: 0.021131 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.383s | Train Loss: 0.021169 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.384s | Train Loss: 0.021060 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.384s | Train Loss: 0.021007 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.383s | Train Loss: 0.020643 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.384s | Train Loss: 0.020467 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.384s | Train Loss: 0.020258 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.385s | Train Loss: 0.020220 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.385s | Train Loss: 0.020350 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.384s | Train Loss: 0.020177 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.382s | Train Loss: 0.019821 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.385s | Train Loss: 0.019704 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.383s | Train Loss: 0.019705 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.383s | Train Loss: 0.019536 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.384s | Train Loss: 0.019444 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.386s | Train Loss: 0.019545 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.383s | Train Loss: 0.019260 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.382s | Train Loss: 0.019116 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.385s | Train Loss: 0.019082 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.376s | Train Loss: 0.018888 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.390s | Train Loss: 0.018851 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.384s | Train Loss: 0.018566 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.387s | Train Loss: 0.018400 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.385s | Train Loss: 0.018276 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.387s | Train Loss: 0.018316 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.388s | Train Loss: 0.017886 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.386s | Train Loss: 0.017978 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.382s | Train Loss: 0.018023 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.383s | Train Loss: 0.017871 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.382s | Train Loss: 0.017568 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.381s | Train Loss: 0.017411 |\n",
      "INFO:root:Pretraining Time: 26.928s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017218\n",
      "INFO:root:Test AUC: 63.06%\n",
      "INFO:root:Test AUC: 63.06%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.642s | Train Loss: 0.768970 || Validation Loss: 0.451725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.661s | Train Loss: 0.455418 || Validation Loss: 0.274031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.695s | Train Loss: 0.396304 || Validation Loss: 0.227935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.735s | Train Loss: 0.384051 || Validation Loss: 0.215491 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.714s | Train Loss: 0.371931 || Validation Loss: 0.211323 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.721s | Train Loss: 0.371139 || Validation Loss: 0.205743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.769s | Train Loss: 0.357609 || Validation Loss: 0.194147 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.773s | Train Loss: 0.353005 || Validation Loss: 0.191760 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.755s | Train Loss: 0.353471 || Validation Loss: 0.191474 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.825s | Train Loss: 0.350102 || Validation Loss: 0.183742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.848s | Train Loss: 0.334871 || Validation Loss: 0.177683 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.819s | Train Loss: 0.356255 || Validation Loss: 0.187292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.868s | Train Loss: 0.350377 || Validation Loss: 0.189251 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.876s | Train Loss: 0.347113 || Validation Loss: 0.182608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.885s | Train Loss: 0.349882 || Validation Loss: 0.184172 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.898s | Train Loss: 0.341563 || Validation Loss: 0.178904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.951s | Train Loss: 0.349569 || Validation Loss: 0.181453 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.927s | Train Loss: 0.349670 || Validation Loss: 0.180526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.972s | Train Loss: 0.339964 || Validation Loss: 0.178995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.946s | Train Loss: 0.337132 || Validation Loss: 0.177654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.034s | Train Loss: 0.333401 || Validation Loss: 0.178026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.979s | Train Loss: 0.333745 || Validation Loss: 0.177672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.024s | Train Loss: 0.335082 || Validation Loss: 0.176171 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.039s | Train Loss: 0.333668 || Validation Loss: 0.176340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.056s | Train Loss: 0.340754 || Validation Loss: 0.179847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.101s | Train Loss: 0.337044 || Validation Loss: 0.178451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.037s | Train Loss: 0.335653 || Validation Loss: 0.177655 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.163s | Train Loss: 0.329414 || Validation Loss: 0.172906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.096s | Train Loss: 0.339316 || Validation Loss: 0.178987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.111s | Train Loss: 0.342315 || Validation Loss: 0.178412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.212s | Train Loss: 0.340787 || Validation Loss: 0.180400 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.141s | Train Loss: 0.324252 || Validation Loss: 0.168957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.207s | Train Loss: 0.334530 || Validation Loss: 0.178931 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.169s | Train Loss: 0.326443 || Validation Loss: 0.171430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.213s | Train Loss: 0.330973 || Validation Loss: 0.174254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.274s | Train Loss: 0.328514 || Validation Loss: 0.171877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.252s | Train Loss: 0.331740 || Validation Loss: 0.176008 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.262s | Train Loss: 0.335947 || Validation Loss: 0.178844 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.279s | Train Loss: 0.328578 || Validation Loss: 0.177164 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.252s | Train Loss: 0.331313 || Validation Loss: 0.175809 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.310s | Train Loss: 0.326146 || Validation Loss: 0.171263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.325s | Train Loss: 0.328576 || Validation Loss: 0.168685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.271s | Train Loss: 0.324060 || Validation Loss: 0.172478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.345s | Train Loss: 0.328756 || Validation Loss: 0.171713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.399s | Train Loss: 0.324979 || Validation Loss: 0.170896 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.411s | Train Loss: 0.329954 || Validation Loss: 0.173380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.418s | Train Loss: 0.330965 || Validation Loss: 0.173800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.488s | Train Loss: 0.319121 || Validation Loss: 0.166175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.368s | Train Loss: 0.326090 || Validation Loss: 0.167863 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.469s | Train Loss: 0.328601 || Validation Loss: 0.168918 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.575s | Train Loss: 0.333216 || Validation Loss: 0.170609 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.445s | Train Loss: 0.333676 || Validation Loss: 0.171309 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.545s | Train Loss: 0.322174 || Validation Loss: 0.171062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.609s | Train Loss: 0.322247 || Validation Loss: 0.171204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.550s | Train Loss: 0.323770 || Validation Loss: 0.171249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.458s | Train Loss: 0.324845 || Validation Loss: 0.171233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.642s | Train Loss: 0.330586 || Validation Loss: 0.171850 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.611s | Train Loss: 0.325826 || Validation Loss: 0.171456 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.668s | Train Loss: 0.327775 || Validation Loss: 0.171488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.651s | Train Loss: 0.321344 || Validation Loss: 0.170866 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.724s | Train Loss: 0.315913 || Validation Loss: 0.170206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.579s | Train Loss: 0.317133 || Validation Loss: 0.169392 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.734s | Train Loss: 0.323483 || Validation Loss: 0.169421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.677s | Train Loss: 0.315163 || Validation Loss: 0.168883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.569s | Train Loss: 0.312208 || Validation Loss: 0.167759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.825s | Train Loss: 0.323433 || Validation Loss: 0.168442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.695s | Train Loss: 0.320534 || Validation Loss: 0.168522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.810s | Train Loss: 0.327828 || Validation Loss: 0.169604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.783s | Train Loss: 0.331932 || Validation Loss: 0.170574 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 1.780s | Train Loss: 0.324167 || Validation Loss: 0.170157 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 87.452s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.582%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.167661\n",
      "INFO:root:Test AUC: 46.87%\n",
      "INFO:root:Test Time: 0.179s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.4865243623781222\n",
      "validation AUC 0.5048485773757286\n",
      "validation AUC 0.49783161307804125\n",
      "validation AUC 0.5148365260811789\n",
      "validation AUC 0.48177410519656805\n",
      "validation AUC 0.46980473837723496\n",
      "validation AUC 0.4811085887251351\n",
      "validation AUC 0.4998363786793778\n",
      "validation AUC 0.4958407415598367\n",
      "validation AUC 0.5048419629194539\n",
      "validation AUC 0.4976007297080934\n",
      "validation AUC 0.48125393647950865\n",
      "validation AUC 0.4909642083948572\n",
      "validation AUC 0.5064069070887601\n",
      "validation AUC 0.4841473886041486\n",
      "validation AUC 0.47530984443948254\n",
      "validation AUC 0.5007218351343314\n",
      "validation AUC 0.5070143541683207\n",
      "validation AUC 0.504794911413114\n",
      "validation AUC 0.47027327921434525\n",
      "validation AUC 0.49125121619789347\n",
      "validation AUC 0.5004369691805715\n",
      "validation AUC 0.49742236022531083\n",
      "validation AUC 0.5156851480499433\n",
      "validation AUC 0.5021635950330807\n",
      "validation AUC 0.4782873528403209\n",
      "validation AUC 0.4877921748481229\n",
      "validation AUC 0.4859365724196224\n",
      "validation AUC 0.49282523595463046\n",
      "validation AUC 0.47881420253071333\n",
      "validation AUC 0.4941545261505695\n",
      "validation AUC 0.47445474370924234\n",
      "validation AUC 0.4622000506381061\n",
      "validation AUC 0.4975505705247909\n",
      "validation AUC 0.4821164206032172\n",
      "validation AUC 0.5136810662439658\n",
      "validation AUC 0.4827993672045998\n",
      "validation AUC 0.4846688637162325\n",
      "validation AUC 0.4766474120181531\n",
      "validation AUC 0.4759084234648342\n",
      "validation AUC 0.4694527859791833\n",
      "validation AUC 0.4750963433072026\n",
      "validation AUC 0.45202908955920584\n",
      "validation AUC 0.47623105780428143\n",
      "validation AUC 0.4620970157148413\n",
      "validation AUC 0.4650721055554409\n",
      "validation AUC 0.47291949275474915\n",
      "validation AUC 0.47882526830852756\n",
      "validation AUC 0.5019622159695005\n",
      "validation AUC 0.4977352990383443\n",
      "validation AUC 0.49442232914854123\n",
      "validation AUC 0.4927408098435881\n",
      "validation AUC 0.49224783329995064\n",
      "validation AUC 0.491758408767225\n",
      "validation AUC 0.490465436885103\n",
      "validation AUC 0.4870756131861713\n",
      "validation AUC 0.4848251069647511\n",
      "validation AUC 0.4843405381772793\n",
      "validation AUC 0.48169062628843545\n",
      "validation AUC 0.48013358700633435\n",
      "validation AUC 0.47930639151238075\n",
      "validation AUC 0.4799389740579215\n",
      "validation AUC 0.47923197754894964\n",
      "validation AUC 0.47848454398991236\n",
      "validation AUC 0.4798859253736077\n",
      "validation AUC 0.4796191786665213\n",
      "validation AUC 0.47924135911484844\n",
      "validation AUC 0.4772272731433067\n",
      "validation AUC 0.47717038775507165\n",
      "validation AUC 0.47882451001406334\n",
      "validation AUC 0.47882451001406334\n",
      "train auc 0.5818489144649688\n",
      "Test AUC 0.4687360188168093\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 4.35812411e-04 ... 9.99677878e-01\n",
      " 9.99677878e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.01454067 1.01454067 0.66311502 ... 0.02626378 0.02622353 0.01577833]\n",
      "Itration 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.391s | Train Loss: 0.196675 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.390s | Train Loss: 0.154240 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.384s | Train Loss: 0.121265 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.382s | Train Loss: 0.097845 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.382s | Train Loss: 0.081292 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.382s | Train Loss: 0.070279 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.382s | Train Loss: 0.062079 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.383s | Train Loss: 0.056515 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.383s | Train Loss: 0.051716 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.382s | Train Loss: 0.048133 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.383s | Train Loss: 0.045354 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.383s | Train Loss: 0.042828 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.384s | Train Loss: 0.040655 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.381s | Train Loss: 0.038516 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.382s | Train Loss: 0.037010 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.382s | Train Loss: 0.034929 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.383s | Train Loss: 0.034055 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.383s | Train Loss: 0.032709 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.381s | Train Loss: 0.031615 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.383s | Train Loss: 0.030476 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.385s | Train Loss: 0.029307 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.383s | Train Loss: 0.028465 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.383s | Train Loss: 0.027902 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.382s | Train Loss: 0.027216 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.381s | Train Loss: 0.026377 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.382s | Train Loss: 0.025772 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.385s | Train Loss: 0.025334 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.385s | Train Loss: 0.024609 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.384s | Train Loss: 0.024344 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.381s | Train Loss: 0.024123 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.385s | Train Loss: 0.023600 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.386s | Train Loss: 0.023346 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.382s | Train Loss: 0.022993 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.383s | Train Loss: 0.022617 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.383s | Train Loss: 0.022387 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.382s | Train Loss: 0.022211 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.384s | Train Loss: 0.021841 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.383s | Train Loss: 0.021669 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.383s | Train Loss: 0.021505 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.381s | Train Loss: 0.021237 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.384s | Train Loss: 0.021157 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.387s | Train Loss: 0.020733 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.383s | Train Loss: 0.020584 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.384s | Train Loss: 0.020542 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.383s | Train Loss: 0.020506 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.385s | Train Loss: 0.020181 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.384s | Train Loss: 0.020108 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.386s | Train Loss: 0.020144 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.384s | Train Loss: 0.020038 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.380s | Train Loss: 0.019858 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.382s | Train Loss: 0.019653 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.383s | Train Loss: 0.019605 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.384s | Train Loss: 0.019526 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.381s | Train Loss: 0.019458 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.381s | Train Loss: 0.019376 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.385s | Train Loss: 0.019434 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.386s | Train Loss: 0.019088 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.383s | Train Loss: 0.019132 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.382s | Train Loss: 0.019025 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.384s | Train Loss: 0.018804 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.384s | Train Loss: 0.018991 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.387s | Train Loss: 0.019096 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.368s | Train Loss: 0.018880 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.385s | Train Loss: 0.018542 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.386s | Train Loss: 0.018628 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.384s | Train Loss: 0.018621 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.386s | Train Loss: 0.018421 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.384s | Train Loss: 0.018488 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.381s | Train Loss: 0.018498 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.380s | Train Loss: 0.018180 |\n",
      "INFO:root:Pretraining Time: 26.837s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017863\n",
      "INFO:root:Test AUC: 63.52%\n",
      "INFO:root:Test AUC: 63.52%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.634s | Train Loss: 0.993061 || Validation Loss: 0.615585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.648s | Train Loss: 0.539532 || Validation Loss: 0.355569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.689s | Train Loss: 0.428366 || Validation Loss: 0.269126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.724s | Train Loss: 0.392344 || Validation Loss: 0.235322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.703s | Train Loss: 0.378246 || Validation Loss: 0.216982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.710s | Train Loss: 0.381489 || Validation Loss: 0.213293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.758s | Train Loss: 0.367529 || Validation Loss: 0.204444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.763s | Train Loss: 0.373386 || Validation Loss: 0.202514 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.751s | Train Loss: 0.354089 || Validation Loss: 0.192666 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.816s | Train Loss: 0.351893 || Validation Loss: 0.190789 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.837s | Train Loss: 0.344730 || Validation Loss: 0.187684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.810s | Train Loss: 0.347373 || Validation Loss: 0.188879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.862s | Train Loss: 0.350685 || Validation Loss: 0.188120 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.866s | Train Loss: 0.350130 || Validation Loss: 0.187928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.880s | Train Loss: 0.344670 || Validation Loss: 0.185243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.890s | Train Loss: 0.339097 || Validation Loss: 0.183001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.941s | Train Loss: 0.349151 || Validation Loss: 0.188623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.920s | Train Loss: 0.349513 || Validation Loss: 0.184788 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.971s | Train Loss: 0.344970 || Validation Loss: 0.182344 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.948s | Train Loss: 0.337536 || Validation Loss: 0.181531 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.031s | Train Loss: 0.329857 || Validation Loss: 0.174966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.969s | Train Loss: 0.339393 || Validation Loss: 0.179285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.018s | Train Loss: 0.331591 || Validation Loss: 0.174159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.032s | Train Loss: 0.336707 || Validation Loss: 0.175250 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.045s | Train Loss: 0.339754 || Validation Loss: 0.179158 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.095s | Train Loss: 0.333987 || Validation Loss: 0.180099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.033s | Train Loss: 0.340312 || Validation Loss: 0.180977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.160s | Train Loss: 0.329747 || Validation Loss: 0.171570 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.100s | Train Loss: 0.336568 || Validation Loss: 0.178196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.108s | Train Loss: 0.328651 || Validation Loss: 0.173843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.197s | Train Loss: 0.328435 || Validation Loss: 0.175159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.135s | Train Loss: 0.334717 || Validation Loss: 0.171077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.194s | Train Loss: 0.338367 || Validation Loss: 0.174589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.163s | Train Loss: 0.325929 || Validation Loss: 0.175495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.213s | Train Loss: 0.325117 || Validation Loss: 0.173770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.259s | Train Loss: 0.333879 || Validation Loss: 0.176000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.242s | Train Loss: 0.322381 || Validation Loss: 0.172423 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.259s | Train Loss: 0.328735 || Validation Loss: 0.170949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.271s | Train Loss: 0.323705 || Validation Loss: 0.172077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.250s | Train Loss: 0.331967 || Validation Loss: 0.173128 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.305s | Train Loss: 0.330418 || Validation Loss: 0.177637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.320s | Train Loss: 0.319349 || Validation Loss: 0.169985 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.287s | Train Loss: 0.325403 || Validation Loss: 0.169218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.338s | Train Loss: 0.327147 || Validation Loss: 0.176205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.386s | Train Loss: 0.314597 || Validation Loss: 0.166297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.414s | Train Loss: 0.314824 || Validation Loss: 0.167141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.417s | Train Loss: 0.326096 || Validation Loss: 0.169535 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.479s | Train Loss: 0.323355 || Validation Loss: 0.173279 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.361s | Train Loss: 0.333516 || Validation Loss: 0.170605 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.487s | Train Loss: 0.329326 || Validation Loss: 0.170892 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.561s | Train Loss: 0.320162 || Validation Loss: 0.170579 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.449s | Train Loss: 0.336885 || Validation Loss: 0.171641 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.543s | Train Loss: 0.323827 || Validation Loss: 0.171272 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.608s | Train Loss: 0.321625 || Validation Loss: 0.170933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.547s | Train Loss: 0.321724 || Validation Loss: 0.170633 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.447s | Train Loss: 0.315196 || Validation Loss: 0.170014 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.640s | Train Loss: 0.318138 || Validation Loss: 0.169703 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.610s | Train Loss: 0.316244 || Validation Loss: 0.169525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.671s | Train Loss: 0.332829 || Validation Loss: 0.170166 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.640s | Train Loss: 0.329221 || Validation Loss: 0.170242 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.715s | Train Loss: 0.328745 || Validation Loss: 0.170852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.573s | Train Loss: 0.319627 || Validation Loss: 0.170884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.737s | Train Loss: 0.318174 || Validation Loss: 0.169942 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.668s | Train Loss: 0.320910 || Validation Loss: 0.169867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.565s | Train Loss: 0.325439 || Validation Loss: 0.169923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.817s | Train Loss: 0.318408 || Validation Loss: 0.169402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.693s | Train Loss: 0.325929 || Validation Loss: 0.169387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.801s | Train Loss: 0.323570 || Validation Loss: 0.169335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.780s | Train Loss: 0.323235 || Validation Loss: 0.168961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 1.761s | Train Loss: 0.330808 || Validation Loss: 0.169744 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 87.061s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.580%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.167017\n",
      "INFO:root:Test AUC: 47.41%\n",
      "INFO:root:Test Time: 0.180s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5672366610948432\n",
      "validation AUC 0.5485192611051026\n",
      "validation AUC 0.5222488464877822\n",
      "validation AUC 0.4954747780086304\n",
      "validation AUC 0.49019815272018585\n",
      "validation AUC 0.47827208584510617\n",
      "validation AUC 0.4773461071982525\n",
      "validation AUC 0.4700299332083593\n",
      "validation AUC 0.47718817441645384\n",
      "validation AUC 0.48817682171065696\n",
      "validation AUC 0.4711837061729746\n",
      "validation AUC 0.4696164285852641\n",
      "validation AUC 0.47412231539813493\n",
      "validation AUC 0.47273732115691575\n",
      "validation AUC 0.4746815056822596\n",
      "validation AUC 0.4896488362281912\n",
      "validation AUC 0.4822735977512339\n",
      "validation AUC 0.48611356100827513\n",
      "validation AUC 0.48761253089850387\n",
      "validation AUC 0.48127565296869357\n",
      "validation AUC 0.47663188959743447\n",
      "validation AUC 0.4797739319329346\n",
      "validation AUC 0.46730313026083414\n",
      "validation AUC 0.48489984819210896\n",
      "validation AUC 0.4845964026936322\n",
      "validation AUC 0.4799312367937033\n",
      "validation AUC 0.46912693487479784\n",
      "validation AUC 0.48048535049594054\n",
      "validation AUC 0.4727464100477582\n",
      "validation AUC 0.48676290851318305\n",
      "validation AUC 0.47732997814196254\n",
      "validation AUC 0.5049785330828176\n",
      "validation AUC 0.49104713122038257\n",
      "validation AUC 0.4704109615429233\n",
      "validation AUC 0.47002408502859594\n",
      "validation AUC 0.48133210200502635\n",
      "validation AUC 0.4797997006413096\n",
      "validation AUC 0.48320932513814796\n",
      "validation AUC 0.4881097405877299\n",
      "validation AUC 0.47599779046296514\n",
      "validation AUC 0.4798523768300971\n",
      "validation AUC 0.475837016072437\n",
      "validation AUC 0.4755742337820237\n",
      "validation AUC 0.46949579324838026\n",
      "validation AUC 0.47595384663358764\n",
      "validation AUC 0.47415932016799334\n",
      "validation AUC 0.4778212039566262\n",
      "validation AUC 0.4730651784157786\n",
      "validation AUC 0.4810831073704519\n",
      "validation AUC 0.48038420199644827\n",
      "validation AUC 0.4811470196207229\n",
      "validation AUC 0.47708530445549346\n",
      "validation AUC 0.47648159563460785\n",
      "validation AUC 0.4752035421983154\n",
      "validation AUC 0.4734467495295382\n",
      "validation AUC 0.4740805054359869\n",
      "validation AUC 0.4747234300537479\n",
      "validation AUC 0.4752935717063467\n",
      "validation AUC 0.47568539442912716\n",
      "validation AUC 0.4747353951421901\n",
      "validation AUC 0.47439967024567464\n",
      "validation AUC 0.4720859994257183\n",
      "validation AUC 0.47475324565994814\n",
      "validation AUC 0.47450309894992443\n",
      "validation AUC 0.47270349590244276\n",
      "validation AUC 0.4750176403238541\n",
      "validation AUC 0.4753452634426718\n",
      "validation AUC 0.4761769767752232\n",
      "validation AUC 0.4745796973335493\n",
      "validation AUC 0.4726404350705177\n",
      "validation AUC 0.4726404350705177\n",
      "train auc 0.5797765438760502\n",
      "Test AUC 0.4740529159105268\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.32638560e-04 ... 9.99431549e-01\n",
      " 9.99431549e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [1.92286807 0.92286807 0.75062317 ... 0.01747718 0.01713083 0.0054448 ]\n",
      "Itration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is dta\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.414s | Train Loss: 0.193750 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.395s | Train Loss: 0.151132 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.389s | Train Loss: 0.119211 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.391s | Train Loss: 0.096132 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.392s | Train Loss: 0.080506 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.392s | Train Loss: 0.069158 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.387s | Train Loss: 0.061245 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.388s | Train Loss: 0.054941 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.388s | Train Loss: 0.050441 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.392s | Train Loss: 0.046427 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.390s | Train Loss: 0.043327 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.389s | Train Loss: 0.040302 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.389s | Train Loss: 0.038182 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.390s | Train Loss: 0.036221 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.390s | Train Loss: 0.034638 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.391s | Train Loss: 0.033177 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.390s | Train Loss: 0.031943 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.388s | Train Loss: 0.030660 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.391s | Train Loss: 0.029805 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.386s | Train Loss: 0.028971 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.391s | Train Loss: 0.028037 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.389s | Train Loss: 0.027293 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.391s | Train Loss: 0.026592 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.392s | Train Loss: 0.025972 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.391s | Train Loss: 0.025851 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.390s | Train Loss: 0.025212 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.392s | Train Loss: 0.024565 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.389s | Train Loss: 0.023853 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.390s | Train Loss: 0.023777 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.389s | Train Loss: 0.023628 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.391s | Train Loss: 0.023264 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.388s | Train Loss: 0.022731 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.390s | Train Loss: 0.022441 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.388s | Train Loss: 0.022226 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.391s | Train Loss: 0.022349 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.391s | Train Loss: 0.021975 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.390s | Train Loss: 0.021846 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.390s | Train Loss: 0.021770 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.390s | Train Loss: 0.021313 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.391s | Train Loss: 0.021340 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.391s | Train Loss: 0.021192 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.392s | Train Loss: 0.021133 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.391s | Train Loss: 0.021139 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.390s | Train Loss: 0.020630 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.389s | Train Loss: 0.020707 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.390s | Train Loss: 0.020613 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.389s | Train Loss: 0.020501 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.388s | Train Loss: 0.020492 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.388s | Train Loss: 0.020325 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.391s | Train Loss: 0.020294 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.390s | Train Loss: 0.020358 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.390s | Train Loss: 0.020026 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.388s | Train Loss: 0.019896 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.391s | Train Loss: 0.019864 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.389s | Train Loss: 0.019713 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.393s | Train Loss: 0.019765 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.391s | Train Loss: 0.019620 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.391s | Train Loss: 0.019625 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.392s | Train Loss: 0.019453 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.391s | Train Loss: 0.019445 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.390s | Train Loss: 0.019410 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.389s | Train Loss: 0.019286 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.389s | Train Loss: 0.019147 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.387s | Train Loss: 0.019304 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.392s | Train Loss: 0.019148 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.378s | Train Loss: 0.018754 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.386s | Train Loss: 0.018911 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.390s | Train Loss: 0.018768 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.387s | Train Loss: 0.018641 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.388s | Train Loss: 0.018776 |\n",
      "INFO:root:Pretraining Time: 27.317s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018240\n",
      "INFO:root:Test AUC: 61.14%\n",
      "INFO:root:Test AUC: 61.14%\n",
      "INFO:root:Test Time: 0.236s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 70\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.652s | Train Loss: 1.027928 || Validation Loss: 0.643176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.672s | Train Loss: 0.552489 || Validation Loss: 0.357079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.708s | Train Loss: 0.424277 || Validation Loss: 0.263161 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.746s | Train Loss: 0.399188 || Validation Loss: 0.237149 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.724s | Train Loss: 0.384993 || Validation Loss: 0.222905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.728s | Train Loss: 0.371011 || Validation Loss: 0.210622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.777s | Train Loss: 0.358142 || Validation Loss: 0.199843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.783s | Train Loss: 0.366956 || Validation Loss: 0.201688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.762s | Train Loss: 0.347389 || Validation Loss: 0.191504 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.834s | Train Loss: 0.347759 || Validation Loss: 0.188937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.856s | Train Loss: 0.346597 || Validation Loss: 0.190156 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.831s | Train Loss: 0.354699 || Validation Loss: 0.188292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.877s | Train Loss: 0.342049 || Validation Loss: 0.183953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.883s | Train Loss: 0.349637 || Validation Loss: 0.189140 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.902s | Train Loss: 0.350003 || Validation Loss: 0.185406 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.909s | Train Loss: 0.335766 || Validation Loss: 0.179911 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.962s | Train Loss: 0.342368 || Validation Loss: 0.184089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.936s | Train Loss: 0.343121 || Validation Loss: 0.181301 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.986s | Train Loss: 0.333043 || Validation Loss: 0.179680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.966s | Train Loss: 0.343878 || Validation Loss: 0.180999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 1.043s | Train Loss: 0.339571 || Validation Loss: 0.182030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.990s | Train Loss: 0.335863 || Validation Loss: 0.179925 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 1.033s | Train Loss: 0.342239 || Validation Loss: 0.180228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 1.049s | Train Loss: 0.332711 || Validation Loss: 0.179899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 1.058s | Train Loss: 0.338939 || Validation Loss: 0.178835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 1.114s | Train Loss: 0.344367 || Validation Loss: 0.179697 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 1.055s | Train Loss: 0.336945 || Validation Loss: 0.179265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 1.174s | Train Loss: 0.331772 || Validation Loss: 0.175396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 1.122s | Train Loss: 0.338065 || Validation Loss: 0.176563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 1.131s | Train Loss: 0.333216 || Validation Loss: 0.175551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 1.218s | Train Loss: 0.333388 || Validation Loss: 0.175619 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 1.154s | Train Loss: 0.326292 || Validation Loss: 0.174726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 1.211s | Train Loss: 0.336658 || Validation Loss: 0.176688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 1.184s | Train Loss: 0.318411 || Validation Loss: 0.171796 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 1.232s | Train Loss: 0.320018 || Validation Loss: 0.170522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 1.283s | Train Loss: 0.332011 || Validation Loss: 0.172433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 1.256s | Train Loss: 0.341353 || Validation Loss: 0.177614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 1.282s | Train Loss: 0.330989 || Validation Loss: 0.172834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 1.294s | Train Loss: 0.340584 || Validation Loss: 0.175348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 1.264s | Train Loss: 0.324649 || Validation Loss: 0.169715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 1.324s | Train Loss: 0.330639 || Validation Loss: 0.174489 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 1.339s | Train Loss: 0.325306 || Validation Loss: 0.168148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 1.307s | Train Loss: 0.322377 || Validation Loss: 0.165752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 1.354s | Train Loss: 0.326521 || Validation Loss: 0.169125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 1.420s | Train Loss: 0.315934 || Validation Loss: 0.166610 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 1.431s | Train Loss: 0.322534 || Validation Loss: 0.167891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 1.426s | Train Loss: 0.322037 || Validation Loss: 0.169552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 1.506s | Train Loss: 0.316099 || Validation Loss: 0.163982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 1.378s | Train Loss: 0.333768 || Validation Loss: 0.174714 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 1.517s | Train Loss: 0.328058 || Validation Loss: 0.173977 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:418: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
      "INFO:root:  LR scheduler: new learning rate is 1e-05\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 1.582s | Train Loss: 0.320735 || Validation Loss: 0.172976 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 1.461s | Train Loss: 0.325451 || Validation Loss: 0.172877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 1.560s | Train Loss: 0.325625 || Validation Loss: 0.172626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 1.623s | Train Loss: 0.316662 || Validation Loss: 0.171265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 1.565s | Train Loss: 0.336309 || Validation Loss: 0.172242 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 1.464s | Train Loss: 0.310673 || Validation Loss: 0.170984 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 1.655s | Train Loss: 0.316749 || Validation Loss: 0.169917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 1.621s | Train Loss: 0.321145 || Validation Loss: 0.169857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 1.690s | Train Loss: 0.323986 || Validation Loss: 0.170107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 1.658s | Train Loss: 0.323258 || Validation Loss: 0.169958 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 1.735s | Train Loss: 0.320831 || Validation Loss: 0.169882 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 1.590s | Train Loss: 0.317124 || Validation Loss: 0.169211 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 1.747s | Train Loss: 0.326712 || Validation Loss: 0.169411 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 1.695s | Train Loss: 0.322383 || Validation Loss: 0.169185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 1.589s | Train Loss: 0.316744 || Validation Loss: 0.168600 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 1.830s | Train Loss: 0.319405 || Validation Loss: 0.168137 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 1.714s | Train Loss: 0.319852 || Validation Loss: 0.168020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 1.817s | Train Loss: 0.322461 || Validation Loss: 0.168083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 1.800s | Train Loss: 0.326467 || Validation Loss: 0.168310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 1.783s | Train Loss: 0.316484 || Validation Loss: 0.168204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 88.370s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.580%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.164370\n",
      "INFO:root:Test AUC: 47.05%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root dta dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5107739568369212\n",
      "validation AUC 0.5078427177018175\n",
      "validation AUC 0.4993696949985345\n",
      "validation AUC 0.4821036307032524\n",
      "validation AUC 0.4799156398738794\n",
      "validation AUC 0.4687577119877362\n",
      "validation AUC 0.4803656038269552\n",
      "validation AUC 0.48328814519151914\n",
      "validation AUC 0.48285753770240075\n",
      "validation AUC 0.4760573498369427\n",
      "validation AUC 0.4810087412993027\n",
      "validation AUC 0.4707053926496628\n",
      "validation AUC 0.4682916961381579\n",
      "validation AUC 0.4771770288181696\n",
      "validation AUC 0.46815248391723957\n",
      "validation AUC 0.47432407227860623\n",
      "validation AUC 0.47190402205769505\n",
      "validation AUC 0.4580212654503162\n",
      "validation AUC 0.4683061170363916\n",
      "validation AUC 0.4701608547431345\n",
      "validation AUC 0.46336283799445854\n",
      "validation AUC 0.4681765311641464\n",
      "validation AUC 0.469271556262895\n",
      "validation AUC 0.4613752444368857\n",
      "validation AUC 0.46384744669216527\n",
      "validation AUC 0.4667526271045199\n",
      "validation AUC 0.46405674660704466\n",
      "validation AUC 0.47956714902354025\n",
      "validation AUC 0.4856208611756001\n",
      "validation AUC 0.4806079414343376\n",
      "validation AUC 0.47609968129282765\n",
      "validation AUC 0.4793402433736771\n",
      "validation AUC 0.4717775225769538\n",
      "validation AUC 0.4800069438487472\n",
      "validation AUC 0.48595160527479203\n",
      "validation AUC 0.4803775023983391\n",
      "validation AUC 0.4523944784030286\n",
      "validation AUC 0.46366066945321704\n",
      "validation AUC 0.4602598890644466\n",
      "validation AUC 0.47728982578490664\n",
      "validation AUC 0.46910580639640803\n",
      "validation AUC 0.4705584032542912\n",
      "validation AUC 0.4743486995542612\n",
      "validation AUC 0.46938836287790464\n",
      "validation AUC 0.47077970018580073\n",
      "validation AUC 0.4600635067622838\n",
      "validation AUC 0.464730202554553\n",
      "validation AUC 0.4712345198841305\n",
      "validation AUC 0.4584804699531273\n",
      "validation AUC 0.4595599034534167\n",
      "validation AUC 0.4618221060343637\n",
      "validation AUC 0.4620969092875481\n",
      "validation AUC 0.4624325570242759\n",
      "validation AUC 0.46389238827741675\n",
      "validation AUC 0.46365706156797626\n",
      "validation AUC 0.46536926918293425\n",
      "validation AUC 0.46543695428075055\n",
      "validation AUC 0.4667436765691587\n",
      "validation AUC 0.46737765331224085\n",
      "validation AUC 0.4678824512674746\n",
      "validation AUC 0.4686427199580762\n",
      "validation AUC 0.46839164200923666\n",
      "validation AUC 0.46735363267215735\n",
      "validation AUC 0.46710618921538183\n",
      "validation AUC 0.46787839372691986\n",
      "validation AUC 0.468264070273516\n",
      "validation AUC 0.46900926353803063\n",
      "validation AUC 0.4680878559434215\n",
      "validation AUC 0.4681470933748371\n",
      "validation AUC 0.4677161027474631\n",
      "validation AUC 0.4677161027474631\n",
      "train auc 0.5798563598091047\n",
      "Test AUC 0.47048835540174305\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.30554240e-04 ... 9.99753671e-01\n",
      " 9.99753671e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.2027092  1.2027092  0.65364146 ... 0.02011227 0.02002566 0.01195861]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {5..10}\n",
    "do\n",
    "    echo \"Itration $i\"\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test dta --experiment_name \"5_per_labeled_5_unlabeled_exclude_balance_loss\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 0 --balanced_batches 0 --minority_loss 0 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 70 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1\n",
    "done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f611dee2",
   "metadata": {},
   "source": [
    "# Enhancements effect of smote for different levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cfc549",
   "metadata": {},
   "outputs": [],
   "source": [
    " Below multiplier = 4 (0.24 share of minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3efabcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.419s | Train Loss: 0.196734 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.395s | Train Loss: 0.150767 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.415s | Train Loss: 0.117608 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.395s | Train Loss: 0.094378 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.390s | Train Loss: 0.078222 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.392s | Train Loss: 0.066410 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.058168 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.389s | Train Loss: 0.051901 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.399s | Train Loss: 0.047080 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.394s | Train Loss: 0.043450 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.400s | Train Loss: 0.040494 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.400s | Train Loss: 0.037878 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.378s | Train Loss: 0.035904 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.383s | Train Loss: 0.034266 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.386s | Train Loss: 0.032743 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.400s | Train Loss: 0.031451 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.404s | Train Loss: 0.029984 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.400s | Train Loss: 0.028932 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.408s | Train Loss: 0.027917 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.403s | Train Loss: 0.027281 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.392s | Train Loss: 0.026432 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.394s | Train Loss: 0.025806 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.381s | Train Loss: 0.025310 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.388s | Train Loss: 0.024662 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.395s | Train Loss: 0.024181 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.392s | Train Loss: 0.023712 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.391s | Train Loss: 0.023261 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.404s | Train Loss: 0.022983 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.405s | Train Loss: 0.022641 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.407s | Train Loss: 0.022525 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.396s | Train Loss: 0.021968 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.392s | Train Loss: 0.022149 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.395s | Train Loss: 0.021638 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.413s | Train Loss: 0.021313 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.445s | Train Loss: 0.021240 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.445s | Train Loss: 0.021042 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.548s | Train Loss: 0.020626 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.424s | Train Loss: 0.020560 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.381s | Train Loss: 0.020250 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.396s | Train Loss: 0.020202 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.382s | Train Loss: 0.019845 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.394s | Train Loss: 0.019829 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.439s | Train Loss: 0.019703 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.404s | Train Loss: 0.019423 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.378s | Train Loss: 0.019340 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.394s | Train Loss: 0.019072 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.395s | Train Loss: 0.019017 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.393s | Train Loss: 0.018827 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.391s | Train Loss: 0.018541 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.395s | Train Loss: 0.018671 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.393s | Train Loss: 0.018520 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.394s | Train Loss: 0.018565 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.392s | Train Loss: 0.018330 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.397s | Train Loss: 0.018058 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.394s | Train Loss: 0.018202 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.393s | Train Loss: 0.018093 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.392s | Train Loss: 0.017981 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.394s | Train Loss: 0.017884 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.398s | Train Loss: 0.017489 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.403s | Train Loss: 0.017588 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.391s | Train Loss: 0.017646 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.391s | Train Loss: 0.017436 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.404s | Train Loss: 0.017412 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.404s | Train Loss: 0.017401 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.399s | Train Loss: 0.017215 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.392s | Train Loss: 0.017255 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.396s | Train Loss: 0.017318 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.381s | Train Loss: 0.017223 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.389s | Train Loss: 0.017059 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.379s | Train Loss: 0.016987 |\n",
      "INFO:root:Pretraining Time: 27.982s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018129\n",
      "INFO:root:Test AUC: 52.54%\n",
      "INFO:root:Test AUC: 52.54%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.669s | Train Loss: 0.892635 || Validation Loss: 0.017385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.654s | Train Loss: 0.683568 || Validation Loss: 0.017680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.706s | Train Loss: 0.613993 || Validation Loss: 0.016959 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.729s | Train Loss: 0.579644 || Validation Loss: 0.017603 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.731s | Train Loss: 0.539285 || Validation Loss: 0.017193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.774s | Train Loss: 0.522557 || Validation Loss: 0.017569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.783s | Train Loss: 0.504893 || Validation Loss: 0.018974 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.807s | Train Loss: 0.492714 || Validation Loss: 0.019660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.822s | Train Loss: 0.472666 || Validation Loss: 0.021588 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.833s | Train Loss: 0.471393 || Validation Loss: 0.019062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.893s | Train Loss: 0.463912 || Validation Loss: 0.020525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.847s | Train Loss: 0.452920 || Validation Loss: 0.019983 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.931s | Train Loss: 0.445647 || Validation Loss: 0.022238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.885s | Train Loss: 0.430243 || Validation Loss: 0.021548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.959s | Train Loss: 0.428974 || Validation Loss: 0.020335 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.942s | Train Loss: 0.426580 || Validation Loss: 0.020090 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.985s | Train Loss: 0.417197 || Validation Loss: 0.021321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.016s | Train Loss: 0.408128 || Validation Loss: 0.022494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.066s | Train Loss: 0.409638 || Validation Loss: 0.022018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.001s | Train Loss: 0.402887 || Validation Loss: 0.023549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.089s | Train Loss: 0.398638 || Validation Loss: 0.021369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.040s | Train Loss: 0.397982 || Validation Loss: 0.023284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.126s | Train Loss: 0.395212 || Validation Loss: 0.023524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.091s | Train Loss: 0.394727 || Validation Loss: 0.023435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.143s | Train Loss: 0.392519 || Validation Loss: 0.020845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.163s | Train Loss: 0.382606 || Validation Loss: 0.025243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.182s | Train Loss: 0.383162 || Validation Loss: 0.024135 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.189s | Train Loss: 0.379185 || Validation Loss: 0.024956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.256s | Train Loss: 0.373467 || Validation Loss: 0.025704 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.250s | Train Loss: 0.373878 || Validation Loss: 0.025033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.326s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.895%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021614\n",
      "INFO:root:Test AUC: 86.56%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6787924381492464\n",
      "validation AUC 0.7509535034055669\n",
      "validation AUC 0.7874857547068003\n",
      "validation AUC 0.8081711044364643\n",
      "validation AUC 0.821393929344623\n",
      "validation AUC 0.8279060770623002\n",
      "validation AUC 0.8357686434543063\n",
      "validation AUC 0.8406703440134542\n",
      "validation AUC 0.8443090745444431\n",
      "validation AUC 0.8474961622318059\n",
      "validation AUC 0.8504202920237214\n",
      "validation AUC 0.8527911063180988\n",
      "validation AUC 0.8539152446029123\n",
      "validation AUC 0.8547326195183824\n",
      "validation AUC 0.8562936897342148\n",
      "validation AUC 0.8584366804433251\n",
      "validation AUC 0.8593156155658856\n",
      "validation AUC 0.8602165199424529\n",
      "validation AUC 0.8609409811702446\n",
      "validation AUC 0.8616227916602721\n",
      "validation AUC 0.8604898944092252\n",
      "validation AUC 0.8618933351610447\n",
      "validation AUC 0.8611791148995445\n",
      "validation AUC 0.8613008224914077\n",
      "validation AUC 0.8594808465993179\n",
      "validation AUC 0.860992923010709\n",
      "validation AUC 0.8598235690903637\n",
      "validation AUC 0.860703666931103\n",
      "validation AUC 0.86281350553837\n",
      "validation AUC 0.8604587298370832\n",
      "validation AUC 0.8604587298370832\n",
      "train auc 0.8951605720078402\n",
      "Test AUC 0.8655613824668205\n",
      "test false positive rates [0.         0.         0.         ... 0.99384178 0.99387968 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 4.19932811e-03 ... 1.00000000e+00\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.37388878e+01 5.27388878e+01 4.02583809e+01 ... 3.41059640e-02\n",
      " 3.38868238e-02 3.93993920e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.417s | Train Loss: 0.201915 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.399s | Train Loss: 0.154228 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.386s | Train Loss: 0.118145 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.396s | Train Loss: 0.093168 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.383s | Train Loss: 0.076795 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.392s | Train Loss: 0.065367 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.057220 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.395s | Train Loss: 0.051146 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.384s | Train Loss: 0.047061 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.395s | Train Loss: 0.043073 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.382s | Train Loss: 0.040153 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.405s | Train Loss: 0.037930 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.401s | Train Loss: 0.035992 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.389s | Train Loss: 0.034118 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.400s | Train Loss: 0.032460 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.401s | Train Loss: 0.031436 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.399s | Train Loss: 0.030113 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.403s | Train Loss: 0.029135 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.381s | Train Loss: 0.028159 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.394s | Train Loss: 0.027418 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.398s | Train Loss: 0.026526 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.399s | Train Loss: 0.025705 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.403s | Train Loss: 0.025179 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.400s | Train Loss: 0.024614 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.402s | Train Loss: 0.023995 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.401s | Train Loss: 0.023534 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.397s | Train Loss: 0.023185 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.401s | Train Loss: 0.022543 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.386s | Train Loss: 0.022083 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.396s | Train Loss: 0.021876 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.393s | Train Loss: 0.021636 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.396s | Train Loss: 0.021266 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.392s | Train Loss: 0.020904 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.393s | Train Loss: 0.020762 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.385s | Train Loss: 0.020745 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.398s | Train Loss: 0.020070 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.380s | Train Loss: 0.020211 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.398s | Train Loss: 0.020003 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.401s | Train Loss: 0.019747 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.393s | Train Loss: 0.019566 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.402s | Train Loss: 0.019591 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.400s | Train Loss: 0.019298 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.402s | Train Loss: 0.019200 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.404s | Train Loss: 0.019060 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.397s | Train Loss: 0.019106 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.401s | Train Loss: 0.018910 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.401s | Train Loss: 0.018658 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.402s | Train Loss: 0.018675 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.403s | Train Loss: 0.018573 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.402s | Train Loss: 0.018401 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.400s | Train Loss: 0.018351 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.403s | Train Loss: 0.018152 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.401s | Train Loss: 0.018319 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.403s | Train Loss: 0.018238 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.402s | Train Loss: 0.018056 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.402s | Train Loss: 0.017951 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.399s | Train Loss: 0.017739 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.401s | Train Loss: 0.017689 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.404s | Train Loss: 0.017537 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.400s | Train Loss: 0.017519 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.402s | Train Loss: 0.017401 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.403s | Train Loss: 0.017383 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.405s | Train Loss: 0.017281 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.402s | Train Loss: 0.017031 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.401s | Train Loss: 0.017277 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.403s | Train Loss: 0.016968 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.407s | Train Loss: 0.016841 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.404s | Train Loss: 0.016786 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.404s | Train Loss: 0.016792 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.401s | Train Loss: 0.016508 |\n",
      "INFO:root:Pretraining Time: 27.880s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018326\n",
      "INFO:root:Test AUC: 49.00%\n",
      "INFO:root:Test AUC: 49.00%\n",
      "INFO:root:Test Time: 0.232s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.698s | Train Loss: 0.800125 || Validation Loss: 0.014500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.713s | Train Loss: 0.623390 || Validation Loss: 0.014568 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.736s | Train Loss: 0.557235 || Validation Loss: 0.016155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.753s | Train Loss: 0.523300 || Validation Loss: 0.016554 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.753s | Train Loss: 0.491337 || Validation Loss: 0.019418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.806s | Train Loss: 0.477041 || Validation Loss: 0.019000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.803s | Train Loss: 0.452240 || Validation Loss: 0.021779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.824s | Train Loss: 0.451958 || Validation Loss: 0.020405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.831s | Train Loss: 0.441072 || Validation Loss: 0.020123 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.855s | Train Loss: 0.433711 || Validation Loss: 0.021422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.908s | Train Loss: 0.423584 || Validation Loss: 0.020007 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.884s | Train Loss: 0.421676 || Validation Loss: 0.022449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.963s | Train Loss: 0.413107 || Validation Loss: 0.022644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.920s | Train Loss: 0.413641 || Validation Loss: 0.021038 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.992s | Train Loss: 0.402785 || Validation Loss: 0.021411 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.974s | Train Loss: 0.407400 || Validation Loss: 0.021677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.022s | Train Loss: 0.389320 || Validation Loss: 0.023357 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.045s | Train Loss: 0.401926 || Validation Loss: 0.022183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.040s | Train Loss: 0.390993 || Validation Loss: 0.022905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.037s | Train Loss: 0.389722 || Validation Loss: 0.024062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.106s | Train Loss: 0.382456 || Validation Loss: 0.027349 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.056s | Train Loss: 0.380918 || Validation Loss: 0.022947 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.143s | Train Loss: 0.378310 || Validation Loss: 0.025804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.123s | Train Loss: 0.375778 || Validation Loss: 0.023491 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.180s | Train Loss: 0.373361 || Validation Loss: 0.024677 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.202s | Train Loss: 0.371915 || Validation Loss: 0.023110 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.192s | Train Loss: 0.372015 || Validation Loss: 0.025781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.212s | Train Loss: 0.369051 || Validation Loss: 0.026495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.273s | Train Loss: 0.364759 || Validation Loss: 0.024008 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.276s | Train Loss: 0.363120 || Validation Loss: 0.024313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.068s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.909%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020938\n",
      "INFO:root:Test AUC: 86.06%\n",
      "INFO:root:Test Time: 0.205s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.685029442046401\n",
      "validation AUC 0.7755239761534749\n",
      "validation AUC 0.8109562401834125\n",
      "validation AUC 0.8288223255938697\n",
      "validation AUC 0.840013982949922\n",
      "validation AUC 0.8459301564502496\n",
      "validation AUC 0.8504253765876557\n",
      "validation AUC 0.8520070325026826\n",
      "validation AUC 0.8552909743482551\n",
      "validation AUC 0.8552517505693329\n",
      "validation AUC 0.8580633015704625\n",
      "validation AUC 0.8590071892700855\n",
      "validation AUC 0.8603419657929908\n",
      "validation AUC 0.8597938811969154\n",
      "validation AUC 0.858970626173494\n",
      "validation AUC 0.8595344859551095\n",
      "validation AUC 0.8614870675938897\n",
      "validation AUC 0.8619588145532086\n",
      "validation AUC 0.860541913409477\n",
      "validation AUC 0.8611582418466582\n",
      "validation AUC 0.8618171039515815\n",
      "validation AUC 0.8607721502336185\n",
      "validation AUC 0.8616199686763192\n",
      "validation AUC 0.8627084990494979\n",
      "validation AUC 0.8621530895949653\n",
      "validation AUC 0.8613442315236366\n",
      "validation AUC 0.8597619077773447\n",
      "validation AUC 0.8617915693832517\n",
      "validation AUC 0.8577651189016363\n",
      "validation AUC 0.8605960689376406\n",
      "validation AUC 0.8605960689376406\n",
      "train auc 0.9086617609420072\n",
      "Test AUC 0.8605674722551262\n",
      "test false positive rates [0.         0.         0.         ... 0.99511132 0.99511132 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.67973124e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.90132675e+01 6.80132675e+01 4.81480331e+01 ... 1.82282645e-02\n",
      " 1.82098132e-02 4.84284386e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.418s | Train Loss: 0.193023 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.405s | Train Loss: 0.147977 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.385s | Train Loss: 0.114980 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.405s | Train Loss: 0.091839 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.403s | Train Loss: 0.075565 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.405s | Train Loss: 0.064202 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.403s | Train Loss: 0.055841 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.403s | Train Loss: 0.049842 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.403s | Train Loss: 0.045141 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.399s | Train Loss: 0.041393 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.403s | Train Loss: 0.038422 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.403s | Train Loss: 0.036452 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.403s | Train Loss: 0.034164 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.404s | Train Loss: 0.032404 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.404s | Train Loss: 0.030790 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.404s | Train Loss: 0.029587 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.402s | Train Loss: 0.028488 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.405s | Train Loss: 0.027339 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.401s | Train Loss: 0.026592 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.405s | Train Loss: 0.025787 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.404s | Train Loss: 0.025090 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.405s | Train Loss: 0.024559 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.404s | Train Loss: 0.023986 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.405s | Train Loss: 0.023687 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.401s | Train Loss: 0.023275 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.406s | Train Loss: 0.022816 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.401s | Train Loss: 0.022581 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.406s | Train Loss: 0.022153 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.403s | Train Loss: 0.022098 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.406s | Train Loss: 0.021571 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.404s | Train Loss: 0.021246 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.401s | Train Loss: 0.021104 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.402s | Train Loss: 0.020945 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.403s | Train Loss: 0.020730 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.405s | Train Loss: 0.020459 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.402s | Train Loss: 0.020237 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.403s | Train Loss: 0.020096 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.403s | Train Loss: 0.019878 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.404s | Train Loss: 0.019528 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.406s | Train Loss: 0.019402 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.407s | Train Loss: 0.019465 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.397s | Train Loss: 0.019180 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.405s | Train Loss: 0.018970 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.405s | Train Loss: 0.018911 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.404s | Train Loss: 0.018966 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.403s | Train Loss: 0.018627 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.404s | Train Loss: 0.018445 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.407s | Train Loss: 0.018471 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.403s | Train Loss: 0.018215 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.405s | Train Loss: 0.018196 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.406s | Train Loss: 0.018150 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.404s | Train Loss: 0.018350 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.406s | Train Loss: 0.017860 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.405s | Train Loss: 0.018015 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.401s | Train Loss: 0.017826 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.403s | Train Loss: 0.017586 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.401s | Train Loss: 0.017669 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.400s | Train Loss: 0.017654 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.400s | Train Loss: 0.017549 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.404s | Train Loss: 0.017651 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.405s | Train Loss: 0.017405 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.398s | Train Loss: 0.017176 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.402s | Train Loss: 0.017214 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.402s | Train Loss: 0.016981 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.403s | Train Loss: 0.017013 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.401s | Train Loss: 0.016787 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.403s | Train Loss: 0.016999 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.402s | Train Loss: 0.016900 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.403s | Train Loss: 0.016838 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.408s | Train Loss: 0.016664 |\n",
      "INFO:root:Pretraining Time: 28.242s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018377\n",
      "INFO:root:Test AUC: 51.31%\n",
      "INFO:root:Test AUC: 51.31%\n",
      "INFO:root:Test Time: 0.234s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.678s | Train Loss: 1.180138 || Validation Loss: 0.014750 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.689s | Train Loss: 0.782753 || Validation Loss: 0.015127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.728s | Train Loss: 0.677630 || Validation Loss: 0.016206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.733s | Train Loss: 0.622944 || Validation Loss: 0.015030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.755s | Train Loss: 0.580163 || Validation Loss: 0.016815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.797s | Train Loss: 0.554139 || Validation Loss: 0.016433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.813s | Train Loss: 0.528451 || Validation Loss: 0.016872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.827s | Train Loss: 0.526091 || Validation Loss: 0.018431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.837s | Train Loss: 0.503290 || Validation Loss: 0.019113 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.851s | Train Loss: 0.495023 || Validation Loss: 0.019253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.904s | Train Loss: 0.479267 || Validation Loss: 0.018902 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.876s | Train Loss: 0.466625 || Validation Loss: 0.020331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.953s | Train Loss: 0.464759 || Validation Loss: 0.020078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.908s | Train Loss: 0.448500 || Validation Loss: 0.019648 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.988s | Train Loss: 0.453602 || Validation Loss: 0.020653 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.965s | Train Loss: 0.447144 || Validation Loss: 0.019910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.016s | Train Loss: 0.433717 || Validation Loss: 0.021628 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.031s | Train Loss: 0.435144 || Validation Loss: 0.020018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.044s | Train Loss: 0.427596 || Validation Loss: 0.022268 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.021s | Train Loss: 0.426274 || Validation Loss: 0.020259 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.117s | Train Loss: 0.412178 || Validation Loss: 0.022099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.051s | Train Loss: 0.414283 || Validation Loss: 0.022458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.136s | Train Loss: 0.409235 || Validation Loss: 0.022418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.108s | Train Loss: 0.408227 || Validation Loss: 0.021470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.166s | Train Loss: 0.415446 || Validation Loss: 0.022072 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.188s | Train Loss: 0.413620 || Validation Loss: 0.021433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.193s | Train Loss: 0.400054 || Validation Loss: 0.021726 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.196s | Train Loss: 0.392589 || Validation Loss: 0.022447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.269s | Train Loss: 0.389654 || Validation Loss: 0.023096 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.273s | Train Loss: 0.389678 || Validation Loss: 0.023819 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.867s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.876%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019709\n",
      "INFO:root:Test AUC: 85.83%\n",
      "INFO:root:Test Time: 0.206s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.571309266432853\n",
      "validation AUC 0.6631063668425343\n",
      "validation AUC 0.7168322240771317\n",
      "validation AUC 0.7529218176589699\n",
      "validation AUC 0.7762394974886352\n",
      "validation AUC 0.7937407687626528\n",
      "validation AUC 0.8066821227477058\n",
      "validation AUC 0.8145614168495265\n",
      "validation AUC 0.8228883601107951\n",
      "validation AUC 0.8279337694440003\n",
      "validation AUC 0.83340552385323\n",
      "validation AUC 0.8370054456717404\n",
      "validation AUC 0.8398995363601396\n",
      "validation AUC 0.8417829375976338\n",
      "validation AUC 0.8428197469669817\n",
      "validation AUC 0.8455289761077112\n",
      "validation AUC 0.8462489913353285\n",
      "validation AUC 0.8493050164717522\n",
      "validation AUC 0.8488443299688828\n",
      "validation AUC 0.849571976033425\n",
      "validation AUC 0.850459510481279\n",
      "validation AUC 0.8516660926687986\n",
      "validation AUC 0.8512451594206355\n",
      "validation AUC 0.8519743832697998\n",
      "validation AUC 0.8526027300090697\n",
      "validation AUC 0.8524012764463842\n",
      "validation AUC 0.8532835932961874\n",
      "validation AUC 0.8543195246403663\n",
      "validation AUC 0.85413853704617\n",
      "validation AUC 0.8544185020230765\n",
      "validation AUC 0.8544185020230765\n",
      "train auc 0.8764203253358719\n",
      "Test AUC 0.8583316897624188\n",
      "test false positive rates [0.         0.         0.         ... 0.99575557 0.99575557 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.95968645e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.59915886e+01 3.49915886e+01 3.25560417e+01 ... 3.62714380e-02\n",
      " 3.62602547e-02 9.75234527e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.410s | Train Loss: 0.192240 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.395s | Train Loss: 0.147325 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.396s | Train Loss: 0.114337 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.396s | Train Loss: 0.090666 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.392s | Train Loss: 0.074477 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.395s | Train Loss: 0.063376 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.395s | Train Loss: 0.055630 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.392s | Train Loss: 0.049893 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.393s | Train Loss: 0.045640 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.391s | Train Loss: 0.042624 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.393s | Train Loss: 0.039580 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.396s | Train Loss: 0.037340 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.395s | Train Loss: 0.035353 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.390s | Train Loss: 0.033930 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.395s | Train Loss: 0.032286 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.393s | Train Loss: 0.030855 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.393s | Train Loss: 0.029768 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.395s | Train Loss: 0.028718 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.396s | Train Loss: 0.027597 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.386s | Train Loss: 0.026901 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.392s | Train Loss: 0.026203 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.394s | Train Loss: 0.025478 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.393s | Train Loss: 0.024981 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.395s | Train Loss: 0.024384 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.391s | Train Loss: 0.024014 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.393s | Train Loss: 0.023707 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.397s | Train Loss: 0.023079 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.389s | Train Loss: 0.023069 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.393s | Train Loss: 0.022509 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.395s | Train Loss: 0.022408 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.395s | Train Loss: 0.022101 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.394s | Train Loss: 0.021666 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.392s | Train Loss: 0.021529 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.393s | Train Loss: 0.020995 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.393s | Train Loss: 0.021292 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.394s | Train Loss: 0.020880 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.394s | Train Loss: 0.020656 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.392s | Train Loss: 0.020500 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.395s | Train Loss: 0.020443 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.392s | Train Loss: 0.020355 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.390s | Train Loss: 0.020189 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.391s | Train Loss: 0.019879 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.394s | Train Loss: 0.019619 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.396s | Train Loss: 0.019702 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.392s | Train Loss: 0.019652 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.392s | Train Loss: 0.019350 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.393s | Train Loss: 0.019249 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.394s | Train Loss: 0.019076 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.394s | Train Loss: 0.018834 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.393s | Train Loss: 0.018865 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.393s | Train Loss: 0.018594 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.394s | Train Loss: 0.018410 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.396s | Train Loss: 0.018254 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.389s | Train Loss: 0.018156 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.393s | Train Loss: 0.018004 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.394s | Train Loss: 0.017931 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.394s | Train Loss: 0.017827 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.393s | Train Loss: 0.017546 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.398s | Train Loss: 0.017450 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.395s | Train Loss: 0.017473 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.394s | Train Loss: 0.017276 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.396s | Train Loss: 0.017045 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.397s | Train Loss: 0.017107 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.397s | Train Loss: 0.016816 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.395s | Train Loss: 0.016677 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.395s | Train Loss: 0.016584 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.394s | Train Loss: 0.016444 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.396s | Train Loss: 0.016463 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.393s | Train Loss: 0.016367 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.393s | Train Loss: 0.016264 |\n",
      "INFO:root:Pretraining Time: 27.589s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017715\n",
      "INFO:root:Test AUC: 50.12%\n",
      "INFO:root:Test AUC: 50.12%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.661s | Train Loss: 0.882604 || Validation Loss: 0.012911 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.656s | Train Loss: 0.670027 || Validation Loss: 0.013977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.713s | Train Loss: 0.597331 || Validation Loss: 0.014938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.724s | Train Loss: 0.559845 || Validation Loss: 0.015882 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.731s | Train Loss: 0.529046 || Validation Loss: 0.016074 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.775s | Train Loss: 0.501852 || Validation Loss: 0.018134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.790s | Train Loss: 0.482126 || Validation Loss: 0.018773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.803s | Train Loss: 0.466469 || Validation Loss: 0.020023 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.819s | Train Loss: 0.450832 || Validation Loss: 0.019518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.820s | Train Loss: 0.439832 || Validation Loss: 0.022840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.871s | Train Loss: 0.437022 || Validation Loss: 0.021047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.851s | Train Loss: 0.426112 || Validation Loss: 0.022075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.935s | Train Loss: 0.422373 || Validation Loss: 0.023248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.876s | Train Loss: 0.412446 || Validation Loss: 0.021757 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.973s | Train Loss: 0.393744 || Validation Loss: 0.022297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.934s | Train Loss: 0.402536 || Validation Loss: 0.021878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.963s | Train Loss: 0.391919 || Validation Loss: 0.024152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.000s | Train Loss: 0.394040 || Validation Loss: 0.024871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.016s | Train Loss: 0.383977 || Validation Loss: 0.022542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.992s | Train Loss: 0.376843 || Validation Loss: 0.026080 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.084s | Train Loss: 0.380193 || Validation Loss: 0.023515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.030s | Train Loss: 0.381004 || Validation Loss: 0.025125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.120s | Train Loss: 0.368303 || Validation Loss: 0.023348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.086s | Train Loss: 0.369772 || Validation Loss: 0.025043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.141s | Train Loss: 0.368839 || Validation Loss: 0.025699 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.161s | Train Loss: 0.363031 || Validation Loss: 0.023266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.167s | Train Loss: 0.363020 || Validation Loss: 0.024861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.175s | Train Loss: 0.354666 || Validation Loss: 0.024541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.243s | Train Loss: 0.360305 || Validation Loss: 0.026129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.241s | Train Loss: 0.353607 || Validation Loss: 0.027185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.109s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.900%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021900\n",
      "INFO:root:Test AUC: 86.39%\n",
      "INFO:root:Test Time: 0.184s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6434131429831528\n",
      "validation AUC 0.7316958385651215\n",
      "validation AUC 0.7759641993485372\n",
      "validation AUC 0.8011205144354787\n",
      "validation AUC 0.8171110449606358\n",
      "validation AUC 0.8284112022814606\n",
      "validation AUC 0.8364680650006822\n",
      "validation AUC 0.8418398815208801\n",
      "validation AUC 0.8468480093732645\n",
      "validation AUC 0.8491814251167985\n",
      "validation AUC 0.8521177807440248\n",
      "validation AUC 0.854238983125527\n",
      "validation AUC 0.8555994671611137\n",
      "validation AUC 0.8572858209258621\n",
      "validation AUC 0.8571545375382899\n",
      "validation AUC 0.8587934007841137\n",
      "validation AUC 0.8583069748403749\n",
      "validation AUC 0.8584552041137129\n",
      "validation AUC 0.858331152460716\n",
      "validation AUC 0.8597705257274145\n",
      "validation AUC 0.8590793629389938\n",
      "validation AUC 0.8599201146094237\n",
      "validation AUC 0.8608680997598361\n",
      "validation AUC 0.8616210595560747\n",
      "validation AUC 0.8607251466195603\n",
      "validation AUC 0.8613430342165876\n",
      "validation AUC 0.861382202121181\n",
      "validation AUC 0.8614063079030992\n",
      "validation AUC 0.8598775250673525\n",
      "validation AUC 0.860466067998952\n",
      "validation AUC 0.860466067998952\n",
      "train auc 0.9002784137519312\n",
      "Test AUC 0.8638540217269642\n",
      "test false positive rates [0.         0.         0.         ... 0.99522501 0.99522501 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.25979843e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.31540985e+01 5.21540985e+01 3.06562080e+01 ... 1.08468356e-02\n",
      " 1.08461119e-02 1.68623426e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.406s | Train Loss: 0.196335 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.402s | Train Loss: 0.151927 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.399s | Train Loss: 0.118161 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.395s | Train Loss: 0.093995 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.396s | Train Loss: 0.077523 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.396s | Train Loss: 0.065705 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.397s | Train Loss: 0.057324 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.397s | Train Loss: 0.051102 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.397s | Train Loss: 0.046538 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.394s | Train Loss: 0.042564 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.395s | Train Loss: 0.039418 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.394s | Train Loss: 0.036951 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.398s | Train Loss: 0.034730 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.395s | Train Loss: 0.033063 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.398s | Train Loss: 0.031415 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.400s | Train Loss: 0.029978 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.396s | Train Loss: 0.029141 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.383s | Train Loss: 0.028057 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.384s | Train Loss: 0.027257 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.390s | Train Loss: 0.026545 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.395s | Train Loss: 0.025754 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.397s | Train Loss: 0.025245 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.397s | Train Loss: 0.024618 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.396s | Train Loss: 0.024209 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.393s | Train Loss: 0.023701 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.397s | Train Loss: 0.023583 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.397s | Train Loss: 0.023087 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.398s | Train Loss: 0.022948 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.398s | Train Loss: 0.022677 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.395s | Train Loss: 0.022266 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.394s | Train Loss: 0.022201 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.396s | Train Loss: 0.021943 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.394s | Train Loss: 0.021717 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.395s | Train Loss: 0.021659 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.396s | Train Loss: 0.021219 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.394s | Train Loss: 0.021132 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.393s | Train Loss: 0.020915 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.395s | Train Loss: 0.020849 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.397s | Train Loss: 0.020640 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.396s | Train Loss: 0.020512 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.396s | Train Loss: 0.020267 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.398s | Train Loss: 0.020210 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.398s | Train Loss: 0.020242 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.397s | Train Loss: 0.020020 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.394s | Train Loss: 0.019836 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.399s | Train Loss: 0.019693 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.396s | Train Loss: 0.019534 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.398s | Train Loss: 0.019411 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.399s | Train Loss: 0.019380 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.393s | Train Loss: 0.019236 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.397s | Train Loss: 0.018993 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.399s | Train Loss: 0.018962 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.396s | Train Loss: 0.018683 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.399s | Train Loss: 0.018825 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.394s | Train Loss: 0.018377 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.398s | Train Loss: 0.018370 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.397s | Train Loss: 0.018416 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.396s | Train Loss: 0.018225 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.398s | Train Loss: 0.018106 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.398s | Train Loss: 0.017958 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.395s | Train Loss: 0.017971 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.398s | Train Loss: 0.017931 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.396s | Train Loss: 0.017885 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.395s | Train Loss: 0.017920 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.398s | Train Loss: 0.017766 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.395s | Train Loss: 0.017760 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.398s | Train Loss: 0.017681 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.397s | Train Loss: 0.017744 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.394s | Train Loss: 0.017591 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.389s | Train Loss: 0.017344 |\n",
      "INFO:root:Pretraining Time: 27.729s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018770\n",
      "INFO:root:Test AUC: 51.14%\n",
      "INFO:root:Test AUC: 51.14%\n",
      "INFO:root:Test Time: 0.214s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.661s | Train Loss: 0.829321 || Validation Loss: 0.016749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.673s | Train Loss: 0.659661 || Validation Loss: 0.015458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.707s | Train Loss: 0.590326 || Validation Loss: 0.016820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.722s | Train Loss: 0.555371 || Validation Loss: 0.018105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.731s | Train Loss: 0.534962 || Validation Loss: 0.019338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.774s | Train Loss: 0.516691 || Validation Loss: 0.018331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.786s | Train Loss: 0.497552 || Validation Loss: 0.018702 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.799s | Train Loss: 0.494186 || Validation Loss: 0.020686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.812s | Train Loss: 0.465557 || Validation Loss: 0.019750 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.825s | Train Loss: 0.454493 || Validation Loss: 0.020456 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.873s | Train Loss: 0.450921 || Validation Loss: 0.020422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.851s | Train Loss: 0.449289 || Validation Loss: 0.021838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.934s | Train Loss: 0.439366 || Validation Loss: 0.021713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.881s | Train Loss: 0.432879 || Validation Loss: 0.021371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.960s | Train Loss: 0.425986 || Validation Loss: 0.020870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.938s | Train Loss: 0.416636 || Validation Loss: 0.021478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.985s | Train Loss: 0.420004 || Validation Loss: 0.021221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.000s | Train Loss: 0.408452 || Validation Loss: 0.022170 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.020s | Train Loss: 0.400416 || Validation Loss: 0.022309 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.998s | Train Loss: 0.398307 || Validation Loss: 0.024278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.087s | Train Loss: 0.399046 || Validation Loss: 0.023212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.028s | Train Loss: 0.399813 || Validation Loss: 0.023190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.112s | Train Loss: 0.386492 || Validation Loss: 0.023686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.087s | Train Loss: 0.382340 || Validation Loss: 0.024111 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.146s | Train Loss: 0.382573 || Validation Loss: 0.021849 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.158s | Train Loss: 0.383567 || Validation Loss: 0.023660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.168s | Train Loss: 0.379509 || Validation Loss: 0.023105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.181s | Train Loss: 0.374805 || Validation Loss: 0.025637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.241s | Train Loss: 0.374976 || Validation Loss: 0.022816 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.246s | Train Loss: 0.372702 || Validation Loss: 0.023585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.140s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.899%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021249\n",
      "INFO:root:Test AUC: 85.81%\n",
      "INFO:root:Test Time: 0.184s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7249442400803909\n",
      "validation AUC 0.7754753974154769\n",
      "validation AUC 0.8008280415909347\n",
      "validation AUC 0.8149901325935075\n",
      "validation AUC 0.8237590923497294\n",
      "validation AUC 0.8306126747882044\n",
      "validation AUC 0.834986892414565\n",
      "validation AUC 0.8394509080695941\n",
      "validation AUC 0.8425787503434942\n",
      "validation AUC 0.8462884386115666\n",
      "validation AUC 0.8475341620968561\n",
      "validation AUC 0.8502491862037022\n",
      "validation AUC 0.8505412226963441\n",
      "validation AUC 0.8504122620844466\n",
      "validation AUC 0.8522736248901936\n",
      "validation AUC 0.8546910250715033\n",
      "validation AUC 0.8546599403198312\n",
      "validation AUC 0.8548275686280435\n",
      "validation AUC 0.8563418028528049\n",
      "validation AUC 0.8558637553977262\n",
      "validation AUC 0.8557052505692264\n",
      "validation AUC 0.8530573022639427\n",
      "validation AUC 0.8555700639606747\n",
      "validation AUC 0.8559466968480279\n",
      "validation AUC 0.8567571194005675\n",
      "validation AUC 0.8558793922277851\n",
      "validation AUC 0.8563122666182492\n",
      "validation AUC 0.854944854165873\n",
      "validation AUC 0.8547447389263998\n",
      "validation AUC 0.8519947082221258\n",
      "validation AUC 0.8519947082221258\n",
      "train auc 0.8994926504347572\n",
      "Test AUC 0.8580561586697525\n",
      "test false positive rates [0.         0.         0.         ... 0.99982946 0.99982946 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.17244606e+01 6.07244606e+01 6.06599579e+01 ... 1.64357051e-02\n",
      " 1.63350496e-02 7.80904153e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.421s | Train Loss: 0.188176 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.398s | Train Loss: 0.142823 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.397s | Train Loss: 0.110455 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.396s | Train Loss: 0.088236 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.397s | Train Loss: 0.073620 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.395s | Train Loss: 0.063179 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.396s | Train Loss: 0.055499 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.397s | Train Loss: 0.049622 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.400s | Train Loss: 0.045286 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.395s | Train Loss: 0.041436 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.398s | Train Loss: 0.038561 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.394s | Train Loss: 0.036049 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.393s | Train Loss: 0.033938 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.399s | Train Loss: 0.032728 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.397s | Train Loss: 0.031183 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.398s | Train Loss: 0.029690 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.397s | Train Loss: 0.028818 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.397s | Train Loss: 0.027714 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.398s | Train Loss: 0.026981 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.399s | Train Loss: 0.026321 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.395s | Train Loss: 0.025616 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.396s | Train Loss: 0.025025 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.398s | Train Loss: 0.024766 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.397s | Train Loss: 0.024094 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.395s | Train Loss: 0.023780 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.396s | Train Loss: 0.023365 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.395s | Train Loss: 0.023262 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.395s | Train Loss: 0.022846 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.398s | Train Loss: 0.022494 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.394s | Train Loss: 0.022124 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.397s | Train Loss: 0.021691 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.396s | Train Loss: 0.021631 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.394s | Train Loss: 0.021537 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.396s | Train Loss: 0.021156 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.392s | Train Loss: 0.020973 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.393s | Train Loss: 0.020557 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.394s | Train Loss: 0.020395 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.394s | Train Loss: 0.020269 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.393s | Train Loss: 0.020115 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.396s | Train Loss: 0.019807 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.391s | Train Loss: 0.019669 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.395s | Train Loss: 0.019573 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.398s | Train Loss: 0.019435 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.394s | Train Loss: 0.019170 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.398s | Train Loss: 0.019249 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.396s | Train Loss: 0.018975 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.393s | Train Loss: 0.018742 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.401s | Train Loss: 0.018704 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.394s | Train Loss: 0.018392 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.391s | Train Loss: 0.018306 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.397s | Train Loss: 0.018236 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.396s | Train Loss: 0.018083 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.396s | Train Loss: 0.017968 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.396s | Train Loss: 0.017934 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.404s | Train Loss: 0.017726 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.397s | Train Loss: 0.017768 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.396s | Train Loss: 0.017668 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.394s | Train Loss: 0.017644 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.382s | Train Loss: 0.017658 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.382s | Train Loss: 0.017573 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.396s | Train Loss: 0.017336 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.398s | Train Loss: 0.017267 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.396s | Train Loss: 0.017401 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.396s | Train Loss: 0.017358 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.396s | Train Loss: 0.017055 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.396s | Train Loss: 0.017117 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.394s | Train Loss: 0.017058 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.379s | Train Loss: 0.016690 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.396s | Train Loss: 0.016822 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.396s | Train Loss: 0.016803 |\n",
      "INFO:root:Pretraining Time: 27.723s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017949\n",
      "INFO:root:Test AUC: 50.45%\n",
      "INFO:root:Test AUC: 50.45%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.662s | Train Loss: 0.774818 || Validation Loss: 0.014446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.675s | Train Loss: 0.615055 || Validation Loss: 0.014685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.725s | Train Loss: 0.556637 || Validation Loss: 0.015657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.728s | Train Loss: 0.522452 || Validation Loss: 0.017599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.741s | Train Loss: 0.498826 || Validation Loss: 0.018430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.777s | Train Loss: 0.477902 || Validation Loss: 0.018735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.790s | Train Loss: 0.468923 || Validation Loss: 0.020701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.803s | Train Loss: 0.448324 || Validation Loss: 0.020194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.825s | Train Loss: 0.452391 || Validation Loss: 0.019659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.826s | Train Loss: 0.438812 || Validation Loss: 0.020062 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.882s | Train Loss: 0.431079 || Validation Loss: 0.021443 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.855s | Train Loss: 0.425016 || Validation Loss: 0.019837 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.921s | Train Loss: 0.422698 || Validation Loss: 0.022581 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.883s | Train Loss: 0.425108 || Validation Loss: 0.021675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.963s | Train Loss: 0.402344 || Validation Loss: 0.021835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.948s | Train Loss: 0.399939 || Validation Loss: 0.022444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.003s | Train Loss: 0.404917 || Validation Loss: 0.024125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.012s | Train Loss: 0.395926 || Validation Loss: 0.022698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.026s | Train Loss: 0.394110 || Validation Loss: 0.021031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.000s | Train Loss: 0.392183 || Validation Loss: 0.021069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.091s | Train Loss: 0.390262 || Validation Loss: 0.021331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.031s | Train Loss: 0.379431 || Validation Loss: 0.024577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.114s | Train Loss: 0.383932 || Validation Loss: 0.024159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.091s | Train Loss: 0.386909 || Validation Loss: 0.023153 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.148s | Train Loss: 0.377208 || Validation Loss: 0.022907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.160s | Train Loss: 0.379698 || Validation Loss: 0.021548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.168s | Train Loss: 0.371432 || Validation Loss: 0.023039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.179s | Train Loss: 0.374607 || Validation Loss: 0.024819 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.243s | Train Loss: 0.368672 || Validation Loss: 0.023559 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.243s | Train Loss: 0.371185 || Validation Loss: 0.026385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.263s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.911%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021653\n",
      "INFO:root:Test AUC: 87.53%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7134977212852245\n",
      "validation AUC 0.789476466584065\n",
      "validation AUC 0.8193746497211711\n",
      "validation AUC 0.8336436708859412\n",
      "validation AUC 0.8415273471315611\n",
      "validation AUC 0.8485121757080554\n",
      "validation AUC 0.8521595614386671\n",
      "validation AUC 0.8559422215803474\n",
      "validation AUC 0.8576973726081263\n",
      "validation AUC 0.85923021564511\n",
      "validation AUC 0.8607078255775862\n",
      "validation AUC 0.8618852759542643\n",
      "validation AUC 0.8633723579158387\n",
      "validation AUC 0.8645162517669591\n",
      "validation AUC 0.866268603756926\n",
      "validation AUC 0.8668161429131236\n",
      "validation AUC 0.8667225188232633\n",
      "validation AUC 0.868541137767364\n",
      "validation AUC 0.8680611533355485\n",
      "validation AUC 0.8695099267929222\n",
      "validation AUC 0.8700750956302444\n",
      "validation AUC 0.8704426023772025\n",
      "validation AUC 0.8707940811737994\n",
      "validation AUC 0.8713806312543328\n",
      "validation AUC 0.8720408848961154\n",
      "validation AUC 0.8719171152754455\n",
      "validation AUC 0.8720997445106398\n",
      "validation AUC 0.8711262593807677\n",
      "validation AUC 0.8725994073489748\n",
      "validation AUC 0.8720266316208686\n",
      "validation AUC 0.8720266316208686\n",
      "train auc 0.9110909815280908\n",
      "Test AUC 0.8752824245420591\n",
      "test false positive rates [0.         0.         0.         ... 0.98741829 0.98741829 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 7.27883539e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.16354256e+01 5.06354256e+01 3.95511208e+01 ... 3.11299264e-02\n",
      " 3.10801417e-02 6.79303426e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.418s | Train Loss: 0.202737 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.399s | Train Loss: 0.156391 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.392s | Train Loss: 0.120163 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.394s | Train Loss: 0.094786 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.399s | Train Loss: 0.078408 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.396s | Train Loss: 0.067204 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.059117 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.396s | Train Loss: 0.053110 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.398s | Train Loss: 0.048443 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.394s | Train Loss: 0.044657 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.394s | Train Loss: 0.041330 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.390s | Train Loss: 0.038355 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.395s | Train Loss: 0.036318 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.395s | Train Loss: 0.034338 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.395s | Train Loss: 0.032768 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.394s | Train Loss: 0.031346 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.392s | Train Loss: 0.030015 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.389s | Train Loss: 0.029034 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.399s | Train Loss: 0.028341 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.397s | Train Loss: 0.027146 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.396s | Train Loss: 0.026697 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.395s | Train Loss: 0.026006 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.395s | Train Loss: 0.025354 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.391s | Train Loss: 0.024752 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.394s | Train Loss: 0.024320 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.396s | Train Loss: 0.023991 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.396s | Train Loss: 0.023708 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.397s | Train Loss: 0.023031 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.395s | Train Loss: 0.022878 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.396s | Train Loss: 0.022811 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.395s | Train Loss: 0.022473 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.394s | Train Loss: 0.021959 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.399s | Train Loss: 0.021822 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.393s | Train Loss: 0.021467 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.394s | Train Loss: 0.021222 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.395s | Train Loss: 0.021140 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.393s | Train Loss: 0.020647 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.397s | Train Loss: 0.020618 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.394s | Train Loss: 0.020202 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.396s | Train Loss: 0.020097 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.394s | Train Loss: 0.019986 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.394s | Train Loss: 0.019884 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.398s | Train Loss: 0.019746 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.393s | Train Loss: 0.019607 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.395s | Train Loss: 0.019432 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.393s | Train Loss: 0.019181 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.394s | Train Loss: 0.019040 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.395s | Train Loss: 0.018940 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.397s | Train Loss: 0.019103 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.396s | Train Loss: 0.018819 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.395s | Train Loss: 0.018690 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.396s | Train Loss: 0.018529 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.396s | Train Loss: 0.018401 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.395s | Train Loss: 0.018367 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.394s | Train Loss: 0.018352 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.396s | Train Loss: 0.018230 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.397s | Train Loss: 0.018010 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.394s | Train Loss: 0.017838 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.394s | Train Loss: 0.017851 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.399s | Train Loss: 0.017775 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.396s | Train Loss: 0.017660 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.397s | Train Loss: 0.017724 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.393s | Train Loss: 0.017449 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.396s | Train Loss: 0.017518 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.382s | Train Loss: 0.017555 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.395s | Train Loss: 0.017318 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.397s | Train Loss: 0.017241 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.394s | Train Loss: 0.017089 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.391s | Train Loss: 0.017229 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.395s | Train Loss: 0.017010 |\n",
      "INFO:root:Pretraining Time: 27.669s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018291\n",
      "INFO:root:Test AUC: 51.25%\n",
      "INFO:root:Test AUC: 51.25%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.674s | Train Loss: 0.740408 || Validation Loss: 0.014492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.675s | Train Loss: 0.614573 || Validation Loss: 0.014253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.720s | Train Loss: 0.554742 || Validation Loss: 0.015137 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.740s | Train Loss: 0.523133 || Validation Loss: 0.016399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.741s | Train Loss: 0.498937 || Validation Loss: 0.017047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.774s | Train Loss: 0.482558 || Validation Loss: 0.018067 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.790s | Train Loss: 0.472371 || Validation Loss: 0.019715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.813s | Train Loss: 0.459467 || Validation Loss: 0.019464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.824s | Train Loss: 0.451861 || Validation Loss: 0.018662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.842s | Train Loss: 0.440812 || Validation Loss: 0.021484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.878s | Train Loss: 0.433390 || Validation Loss: 0.019379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.857s | Train Loss: 0.425693 || Validation Loss: 0.021737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.937s | Train Loss: 0.427484 || Validation Loss: 0.020594 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.896s | Train Loss: 0.417667 || Validation Loss: 0.022020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.979s | Train Loss: 0.411675 || Validation Loss: 0.022903 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.939s | Train Loss: 0.418602 || Validation Loss: 0.021650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.005s | Train Loss: 0.406728 || Validation Loss: 0.021734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.000s | Train Loss: 0.403281 || Validation Loss: 0.020524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.033s | Train Loss: 0.397533 || Validation Loss: 0.024740 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.009s | Train Loss: 0.388931 || Validation Loss: 0.024268 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.091s | Train Loss: 0.391818 || Validation Loss: 0.024964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.038s | Train Loss: 0.393466 || Validation Loss: 0.023859 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.133s | Train Loss: 0.391583 || Validation Loss: 0.023606 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.098s | Train Loss: 0.385127 || Validation Loss: 0.024237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.156s | Train Loss: 0.376406 || Validation Loss: 0.023916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.170s | Train Loss: 0.371981 || Validation Loss: 0.025515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.171s | Train Loss: 0.376961 || Validation Loss: 0.024442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.188s | Train Loss: 0.367779 || Validation Loss: 0.022806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.249s | Train Loss: 0.376039 || Validation Loss: 0.024184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.242s | Train Loss: 0.369178 || Validation Loss: 0.025387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.417s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.910%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021565\n",
      "INFO:root:Test AUC: 87.01%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.713900662999466\n",
      "validation AUC 0.7774324516612555\n",
      "validation AUC 0.8053469604258624\n",
      "validation AUC 0.8214118144512512\n",
      "validation AUC 0.8325443035536285\n",
      "validation AUC 0.8402515446325205\n",
      "validation AUC 0.844024285750471\n",
      "validation AUC 0.8486740808885742\n",
      "validation AUC 0.85242317652265\n",
      "validation AUC 0.8548026673021086\n",
      "validation AUC 0.8571408004354153\n",
      "validation AUC 0.8583510091329518\n",
      "validation AUC 0.8591783562857982\n",
      "validation AUC 0.8603502165688991\n",
      "validation AUC 0.8622770454633983\n",
      "validation AUC 0.8618116708382617\n",
      "validation AUC 0.8623642014345122\n",
      "validation AUC 0.8640937700016795\n",
      "validation AUC 0.8644037421539139\n",
      "validation AUC 0.8650636339428992\n",
      "validation AUC 0.864510999580038\n",
      "validation AUC 0.8655381081016074\n",
      "validation AUC 0.8659014136311652\n",
      "validation AUC 0.8668696811629863\n",
      "validation AUC 0.8650996569209776\n",
      "validation AUC 0.8656675715824653\n",
      "validation AUC 0.8651330511449129\n",
      "validation AUC 0.8652908535532773\n",
      "validation AUC 0.8651576917239795\n",
      "validation AUC 0.8647936385426869\n",
      "validation AUC 0.8647936385426869\n",
      "train auc 0.9097644306494257\n",
      "Test AUC 0.8700727114358053\n",
      "test false positive rates [0.         0.         0.         ... 0.99725249 0.99725249 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.72718887e+01 7.62718887e+01 6.58679047e+01 ... 2.91377511e-02\n",
      " 2.89254095e-02 8.26559495e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.403s | Train Loss: 0.195127 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.400s | Train Loss: 0.151011 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.398s | Train Loss: 0.117289 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.396s | Train Loss: 0.093783 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.396s | Train Loss: 0.078006 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.398s | Train Loss: 0.066564 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.397s | Train Loss: 0.058069 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.398s | Train Loss: 0.051741 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.397s | Train Loss: 0.047112 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.397s | Train Loss: 0.043679 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.394s | Train Loss: 0.040738 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.396s | Train Loss: 0.038339 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.396s | Train Loss: 0.036383 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.396s | Train Loss: 0.034696 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.394s | Train Loss: 0.033477 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.395s | Train Loss: 0.032103 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.398s | Train Loss: 0.030939 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.396s | Train Loss: 0.029822 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.398s | Train Loss: 0.028858 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.398s | Train Loss: 0.027563 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.396s | Train Loss: 0.026928 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.398s | Train Loss: 0.026311 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.396s | Train Loss: 0.025514 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.397s | Train Loss: 0.024804 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.396s | Train Loss: 0.024265 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.394s | Train Loss: 0.023657 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.397s | Train Loss: 0.022998 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.394s | Train Loss: 0.023017 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.393s | Train Loss: 0.022493 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.394s | Train Loss: 0.022431 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.397s | Train Loss: 0.021926 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.396s | Train Loss: 0.021734 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.398s | Train Loss: 0.021661 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.395s | Train Loss: 0.021062 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.398s | Train Loss: 0.020820 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.397s | Train Loss: 0.020752 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.394s | Train Loss: 0.020341 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.393s | Train Loss: 0.020217 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.395s | Train Loss: 0.020206 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.394s | Train Loss: 0.020012 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.396s | Train Loss: 0.019768 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.395s | Train Loss: 0.019524 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.390s | Train Loss: 0.019446 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.393s | Train Loss: 0.019099 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.398s | Train Loss: 0.019177 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.398s | Train Loss: 0.019187 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.399s | Train Loss: 0.019102 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.398s | Train Loss: 0.018768 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.398s | Train Loss: 0.018459 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.396s | Train Loss: 0.018314 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.398s | Train Loss: 0.018244 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.399s | Train Loss: 0.017972 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.398s | Train Loss: 0.017906 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.398s | Train Loss: 0.017620 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.397s | Train Loss: 0.017644 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.399s | Train Loss: 0.017611 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.399s | Train Loss: 0.017456 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.398s | Train Loss: 0.017192 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.396s | Train Loss: 0.017241 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.397s | Train Loss: 0.017106 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.399s | Train Loss: 0.017035 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.378s | Train Loss: 0.016843 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.399s | Train Loss: 0.016806 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.396s | Train Loss: 0.016700 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.397s | Train Loss: 0.016659 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.396s | Train Loss: 0.016515 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.398s | Train Loss: 0.016261 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.402s | Train Loss: 0.016290 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.400s | Train Loss: 0.016302 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.395s | Train Loss: 0.016215 |\n",
      "INFO:root:Pretraining Time: 27.763s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017586\n",
      "INFO:root:Test AUC: 49.42%\n",
      "INFO:root:Test AUC: 49.42%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.665s | Train Loss: 0.829016 || Validation Loss: 0.013066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.672s | Train Loss: 0.654180 || Validation Loss: 0.015418 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.712s | Train Loss: 0.581659 || Validation Loss: 0.016450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.726s | Train Loss: 0.547114 || Validation Loss: 0.016297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.738s | Train Loss: 0.524494 || Validation Loss: 0.015999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.775s | Train Loss: 0.511021 || Validation Loss: 0.015259 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.789s | Train Loss: 0.491772 || Validation Loss: 0.019664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.803s | Train Loss: 0.480856 || Validation Loss: 0.017431 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.815s | Train Loss: 0.473111 || Validation Loss: 0.018755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.830s | Train Loss: 0.453783 || Validation Loss: 0.019519 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.879s | Train Loss: 0.446496 || Validation Loss: 0.019076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.828s | Train Loss: 0.441859 || Validation Loss: 0.020170 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.929s | Train Loss: 0.435673 || Validation Loss: 0.019267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.881s | Train Loss: 0.431658 || Validation Loss: 0.021917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.956s | Train Loss: 0.427698 || Validation Loss: 0.020387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.938s | Train Loss: 0.424918 || Validation Loss: 0.022104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.989s | Train Loss: 0.407972 || Validation Loss: 0.021447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.998s | Train Loss: 0.407550 || Validation Loss: 0.020376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.027s | Train Loss: 0.403313 || Validation Loss: 0.019878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.997s | Train Loss: 0.398405 || Validation Loss: 0.020274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.087s | Train Loss: 0.387958 || Validation Loss: 0.022801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.024s | Train Loss: 0.387916 || Validation Loss: 0.020717 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.113s | Train Loss: 0.392949 || Validation Loss: 0.022486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.092s | Train Loss: 0.390779 || Validation Loss: 0.019318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.140s | Train Loss: 0.379970 || Validation Loss: 0.021027 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.153s | Train Loss: 0.382129 || Validation Loss: 0.021905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.173s | Train Loss: 0.377097 || Validation Loss: 0.020720 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.179s | Train Loss: 0.366187 || Validation Loss: 0.022971 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.226s | Train Loss: 0.369794 || Validation Loss: 0.021878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.242s | Train Loss: 0.359377 || Validation Loss: 0.023235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.130s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.903%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019989\n",
      "INFO:root:Test AUC: 87.13%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7199646799741934\n",
      "validation AUC 0.7662459719930192\n",
      "validation AUC 0.7885872638883893\n",
      "validation AUC 0.805516405980107\n",
      "validation AUC 0.8177537726879045\n",
      "validation AUC 0.8266565221944543\n",
      "validation AUC 0.833635058257236\n",
      "validation AUC 0.8386600362108222\n",
      "validation AUC 0.8422976785227381\n",
      "validation AUC 0.8453767398467831\n",
      "validation AUC 0.8479096402480778\n",
      "validation AUC 0.8508294650559818\n",
      "validation AUC 0.8530927638380492\n",
      "validation AUC 0.8561383191895008\n",
      "validation AUC 0.8574462281422285\n",
      "validation AUC 0.8570994321465342\n",
      "validation AUC 0.8601969240170854\n",
      "validation AUC 0.8603501394091114\n",
      "validation AUC 0.8605081972429797\n",
      "validation AUC 0.8625488980198777\n",
      "validation AUC 0.863359738299543\n",
      "validation AUC 0.8638016909381204\n",
      "validation AUC 0.8634395082165064\n",
      "validation AUC 0.8628791658569324\n",
      "validation AUC 0.8641207706059737\n",
      "validation AUC 0.8638722841617245\n",
      "validation AUC 0.864981935031246\n",
      "validation AUC 0.864904290999465\n",
      "validation AUC 0.8648327745190922\n",
      "validation AUC 0.8658399492086385\n",
      "validation AUC 0.8658399492086385\n",
      "train auc 0.9033681540966939\n",
      "Test AUC 0.871271078863289\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.99393652e-01\n",
      " 9.99393652e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.45357628e+01 4.35357628e+01 3.62187843e+01 ... 2.97487099e-02\n",
      " 2.96175834e-02 1.01105515e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.414s | Train Loss: 0.207798 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.398s | Train Loss: 0.161709 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.396s | Train Loss: 0.126482 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.396s | Train Loss: 0.100308 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.395s | Train Loss: 0.082176 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.397s | Train Loss: 0.069401 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.394s | Train Loss: 0.060314 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.396s | Train Loss: 0.053783 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.398s | Train Loss: 0.048894 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.396s | Train Loss: 0.044657 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.396s | Train Loss: 0.041974 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.396s | Train Loss: 0.039504 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.395s | Train Loss: 0.037572 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.396s | Train Loss: 0.035731 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.393s | Train Loss: 0.034188 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.396s | Train Loss: 0.032552 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.398s | Train Loss: 0.031277 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.394s | Train Loss: 0.030097 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.397s | Train Loss: 0.029265 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.397s | Train Loss: 0.027933 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.394s | Train Loss: 0.026910 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.395s | Train Loss: 0.026560 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.398s | Train Loss: 0.025681 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.397s | Train Loss: 0.025114 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.398s | Train Loss: 0.024607 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.394s | Train Loss: 0.023968 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.397s | Train Loss: 0.023495 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.397s | Train Loss: 0.023185 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.399s | Train Loss: 0.022587 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.397s | Train Loss: 0.022324 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.396s | Train Loss: 0.022087 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.397s | Train Loss: 0.021804 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.398s | Train Loss: 0.021590 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.395s | Train Loss: 0.021046 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.398s | Train Loss: 0.021086 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.394s | Train Loss: 0.020824 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.401s | Train Loss: 0.020825 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.402s | Train Loss: 0.020538 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.395s | Train Loss: 0.020274 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.389s | Train Loss: 0.020209 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.383s | Train Loss: 0.019953 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.394s | Train Loss: 0.019885 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.394s | Train Loss: 0.019757 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.388s | Train Loss: 0.019684 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.394s | Train Loss: 0.019240 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.393s | Train Loss: 0.019272 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.391s | Train Loss: 0.019038 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.394s | Train Loss: 0.018953 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.395s | Train Loss: 0.018945 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.385s | Train Loss: 0.018744 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.386s | Train Loss: 0.018597 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.395s | Train Loss: 0.018458 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.398s | Train Loss: 0.018373 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.386s | Train Loss: 0.018403 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.389s | Train Loss: 0.018304 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.403s | Train Loss: 0.018249 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.392s | Train Loss: 0.018160 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.393s | Train Loss: 0.017839 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.395s | Train Loss: 0.017913 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.379s | Train Loss: 0.017910 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.393s | Train Loss: 0.017957 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.391s | Train Loss: 0.017707 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.394s | Train Loss: 0.017647 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.391s | Train Loss: 0.017608 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.378s | Train Loss: 0.017536 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.394s | Train Loss: 0.017435 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.379s | Train Loss: 0.017482 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.399s | Train Loss: 0.017363 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.388s | Train Loss: 0.017210 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.392s | Train Loss: 0.017194 |\n",
      "INFO:root:Pretraining Time: 27.608s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018790\n",
      "INFO:root:Test AUC: 50.33%\n",
      "INFO:root:Test AUC: 50.33%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.641s | Train Loss: 0.786982 || Validation Loss: 0.015042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.655s | Train Loss: 0.628765 || Validation Loss: 0.014635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.687s | Train Loss: 0.570624 || Validation Loss: 0.015691 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.720s | Train Loss: 0.523221 || Validation Loss: 0.018141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.733s | Train Loss: 0.504193 || Validation Loss: 0.017922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.781s | Train Loss: 0.482172 || Validation Loss: 0.019690 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.793s | Train Loss: 0.474321 || Validation Loss: 0.018921 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.803s | Train Loss: 0.459157 || Validation Loss: 0.019870 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.807s | Train Loss: 0.452332 || Validation Loss: 0.019922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.821s | Train Loss: 0.443133 || Validation Loss: 0.021083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.871s | Train Loss: 0.440298 || Validation Loss: 0.020790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.851s | Train Loss: 0.424229 || Validation Loss: 0.023832 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.945s | Train Loss: 0.422746 || Validation Loss: 0.023467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.889s | Train Loss: 0.424036 || Validation Loss: 0.023360 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.982s | Train Loss: 0.413501 || Validation Loss: 0.022285 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.943s | Train Loss: 0.416117 || Validation Loss: 0.024163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.028s | Train Loss: 0.411335 || Validation Loss: 0.023644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.015s | Train Loss: 0.409888 || Validation Loss: 0.023614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.026s | Train Loss: 0.409639 || Validation Loss: 0.022960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.025s | Train Loss: 0.403979 || Validation Loss: 0.022042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.088s | Train Loss: 0.395236 || Validation Loss: 0.022384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.028s | Train Loss: 0.393265 || Validation Loss: 0.023240 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.113s | Train Loss: 0.393255 || Validation Loss: 0.025320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.086s | Train Loss: 0.389940 || Validation Loss: 0.026146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.156s | Train Loss: 0.383301 || Validation Loss: 0.024101 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.191s | Train Loss: 0.387746 || Validation Loss: 0.023865 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.253s | Train Loss: 0.386683 || Validation Loss: 0.023348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.255s | Train Loss: 0.388145 || Validation Loss: 0.024718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.341s | Train Loss: 0.378677 || Validation Loss: 0.026019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.307s | Train Loss: 0.383764 || Validation Loss: 0.023640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.652s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.903%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022486\n",
      "INFO:root:Test AUC: 85.58%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6905226921081821\n",
      "validation AUC 0.7700760774220243\n",
      "validation AUC 0.8061402402021436\n",
      "validation AUC 0.8213372408468803\n",
      "validation AUC 0.8318258102363048\n",
      "validation AUC 0.837743854196311\n",
      "validation AUC 0.8410542272601592\n",
      "validation AUC 0.8439586094678146\n",
      "validation AUC 0.8448035304274567\n",
      "validation AUC 0.8463018644146083\n",
      "validation AUC 0.8474722214121923\n",
      "validation AUC 0.8480732748721117\n",
      "validation AUC 0.8496248597554346\n",
      "validation AUC 0.8491022086217601\n",
      "validation AUC 0.8496108858518323\n",
      "validation AUC 0.8499815268825763\n",
      "validation AUC 0.8503566724485067\n",
      "validation AUC 0.8509697974241615\n",
      "validation AUC 0.8499071927396151\n",
      "validation AUC 0.8502820243450305\n",
      "validation AUC 0.8506149049721341\n",
      "validation AUC 0.8500279664319804\n",
      "validation AUC 0.8500642448355624\n",
      "validation AUC 0.8503354189180474\n",
      "validation AUC 0.8500423979729431\n",
      "validation AUC 0.8521289635918615\n",
      "validation AUC 0.8514080969458985\n",
      "validation AUC 0.8501068530024097\n",
      "validation AUC 0.8509010666781891\n",
      "validation AUC 0.850347187115997\n",
      "validation AUC 0.850347187115997\n",
      "train auc 0.903498802186456\n",
      "Test AUC 0.8557675944752677\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.86906679e-01\n",
      " 9.86906679e-01 1.00000000e+00] true positive rate [0.         0.         0.00139978 ... 0.99972004 1.         1.        ] tresholds [6.93691254e+01 6.83691254e+01 5.46676292e+01 ... 4.97790053e-02\n",
      " 4.97618653e-02 5.05259121e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.408s | Train Loss: 0.192330 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.393s | Train Loss: 0.146324 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.402s | Train Loss: 0.113608 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.394s | Train Loss: 0.091801 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.401s | Train Loss: 0.076688 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.401s | Train Loss: 0.066523 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.398s | Train Loss: 0.059124 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.402s | Train Loss: 0.053848 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.395s | Train Loss: 0.049704 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.401s | Train Loss: 0.046062 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.400s | Train Loss: 0.043423 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.399s | Train Loss: 0.040867 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.406s | Train Loss: 0.038972 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.409s | Train Loss: 0.037277 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.403s | Train Loss: 0.035586 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.397s | Train Loss: 0.034244 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.396s | Train Loss: 0.032700 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.403s | Train Loss: 0.031245 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.399s | Train Loss: 0.030164 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.398s | Train Loss: 0.029045 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.397s | Train Loss: 0.028243 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.396s | Train Loss: 0.027373 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.401s | Train Loss: 0.026781 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.396s | Train Loss: 0.026052 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.389s | Train Loss: 0.025521 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.389s | Train Loss: 0.024895 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.392s | Train Loss: 0.024259 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.402s | Train Loss: 0.024033 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.391s | Train Loss: 0.023568 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.396s | Train Loss: 0.023290 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.395s | Train Loss: 0.022910 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.383s | Train Loss: 0.022757 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.394s | Train Loss: 0.022419 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.397s | Train Loss: 0.022076 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.387s | Train Loss: 0.022178 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.398s | Train Loss: 0.021818 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.392s | Train Loss: 0.021556 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.390s | Train Loss: 0.021288 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.394s | Train Loss: 0.021242 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.399s | Train Loss: 0.021086 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.391s | Train Loss: 0.020929 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.390s | Train Loss: 0.020611 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.396s | Train Loss: 0.020539 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.395s | Train Loss: 0.020356 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.392s | Train Loss: 0.020187 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.399s | Train Loss: 0.020057 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.402s | Train Loss: 0.019978 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.398s | Train Loss: 0.020032 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.401s | Train Loss: 0.019818 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.400s | Train Loss: 0.019768 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.402s | Train Loss: 0.019738 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.398s | Train Loss: 0.019497 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.395s | Train Loss: 0.019549 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.399s | Train Loss: 0.019298 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.374s | Train Loss: 0.019265 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.397s | Train Loss: 0.019066 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.397s | Train Loss: 0.018920 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.388s | Train Loss: 0.019017 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.389s | Train Loss: 0.018935 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.401s | Train Loss: 0.018616 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.397s | Train Loss: 0.018724 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.396s | Train Loss: 0.018731 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.395s | Train Loss: 0.018496 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.396s | Train Loss: 0.018226 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.397s | Train Loss: 0.018178 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.392s | Train Loss: 0.018373 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.397s | Train Loss: 0.018279 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.394s | Train Loss: 0.018102 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.399s | Train Loss: 0.018063 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.393s | Train Loss: 0.017833 |\n",
      "INFO:root:Pretraining Time: 27.759s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018400\n",
      "INFO:root:Test AUC: 52.65%\n",
      "INFO:root:Test AUC: 52.65%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.666s | Train Loss: 0.763802 || Validation Loss: 0.013478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.688s | Train Loss: 0.622520 || Validation Loss: 0.015153 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.716s | Train Loss: 0.564499 || Validation Loss: 0.016369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.729s | Train Loss: 0.528994 || Validation Loss: 0.017969 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.754s | Train Loss: 0.513079 || Validation Loss: 0.018878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.793s | Train Loss: 0.489092 || Validation Loss: 0.018694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.821s | Train Loss: 0.467976 || Validation Loss: 0.018659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.842s | Train Loss: 0.468922 || Validation Loss: 0.018990 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.863s | Train Loss: 0.454528 || Validation Loss: 0.020950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.840s | Train Loss: 0.447532 || Validation Loss: 0.021280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.955s | Train Loss: 0.441426 || Validation Loss: 0.019928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.856s | Train Loss: 0.431513 || Validation Loss: 0.021980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.977s | Train Loss: 0.424195 || Validation Loss: 0.022298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.902s | Train Loss: 0.417553 || Validation Loss: 0.021355 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.976s | Train Loss: 0.416868 || Validation Loss: 0.021317 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.945s | Train Loss: 0.413509 || Validation Loss: 0.022376 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.008s | Train Loss: 0.405239 || Validation Loss: 0.021165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.025s | Train Loss: 0.398781 || Validation Loss: 0.023989 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.096s | Train Loss: 0.397654 || Validation Loss: 0.025102 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.074s | Train Loss: 0.401667 || Validation Loss: 0.023787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.111s | Train Loss: 0.396811 || Validation Loss: 0.023280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.056s | Train Loss: 0.391390 || Validation Loss: 0.022444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.133s | Train Loss: 0.384603 || Validation Loss: 0.023533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.113s | Train Loss: 0.387454 || Validation Loss: 0.022557 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.169s | Train Loss: 0.383744 || Validation Loss: 0.023878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.195s | Train Loss: 0.377256 || Validation Loss: 0.023319 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.182s | Train Loss: 0.378202 || Validation Loss: 0.024028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.202s | Train Loss: 0.373917 || Validation Loss: 0.024507 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.277s | Train Loss: 0.368165 || Validation Loss: 0.024652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.257s | Train Loss: 0.362711 || Validation Loss: 0.024533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.991s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.907%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022243\n",
      "INFO:root:Test AUC: 86.26%\n",
      "INFO:root:Test Time: 0.210s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 3\n",
      "experiment_method type 3\n",
      "method num 3\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 3\n",
      "normal random 12313 random outlier 834 unlabeled 834 12313\n",
      "saving in path /Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/5_per_labeled_5_unlabeled_smote_m4\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6842434206115184\n",
      "validation AUC 0.7526209343975777\n",
      "validation AUC 0.7908515125340487\n",
      "validation AUC 0.8131414825279377\n",
      "validation AUC 0.8237894108248903\n",
      "validation AUC 0.8312828713998575\n",
      "validation AUC 0.8372332692570077\n",
      "validation AUC 0.8418192026978042\n",
      "validation AUC 0.8447879813999147\n",
      "validation AUC 0.8460489000419963\n",
      "validation AUC 0.848544340696754\n",
      "validation AUC 0.850949813039174\n",
      "validation AUC 0.8538676316926005\n",
      "validation AUC 0.8549983072739011\n",
      "validation AUC 0.8551779645269316\n",
      "validation AUC 0.8562802719132201\n",
      "validation AUC 0.8572200195911361\n",
      "validation AUC 0.8584640189542753\n",
      "validation AUC 0.8582517843067847\n",
      "validation AUC 0.8595532544082719\n",
      "validation AUC 0.8595447561889068\n",
      "validation AUC 0.8605602002791374\n",
      "validation AUC 0.8613221159321021\n",
      "validation AUC 0.8610421615979249\n",
      "validation AUC 0.8610842908419527\n",
      "validation AUC 0.8618632667900231\n",
      "validation AUC 0.8607958382884107\n",
      "validation AUC 0.8595584773276873\n",
      "validation AUC 0.8591486258214325\n",
      "validation AUC 0.8609930001704964\n",
      "validation AUC 0.8609930001704964\n",
      "train auc 0.906881431783174\n",
      "Test AUC 0.8625616286046057\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.97328280e-01\n",
      " 9.97328280e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.46748314e+01 5.36748314e+01 4.87538910e+01 ... 2.29094401e-02\n",
      " 2.27506310e-02 6.36560470e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m4\" --iteration_num $i --experiment_method 3 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f20964b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#basline smote multiplier =4 (0.24 share of minority). Below multiplier =1 (0.068 share of minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d9cf018f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.404s | Train Loss: 0.192294 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.377s | Train Loss: 0.150469 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.375s | Train Loss: 0.119175 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.375s | Train Loss: 0.097152 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.377s | Train Loss: 0.081848 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.375s | Train Loss: 0.071049 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.373s | Train Loss: 0.063073 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.372s | Train Loss: 0.057114 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.376s | Train Loss: 0.052567 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.374s | Train Loss: 0.048503 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.376s | Train Loss: 0.045484 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.377s | Train Loss: 0.042866 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.374s | Train Loss: 0.040596 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.375s | Train Loss: 0.038688 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.376s | Train Loss: 0.037154 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.375s | Train Loss: 0.035742 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.373s | Train Loss: 0.034412 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.375s | Train Loss: 0.033191 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.373s | Train Loss: 0.032288 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.373s | Train Loss: 0.031540 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.374s | Train Loss: 0.030645 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.379s | Train Loss: 0.029778 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.377s | Train Loss: 0.029164 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.377s | Train Loss: 0.028421 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.372s | Train Loss: 0.027793 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.377s | Train Loss: 0.027276 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.377s | Train Loss: 0.026953 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.374s | Train Loss: 0.026409 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.378s | Train Loss: 0.026037 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.377s | Train Loss: 0.025447 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.379s | Train Loss: 0.024951 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.374s | Train Loss: 0.024829 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.380s | Train Loss: 0.024590 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.377s | Train Loss: 0.024200 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.376s | Train Loss: 0.023955 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.381s | Train Loss: 0.023821 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.380s | Train Loss: 0.023449 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.380s | Train Loss: 0.023247 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.379s | Train Loss: 0.022901 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.377s | Train Loss: 0.022462 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.377s | Train Loss: 0.022469 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.378s | Train Loss: 0.022038 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.371s | Train Loss: 0.021929 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.376s | Train Loss: 0.021794 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.376s | Train Loss: 0.021475 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.373s | Train Loss: 0.021270 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.369s | Train Loss: 0.021135 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.379s | Train Loss: 0.020962 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.374s | Train Loss: 0.020738 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.371s | Train Loss: 0.020703 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.370s | Train Loss: 0.020399 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.370s | Train Loss: 0.020151 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.380s | Train Loss: 0.020306 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.381s | Train Loss: 0.020084 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.373s | Train Loss: 0.019869 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.379s | Train Loss: 0.019712 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.378s | Train Loss: 0.019637 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.373s | Train Loss: 0.019654 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.378s | Train Loss: 0.019559 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.374s | Train Loss: 0.019542 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.373s | Train Loss: 0.018962 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.382s | Train Loss: 0.019070 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.370s | Train Loss: 0.018971 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.371s | Train Loss: 0.018847 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.368s | Train Loss: 0.018724 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.369s | Train Loss: 0.018557 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.376s | Train Loss: 0.018315 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.377s | Train Loss: 0.018419 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.372s | Train Loss: 0.018309 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.370s | Train Loss: 0.018228 |\n",
      "INFO:root:Pretraining Time: 26.310s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017870\n",
      "INFO:root:Test AUC: 50.49%\n",
      "INFO:root:Test AUC: 50.49%\n",
      "INFO:root:Test Time: 0.232s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.664s | Train Loss: 0.866060 || Validation Loss: 0.013656 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.672s | Train Loss: 0.670448 || Validation Loss: 0.014427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.721s | Train Loss: 0.613280 || Validation Loss: 0.016555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.747s | Train Loss: 0.565762 || Validation Loss: 0.015728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.736s | Train Loss: 0.544597 || Validation Loss: 0.015513 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.754s | Train Loss: 0.520768 || Validation Loss: 0.015483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.790s | Train Loss: 0.514785 || Validation Loss: 0.017637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.771s | Train Loss: 0.500748 || Validation Loss: 0.018569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.828s | Train Loss: 0.491641 || Validation Loss: 0.018427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.846s | Train Loss: 0.478879 || Validation Loss: 0.019284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.848s | Train Loss: 0.472747 || Validation Loss: 0.020616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.895s | Train Loss: 0.469711 || Validation Loss: 0.020802 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.844s | Train Loss: 0.462722 || Validation Loss: 0.021293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.899s | Train Loss: 0.453122 || Validation Loss: 0.020150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.938s | Train Loss: 0.442792 || Validation Loss: 0.021979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.884s | Train Loss: 0.443557 || Validation Loss: 0.020191 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.965s | Train Loss: 0.435137 || Validation Loss: 0.020085 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.989s | Train Loss: 0.426435 || Validation Loss: 0.020876 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.961s | Train Loss: 0.426446 || Validation Loss: 0.022086 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.049s | Train Loss: 0.421785 || Validation Loss: 0.022723 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.027s | Train Loss: 0.424375 || Validation Loss: 0.022068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.039s | Train Loss: 0.418666 || Validation Loss: 0.025071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.016s | Train Loss: 0.404911 || Validation Loss: 0.025025 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.102s | Train Loss: 0.407570 || Validation Loss: 0.023459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.076s | Train Loss: 0.408696 || Validation Loss: 0.025844 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.080s | Train Loss: 0.406253 || Validation Loss: 0.024806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.220s | Train Loss: 0.397371 || Validation Loss: 0.024236 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.253s | Train Loss: 0.407405 || Validation Loss: 0.023011 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.234s | Train Loss: 0.399872 || Validation Loss: 0.025509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.200s | Train Loss: 0.397119 || Validation Loss: 0.024602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.821s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.887%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021215\n",
      "INFO:root:Test AUC: 86.27%\n",
      "INFO:root:Test Time: 0.200s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 837})\n",
      "class weights 0.03182872571015705 0.968171274289843\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6446193553358281\n",
      "validation AUC 0.7201399657261545\n",
      "validation AUC 0.7633397778606965\n",
      "validation AUC 0.7883456925575181\n",
      "validation AUC 0.8056661731278322\n",
      "validation AUC 0.8185918849614616\n",
      "validation AUC 0.825972519302186\n",
      "validation AUC 0.8320535779472431\n",
      "validation AUC 0.8375807783155667\n",
      "validation AUC 0.8409149405401356\n",
      "validation AUC 0.8434042456614382\n",
      "validation AUC 0.8478634029105311\n",
      "validation AUC 0.8484232344190975\n",
      "validation AUC 0.8514581257557668\n",
      "validation AUC 0.8531896978167293\n",
      "validation AUC 0.8530494665544781\n",
      "validation AUC 0.8545280955282869\n",
      "validation AUC 0.8550916120818818\n",
      "validation AUC 0.8564769894613565\n",
      "validation AUC 0.8570080776187022\n",
      "validation AUC 0.8568428252998109\n",
      "validation AUC 0.8571338267870261\n",
      "validation AUC 0.8587115741597086\n",
      "validation AUC 0.858454217000568\n",
      "validation AUC 0.8585837656232607\n",
      "validation AUC 0.8583778314715297\n",
      "validation AUC 0.8580877585624482\n",
      "validation AUC 0.8585123954804159\n",
      "validation AUC 0.860559915586128\n",
      "validation AUC 0.860010096225173\n",
      "validation AUC 0.860010096225173\n",
      "train auc 0.8868355911648853\n",
      "Test AUC 0.8627065979249099\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99526291e-01\n",
      " 9.99526291e-01 1.00000000e+00] true positive rate [0.         0.         0.00167973 ... 0.99972004 1.         1.        ] tresholds [6.74784546e+01 6.64784546e+01 4.18552780e+01 ... 3.62134539e-02\n",
      " 3.61433588e-02 2.02160664e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.385s | Train Loss: 0.189406 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.373s | Train Loss: 0.149296 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.365s | Train Loss: 0.118454 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.366s | Train Loss: 0.096611 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.363s | Train Loss: 0.081166 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.367s | Train Loss: 0.070042 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.364s | Train Loss: 0.061789 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.370s | Train Loss: 0.055934 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.370s | Train Loss: 0.051057 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.365s | Train Loss: 0.047321 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.362s | Train Loss: 0.044110 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.367s | Train Loss: 0.041440 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.367s | Train Loss: 0.039187 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.367s | Train Loss: 0.037553 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.371s | Train Loss: 0.035681 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.368s | Train Loss: 0.034787 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.364s | Train Loss: 0.033200 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.366s | Train Loss: 0.032214 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.367s | Train Loss: 0.031103 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.365s | Train Loss: 0.030267 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.367s | Train Loss: 0.029369 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.367s | Train Loss: 0.028585 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.363s | Train Loss: 0.028060 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.363s | Train Loss: 0.027206 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.368s | Train Loss: 0.026624 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.366s | Train Loss: 0.026393 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.364s | Train Loss: 0.025729 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.370s | Train Loss: 0.025233 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.366s | Train Loss: 0.024688 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.366s | Train Loss: 0.024130 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.367s | Train Loss: 0.024055 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.371s | Train Loss: 0.023696 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.365s | Train Loss: 0.023204 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.365s | Train Loss: 0.023140 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.369s | Train Loss: 0.022677 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.365s | Train Loss: 0.022521 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.366s | Train Loss: 0.022415 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.368s | Train Loss: 0.022129 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.370s | Train Loss: 0.021906 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.364s | Train Loss: 0.021626 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.368s | Train Loss: 0.021384 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.369s | Train Loss: 0.021201 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.372s | Train Loss: 0.021298 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.368s | Train Loss: 0.020757 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.370s | Train Loss: 0.020618 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.365s | Train Loss: 0.020508 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.365s | Train Loss: 0.020449 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.369s | Train Loss: 0.020208 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.364s | Train Loss: 0.020196 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.348s | Train Loss: 0.020026 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.381s | Train Loss: 0.019820 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.368s | Train Loss: 0.019762 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.365s | Train Loss: 0.019683 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.368s | Train Loss: 0.019295 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.371s | Train Loss: 0.019365 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.367s | Train Loss: 0.019018 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.367s | Train Loss: 0.018984 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.368s | Train Loss: 0.018856 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.367s | Train Loss: 0.018825 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.367s | Train Loss: 0.018816 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.368s | Train Loss: 0.018636 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.368s | Train Loss: 0.018578 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.365s | Train Loss: 0.018458 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.370s | Train Loss: 0.018225 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.367s | Train Loss: 0.018144 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.366s | Train Loss: 0.018027 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.368s | Train Loss: 0.018108 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.369s | Train Loss: 0.017931 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.364s | Train Loss: 0.018028 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.366s | Train Loss: 0.017839 |\n",
      "INFO:root:Pretraining Time: 25.710s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017517\n",
      "INFO:root:Test AUC: 51.87%\n",
      "INFO:root:Test AUC: 51.87%\n",
      "INFO:root:Test Time: 0.220s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.648s | Train Loss: 0.886540 || Validation Loss: 0.016317 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.658s | Train Loss: 0.674319 || Validation Loss: 0.015527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.704s | Train Loss: 0.585726 || Validation Loss: 0.014930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.745s | Train Loss: 0.553903 || Validation Loss: 0.016486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.710s | Train Loss: 0.538518 || Validation Loss: 0.018824 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.730s | Train Loss: 0.516555 || Validation Loss: 0.019204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.744s | Train Loss: 0.496860 || Validation Loss: 0.018348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.763s | Train Loss: 0.487453 || Validation Loss: 0.020946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.799s | Train Loss: 0.472999 || Validation Loss: 0.019519 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.851s | Train Loss: 0.468231 || Validation Loss: 0.020430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.824s | Train Loss: 0.464164 || Validation Loss: 0.020131 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.872s | Train Loss: 0.459203 || Validation Loss: 0.020542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.842s | Train Loss: 0.445354 || Validation Loss: 0.020572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.886s | Train Loss: 0.448620 || Validation Loss: 0.022918 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.940s | Train Loss: 0.442279 || Validation Loss: 0.021950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.879s | Train Loss: 0.431910 || Validation Loss: 0.023907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.964s | Train Loss: 0.431705 || Validation Loss: 0.022327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.979s | Train Loss: 0.428815 || Validation Loss: 0.022656 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.957s | Train Loss: 0.434176 || Validation Loss: 0.022550 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.050s | Train Loss: 0.423153 || Validation Loss: 0.020588 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.017s | Train Loss: 0.415269 || Validation Loss: 0.022465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.041s | Train Loss: 0.413354 || Validation Loss: 0.023707 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.008s | Train Loss: 0.412352 || Validation Loss: 0.021391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.103s | Train Loss: 0.403643 || Validation Loss: 0.024048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.085s | Train Loss: 0.399183 || Validation Loss: 0.024184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.068s | Train Loss: 0.398603 || Validation Loss: 0.023189 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.184s | Train Loss: 0.391667 || Validation Loss: 0.025545 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.068s | Train Loss: 0.388816 || Validation Loss: 0.025817 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.212s | Train Loss: 0.393653 || Validation Loss: 0.024231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.157s | Train Loss: 0.383515 || Validation Loss: 0.025804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.236s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.892%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022106\n",
      "INFO:root:Test AUC: 87.03%\n",
      "INFO:root:Test Time: 0.193s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 837})\n",
      "class weights 0.03182872571015705 0.968171274289843\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6309469389701201\n",
      "validation AUC 0.742564188429097\n",
      "validation AUC 0.7900989837683478\n",
      "validation AUC 0.8128032512686665\n",
      "validation AUC 0.8261050478890891\n",
      "validation AUC 0.8353175939428397\n",
      "validation AUC 0.8405816475072706\n",
      "validation AUC 0.8450811433611827\n",
      "validation AUC 0.8484810616888776\n",
      "validation AUC 0.8515513507432777\n",
      "validation AUC 0.8541008484809526\n",
      "validation AUC 0.8551573256140909\n",
      "validation AUC 0.8572338897281272\n",
      "validation AUC 0.8585310894344732\n",
      "validation AUC 0.8581424249416194\n",
      "validation AUC 0.859586672578348\n",
      "validation AUC 0.8608331437153726\n",
      "validation AUC 0.8608110041776971\n",
      "validation AUC 0.8615680720674546\n",
      "validation AUC 0.8619750127872392\n",
      "validation AUC 0.8623040859779274\n",
      "validation AUC 0.8635236842234096\n",
      "validation AUC 0.862976490955915\n",
      "validation AUC 0.8646174321946395\n",
      "validation AUC 0.8655056264917115\n",
      "validation AUC 0.8640083647595392\n",
      "validation AUC 0.8650818037425371\n",
      "validation AUC 0.8658558068753308\n",
      "validation AUC 0.8654500102489483\n",
      "validation AUC 0.8649957306691317\n",
      "validation AUC 0.8649957306691317\n",
      "train auc 0.8916452684732825\n",
      "Test AUC 0.8702885116780178\n",
      "test false positive rates [0.         0.         0.         ... 0.99020369 0.99020369 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.68222885e+01 5.58222885e+01 5.57320366e+01 ... 5.13288938e-02\n",
      " 5.13077974e-02 1.42042013e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.392s | Train Loss: 0.199781 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.368s | Train Loss: 0.156427 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.372s | Train Loss: 0.123488 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.365s | Train Loss: 0.099761 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.366s | Train Loss: 0.083421 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.369s | Train Loss: 0.071297 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.362s | Train Loss: 0.062869 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.367s | Train Loss: 0.056650 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.368s | Train Loss: 0.051955 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.366s | Train Loss: 0.047923 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.367s | Train Loss: 0.045106 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.368s | Train Loss: 0.042456 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.368s | Train Loss: 0.040716 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.365s | Train Loss: 0.038582 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.369s | Train Loss: 0.036976 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.369s | Train Loss: 0.035679 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.366s | Train Loss: 0.034472 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.364s | Train Loss: 0.033082 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.371s | Train Loss: 0.032518 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.365s | Train Loss: 0.031134 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.365s | Train Loss: 0.030327 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.367s | Train Loss: 0.029670 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.367s | Train Loss: 0.028937 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.367s | Train Loss: 0.028187 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.369s | Train Loss: 0.027626 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.370s | Train Loss: 0.027322 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.362s | Train Loss: 0.026538 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.368s | Train Loss: 0.026203 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.370s | Train Loss: 0.025668 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.364s | Train Loss: 0.025411 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.368s | Train Loss: 0.024931 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.370s | Train Loss: 0.024490 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.369s | Train Loss: 0.024124 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.359s | Train Loss: 0.023857 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.366s | Train Loss: 0.023688 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.367s | Train Loss: 0.023339 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.366s | Train Loss: 0.023415 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.369s | Train Loss: 0.023029 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.368s | Train Loss: 0.022611 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.366s | Train Loss: 0.022736 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.369s | Train Loss: 0.022503 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.369s | Train Loss: 0.022410 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.366s | Train Loss: 0.022082 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.367s | Train Loss: 0.021972 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.371s | Train Loss: 0.021886 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.367s | Train Loss: 0.021491 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.366s | Train Loss: 0.021340 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.370s | Train Loss: 0.021222 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.368s | Train Loss: 0.021147 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.368s | Train Loss: 0.021228 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.370s | Train Loss: 0.020837 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.370s | Train Loss: 0.020464 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.367s | Train Loss: 0.020419 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.352s | Train Loss: 0.020268 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.367s | Train Loss: 0.020106 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.368s | Train Loss: 0.019832 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.368s | Train Loss: 0.019893 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.370s | Train Loss: 0.019629 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.366s | Train Loss: 0.019502 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.372s | Train Loss: 0.019135 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.372s | Train Loss: 0.019061 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.367s | Train Loss: 0.018828 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.368s | Train Loss: 0.018606 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.369s | Train Loss: 0.018416 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.370s | Train Loss: 0.018114 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.370s | Train Loss: 0.018316 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.370s | Train Loss: 0.017910 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.369s | Train Loss: 0.018027 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.368s | Train Loss: 0.017856 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.367s | Train Loss: 0.017639 |\n",
      "INFO:root:Pretraining Time: 25.753s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017014\n",
      "INFO:root:Test AUC: 51.29%\n",
      "INFO:root:Test AUC: 51.29%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.638s | Train Loss: 1.014990 || Validation Loss: 0.013626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.655s | Train Loss: 0.749490 || Validation Loss: 0.014355 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.691s | Train Loss: 0.664812 || Validation Loss: 0.015135 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.734s | Train Loss: 0.612346 || Validation Loss: 0.013982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.709s | Train Loss: 0.582949 || Validation Loss: 0.015057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.720s | Train Loss: 0.551523 || Validation Loss: 0.016626 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.767s | Train Loss: 0.533732 || Validation Loss: 0.018207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.745s | Train Loss: 0.514838 || Validation Loss: 0.019322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.785s | Train Loss: 0.497005 || Validation Loss: 0.018274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.834s | Train Loss: 0.489715 || Validation Loss: 0.019048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.813s | Train Loss: 0.488441 || Validation Loss: 0.020736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.863s | Train Loss: 0.475279 || Validation Loss: 0.018990 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.831s | Train Loss: 0.464250 || Validation Loss: 0.020281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.870s | Train Loss: 0.450572 || Validation Loss: 0.022148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.929s | Train Loss: 0.457824 || Validation Loss: 0.020386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.872s | Train Loss: 0.441410 || Validation Loss: 0.019748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.951s | Train Loss: 0.442180 || Validation Loss: 0.021980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.973s | Train Loss: 0.439097 || Validation Loss: 0.024214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.942s | Train Loss: 0.431486 || Validation Loss: 0.023946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.033s | Train Loss: 0.429114 || Validation Loss: 0.021948 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.006s | Train Loss: 0.429437 || Validation Loss: 0.023398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.032s | Train Loss: 0.425601 || Validation Loss: 0.021619 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.997s | Train Loss: 0.418124 || Validation Loss: 0.021600 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.086s | Train Loss: 0.419045 || Validation Loss: 0.025045 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.066s | Train Loss: 0.413252 || Validation Loss: 0.022853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.071s | Train Loss: 0.410423 || Validation Loss: 0.022298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.170s | Train Loss: 0.413690 || Validation Loss: 0.023742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.065s | Train Loss: 0.413908 || Validation Loss: 0.022537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.191s | Train Loss: 0.405425 || Validation Loss: 0.022857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.159s | Train Loss: 0.409691 || Validation Loss: 0.022550 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 27.952s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.875%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021072\n",
      "INFO:root:Test AUC: 87.13%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 837})\n",
      "class weights 0.03182872571015705 0.968171274289843\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5410867700657486\n",
      "validation AUC 0.6447397858001725\n",
      "validation AUC 0.7202853400873512\n",
      "validation AUC 0.7665173828762316\n",
      "validation AUC 0.7949673433172066\n",
      "validation AUC 0.8123748441638359\n",
      "validation AUC 0.8246762508346561\n",
      "validation AUC 0.834084966335983\n",
      "validation AUC 0.8399098943964541\n",
      "validation AUC 0.8444360795548274\n",
      "validation AUC 0.847206528335309\n",
      "validation AUC 0.8508326206252264\n",
      "validation AUC 0.8528287629551283\n",
      "validation AUC 0.855004594466249\n",
      "validation AUC 0.8563681116796931\n",
      "validation AUC 0.8575667410877252\n",
      "validation AUC 0.8579780027556154\n",
      "validation AUC 0.8594952808009634\n",
      "validation AUC 0.8602907955504026\n",
      "validation AUC 0.8613647666698666\n",
      "validation AUC 0.8608687782338306\n",
      "validation AUC 0.8615765729475018\n",
      "validation AUC 0.8632683012905588\n",
      "validation AUC 0.8642941592914327\n",
      "validation AUC 0.863632439613686\n",
      "validation AUC 0.8630901712691859\n",
      "validation AUC 0.8632399916305575\n",
      "validation AUC 0.8629991945582448\n",
      "validation AUC 0.8634355996741622\n",
      "validation AUC 0.8639785864028917\n",
      "validation AUC 0.8639785864028917\n",
      "train auc 0.8751848626070318\n",
      "Test AUC 0.8712510297736541\n",
      "test false positive rates [0.         0.         0.         ... 0.99602084 0.99602084 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.55618210e+01 7.45618210e+01 5.94062271e+01 ... 5.33978529e-02\n",
      " 5.33710197e-02 2.36578323e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.377s | Train Loss: 0.198517 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.367s | Train Loss: 0.155519 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.371s | Train Loss: 0.122766 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.366s | Train Loss: 0.099250 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.362s | Train Loss: 0.082705 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.369s | Train Loss: 0.071351 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.368s | Train Loss: 0.063524 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.365s | Train Loss: 0.057497 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.368s | Train Loss: 0.052434 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.365s | Train Loss: 0.048936 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.366s | Train Loss: 0.045632 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.369s | Train Loss: 0.043036 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.369s | Train Loss: 0.040675 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.366s | Train Loss: 0.038431 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.369s | Train Loss: 0.036882 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.369s | Train Loss: 0.035385 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.366s | Train Loss: 0.033744 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.369s | Train Loss: 0.032531 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.371s | Train Loss: 0.031582 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.364s | Train Loss: 0.030485 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.366s | Train Loss: 0.029533 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.370s | Train Loss: 0.028960 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.366s | Train Loss: 0.028111 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.366s | Train Loss: 0.027268 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.371s | Train Loss: 0.026793 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.371s | Train Loss: 0.026233 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.368s | Train Loss: 0.025656 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.370s | Train Loss: 0.025240 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.370s | Train Loss: 0.024691 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.366s | Train Loss: 0.024419 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.366s | Train Loss: 0.024107 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.370s | Train Loss: 0.023948 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.369s | Train Loss: 0.023392 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.366s | Train Loss: 0.023228 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.369s | Train Loss: 0.023050 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.366s | Train Loss: 0.022783 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.365s | Train Loss: 0.022410 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.368s | Train Loss: 0.022077 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.362s | Train Loss: 0.022099 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.367s | Train Loss: 0.022089 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.370s | Train Loss: 0.021656 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.367s | Train Loss: 0.021555 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.366s | Train Loss: 0.021537 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.370s | Train Loss: 0.021229 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.372s | Train Loss: 0.021015 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.368s | Train Loss: 0.021087 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.367s | Train Loss: 0.020647 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.368s | Train Loss: 0.020966 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.368s | Train Loss: 0.020759 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.364s | Train Loss: 0.020493 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.370s | Train Loss: 0.020586 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.369s | Train Loss: 0.020319 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.368s | Train Loss: 0.020165 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.369s | Train Loss: 0.020032 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.371s | Train Loss: 0.020040 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.367s | Train Loss: 0.019967 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.370s | Train Loss: 0.019848 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.370s | Train Loss: 0.019667 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.365s | Train Loss: 0.019569 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.355s | Train Loss: 0.019289 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.370s | Train Loss: 0.019233 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.367s | Train Loss: 0.019103 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.366s | Train Loss: 0.018930 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.367s | Train Loss: 0.018954 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.369s | Train Loss: 0.019005 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.367s | Train Loss: 0.019105 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.370s | Train Loss: 0.018752 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.370s | Train Loss: 0.018748 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.365s | Train Loss: 0.018517 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.365s | Train Loss: 0.018319 |\n",
      "INFO:root:Pretraining Time: 25.748s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017924\n",
      "INFO:root:Test AUC: 51.08%\n",
      "INFO:root:Test AUC: 51.08%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.647s | Train Loss: 0.943832 || Validation Loss: 0.014271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.649s | Train Loss: 0.706257 || Validation Loss: 0.015424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.697s | Train Loss: 0.622731 || Validation Loss: 0.014898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.730s | Train Loss: 0.593244 || Validation Loss: 0.016108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.709s | Train Loss: 0.558360 || Validation Loss: 0.017774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.717s | Train Loss: 0.534802 || Validation Loss: 0.017759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.767s | Train Loss: 0.514653 || Validation Loss: 0.018162 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.748s | Train Loss: 0.510932 || Validation Loss: 0.017407 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.786s | Train Loss: 0.495649 || Validation Loss: 0.018387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.829s | Train Loss: 0.486578 || Validation Loss: 0.017762 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.802s | Train Loss: 0.477029 || Validation Loss: 0.021321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.863s | Train Loss: 0.470690 || Validation Loss: 0.019842 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.836s | Train Loss: 0.456784 || Validation Loss: 0.019484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.880s | Train Loss: 0.462234 || Validation Loss: 0.021558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.937s | Train Loss: 0.457397 || Validation Loss: 0.019011 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.877s | Train Loss: 0.454844 || Validation Loss: 0.021451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.950s | Train Loss: 0.447667 || Validation Loss: 0.022266 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.974s | Train Loss: 0.435759 || Validation Loss: 0.022134 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.951s | Train Loss: 0.444130 || Validation Loss: 0.020253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.037s | Train Loss: 0.435231 || Validation Loss: 0.021451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.017s | Train Loss: 0.440166 || Validation Loss: 0.021354 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.027s | Train Loss: 0.429666 || Validation Loss: 0.020031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.007s | Train Loss: 0.433495 || Validation Loss: 0.020729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.093s | Train Loss: 0.430696 || Validation Loss: 0.021489 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.071s | Train Loss: 0.425777 || Validation Loss: 0.022630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.081s | Train Loss: 0.423666 || Validation Loss: 0.019936 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.177s | Train Loss: 0.424233 || Validation Loss: 0.021597 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.067s | Train Loss: 0.422451 || Validation Loss: 0.019738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.207s | Train Loss: 0.411599 || Validation Loss: 0.021804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.162s | Train Loss: 0.422030 || Validation Loss: 0.022200 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.042s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.877%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019395\n",
      "INFO:root:Test AUC: 86.18%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 837})\n",
      "class weights 0.03182872571015705 0.968171274289843\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6052442234990186\n",
      "validation AUC 0.7146198055232786\n",
      "validation AUC 0.7654778516288591\n",
      "validation AUC 0.7935787332087\n",
      "validation AUC 0.8101737867235359\n",
      "validation AUC 0.8209486322283555\n",
      "validation AUC 0.8284558139421031\n",
      "validation AUC 0.8334128088014521\n",
      "validation AUC 0.8381447445244754\n",
      "validation AUC 0.841465100468429\n",
      "validation AUC 0.8438407811507896\n",
      "validation AUC 0.8438941597597127\n",
      "validation AUC 0.8477137182439581\n",
      "validation AUC 0.8482163184755865\n",
      "validation AUC 0.8483303633023368\n",
      "validation AUC 0.8489122571887913\n",
      "validation AUC 0.8501198025433142\n",
      "validation AUC 0.8502251895097599\n",
      "validation AUC 0.8515560867578266\n",
      "validation AUC 0.8522514986559295\n",
      "validation AUC 0.8517690052006761\n",
      "validation AUC 0.8523800229159247\n",
      "validation AUC 0.8523475519487582\n",
      "validation AUC 0.8530516589567188\n",
      "validation AUC 0.8538317311059095\n",
      "validation AUC 0.8534938483960235\n",
      "validation AUC 0.8526553104132935\n",
      "validation AUC 0.8533856836772247\n",
      "validation AUC 0.8534087384896222\n",
      "validation AUC 0.8539617320445981\n",
      "validation AUC 0.8539617320445981\n",
      "train auc 0.8774016509657849\n",
      "Test AUC 0.8617649034041812\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.95755566e-01\n",
      " 9.95755566e-01 1.00000000e+00] true positive rate [0.         0.         0.00111982 ... 0.99972004 1.         1.        ] tresholds [4.04097404e+01 3.94097404e+01 3.25557289e+01 ... 2.99756080e-02\n",
      " 2.98603885e-02 6.74069533e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.391s | Train Loss: 0.192215 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.368s | Train Loss: 0.150469 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.369s | Train Loss: 0.119778 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.368s | Train Loss: 0.097402 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.365s | Train Loss: 0.081491 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.370s | Train Loss: 0.070237 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.371s | Train Loss: 0.062052 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.369s | Train Loss: 0.055705 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.370s | Train Loss: 0.050614 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.369s | Train Loss: 0.047130 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.367s | Train Loss: 0.043991 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.369s | Train Loss: 0.041147 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.369s | Train Loss: 0.039171 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.367s | Train Loss: 0.037374 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.368s | Train Loss: 0.035631 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.371s | Train Loss: 0.034398 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.368s | Train Loss: 0.033021 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.368s | Train Loss: 0.031855 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.370s | Train Loss: 0.030784 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.370s | Train Loss: 0.029824 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.366s | Train Loss: 0.029093 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.368s | Train Loss: 0.028234 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.372s | Train Loss: 0.027646 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.368s | Train Loss: 0.027183 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.366s | Train Loss: 0.026364 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.364s | Train Loss: 0.026099 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.368s | Train Loss: 0.025544 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.366s | Train Loss: 0.025170 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.366s | Train Loss: 0.024828 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.368s | Train Loss: 0.024523 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.365s | Train Loss: 0.024277 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.369s | Train Loss: 0.023886 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.368s | Train Loss: 0.023480 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.365s | Train Loss: 0.023196 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.369s | Train Loss: 0.023107 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.369s | Train Loss: 0.022792 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.367s | Train Loss: 0.022497 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.368s | Train Loss: 0.022541 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.369s | Train Loss: 0.022099 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.368s | Train Loss: 0.021915 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.367s | Train Loss: 0.021700 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.369s | Train Loss: 0.021488 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.371s | Train Loss: 0.021310 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.367s | Train Loss: 0.020945 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.367s | Train Loss: 0.020765 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.371s | Train Loss: 0.020611 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.367s | Train Loss: 0.020436 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.368s | Train Loss: 0.020281 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.372s | Train Loss: 0.019945 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.370s | Train Loss: 0.019941 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.367s | Train Loss: 0.019837 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.369s | Train Loss: 0.019840 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.370s | Train Loss: 0.019541 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.365s | Train Loss: 0.019410 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.370s | Train Loss: 0.019324 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.369s | Train Loss: 0.019042 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.368s | Train Loss: 0.018806 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.370s | Train Loss: 0.018771 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.369s | Train Loss: 0.018640 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.368s | Train Loss: 0.018601 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.369s | Train Loss: 0.018418 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.371s | Train Loss: 0.018348 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.367s | Train Loss: 0.018395 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.364s | Train Loss: 0.018210 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.354s | Train Loss: 0.018048 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.365s | Train Loss: 0.017985 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.366s | Train Loss: 0.017868 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.367s | Train Loss: 0.017860 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.363s | Train Loss: 0.017663 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.365s | Train Loss: 0.017747 |\n",
      "INFO:root:Pretraining Time: 25.780s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017354\n",
      "INFO:root:Test AUC: 50.90%\n",
      "INFO:root:Test AUC: 50.90%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.668s | Train Loss: 0.762635 || Validation Loss: 0.013675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.666s | Train Loss: 0.638355 || Validation Loss: 0.014601 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.705s | Train Loss: 0.579603 || Validation Loss: 0.015981 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.747s | Train Loss: 0.547250 || Validation Loss: 0.015785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.719s | Train Loss: 0.523432 || Validation Loss: 0.015401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.737s | Train Loss: 0.508928 || Validation Loss: 0.018174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.780s | Train Loss: 0.499460 || Validation Loss: 0.017657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.753s | Train Loss: 0.487612 || Validation Loss: 0.017890 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.801s | Train Loss: 0.478402 || Validation Loss: 0.019146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.840s | Train Loss: 0.467065 || Validation Loss: 0.019679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.822s | Train Loss: 0.459229 || Validation Loss: 0.021035 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.863s | Train Loss: 0.459750 || Validation Loss: 0.018904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.850s | Train Loss: 0.447574 || Validation Loss: 0.019086 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.890s | Train Loss: 0.443222 || Validation Loss: 0.019761 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.932s | Train Loss: 0.439996 || Validation Loss: 0.020139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.885s | Train Loss: 0.434070 || Validation Loss: 0.021499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.962s | Train Loss: 0.434836 || Validation Loss: 0.020957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.979s | Train Loss: 0.437535 || Validation Loss: 0.020534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.944s | Train Loss: 0.429924 || Validation Loss: 0.021466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.045s | Train Loss: 0.418263 || Validation Loss: 0.021342 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.023s | Train Loss: 0.423786 || Validation Loss: 0.022362 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.037s | Train Loss: 0.409802 || Validation Loss: 0.021126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.013s | Train Loss: 0.414484 || Validation Loss: 0.019188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.104s | Train Loss: 0.408393 || Validation Loss: 0.021549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.082s | Train Loss: 0.407993 || Validation Loss: 0.022171 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.074s | Train Loss: 0.403007 || Validation Loss: 0.024742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.182s | Train Loss: 0.411068 || Validation Loss: 0.022075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.087s | Train Loss: 0.400161 || Validation Loss: 0.022132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.200s | Train Loss: 0.405101 || Validation Loss: 0.022015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.165s | Train Loss: 0.410630 || Validation Loss: 0.022417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.306s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.893%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019296\n",
      "INFO:root:Test AUC: 86.20%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 837})\n",
      "class weights 0.03182872571015705 0.968171274289843\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6793201392579846\n",
      "validation AUC 0.7423277601971714\n",
      "validation AUC 0.7763127700193464\n",
      "validation AUC 0.7973563061257634\n",
      "validation AUC 0.8109861994664588\n",
      "validation AUC 0.8232971180766119\n",
      "validation AUC 0.8298970071155158\n",
      "validation AUC 0.8349698055126357\n",
      "validation AUC 0.8397408665693352\n",
      "validation AUC 0.8437838026386731\n",
      "validation AUC 0.8466726970144803\n",
      "validation AUC 0.8473310535897606\n",
      "validation AUC 0.8514114121560828\n",
      "validation AUC 0.8544048234552132\n",
      "validation AUC 0.8536421761146077\n",
      "validation AUC 0.855191531346137\n",
      "validation AUC 0.8576520372418899\n",
      "validation AUC 0.8567069548959003\n",
      "validation AUC 0.8592444024032986\n",
      "validation AUC 0.8598892267482439\n",
      "validation AUC 0.8606448578695426\n",
      "validation AUC 0.8612785951512152\n",
      "validation AUC 0.8608113846552703\n",
      "validation AUC 0.8616196919653565\n",
      "validation AUC 0.862359422849046\n",
      "validation AUC 0.8629141112586666\n",
      "validation AUC 0.8633989567571008\n",
      "validation AUC 0.8637299482997496\n",
      "validation AUC 0.8634376617029687\n",
      "validation AUC 0.8628427783653746\n",
      "validation AUC 0.8628427783653746\n",
      "train auc 0.8927290528210836\n",
      "Test AUC 0.8619807911738385\n",
      "test false positive rates [0.         0.         0.         ... 0.99154903 0.99154903 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 8.39865622e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.61819344e+01 4.51819344e+01 4.00988960e+01 ... 4.80353646e-02\n",
      " 4.80261743e-02 9.15332139e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {6..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m1\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ca795035",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below multiplier =1.5 (0.102 share of minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "83276483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.406s | Train Loss: 0.197497 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.382s | Train Loss: 0.154781 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.377s | Train Loss: 0.122939 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.376s | Train Loss: 0.100407 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.378s | Train Loss: 0.084252 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.378s | Train Loss: 0.071941 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.388s | Train Loss: 0.062438 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.380s | Train Loss: 0.055326 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.381s | Train Loss: 0.050054 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.377s | Train Loss: 0.045538 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.382s | Train Loss: 0.042377 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.383s | Train Loss: 0.039311 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.384s | Train Loss: 0.037030 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.382s | Train Loss: 0.035114 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.382s | Train Loss: 0.033551 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.382s | Train Loss: 0.032189 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.383s | Train Loss: 0.030866 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.381s | Train Loss: 0.029756 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.383s | Train Loss: 0.028410 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.383s | Train Loss: 0.027719 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.381s | Train Loss: 0.026941 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.380s | Train Loss: 0.026316 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.376s | Train Loss: 0.025583 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.385s | Train Loss: 0.025055 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.384s | Train Loss: 0.024672 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.382s | Train Loss: 0.024084 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.386s | Train Loss: 0.023654 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.382s | Train Loss: 0.023320 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.385s | Train Loss: 0.022997 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.384s | Train Loss: 0.022751 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.381s | Train Loss: 0.022443 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.383s | Train Loss: 0.022062 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.385s | Train Loss: 0.021852 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.386s | Train Loss: 0.021671 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.387s | Train Loss: 0.021419 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.381s | Train Loss: 0.021158 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.384s | Train Loss: 0.020785 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.366s | Train Loss: 0.020807 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.383s | Train Loss: 0.020315 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.384s | Train Loss: 0.020296 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.383s | Train Loss: 0.020235 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.387s | Train Loss: 0.020046 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.384s | Train Loss: 0.019859 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.385s | Train Loss: 0.019841 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.383s | Train Loss: 0.019756 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.385s | Train Loss: 0.019616 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.386s | Train Loss: 0.019377 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.385s | Train Loss: 0.019415 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.382s | Train Loss: 0.019181 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.387s | Train Loss: 0.019127 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.391s | Train Loss: 0.018933 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.392s | Train Loss: 0.018801 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.386s | Train Loss: 0.018626 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.387s | Train Loss: 0.018759 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.379s | Train Loss: 0.018480 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.387s | Train Loss: 0.018448 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.384s | Train Loss: 0.018344 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.384s | Train Loss: 0.018279 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.387s | Train Loss: 0.018111 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.386s | Train Loss: 0.017885 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.385s | Train Loss: 0.018013 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.377s | Train Loss: 0.017832 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.386s | Train Loss: 0.017811 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.389s | Train Loss: 0.017555 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.388s | Train Loss: 0.017757 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.385s | Train Loss: 0.017488 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.385s | Train Loss: 0.017349 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.384s | Train Loss: 0.017341 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.384s | Train Loss: 0.017496 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.383s | Train Loss: 0.017231 |\n",
      "INFO:root:Pretraining Time: 26.857s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017798\n",
      "INFO:root:Test AUC: 49.26%\n",
      "INFO:root:Test AUC: 49.26%\n",
      "INFO:root:Test Time: 0.236s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.676s | Train Loss: 0.917334 || Validation Loss: 0.012599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.686s | Train Loss: 0.703261 || Validation Loss: 0.014262 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.724s | Train Loss: 0.646180 || Validation Loss: 0.016039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.759s | Train Loss: 0.611051 || Validation Loss: 0.016251 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.747s | Train Loss: 0.571862 || Validation Loss: 0.016036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.754s | Train Loss: 0.551025 || Validation Loss: 0.015121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.792s | Train Loss: 0.525915 || Validation Loss: 0.017496 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.774s | Train Loss: 0.515103 || Validation Loss: 0.018735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.846s | Train Loss: 0.503512 || Validation Loss: 0.018247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.833s | Train Loss: 0.488179 || Validation Loss: 0.016612 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.880s | Train Loss: 0.480042 || Validation Loss: 0.018174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.855s | Train Loss: 0.480124 || Validation Loss: 0.019760 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.904s | Train Loss: 0.472688 || Validation Loss: 0.018462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.917s | Train Loss: 0.462133 || Validation Loss: 0.019487 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.931s | Train Loss: 0.451383 || Validation Loss: 0.020594 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.912s | Train Loss: 0.449319 || Validation Loss: 0.018740 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.027s | Train Loss: 0.444263 || Validation Loss: 0.019721 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.975s | Train Loss: 0.440064 || Validation Loss: 0.019244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.988s | Train Loss: 0.434134 || Validation Loss: 0.021103 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.990s | Train Loss: 0.426865 || Validation Loss: 0.022198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.069s | Train Loss: 0.427813 || Validation Loss: 0.022296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.070s | Train Loss: 0.421484 || Validation Loss: 0.022269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.038s | Train Loss: 0.426563 || Validation Loss: 0.018959 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.124s | Train Loss: 0.409849 || Validation Loss: 0.019238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.135s | Train Loss: 0.408679 || Validation Loss: 0.020404 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.158s | Train Loss: 0.400326 || Validation Loss: 0.023253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.134s | Train Loss: 0.397404 || Validation Loss: 0.021756 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.151s | Train Loss: 0.397989 || Validation Loss: 0.022562 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.161s | Train Loss: 0.398467 || Validation Loss: 0.022501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.290s | Train Loss: 0.392382 || Validation Loss: 0.020221 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.020s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.882%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020096\n",
      "INFO:root:Test AUC: 86.26%\n",
      "INFO:root:Test Time: 0.209s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1255})\n",
      "class weights 0.04697735354669661 0.9530226464533034\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6529555152006911\n",
      "validation AUC 0.7069976663687413\n",
      "validation AUC 0.7410322260779648\n",
      "validation AUC 0.7678149337926452\n",
      "validation AUC 0.7877724618527332\n",
      "validation AUC 0.8045102370284819\n",
      "validation AUC 0.8159090258433139\n",
      "validation AUC 0.8237431309164263\n",
      "validation AUC 0.8300284315192518\n",
      "validation AUC 0.8364524388133527\n",
      "validation AUC 0.8386807629261801\n",
      "validation AUC 0.842083131742302\n",
      "validation AUC 0.8438956018495359\n",
      "validation AUC 0.8457300518535058\n",
      "validation AUC 0.8485576920006905\n",
      "validation AUC 0.8493393658806443\n",
      "validation AUC 0.8494610654904606\n",
      "validation AUC 0.8509484800373261\n",
      "validation AUC 0.8527053472052086\n",
      "validation AUC 0.853587222381745\n",
      "validation AUC 0.8540688936861582\n",
      "validation AUC 0.8554815377381604\n",
      "validation AUC 0.8557707432648021\n",
      "validation AUC 0.8558644099255797\n",
      "validation AUC 0.857777722553838\n",
      "validation AUC 0.8581459263995669\n",
      "validation AUC 0.8594934582335665\n",
      "validation AUC 0.8588015318293168\n",
      "validation AUC 0.8599099907131545\n",
      "validation AUC 0.8605380873482853\n",
      "validation AUC 0.8605380873482853\n",
      "train auc 0.8815625989626996\n",
      "Test AUC 0.8626149672992162\n",
      "test false positive rates [0.         0.         0.         ... 0.99891994 0.99891994 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.51959686e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.87445259e+01 4.77445259e+01 4.33296471e+01 ... 2.18118317e-02\n",
      " 2.17895340e-02 9.97765269e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.415s | Train Loss: 0.197064 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.380s | Train Loss: 0.154602 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.373s | Train Loss: 0.123302 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.367s | Train Loss: 0.100400 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.368s | Train Loss: 0.084295 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.374s | Train Loss: 0.072559 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.381s | Train Loss: 0.064006 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.377s | Train Loss: 0.057796 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.387s | Train Loss: 0.052651 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.378s | Train Loss: 0.048837 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.378s | Train Loss: 0.045294 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.367s | Train Loss: 0.042632 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.379s | Train Loss: 0.040332 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.371s | Train Loss: 0.038232 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.371s | Train Loss: 0.036538 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.374s | Train Loss: 0.035043 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.368s | Train Loss: 0.033471 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.379s | Train Loss: 0.032475 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.359s | Train Loss: 0.031261 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.363s | Train Loss: 0.030179 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.372s | Train Loss: 0.029369 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.374s | Train Loss: 0.028600 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.389s | Train Loss: 0.028051 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.375s | Train Loss: 0.027415 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.376s | Train Loss: 0.026832 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.372s | Train Loss: 0.026103 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.365s | Train Loss: 0.025653 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.379s | Train Loss: 0.025387 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.380s | Train Loss: 0.025130 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.374s | Train Loss: 0.024342 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.376s | Train Loss: 0.024193 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.375s | Train Loss: 0.023926 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.370s | Train Loss: 0.023913 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.383s | Train Loss: 0.023463 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.366s | Train Loss: 0.023341 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.369s | Train Loss: 0.023076 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.369s | Train Loss: 0.022842 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.375s | Train Loss: 0.022400 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.353s | Train Loss: 0.022374 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.370s | Train Loss: 0.022299 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.375s | Train Loss: 0.021967 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.374s | Train Loss: 0.021966 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.381s | Train Loss: 0.021792 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.360s | Train Loss: 0.021745 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.364s | Train Loss: 0.021492 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.373s | Train Loss: 0.021312 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.374s | Train Loss: 0.021256 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.376s | Train Loss: 0.021193 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.377s | Train Loss: 0.021039 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.373s | Train Loss: 0.020786 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.371s | Train Loss: 0.020627 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.379s | Train Loss: 0.020613 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.376s | Train Loss: 0.020403 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.375s | Train Loss: 0.020394 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.374s | Train Loss: 0.020246 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.377s | Train Loss: 0.020297 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.377s | Train Loss: 0.020136 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.377s | Train Loss: 0.020207 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.375s | Train Loss: 0.020170 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.373s | Train Loss: 0.020000 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.373s | Train Loss: 0.019895 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.378s | Train Loss: 0.019516 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.372s | Train Loss: 0.019746 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.372s | Train Loss: 0.019468 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.378s | Train Loss: 0.019422 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.374s | Train Loss: 0.019258 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.376s | Train Loss: 0.019185 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.374s | Train Loss: 0.019064 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.373s | Train Loss: 0.019077 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.376s | Train Loss: 0.018937 |\n",
      "INFO:root:Pretraining Time: 26.211s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.019488\n",
      "INFO:root:Test AUC: 50.16%\n",
      "INFO:root:Test AUC: 50.16%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.637s | Train Loss: 0.828028 || Validation Loss: 0.013764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.657s | Train Loss: 0.637048 || Validation Loss: 0.015658 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.706s | Train Loss: 0.583861 || Validation Loss: 0.016148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.736s | Train Loss: 0.554965 || Validation Loss: 0.016558 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.722s | Train Loss: 0.529150 || Validation Loss: 0.017120 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.726s | Train Loss: 0.511434 || Validation Loss: 0.020304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.780s | Train Loss: 0.489347 || Validation Loss: 0.020717 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.747s | Train Loss: 0.487005 || Validation Loss: 0.021333 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.821s | Train Loss: 0.468532 || Validation Loss: 0.022223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.800s | Train Loss: 0.464651 || Validation Loss: 0.022029 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.849s | Train Loss: 0.463087 || Validation Loss: 0.021941 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.837s | Train Loss: 0.445570 || Validation Loss: 0.020909 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.885s | Train Loss: 0.435999 || Validation Loss: 0.021577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.886s | Train Loss: 0.428370 || Validation Loss: 0.023731 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.901s | Train Loss: 0.429265 || Validation Loss: 0.024737 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.880s | Train Loss: 0.418932 || Validation Loss: 0.024629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.999s | Train Loss: 0.424639 || Validation Loss: 0.021872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.947s | Train Loss: 0.419409 || Validation Loss: 0.024149 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.956s | Train Loss: 0.417617 || Validation Loss: 0.024135 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.969s | Train Loss: 0.408413 || Validation Loss: 0.022083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.055s | Train Loss: 0.408368 || Validation Loss: 0.022198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.024s | Train Loss: 0.408590 || Validation Loss: 0.023183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.005s | Train Loss: 0.402843 || Validation Loss: 0.025398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.093s | Train Loss: 0.402222 || Validation Loss: 0.024010 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.106s | Train Loss: 0.399376 || Validation Loss: 0.024566 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.125s | Train Loss: 0.394105 || Validation Loss: 0.024639 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.102s | Train Loss: 0.390767 || Validation Loss: 0.026770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.124s | Train Loss: 0.397348 || Validation Loss: 0.023996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.134s | Train Loss: 0.390852 || Validation Loss: 0.024082 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.257s | Train Loss: 0.382285 || Validation Loss: 0.024338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.209s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.896%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022470\n",
      "INFO:root:Test AUC: 86.71%\n",
      "INFO:root:Test Time: 0.185s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1255})\n",
      "class weights 0.04697735354669661 0.9530226464533034\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7195396785427465\n",
      "validation AUC 0.7693004220055032\n",
      "validation AUC 0.7952996465762446\n",
      "validation AUC 0.8121062296577533\n",
      "validation AUC 0.8233128107809996\n",
      "validation AUC 0.8318330685777036\n",
      "validation AUC 0.8374878087535598\n",
      "validation AUC 0.8400391849329603\n",
      "validation AUC 0.8440992025828628\n",
      "validation AUC 0.8463570230200106\n",
      "validation AUC 0.8492399760921728\n",
      "validation AUC 0.8509514067878903\n",
      "validation AUC 0.8527185362075229\n",
      "validation AUC 0.8529332639143574\n",
      "validation AUC 0.8540646738439814\n",
      "validation AUC 0.8559723724325213\n",
      "validation AUC 0.8566489174322158\n",
      "validation AUC 0.8585631320317842\n",
      "validation AUC 0.858051921832133\n",
      "validation AUC 0.8580776080593558\n",
      "validation AUC 0.8588528670342098\n",
      "validation AUC 0.8595570059703583\n",
      "validation AUC 0.8604496675530641\n",
      "validation AUC 0.8602231610055506\n",
      "validation AUC 0.8598383358773004\n",
      "validation AUC 0.8607948698000423\n",
      "validation AUC 0.860670991091397\n",
      "validation AUC 0.8614994104992227\n",
      "validation AUC 0.8618750536127489\n",
      "validation AUC 0.8618473665524135\n",
      "validation AUC 0.8618473665524135\n",
      "train auc 0.8960185927330617\n",
      "Test AUC 0.8670851504119359\n",
      "test false positive rates [0.         0.         0.         ... 0.99446708 0.99446708 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 3.63941769e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.35683556e+01 6.25683556e+01 5.47058487e+01 ... 3.70507091e-02\n",
      " 3.70392539e-02 1.02221714e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.397s | Train Loss: 0.193862 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.380s | Train Loss: 0.151634 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.377s | Train Loss: 0.119823 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.374s | Train Loss: 0.096748 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.374s | Train Loss: 0.080857 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.374s | Train Loss: 0.069353 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.375s | Train Loss: 0.061094 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.375s | Train Loss: 0.055115 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.374s | Train Loss: 0.050638 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.373s | Train Loss: 0.047111 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.374s | Train Loss: 0.044034 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.373s | Train Loss: 0.041596 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.373s | Train Loss: 0.039326 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.368s | Train Loss: 0.037433 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.372s | Train Loss: 0.035771 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.374s | Train Loss: 0.034072 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.372s | Train Loss: 0.032667 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.375s | Train Loss: 0.031234 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.370s | Train Loss: 0.030409 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.375s | Train Loss: 0.029720 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.376s | Train Loss: 0.028391 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.375s | Train Loss: 0.027825 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.371s | Train Loss: 0.027022 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.373s | Train Loss: 0.026229 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.378s | Train Loss: 0.025869 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.375s | Train Loss: 0.025186 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.370s | Train Loss: 0.024724 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.375s | Train Loss: 0.024458 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.373s | Train Loss: 0.024148 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.372s | Train Loss: 0.023715 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.384s | Train Loss: 0.023326 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.374s | Train Loss: 0.023091 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.375s | Train Loss: 0.022796 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.375s | Train Loss: 0.022398 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.375s | Train Loss: 0.022308 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.372s | Train Loss: 0.021889 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.371s | Train Loss: 0.021791 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.376s | Train Loss: 0.021578 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.374s | Train Loss: 0.021605 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.372s | Train Loss: 0.021351 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.371s | Train Loss: 0.020999 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.360s | Train Loss: 0.020928 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.370s | Train Loss: 0.020642 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.372s | Train Loss: 0.020398 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.376s | Train Loss: 0.020353 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.374s | Train Loss: 0.020241 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.373s | Train Loss: 0.020046 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.375s | Train Loss: 0.019936 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.372s | Train Loss: 0.019874 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.386s | Train Loss: 0.019708 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.376s | Train Loss: 0.019576 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.373s | Train Loss: 0.019269 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.375s | Train Loss: 0.019410 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.371s | Train Loss: 0.019093 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.375s | Train Loss: 0.018854 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.378s | Train Loss: 0.018916 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.374s | Train Loss: 0.018627 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.374s | Train Loss: 0.018431 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.374s | Train Loss: 0.018428 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.374s | Train Loss: 0.018344 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.377s | Train Loss: 0.018097 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.375s | Train Loss: 0.017933 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.374s | Train Loss: 0.017936 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.375s | Train Loss: 0.017774 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.375s | Train Loss: 0.017825 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.375s | Train Loss: 0.017636 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.367s | Train Loss: 0.017495 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.373s | Train Loss: 0.017299 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.369s | Train Loss: 0.017283 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.373s | Train Loss: 0.017371 |\n",
      "INFO:root:Pretraining Time: 26.194s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017519\n",
      "INFO:root:Test AUC: 52.56%\n",
      "INFO:root:Test AUC: 52.56%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.645s | Train Loss: 0.773636 || Validation Loss: 0.015028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.659s | Train Loss: 0.630544 || Validation Loss: 0.017044 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.699s | Train Loss: 0.580264 || Validation Loss: 0.017006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.732s | Train Loss: 0.548659 || Validation Loss: 0.018713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.717s | Train Loss: 0.523179 || Validation Loss: 0.018716 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.726s | Train Loss: 0.510022 || Validation Loss: 0.019759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.771s | Train Loss: 0.487540 || Validation Loss: 0.019466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.755s | Train Loss: 0.483295 || Validation Loss: 0.019281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.819s | Train Loss: 0.474787 || Validation Loss: 0.019979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.807s | Train Loss: 0.461795 || Validation Loss: 0.020247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.861s | Train Loss: 0.465796 || Validation Loss: 0.020467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.834s | Train Loss: 0.436988 || Validation Loss: 0.020664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.882s | Train Loss: 0.441511 || Validation Loss: 0.021419 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.896s | Train Loss: 0.441066 || Validation Loss: 0.021999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.912s | Train Loss: 0.431125 || Validation Loss: 0.022617 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.886s | Train Loss: 0.433082 || Validation Loss: 0.023998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.009s | Train Loss: 0.424868 || Validation Loss: 0.022921 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.951s | Train Loss: 0.419379 || Validation Loss: 0.024165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.963s | Train Loss: 0.416785 || Validation Loss: 0.022427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.977s | Train Loss: 0.416428 || Validation Loss: 0.023578 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.068s | Train Loss: 0.414346 || Validation Loss: 0.021798 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.052s | Train Loss: 0.412297 || Validation Loss: 0.022247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.005s | Train Loss: 0.400884 || Validation Loss: 0.021780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.113s | Train Loss: 0.401866 || Validation Loss: 0.022967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.113s | Train Loss: 0.396023 || Validation Loss: 0.022775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.145s | Train Loss: 0.396334 || Validation Loss: 0.025483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.110s | Train Loss: 0.396734 || Validation Loss: 0.022533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.136s | Train Loss: 0.400951 || Validation Loss: 0.023580 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.158s | Train Loss: 0.384297 || Validation Loss: 0.024386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.250s | Train Loss: 0.383141 || Validation Loss: 0.022764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.379s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.898%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020898\n",
      "INFO:root:Test AUC: 86.78%\n",
      "INFO:root:Test Time: 0.191s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1255})\n",
      "class weights 0.04697735354669661 0.9530226464533034\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7024238789427854\n",
      "validation AUC 0.7649068186476795\n",
      "validation AUC 0.7969916542909463\n",
      "validation AUC 0.8170516399062333\n",
      "validation AUC 0.8285824437962785\n",
      "validation AUC 0.8360716552615631\n",
      "validation AUC 0.8421494173212123\n",
      "validation AUC 0.8458449640626959\n",
      "validation AUC 0.849328558189016\n",
      "validation AUC 0.8516106679951632\n",
      "validation AUC 0.8545718131996664\n",
      "validation AUC 0.8566984566765352\n",
      "validation AUC 0.8583239287081876\n",
      "validation AUC 0.8591646511111117\n",
      "validation AUC 0.8608884433369384\n",
      "validation AUC 0.8611105544372412\n",
      "validation AUC 0.8618424549328305\n",
      "validation AUC 0.863778218398597\n",
      "validation AUC 0.8639376012522662\n",
      "validation AUC 0.8636289221916446\n",
      "validation AUC 0.8646635843903514\n",
      "validation AUC 0.8640928307808167\n",
      "validation AUC 0.8641065838477852\n",
      "validation AUC 0.8644022734572672\n",
      "validation AUC 0.8655354660440528\n",
      "validation AUC 0.8654409080546943\n",
      "validation AUC 0.8649452894535024\n",
      "validation AUC 0.8646552192051031\n",
      "validation AUC 0.8656019804416435\n",
      "validation AUC 0.8658054348374418\n",
      "validation AUC 0.8658054348374418\n",
      "train auc 0.8976712731469669\n",
      "Test AUC 0.8678208663307381\n",
      "test false positive rates [0.         0.         0.         ... 0.99946945 0.99946945 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 4.19932811e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.95329514e+01 7.85329514e+01 5.53292389e+01 ... 2.45496575e-02\n",
      " 2.45371647e-02 1.14540122e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.387s | Train Loss: 0.195179 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.376s | Train Loss: 0.152844 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.382s | Train Loss: 0.120263 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.382s | Train Loss: 0.097461 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.381s | Train Loss: 0.081614 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.381s | Train Loss: 0.070773 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.378s | Train Loss: 0.062768 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.381s | Train Loss: 0.057001 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.381s | Train Loss: 0.052243 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.379s | Train Loss: 0.048273 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.380s | Train Loss: 0.044925 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.379s | Train Loss: 0.042110 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.378s | Train Loss: 0.039694 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.379s | Train Loss: 0.037677 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.381s | Train Loss: 0.035779 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.379s | Train Loss: 0.034458 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.381s | Train Loss: 0.033009 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.379s | Train Loss: 0.031702 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.383s | Train Loss: 0.030386 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.381s | Train Loss: 0.029825 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.382s | Train Loss: 0.028501 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.381s | Train Loss: 0.027658 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.381s | Train Loss: 0.027039 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.379s | Train Loss: 0.026361 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.380s | Train Loss: 0.025919 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.380s | Train Loss: 0.025141 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.381s | Train Loss: 0.024872 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.378s | Train Loss: 0.024263 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.382s | Train Loss: 0.023891 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.375s | Train Loss: 0.023450 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.381s | Train Loss: 0.023035 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.380s | Train Loss: 0.023009 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.381s | Train Loss: 0.022222 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.382s | Train Loss: 0.021940 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.377s | Train Loss: 0.021898 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.379s | Train Loss: 0.021646 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.382s | Train Loss: 0.021499 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.381s | Train Loss: 0.021098 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.379s | Train Loss: 0.020870 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.379s | Train Loss: 0.020642 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.378s | Train Loss: 0.020321 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.382s | Train Loss: 0.020386 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.377s | Train Loss: 0.020004 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.380s | Train Loss: 0.019707 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.362s | Train Loss: 0.019485 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.380s | Train Loss: 0.019613 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.380s | Train Loss: 0.019358 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.376s | Train Loss: 0.019115 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.381s | Train Loss: 0.018997 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.379s | Train Loss: 0.018828 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.380s | Train Loss: 0.018816 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.378s | Train Loss: 0.018588 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.378s | Train Loss: 0.018495 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.377s | Train Loss: 0.018391 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.375s | Train Loss: 0.018263 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.374s | Train Loss: 0.018272 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.379s | Train Loss: 0.017946 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.379s | Train Loss: 0.018076 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.368s | Train Loss: 0.017836 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.377s | Train Loss: 0.017794 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.377s | Train Loss: 0.017779 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.379s | Train Loss: 0.017507 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.380s | Train Loss: 0.017227 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.375s | Train Loss: 0.017295 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.374s | Train Loss: 0.017376 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.374s | Train Loss: 0.017102 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.375s | Train Loss: 0.017166 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.382s | Train Loss: 0.017237 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.377s | Train Loss: 0.016947 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.373s | Train Loss: 0.016947 |\n",
      "INFO:root:Pretraining Time: 26.523s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017298\n",
      "INFO:root:Test AUC: 51.56%\n",
      "INFO:root:Test AUC: 51.56%\n",
      "INFO:root:Test Time: 0.232s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.664s | Train Loss: 1.007763 || Validation Loss: 0.013377 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.678s | Train Loss: 0.705045 || Validation Loss: 0.014046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.726s | Train Loss: 0.618367 || Validation Loss: 0.015533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.756s | Train Loss: 0.575231 || Validation Loss: 0.016322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.732s | Train Loss: 0.549995 || Validation Loss: 0.016105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.744s | Train Loss: 0.527656 || Validation Loss: 0.016946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.783s | Train Loss: 0.506381 || Validation Loss: 0.018072 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.765s | Train Loss: 0.488369 || Validation Loss: 0.019892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.840s | Train Loss: 0.484947 || Validation Loss: 0.018213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.822s | Train Loss: 0.476953 || Validation Loss: 0.016996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.873s | Train Loss: 0.462122 || Validation Loss: 0.021160 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.847s | Train Loss: 0.459860 || Validation Loss: 0.018051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.902s | Train Loss: 0.459174 || Validation Loss: 0.021656 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.909s | Train Loss: 0.447330 || Validation Loss: 0.021892 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.926s | Train Loss: 0.441842 || Validation Loss: 0.021636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.898s | Train Loss: 0.424248 || Validation Loss: 0.020846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.013s | Train Loss: 0.431789 || Validation Loss: 0.021705 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.962s | Train Loss: 0.429710 || Validation Loss: 0.022640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.975s | Train Loss: 0.419007 || Validation Loss: 0.023346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.986s | Train Loss: 0.416435 || Validation Loss: 0.020864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.073s | Train Loss: 0.417885 || Validation Loss: 0.019930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.055s | Train Loss: 0.412975 || Validation Loss: 0.021773 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.020s | Train Loss: 0.405532 || Validation Loss: 0.019539 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.101s | Train Loss: 0.411467 || Validation Loss: 0.021551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.130s | Train Loss: 0.413854 || Validation Loss: 0.020795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.147s | Train Loss: 0.408947 || Validation Loss: 0.024096 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.130s | Train Loss: 0.411841 || Validation Loss: 0.021778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.139s | Train Loss: 0.399513 || Validation Loss: 0.021908 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.146s | Train Loss: 0.400250 || Validation Loss: 0.023298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.261s | Train Loss: 0.403124 || Validation Loss: 0.024382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.758s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.887%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020185\n",
      "INFO:root:Test AUC: 86.21%\n",
      "INFO:root:Test Time: 0.198s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1255})\n",
      "class weights 0.04697735354669661 0.9530226464533034\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6442819009957976\n",
      "validation AUC 0.7338380949259085\n",
      "validation AUC 0.7839272969617349\n",
      "validation AUC 0.811388063604356\n",
      "validation AUC 0.8256405938600391\n",
      "validation AUC 0.8341774011008414\n",
      "validation AUC 0.8410401602326756\n",
      "validation AUC 0.8449693494716842\n",
      "validation AUC 0.8485647853797846\n",
      "validation AUC 0.849239366795919\n",
      "validation AUC 0.8520329608519972\n",
      "validation AUC 0.8532924134581145\n",
      "validation AUC 0.8545448152560545\n",
      "validation AUC 0.8557528209086209\n",
      "validation AUC 0.8556488148363053\n",
      "validation AUC 0.8565338402607213\n",
      "validation AUC 0.8572740207997245\n",
      "validation AUC 0.8577806067334846\n",
      "validation AUC 0.8583943197199345\n",
      "validation AUC 0.8577852070532347\n",
      "validation AUC 0.8580804443467205\n",
      "validation AUC 0.8575875050526358\n",
      "validation AUC 0.8570799852193774\n",
      "validation AUC 0.8570818556790561\n",
      "validation AUC 0.856061263168515\n",
      "validation AUC 0.8554701207502783\n",
      "validation AUC 0.8561639867919472\n",
      "validation AUC 0.8558843384362382\n",
      "validation AUC 0.8566753752573146\n",
      "validation AUC 0.8551160451277267\n",
      "validation AUC 0.8551160451277267\n",
      "train auc 0.8873063406835868\n",
      "Test AUC 0.8620953964277132\n",
      "test false positive rates [0.         0.         0.         ... 0.98978683 0.98978683 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.91254921e+01 5.81254921e+01 3.96688347e+01 ... 5.80808371e-02\n",
      " 5.80695160e-02 9.27204266e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.394s | Train Loss: 0.192127 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.378s | Train Loss: 0.150410 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.376s | Train Loss: 0.119599 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.371s | Train Loss: 0.097036 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.366s | Train Loss: 0.081267 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.382s | Train Loss: 0.070051 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.391s | Train Loss: 0.062127 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.387s | Train Loss: 0.055394 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.390s | Train Loss: 0.050686 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.389s | Train Loss: 0.046932 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.387s | Train Loss: 0.043787 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.392s | Train Loss: 0.041253 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.393s | Train Loss: 0.039119 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.393s | Train Loss: 0.036824 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.396s | Train Loss: 0.035393 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.392s | Train Loss: 0.034013 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.389s | Train Loss: 0.032570 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.395s | Train Loss: 0.031566 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.468s | Train Loss: 0.030508 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.402s | Train Loss: 0.029623 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.411s | Train Loss: 0.028835 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.416s | Train Loss: 0.028053 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.413s | Train Loss: 0.027480 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.419s | Train Loss: 0.026604 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.415s | Train Loss: 0.025890 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.415s | Train Loss: 0.025324 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.414s | Train Loss: 0.024819 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.412s | Train Loss: 0.024483 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.400s | Train Loss: 0.023746 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.399s | Train Loss: 0.023721 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.391s | Train Loss: 0.023482 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.394s | Train Loss: 0.022883 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.392s | Train Loss: 0.022730 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.392s | Train Loss: 0.022392 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.391s | Train Loss: 0.022098 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.387s | Train Loss: 0.021895 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.382s | Train Loss: 0.021376 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.389s | Train Loss: 0.021370 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.381s | Train Loss: 0.021208 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.381s | Train Loss: 0.021002 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.384s | Train Loss: 0.020770 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.381s | Train Loss: 0.020702 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.381s | Train Loss: 0.020626 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.370s | Train Loss: 0.020602 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.378s | Train Loss: 0.020068 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.386s | Train Loss: 0.020058 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.376s | Train Loss: 0.019937 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.368s | Train Loss: 0.019375 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.370s | Train Loss: 0.019590 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.373s | Train Loss: 0.019370 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.378s | Train Loss: 0.019324 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.374s | Train Loss: 0.019277 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.371s | Train Loss: 0.019008 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.380s | Train Loss: 0.018813 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.380s | Train Loss: 0.018735 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.384s | Train Loss: 0.018797 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.385s | Train Loss: 0.018706 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.385s | Train Loss: 0.018584 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.371s | Train Loss: 0.018466 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.378s | Train Loss: 0.018488 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.385s | Train Loss: 0.018408 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.381s | Train Loss: 0.018053 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.383s | Train Loss: 0.018143 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.382s | Train Loss: 0.018157 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.384s | Train Loss: 0.018018 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.380s | Train Loss: 0.017949 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.359s | Train Loss: 0.017776 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.374s | Train Loss: 0.017803 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.384s | Train Loss: 0.017682 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.378s | Train Loss: 0.017511 |\n",
      "INFO:root:Pretraining Time: 27.178s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017948\n",
      "INFO:root:Test AUC: 51.91%\n",
      "INFO:root:Test AUC: 51.91%\n",
      "INFO:root:Test Time: 0.224s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.662s | Train Loss: 0.743461 || Validation Loss: 0.015492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.665s | Train Loss: 0.633035 || Validation Loss: 0.015859 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.756s | Train Loss: 0.592805 || Validation Loss: 0.018420 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.777s | Train Loss: 0.559857 || Validation Loss: 0.017919 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.742s | Train Loss: 0.537219 || Validation Loss: 0.016292 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.773s | Train Loss: 0.521980 || Validation Loss: 0.018151 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.790s | Train Loss: 0.515934 || Validation Loss: 0.019537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.778s | Train Loss: 0.498418 || Validation Loss: 0.019417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.851s | Train Loss: 0.481117 || Validation Loss: 0.018520 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.815s | Train Loss: 0.480824 || Validation Loss: 0.020751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.916s | Train Loss: 0.470598 || Validation Loss: 0.020198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.851s | Train Loss: 0.467230 || Validation Loss: 0.022715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.904s | Train Loss: 0.461820 || Validation Loss: 0.020987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.869s | Train Loss: 0.447965 || Validation Loss: 0.020998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.906s | Train Loss: 0.446432 || Validation Loss: 0.023462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.881s | Train Loss: 0.445776 || Validation Loss: 0.020198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.019s | Train Loss: 0.440311 || Validation Loss: 0.021620 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.946s | Train Loss: 0.429476 || Validation Loss: 0.021129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.962s | Train Loss: 0.434822 || Validation Loss: 0.021155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.963s | Train Loss: 0.422696 || Validation Loss: 0.020661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.060s | Train Loss: 0.419654 || Validation Loss: 0.023811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.044s | Train Loss: 0.419258 || Validation Loss: 0.024015 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.013s | Train Loss: 0.414614 || Validation Loss: 0.022542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.102s | Train Loss: 0.417473 || Validation Loss: 0.023065 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.115s | Train Loss: 0.408774 || Validation Loss: 0.022075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.138s | Train Loss: 0.402332 || Validation Loss: 0.023271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.131s | Train Loss: 0.407772 || Validation Loss: 0.024912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.127s | Train Loss: 0.402858 || Validation Loss: 0.023099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.131s | Train Loss: 0.403781 || Validation Loss: 0.024480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.254s | Train Loss: 0.394324 || Validation Loss: 0.019976 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.673s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.892%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021087\n",
      "INFO:root:Test AUC: 86.61%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1255})\n",
      "class weights 0.04697735354669661 0.9530226464533034\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7147488459556457\n",
      "validation AUC 0.768725597552002\n",
      "validation AUC 0.7957204148621032\n",
      "validation AUC 0.8097869208519377\n",
      "validation AUC 0.8200118671753325\n",
      "validation AUC 0.8273847429663798\n",
      "validation AUC 0.8319906820776225\n",
      "validation AUC 0.8374728849863655\n",
      "validation AUC 0.840599732165074\n",
      "validation AUC 0.8431793514874172\n",
      "validation AUC 0.8454332846676795\n",
      "validation AUC 0.8465911444403551\n",
      "validation AUC 0.8488172841329888\n",
      "validation AUC 0.8511545285983972\n",
      "validation AUC 0.8516338984125944\n",
      "validation AUC 0.8507863274133507\n",
      "validation AUC 0.8535786124137221\n",
      "validation AUC 0.8531512536177297\n",
      "validation AUC 0.8541762176293406\n",
      "validation AUC 0.8554929387619483\n",
      "validation AUC 0.8566395119201761\n",
      "validation AUC 0.8566006260479097\n",
      "validation AUC 0.8573686639309177\n",
      "validation AUC 0.8583801542472047\n",
      "validation AUC 0.8591173867501852\n",
      "validation AUC 0.8585688285526548\n",
      "validation AUC 0.8588632037850653\n",
      "validation AUC 0.8579426316447082\n",
      "validation AUC 0.8600293675472958\n",
      "validation AUC 0.8610954124940958\n",
      "validation AUC 0.8610954124940958\n",
      "train auc 0.8917507749156323\n",
      "Test AUC 0.8660518517889814\n",
      "test false positive rates [0.         0.         0.         ... 0.99694931 0.99694931 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.95968645e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.91246872e+01 5.81246872e+01 5.03577538e+01 ... 4.08091284e-02\n",
      " 4.07570302e-02 8.98874924e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {6..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m1.5\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c177186",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below multiplier =6 (0.36 share of minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fe8748c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.525s | Train Loss: 0.194922 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.482s | Train Loss: 0.147569 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.565s | Train Loss: 0.112301 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.662s | Train Loss: 0.209025 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.681s | Train Loss: 0.088449 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.624s | Train Loss: 0.157466 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.730s | Train Loss: 0.072282 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.629s | Train Loss: 0.120747 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.676s | Train Loss: 0.061484 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.666s | Train Loss: 0.095581 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.619s | Train Loss: 0.053982 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.622s | Train Loss: 0.078318 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.617s | Train Loss: 0.048558 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.647s | Train Loss: 0.066567 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.609s | Train Loss: 0.044159 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.651s | Train Loss: 0.057897 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.623s | Train Loss: 0.040902 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.659s | Train Loss: 0.051462 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.610s | Train Loss: 0.038079 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.724s | Train Loss: 0.046622 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.681s | Train Loss: 0.036081 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.679s | Train Loss: 0.042573 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.659s | Train Loss: 0.033756 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.636s | Train Loss: 0.039472 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.633s | Train Loss: 0.031953 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.629s | Train Loss: 0.036642 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.690s | Train Loss: 0.030527 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.678s | Train Loss: 0.034372 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.718s | Train Loss: 0.029109 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.642s | Train Loss: 0.032640 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.714s | Train Loss: 0.027901 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.611s | Train Loss: 0.031288 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.661s | Train Loss: 0.026850 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.598s | Train Loss: 0.029744 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.635s | Train Loss: 0.028542 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.749s | Train Loss: 0.025864 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.602s | Train Loss: 0.027499 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.607s | Train Loss: 0.025052 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.592s | Train Loss: 0.026611 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.617s | Train Loss: 0.024459 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.603s | Train Loss: 0.025871 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.619s | Train Loss: 0.023768 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.629s | Train Loss: 0.025062 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.629s | Train Loss: 0.023365 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.727s | Train Loss: 0.024478 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.702s | Train Loss: 0.022878 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.601s | Train Loss: 0.023724 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.612s | Train Loss: 0.022357 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.605s | Train Loss: 0.023251 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.622s | Train Loss: 0.022128 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.603s | Train Loss: 0.022817 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.617s | Train Loss: 0.021685 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.639s | Train Loss: 0.022547 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.657s | Train Loss: 0.021201 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.646s | Train Loss: 0.022110 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.624s | Train Loss: 0.021100 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.636s | Train Loss: 0.021707 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.687s | Train Loss: 0.020823 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.614s | Train Loss: 0.021352 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.621s | Train Loss: 0.020599 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.607s | Train Loss: 0.021216 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.629s | Train Loss: 0.020370 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.608s | Train Loss: 0.020837 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.675s | Train Loss: 0.020057 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.606s | Train Loss: 0.020637 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.647s | Train Loss: 0.019813 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.658s | Train Loss: 0.020254 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.647s | Train Loss: 0.019873 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.688s | Train Loss: 0.020456 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.663s | Train Loss: 0.019623 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.663s | Train Loss: 0.019939 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.648s | Train Loss: 0.019570 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.623s | Train Loss: 0.019692 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.666s | Train Loss: 0.019278 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.650s | Train Loss: 0.019605 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.646s | Train Loss: 0.019075 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.634s | Train Loss: 0.019407 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.675s | Train Loss: 0.019217 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.671s | Train Loss: 0.019162 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.621s | Train Loss: 0.018939 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.607s | Train Loss: 0.019156 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.635s | Train Loss: 0.018833 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.638s | Train Loss: 0.018932 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.631s | Train Loss: 0.018701 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.656s | Train Loss: 0.018825 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.599s | Train Loss: 0.018484 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.631s | Train Loss: 0.018635 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.640s | Train Loss: 0.018392 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.657s | Train Loss: 0.018496 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.619s | Train Loss: 0.018463 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.628s | Train Loss: 0.018465 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.672s | Train Loss: 0.018397 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.623s | Train Loss: 0.018282 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.692s | Train Loss: 0.018318 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.625s | Train Loss: 0.018143 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.667s | Train Loss: 0.018150 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.602s | Train Loss: 0.017944 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.635s | Train Loss: 0.018207 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.633s | Train Loss: 0.018167 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.633s | Train Loss: 0.018154 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.606s | Train Loss: 0.017827 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.646s | Train Loss: 0.017948 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.670s | Train Loss: 0.017628 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.619s | Train Loss: 0.017774 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.667s | Train Loss: 0.017712 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.687s | Train Loss: 0.017949 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.609s | Train Loss: 0.017489 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.678s | Train Loss: 0.017883 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.603s | Train Loss: 0.017518 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.621s | Train Loss: 0.017694 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.611s | Train Loss: 0.017305 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.623s | Train Loss: 0.017729 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.607s | Train Loss: 0.017167 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.623s | Train Loss: 0.017429 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.607s | Train Loss: 0.017042 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.655s | Train Loss: 0.017421 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.599s | Train Loss: 0.017074 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.608s | Train Loss: 0.016986 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.618s | Train Loss: 0.017401 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.643s | Train Loss: 0.016627 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.676s | Train Loss: 0.017300 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.606s | Train Loss: 0.016676 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.693s | Train Loss: 0.017272 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.602s | Train Loss: 0.016787 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.616s | Train Loss: 0.017266 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.599s | Train Loss: 0.016454 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.613s | Train Loss: 0.017319 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.604s | Train Loss: 0.016357 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.622s | Train Loss: 0.017239 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.683s | Train Loss: 0.016252 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.657s | Train Loss: 0.017122 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.670s | Train Loss: 0.016199 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.647s | Train Loss: 0.017146 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.649s | Train Loss: 0.016266 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.619s | Train Loss: 0.016771 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.619s | Train Loss: 0.015997 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.664s | Train Loss: 0.016947 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.621s | Train Loss: 0.016047 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.637s | Train Loss: 0.016757 |\n",
      "INFO:root:Pretraining Time: 45.050s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018361\n",
      "INFO:root:Test AUC: 51.98%\n",
      "INFO:root:Test AUC: 51.98%\n",
      "INFO:root:Test Time: 0.327s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.555s | Train Loss: 0.015810 |\n",
      "INFO:root:Pretraining Time: 44.258s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:Test Loss: 0.017162\n",
      "INFO:root:Test AUC: 49.88%\n",
      "INFO:root:Test AUC: 49.88%\n",
      "INFO:root:Test Time: 0.287s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.958s | Train Loss: 0.863270 || Validation Loss: 0.015762 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.872s | Train Loss: 0.822196 || Validation Loss: 0.015251 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.943s | Train Loss: 0.659413 || Validation Loss: 0.015321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.893s | Train Loss: 0.640833 || Validation Loss: 0.015055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.998s | Train Loss: 0.584793 || Validation Loss: 0.016367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.031s | Train Loss: 0.576029 || Validation Loss: 0.016358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.078s | Train Loss: 0.549915 || Validation Loss: 0.017203 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.023s | Train Loss: 0.540188 || Validation Loss: 0.017805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.044s | Train Loss: 0.521775 || Validation Loss: 0.016787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.014s | Train Loss: 0.524582 || Validation Loss: 0.019166 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.113s | Train Loss: 0.501838 || Validation Loss: 0.017849 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.033s | Train Loss: 0.500321 || Validation Loss: 0.018230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.084s | Train Loss: 0.496055 || Validation Loss: 0.018570 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.035s | Train Loss: 0.483413 || Validation Loss: 0.021155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.105s | Train Loss: 0.475870 || Validation Loss: 0.019225 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.197s | Train Loss: 0.477706 || Validation Loss: 0.019046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.089s | Train Loss: 0.469401 || Validation Loss: 0.020815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.130s | Train Loss: 0.473438 || Validation Loss: 0.019387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.169s | Train Loss: 0.452302 || Validation Loss: 0.019608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.238s | Train Loss: 0.467157 || Validation Loss: 0.018851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.154s | Train Loss: 0.436802 || Validation Loss: 0.023499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.199s | Train Loss: 0.452659 || Validation Loss: 0.019962 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.116s | Train Loss: 0.442743 || Validation Loss: 0.019751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.132s | Train Loss: 0.442026 || Validation Loss: 0.020477 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.210s | Train Loss: 0.437018 || Validation Loss: 0.022399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.234s | Train Loss: 0.445837 || Validation Loss: 0.020483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.173s | Train Loss: 0.424868 || Validation Loss: 0.021391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.220s | Train Loss: 0.432121 || Validation Loss: 0.020474 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.194s | Train Loss: 0.425347 || Validation Loss: 0.021464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.223s | Train Loss: 0.426650 || Validation Loss: 0.021500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.230s | Train Loss: 0.414237 || Validation Loss: 0.023107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.277s | Train Loss: 0.417938 || Validation Loss: 0.021729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.271s | Train Loss: 0.419625 || Validation Loss: 0.021421 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.279s | Train Loss: 0.431547 || Validation Loss: 0.021618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.293s | Train Loss: 0.413023 || Validation Loss: 0.021772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.316s | Train Loss: 0.411345 || Validation Loss: 0.022098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.244s | Train Loss: 0.415900 || Validation Loss: 0.023635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.324s | Train Loss: 0.410118 || Validation Loss: 0.021734 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.314s | Train Loss: 0.409042 || Validation Loss: 0.021571 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.338s | Train Loss: 0.416607 || Validation Loss: 0.020515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.456s | Train Loss: 0.402818 || Validation Loss: 0.022630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.462s | Train Loss: 0.402724 || Validation Loss: 0.022592 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.274s | Train Loss: 0.402326 || Validation Loss: 0.020623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.316s | Train Loss: 0.400432 || Validation Loss: 0.023385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.413s | Train Loss: 0.389202 || Validation Loss: 0.022779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.418s | Train Loss: 0.392831 || Validation Loss: 0.020926 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.428s | Train Loss: 0.383518 || Validation Loss: 0.025089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.461s | Train Loss: 0.391292 || Validation Loss: 0.021831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.430s | Train Loss: 0.397220 || Validation Loss: 0.023340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.529s | Train Loss: 0.399211 || Validation Loss: 0.019220 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.453s | Train Loss: 0.379269 || Validation Loss: 0.024225 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.472s | Train Loss: 0.385615 || Validation Loss: 0.021675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.363s | Train Loss: 0.379377 || Validation Loss: 0.024402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.442s | Train Loss: 0.374956 || Validation Loss: 0.020467 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.610s | Train Loss: 0.383341 || Validation Loss: 0.022905 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.631s | Train Loss: 0.387557 || Validation Loss: 0.023079 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.488s | Train Loss: 0.379851 || Validation Loss: 0.022923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.492s | Train Loss: 0.381826 || Validation Loss: 0.021679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.567s | Train Loss: 0.373889 || Validation Loss: 0.022532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.478s | Train Loss: 0.380712 || Validation Loss: 0.022033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 37.975s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.899%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020988\n",
      "INFO:root:Test AUC: 85.34%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6896974256089184\n",
      "validation AUC 0.763281554149249\n",
      "validation AUC 0.7917596539665559\n",
      "validation AUC 0.8077598560719857\n",
      "validation AUC 0.8189318110753776\n",
      "validation AUC 0.8264332004836906\n",
      "validation AUC 0.831835662742976\n",
      "validation AUC 0.8377797547830023\n",
      "validation AUC 0.8404582477214448\n",
      "validation AUC 0.8420171920520948\n",
      "validation AUC 0.844738575189712\n",
      "validation AUC 0.8466669419585984\n",
      "validation AUC 0.8484758893224263\n",
      "validation AUC 0.8504863274984926\n",
      "validation AUC 0.8497396362698256\n",
      "validation AUC 0.8495149203615209\n",
      "validation AUC 0.8517445428873256\n",
      "validation AUC 0.852407933473576\n",
      "validation AUC 0.8521261352865438\n",
      "validation AUC 0.851152139305664\n",
      "validation AUC 0.8517067345914032\n",
      "validation AUC 0.8535473068254164\n",
      "validation AUC 0.8519752107420048\n",
      "validation AUC 0.8525055938185325\n",
      "validation AUC 0.8525544572495398\n",
      "validation AUC 0.8540844294102885\n",
      "validation AUC 0.8522341084362146\n",
      "validation AUC 0.850606446663004\n",
      "validation AUC 0.8513672315259779\n",
      "validation AUC 0.8497728069964452\n",
      "validation AUC 0.8497728069964452\n",
      "train auc 0.8988075874472586\n",
      "Test AUC 0.8534223761526437\n",
      "test false positive rates [0.         0.         0.         ... 0.97252487 0.97252487 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 6.43896976e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.28112869e+01 5.18112869e+01 3.45108948e+01 ... 5.95142916e-02\n",
      " 5.94898611e-02 5.06568979e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Training Time: 39.042s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.898%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019832\n",
      "INFO:root:Test AUC: 87.10%\n",
      "INFO:root:Test Time: 0.309s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6703422387789979\n",
      "validation AUC 0.7545585470375643\n",
      "validation AUC 0.7971422515715586\n",
      "validation AUC 0.8185919168896497\n",
      "validation AUC 0.8287577455123336\n",
      "validation AUC 0.8350841669605834\n",
      "validation AUC 0.8407315583318415\n",
      "validation AUC 0.844424250161184\n",
      "validation AUC 0.8465027246451342\n",
      "validation AUC 0.8491204316350447\n",
      "validation AUC 0.8512310684470109\n",
      "validation AUC 0.8530364185683271\n",
      "validation AUC 0.8566014242526089\n",
      "validation AUC 0.8568792127913687\n",
      "validation AUC 0.8572817474212133\n",
      "validation AUC 0.8584774075077645\n",
      "validation AUC 0.8594754587175979\n",
      "validation AUC 0.8624680132770176\n",
      "validation AUC 0.8625878291237437\n",
      "validation AUC 0.8621403103377301\n",
      "validation AUC 0.8635927581974028\n",
      "validation AUC 0.8639182820378614\n",
      "validation AUC 0.8639495024843322\n",
      "validation AUC 0.8631394843555072\n",
      "validation AUC 0.8642282621721427\n",
      "validation AUC 0.8641610533364635\n",
      "validation AUC 0.8657736822545046\n",
      "validation AUC 0.8651212536794576\n",
      "validation AUC 0.8637082264891998\n",
      "validation AUC 0.865150031619549\n",
      "validation AUC 0.865150031619549\n",
      "train auc 0.8983364793160541\n",
      "Test AUC 0.8710435287246507\n",
      "test false positive rates [0.         0.         0.         ... 0.97347229 0.97347229 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.76674156e+01 3.66674156e+01 3.39587135e+01 ... 3.54852788e-02\n",
      " 3.54829133e-02 1.07597641e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.492s | Train Loss: 0.189639 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.543s | Train Loss: 0.143135 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.689s | Train Loss: 0.194510 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.608s | Train Loss: 0.109684 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.688s | Train Loss: 0.147532 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.693s | Train Loss: 0.086388 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.650s | Train Loss: 0.112322 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.610s | Train Loss: 0.071063 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.728s | Train Loss: 0.088918 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.617s | Train Loss: 0.060432 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.689s | Train Loss: 0.073569 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.628s | Train Loss: 0.053148 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.604s | Train Loss: 0.047427 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.657s | Train Loss: 0.063095 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.625s | Train Loss: 0.043424 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.634s | Train Loss: 0.055987 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.606s | Train Loss: 0.040233 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.615s | Train Loss: 0.050513 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.603s | Train Loss: 0.037360 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.620s | Train Loss: 0.046221 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.613s | Train Loss: 0.035217 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.618s | Train Loss: 0.043062 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.632s | Train Loss: 0.033422 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.663s | Train Loss: 0.040061 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.643s | Train Loss: 0.031628 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.738s | Train Loss: 0.037664 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.639s | Train Loss: 0.030114 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.656s | Train Loss: 0.035409 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.592s | Train Loss: 0.029003 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.640s | Train Loss: 0.033868 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.615s | Train Loss: 0.027847 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.623s | Train Loss: 0.032254 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.683s | Train Loss: 0.026992 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.623s | Train Loss: 0.031173 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.640s | Train Loss: 0.025836 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.632s | Train Loss: 0.029637 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.607s | Train Loss: 0.025256 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.618s | Train Loss: 0.028445 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.675s | Train Loss: 0.024562 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.647s | Train Loss: 0.027656 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.653s | Train Loss: 0.024089 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.664s | Train Loss: 0.026874 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.652s | Train Loss: 0.023586 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.640s | Train Loss: 0.025804 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.663s | Train Loss: 0.023164 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.616s | Train Loss: 0.025285 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.638s | Train Loss: 0.022579 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.672s | Train Loss: 0.024698 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.606s | Train Loss: 0.022404 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.655s | Train Loss: 0.024104 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.651s | Train Loss: 0.022064 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.646s | Train Loss: 0.023452 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.610s | Train Loss: 0.021854 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.658s | Train Loss: 0.023216 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.619s | Train Loss: 0.021507 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.676s | Train Loss: 0.022657 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.603s | Train Loss: 0.021412 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.699s | Train Loss: 0.022240 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.669s | Train Loss: 0.021189 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.651s | Train Loss: 0.021773 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.695s | Train Loss: 0.020611 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.654s | Train Loss: 0.021489 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.578s | Train Loss: 0.020726 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.658s | Train Loss: 0.021541 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.597s | Train Loss: 0.020487 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.642s | Train Loss: 0.021165 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.633s | Train Loss: 0.020428 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.618s | Train Loss: 0.020546 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.649s | Train Loss: 0.020256 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.608s | Train Loss: 0.020631 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.645s | Train Loss: 0.019980 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.620s | Train Loss: 0.020262 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.627s | Train Loss: 0.019826 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.667s | Train Loss: 0.020287 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.655s | Train Loss: 0.019816 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.667s | Train Loss: 0.020056 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.603s | Train Loss: 0.019613 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.708s | Train Loss: 0.019749 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.606s | Train Loss: 0.019434 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.621s | Train Loss: 0.019601 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.609s | Train Loss: 0.019286 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.615s | Train Loss: 0.019550 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.605s | Train Loss: 0.019186 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.674s | Train Loss: 0.019460 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.657s | Train Loss: 0.019177 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.618s | Train Loss: 0.019360 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.610s | Train Loss: 0.018990 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.621s | Train Loss: 0.019295 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.607s | Train Loss: 0.018964 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.619s | Train Loss: 0.019105 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.610s | Train Loss: 0.018770 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.605s | Train Loss: 0.018850 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.680s | Train Loss: 0.018623 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.618s | Train Loss: 0.018917 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.608s | Train Loss: 0.018485 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.622s | Train Loss: 0.018664 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.612s | Train Loss: 0.018551 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.621s | Train Loss: 0.018628 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.607s | Train Loss: 0.018431 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.616s | Train Loss: 0.018522 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.608s | Train Loss: 0.017984 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.647s | Train Loss: 0.018158 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.676s | Train Loss: 0.018127 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.681s | Train Loss: 0.018371 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.648s | Train Loss: 0.018143 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.637s | Train Loss: 0.017870 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.729s | Train Loss: 0.018172 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.679s | Train Loss: 0.017811 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.646s | Train Loss: 0.018094 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.636s | Train Loss: 0.017632 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.649s | Train Loss: 0.017950 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.607s | Train Loss: 0.017804 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.621s | Train Loss: 0.017845 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.604s | Train Loss: 0.017476 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.616s | Train Loss: 0.017754 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.608s | Train Loss: 0.017459 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.619s | Train Loss: 0.017790 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.602s | Train Loss: 0.017360 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.610s | Train Loss: 0.017779 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.611s | Train Loss: 0.017134 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.623s | Train Loss: 0.017669 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.663s | Train Loss: 0.017114 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.672s | Train Loss: 0.017662 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.647s | Train Loss: 0.016901 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.661s | Train Loss: 0.017290 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.607s | Train Loss: 0.016884 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.662s | Train Loss: 0.017396 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.608s | Train Loss: 0.016842 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.686s | Train Loss: 0.017348 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.610s | Train Loss: 0.016872 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.648s | Train Loss: 0.017249 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.604s | Train Loss: 0.016347 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.612s | Train Loss: 0.017190 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.613s | Train Loss: 0.016338 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.614s | Train Loss: 0.016945 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.624s | Train Loss: 0.016535 |\n",
      "INFO:root:Pretraining Time: 43.726s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.618s | Train Loss: 0.016882 |\n",
      "INFO:root:Test Loss: 0.017893\n",
      "INFO:root:Test AUC: 50.93%\n",
      "INFO:root:Test AUC: 50.93%\n",
      "INFO:root:Test Time: 0.329s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.553s | Train Loss: 0.016912 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.559s | Train Loss: 0.016821 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.949s | Train Loss: 0.769975 || Validation Loss: 0.015871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.570s | Train Loss: 0.016787 |\n",
      "INFO:root:Pretraining Time: 45.005s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018702\n",
      "INFO:root:Test AUC: 50.41%\n",
      "INFO:root:Test AUC: 50.41%\n",
      "INFO:root:Test Time: 0.296s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.915s | Train Loss: 0.627390 || Validation Loss: 0.016503 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.961s | Train Loss: 0.579967 || Validation Loss: 0.015909 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.901s | Train Loss: 0.754180 || Validation Loss: 0.017014 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.938s | Train Loss: 0.627788 || Validation Loss: 0.016768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.028s | Train Loss: 0.539910 || Validation Loss: 0.018492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.045s | Train Loss: 0.582013 || Validation Loss: 0.016513 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.036s | Train Loss: 0.514654 || Validation Loss: 0.017811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.029s | Train Loss: 0.533207 || Validation Loss: 0.018922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.051s | Train Loss: 0.499843 || Validation Loss: 0.019427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.011s | Train Loss: 0.509941 || Validation Loss: 0.018554 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.039s | Train Loss: 0.486313 || Validation Loss: 0.019273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.059s | Train Loss: 0.498751 || Validation Loss: 0.018450 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.116s | Train Loss: 0.472795 || Validation Loss: 0.019265 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.012s | Train Loss: 0.484564 || Validation Loss: 0.020593 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.076s | Train Loss: 0.464480 || Validation Loss: 0.021394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.091s | Train Loss: 0.473622 || Validation Loss: 0.019283 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.120s | Train Loss: 0.456361 || Validation Loss: 0.020397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.066s | Train Loss: 0.464657 || Validation Loss: 0.020427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.129s | Train Loss: 0.448964 || Validation Loss: 0.020517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.112s | Train Loss: 0.455137 || Validation Loss: 0.021701 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.107s | Train Loss: 0.441349 || Validation Loss: 0.021004 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.152s | Train Loss: 0.444826 || Validation Loss: 0.022091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.208s | Train Loss: 0.443782 || Validation Loss: 0.020506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.113s | Train Loss: 0.437130 || Validation Loss: 0.021794 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.185s | Train Loss: 0.427489 || Validation Loss: 0.020069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.239s | Train Loss: 0.439314 || Validation Loss: 0.020644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.148s | Train Loss: 0.425558 || Validation Loss: 0.020506 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.183s | Train Loss: 0.435638 || Validation Loss: 0.022254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.226s | Train Loss: 0.417642 || Validation Loss: 0.021630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.253s | Train Loss: 0.431380 || Validation Loss: 0.022682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.280s | Train Loss: 0.408736 || Validation Loss: 0.023611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.211s | Train Loss: 0.425578 || Validation Loss: 0.023064 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.307s | Train Loss: 0.406550 || Validation Loss: 0.020893 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.306s | Train Loss: 0.414177 || Validation Loss: 0.021517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.241s | Train Loss: 0.403086 || Validation Loss: 0.021580 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.302s | Train Loss: 0.409220 || Validation Loss: 0.023920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.305s | Train Loss: 0.396977 || Validation Loss: 0.022540 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.255s | Train Loss: 0.404862 || Validation Loss: 0.021654 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.478s | Train Loss: 0.404503 || Validation Loss: 0.021160 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.295s | Train Loss: 0.408918 || Validation Loss: 0.021371 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.270s | Train Loss: 0.392059 || Validation Loss: 0.021352 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.462s | Train Loss: 0.409028 || Validation Loss: 0.023609 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.390s | Train Loss: 0.386663 || Validation Loss: 0.024205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.291s | Train Loss: 0.397689 || Validation Loss: 0.023855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.430s | Train Loss: 0.386454 || Validation Loss: 0.020852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.381s | Train Loss: 0.396288 || Validation Loss: 0.022346 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.415s | Train Loss: 0.386515 || Validation Loss: 0.024739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.457s | Train Loss: 0.389312 || Validation Loss: 0.023165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.395s | Train Loss: 0.380490 || Validation Loss: 0.023032 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.458s | Train Loss: 0.390289 || Validation Loss: 0.022659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.360s | Train Loss: 0.382228 || Validation Loss: 0.022019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.468s | Train Loss: 0.389291 || Validation Loss: 0.022542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.572s | Train Loss: 0.371273 || Validation Loss: 0.022417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.372s | Train Loss: 0.383117 || Validation Loss: 0.024920 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.466s | Train Loss: 0.368392 || Validation Loss: 0.023214 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.614s | Train Loss: 0.387226 || Validation Loss: 0.023385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.469s | Train Loss: 0.375013 || Validation Loss: 0.023525 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.479s | Train Loss: 0.381198 || Validation Loss: 0.025071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 37.727s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.902%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020439\n",
      "INFO:root:Test AUC: 85.08%\n",
      "INFO:root:Test Time: 0.237s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7006000716681393\n",
      "validation AUC 0.7631487674761598\n",
      "validation AUC 0.7958201798067834\n",
      "validation AUC 0.8153749630431224\n",
      "validation AUC 0.8246637482883832\n",
      "validation AUC 0.8328363719744581\n",
      "validation AUC 0.8373150080788959\n",
      "validation AUC 0.8408963264065484\n",
      "validation AUC 0.8435520385828738\n",
      "validation AUC 0.8452382805989643\n",
      "validation AUC 0.8481584832237593\n",
      "validation AUC 0.8488129073105547\n",
      "validation AUC 0.8495573981549338\n",
      "validation AUC 0.8501205315702729\n",
      "validation AUC 0.8510895706999703\n",
      "validation AUC 0.8525769426759184\n",
      "validation AUC 0.8523606025955914\n",
      "validation AUC 0.851601693513661\n",
      "validation AUC 0.8513353538909711\n",
      "validation AUC 0.8526968888960786\n",
      "validation AUC 0.8514219564401603\n",
      "validation AUC 0.8522348800340908\n",
      "validation AUC 0.8518120497194257\n",
      "validation AUC 0.8539157980248372\n",
      "validation AUC 0.8513467256472533\n",
      "validation AUC 0.8532353258580222\n",
      "validation AUC 0.851232364199306\n",
      "validation AUC 0.8492553441933163\n",
      "validation AUC 0.851570361318532\n",
      "validation AUC 0.8514678877992391\n",
      "validation AUC 0.8514678877992391\n",
      "train auc 0.9018442198389547\n",
      "Test AUC 0.8508054328550444\n",
      "test false positive rates [0.         0.         0.         ... 0.99412601 0.99412601 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.14781635e-02 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.84477196e+01 3.74477196e+01 2.73063793e+01 ... 1.83585156e-02\n",
      " 1.83578338e-02 6.53031608e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 030/030 | Train Time: 1.554s | Train Loss: 0.382473 || Validation Loss: 0.024772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:Training Time: 38.067s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.902%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021615\n",
      "INFO:root:Test AUC: 85.03%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7313710570550334\n",
      "validation AUC 0.7831081713172912\n",
      "validation AUC 0.8096056086544973\n",
      "validation AUC 0.8237619658866469\n",
      "validation AUC 0.8323752092094517\n",
      "validation AUC 0.8363317023705402\n",
      "validation AUC 0.839994727591893\n",
      "validation AUC 0.8423798669999403\n",
      "validation AUC 0.8443425618922603\n",
      "validation AUC 0.8439732166138113\n",
      "validation AUC 0.844768058210621\n",
      "validation AUC 0.8460009359216167\n",
      "validation AUC 0.8458287019722894\n",
      "validation AUC 0.8462268517976527\n",
      "validation AUC 0.8453408818310093\n",
      "validation AUC 0.845926500672727\n",
      "validation AUC 0.8468169831566037\n",
      "validation AUC 0.8471490309901377\n",
      "validation AUC 0.8482500798736835\n",
      "validation AUC 0.8472048175165702\n",
      "validation AUC 0.8451851680582745\n",
      "validation AUC 0.8463166312015449\n",
      "validation AUC 0.8473531638599306\n",
      "validation AUC 0.8471333010361974\n",
      "validation AUC 0.8474753184464255\n",
      "validation AUC 0.8453380987572912\n",
      "validation AUC 0.8456168451416897\n",
      "validation AUC 0.8457167298170749\n",
      "validation AUC 0.8455005014854057\n",
      "validation AUC 0.8451912876276357\n",
      "validation AUC 0.8451912876276357\n",
      "train auc 0.9018619026699437\n",
      "Test AUC 0.8502825173741977\n",
      "test false positive rates [0.         0.         0.         ... 0.98908574 0.98908574 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.88784103e+01 7.78784103e+01 6.38547020e+01 ... 6.95494860e-02\n",
      " 6.94976076e-02 1.11468248e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.567s | Train Loss: 0.182306 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.449s | Train Loss: 0.136020 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.459s | Train Loss: 0.104764 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.468s | Train Loss: 0.084122 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.607s | Train Loss: 0.070649 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.688s | Train Loss: 0.186331 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.611s | Train Loss: 0.060859 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.611s | Train Loss: 0.139582 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.633s | Train Loss: 0.054187 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.643s | Train Loss: 0.106874 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.675s | Train Loss: 0.048550 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.622s | Train Loss: 0.085065 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.644s | Train Loss: 0.044399 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.638s | Train Loss: 0.070840 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.602s | Train Loss: 0.041100 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.600s | Train Loss: 0.060682 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.603s | Train Loss: 0.038227 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.611s | Train Loss: 0.053670 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.650s | Train Loss: 0.036221 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.607s | Train Loss: 0.048558 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.624s | Train Loss: 0.033977 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.612s | Train Loss: 0.044630 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.651s | Train Loss: 0.032449 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.654s | Train Loss: 0.041530 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.626s | Train Loss: 0.030911 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.660s | Train Loss: 0.038858 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.590s | Train Loss: 0.029743 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.721s | Train Loss: 0.036575 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.639s | Train Loss: 0.028405 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.662s | Train Loss: 0.034342 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.631s | Train Loss: 0.027580 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.653s | Train Loss: 0.032674 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.602s | Train Loss: 0.026757 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.646s | Train Loss: 0.030969 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.727s | Train Loss: 0.026072 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.617s | Train Loss: 0.029999 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.655s | Train Loss: 0.025294 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.632s | Train Loss: 0.028628 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.634s | Train Loss: 0.024665 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.661s | Train Loss: 0.027825 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.631s | Train Loss: 0.024231 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.644s | Train Loss: 0.026795 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.625s | Train Loss: 0.023784 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.732s | Train Loss: 0.025936 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.596s | Train Loss: 0.023030 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.592s | Train Loss: 0.025584 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.601s | Train Loss: 0.022810 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.588s | Train Loss: 0.025064 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.634s | Train Loss: 0.022308 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.613s | Train Loss: 0.024237 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.619s | Train Loss: 0.021947 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.670s | Train Loss: 0.023992 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.674s | Train Loss: 0.021554 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.629s | Train Loss: 0.023605 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.635s | Train Loss: 0.021156 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.661s | Train Loss: 0.023291 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.653s | Train Loss: 0.020764 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.735s | Train Loss: 0.022858 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.600s | Train Loss: 0.020687 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.705s | Train Loss: 0.022694 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.658s | Train Loss: 0.020473 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.626s | Train Loss: 0.022171 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.729s | Train Loss: 0.020434 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.658s | Train Loss: 0.021723 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.669s | Train Loss: 0.020005 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.613s | Train Loss: 0.021714 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.601s | Train Loss: 0.019689 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.614s | Train Loss: 0.021519 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.607s | Train Loss: 0.019614 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.652s | Train Loss: 0.021429 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.608s | Train Loss: 0.019660 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.644s | Train Loss: 0.020960 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.710s | Train Loss: 0.019301 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.672s | Train Loss: 0.020943 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.614s | Train Loss: 0.019364 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.646s | Train Loss: 0.020529 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.621s | Train Loss: 0.019174 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.646s | Train Loss: 0.020328 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.615s | Train Loss: 0.019000 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.665s | Train Loss: 0.020090 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.644s | Train Loss: 0.018842 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.613s | Train Loss: 0.020071 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.590s | Train Loss: 0.018678 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.567s | Train Loss: 0.020090 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.575s | Train Loss: 0.018360 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.616s | Train Loss: 0.019679 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.650s | Train Loss: 0.018463 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.601s | Train Loss: 0.019806 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.609s | Train Loss: 0.018347 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.645s | Train Loss: 0.019457 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.674s | Train Loss: 0.018370 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.623s | Train Loss: 0.019319 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.610s | Train Loss: 0.018041 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.639s | Train Loss: 0.019240 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.649s | Train Loss: 0.017973 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.629s | Train Loss: 0.019156 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.669s | Train Loss: 0.017686 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.603s | Train Loss: 0.019049 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.686s | Train Loss: 0.017999 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.615s | Train Loss: 0.018692 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.591s | Train Loss: 0.017846 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.591s | Train Loss: 0.018757 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.607s | Train Loss: 0.017827 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.622s | Train Loss: 0.018934 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.666s | Train Loss: 0.017695 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.614s | Train Loss: 0.018456 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.611s | Train Loss: 0.017699 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.615s | Train Loss: 0.018410 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.613s | Train Loss: 0.017619 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.717s | Train Loss: 0.018390 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.629s | Train Loss: 0.017308 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.642s | Train Loss: 0.018177 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.629s | Train Loss: 0.017216 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.645s | Train Loss: 0.018351 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.606s | Train Loss: 0.017326 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.611s | Train Loss: 0.018088 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.608s | Train Loss: 0.017148 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.655s | Train Loss: 0.017830 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.634s | Train Loss: 0.017036 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.638s | Train Loss: 0.017743 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.631s | Train Loss: 0.017056 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.602s | Train Loss: 0.017866 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.757s | Train Loss: 0.017099 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.628s | Train Loss: 0.017725 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.618s | Train Loss: 0.017164 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.642s | Train Loss: 0.017593 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.671s | Train Loss: 0.017038 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.611s | Train Loss: 0.017475 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.644s | Train Loss: 0.017000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.615s | Train Loss: 0.017442 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.607s | Train Loss: 0.016816 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.607s | Train Loss: 0.017388 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.612s | Train Loss: 0.016797 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.635s | Train Loss: 0.017290 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.634s | Train Loss: 0.016773 |\n",
      "INFO:root:Pretraining Time: 43.786s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.622s | Train Loss: 0.017299 |\n",
      "INFO:root:Test Loss: 0.018047\n",
      "INFO:root:Test AUC: 52.37%\n",
      "INFO:root:Test AUC: 52.37%\n",
      "INFO:root:Test Time: 0.306s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.598s | Train Loss: 0.017068 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.581s | Train Loss: 0.016949 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 1.021s | Train Loss: 0.769516 || Validation Loss: 0.013484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.576s | Train Loss: 0.016835 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.976s | Train Loss: 0.640821 || Validation Loss: 0.013964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.578s | Train Loss: 0.017012 |\n",
      "INFO:root:Pretraining Time: 44.349s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018436\n",
      "INFO:root:Test AUC: 49.66%\n",
      "INFO:root:Test AUC: 49.66%\n",
      "INFO:root:Test Time: 0.292s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.924s | Train Loss: 0.587558 || Validation Loss: 0.015230 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.878s | Train Loss: 0.881786 || Validation Loss: 0.013231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.009s | Train Loss: 0.545120 || Validation Loss: 0.016697 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.936s | Train Loss: 0.670374 || Validation Loss: 0.014331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.066s | Train Loss: 0.519185 || Validation Loss: 0.016484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.011s | Train Loss: 0.592777 || Validation Loss: 0.016401 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.078s | Train Loss: 0.499141 || Validation Loss: 0.018424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.023s | Train Loss: 0.545865 || Validation Loss: 0.016829 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.015s | Train Loss: 0.485987 || Validation Loss: 0.019179 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.982s | Train Loss: 0.515126 || Validation Loss: 0.018805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.115s | Train Loss: 0.470249 || Validation Loss: 0.018688 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.011s | Train Loss: 0.489729 || Validation Loss: 0.018629 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.141s | Train Loss: 0.455917 || Validation Loss: 0.020533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.010s | Train Loss: 0.475735 || Validation Loss: 0.020019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.133s | Train Loss: 0.447101 || Validation Loss: 0.018782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.124s | Train Loss: 0.452406 || Validation Loss: 0.022644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.167s | Train Loss: 0.443497 || Validation Loss: 0.020522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.129s | Train Loss: 0.434727 || Validation Loss: 0.023497 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.133s | Train Loss: 0.438035 || Validation Loss: 0.019365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.184s | Train Loss: 0.432006 || Validation Loss: 0.022070 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.221s | Train Loss: 0.436170 || Validation Loss: 0.019600 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.160s | Train Loss: 0.421000 || Validation Loss: 0.022566 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.138s | Train Loss: 0.417977 || Validation Loss: 0.023128 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.221s | Train Loss: 0.428193 || Validation Loss: 0.020098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.243s | Train Loss: 0.406623 || Validation Loss: 0.024857 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.244s | Train Loss: 0.426136 || Validation Loss: 0.020207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.240s | Train Loss: 0.401578 || Validation Loss: 0.024845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.314s | Train Loss: 0.420568 || Validation Loss: 0.020834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.218s | Train Loss: 0.392683 || Validation Loss: 0.023563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.287s | Train Loss: 0.414300 || Validation Loss: 0.020718 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.214s | Train Loss: 0.384498 || Validation Loss: 0.024354 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.323s | Train Loss: 0.408870 || Validation Loss: 0.023352 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.262s | Train Loss: 0.380652 || Validation Loss: 0.023832 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.252s | Train Loss: 0.401705 || Validation Loss: 0.023695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.267s | Train Loss: 0.382386 || Validation Loss: 0.026492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.312s | Train Loss: 0.400665 || Validation Loss: 0.023391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.298s | Train Loss: 0.374856 || Validation Loss: 0.026541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.445s | Train Loss: 0.397893 || Validation Loss: 0.021554 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.312s | Train Loss: 0.380738 || Validation Loss: 0.025165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.301s | Train Loss: 0.398784 || Validation Loss: 0.020956 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.389s | Train Loss: 0.370511 || Validation Loss: 0.024975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.389s | Train Loss: 0.399509 || Validation Loss: 0.022867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.309s | Train Loss: 0.366535 || Validation Loss: 0.023874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.435s | Train Loss: 0.392881 || Validation Loss: 0.021900 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.439s | Train Loss: 0.368927 || Validation Loss: 0.025141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.489s | Train Loss: 0.395378 || Validation Loss: 0.021555 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.398s | Train Loss: 0.362108 || Validation Loss: 0.026887 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.419s | Train Loss: 0.389092 || Validation Loss: 0.020789 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.433s | Train Loss: 0.360432 || Validation Loss: 0.024704 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.354s | Train Loss: 0.385967 || Validation Loss: 0.021567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.402s | Train Loss: 0.360642 || Validation Loss: 0.025280 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.574s | Train Loss: 0.385942 || Validation Loss: 0.020728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.400s | Train Loss: 0.350760 || Validation Loss: 0.024637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.476s | Train Loss: 0.379818 || Validation Loss: 0.021769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.613s | Train Loss: 0.350877 || Validation Loss: 0.023766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.527s | Train Loss: 0.384902 || Validation Loss: 0.024781 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.411s | Train Loss: 0.353270 || Validation Loss: 0.026083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.464s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.903%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019787\n",
      "INFO:root:Test AUC: 86.79%\n",
      "INFO:root:Test Time: 0.264s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6685109656297313\n",
      "validation AUC 0.7551865026565318\n",
      "validation AUC 0.7956868077835815\n",
      "validation AUC 0.8180010032900934\n",
      "validation AUC 0.8305501061825105\n",
      "validation AUC 0.8386683056115067\n",
      "validation AUC 0.844654803606523\n",
      "validation AUC 0.848383608877143\n",
      "validation AUC 0.8512210882275876\n",
      "validation AUC 0.854485644341697\n",
      "validation AUC 0.8566851585862454\n",
      "validation AUC 0.8572315669524523\n",
      "validation AUC 0.858062149495013\n",
      "validation AUC 0.859889083071398\n",
      "validation AUC 0.8597798860078548\n",
      "validation AUC 0.8621342147145099\n",
      "validation AUC 0.860713099049966\n",
      "validation AUC 0.8626823817917374\n",
      "validation AUC 0.8624609332013352\n",
      "validation AUC 0.8611535138141562\n",
      "validation AUC 0.8628500872597377\n",
      "validation AUC 0.8632098806886185\n",
      "validation AUC 0.8638595102258536\n",
      "validation AUC 0.8641558171136364\n",
      "validation AUC 0.8635863087034326\n",
      "validation AUC 0.8629023537034464\n",
      "validation AUC 0.8644301574080953\n",
      "validation AUC 0.8654406419864612\n",
      "validation AUC 0.8635926437880624\n",
      "validation AUC 0.8640025644720579\n",
      "validation AUC 0.8640025644720579\n",
      "train auc 0.9025154156672169\n",
      "Test AUC 0.867939574765148\n",
      "test false positive rates [0.        0.        0.        ... 0.9942018 0.9942018 1.       ] true positive rate [0.00000000e+00 2.79955207e-04 4.47928331e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.78758011e+01 5.68758011e+01 4.66464157e+01 ... 3.56597155e-02\n",
      " 3.56094576e-02 5.27647045e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 030/030 | Train Time: 1.501s | Train Loss: 0.349882 || Validation Loss: 0.027428 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:Training Time: 37.896s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.912%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022388\n",
      "INFO:root:Test AUC: 86.84%\n",
      "INFO:root:Test Time: 0.207s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6512787372316515\n",
      "validation AUC 0.7450857649663913\n",
      "validation AUC 0.7865043274401703\n",
      "validation AUC 0.8084164965071627\n",
      "validation AUC 0.8216403351352892\n",
      "validation AUC 0.8300166579999374\n",
      "validation AUC 0.8384598012406443\n",
      "validation AUC 0.8441335972233545\n",
      "validation AUC 0.849169319012193\n",
      "validation AUC 0.8529284720254796\n",
      "validation AUC 0.8553892439894654\n",
      "validation AUC 0.8584157302306513\n",
      "validation AUC 0.8602772792841616\n",
      "validation AUC 0.8618666777847712\n",
      "validation AUC 0.8634253267796825\n",
      "validation AUC 0.8640946320627548\n",
      "validation AUC 0.8643829090112629\n",
      "validation AUC 0.8650948490680056\n",
      "validation AUC 0.8662579237780498\n",
      "validation AUC 0.8657482381493742\n",
      "validation AUC 0.8663215912455462\n",
      "validation AUC 0.8670720633038054\n",
      "validation AUC 0.8654652213698342\n",
      "validation AUC 0.8662527567329631\n",
      "validation AUC 0.8663868098908419\n",
      "validation AUC 0.866525176014779\n",
      "validation AUC 0.866153941651875\n",
      "validation AUC 0.8683287488002983\n",
      "validation AUC 0.8665872816617474\n",
      "validation AUC 0.8656487392729271\n",
      "validation AUC 0.8656487392729271\n",
      "train auc 0.9117019510412493\n",
      "Test AUC 0.8683782092733472\n",
      "test false positive rates [0.       0.       0.       ... 0.993027 0.993027 1.      ] true positive rate [0.00000000e+00 2.79955207e-04 3.35946249e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.14657974e+01 6.04657974e+01 4.12473831e+01 ... 2.64406689e-02\n",
      " 2.63941027e-02 4.49860841e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.553s | Train Loss: 0.187190 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.453s | Train Loss: 0.140758 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.467s | Train Loss: 0.107669 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.469s | Train Loss: 0.086194 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.578s | Train Loss: 0.071939 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.633s | Train Loss: 0.198142 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.636s | Train Loss: 0.062196 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.616s | Train Loss: 0.150805 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.644s | Train Loss: 0.054891 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.649s | Train Loss: 0.116526 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.702s | Train Loss: 0.049149 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.683s | Train Loss: 0.092267 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.643s | Train Loss: 0.044874 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.636s | Train Loss: 0.075371 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.594s | Train Loss: 0.041231 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.592s | Train Loss: 0.064316 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.680s | Train Loss: 0.038347 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.694s | Train Loss: 0.056433 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.676s | Train Loss: 0.035790 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.609s | Train Loss: 0.050638 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.608s | Train Loss: 0.033708 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.649s | Train Loss: 0.046106 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.611s | Train Loss: 0.032116 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.709s | Train Loss: 0.042685 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.603s | Train Loss: 0.030332 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.733s | Train Loss: 0.039689 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.615s | Train Loss: 0.029274 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.656s | Train Loss: 0.037255 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.614s | Train Loss: 0.028091 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.612s | Train Loss: 0.035089 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.604s | Train Loss: 0.026933 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.648s | Train Loss: 0.033105 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.640s | Train Loss: 0.026076 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.609s | Train Loss: 0.031671 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.608s | Train Loss: 0.025427 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.644s | Train Loss: 0.030013 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.601s | Train Loss: 0.024690 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.675s | Train Loss: 0.028936 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.634s | Train Loss: 0.023998 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.615s | Train Loss: 0.027875 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.624s | Train Loss: 0.023332 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.634s | Train Loss: 0.027340 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.628s | Train Loss: 0.022981 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.649s | Train Loss: 0.026180 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.660s | Train Loss: 0.022404 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.606s | Train Loss: 0.025468 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.652s | Train Loss: 0.022071 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.610s | Train Loss: 0.025047 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.615s | Train Loss: 0.021508 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.630s | Train Loss: 0.024601 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.676s | Train Loss: 0.021313 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.623s | Train Loss: 0.024013 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.642s | Train Loss: 0.021015 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.643s | Train Loss: 0.023714 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.639s | Train Loss: 0.020588 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.648s | Train Loss: 0.023240 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.618s | Train Loss: 0.020538 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.606s | Train Loss: 0.023088 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.679s | Train Loss: 0.020182 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.664s | Train Loss: 0.022571 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.637s | Train Loss: 0.019987 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.639s | Train Loss: 0.022505 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.629s | Train Loss: 0.019824 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.659s | Train Loss: 0.022013 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.599s | Train Loss: 0.019459 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.630s | Train Loss: 0.021917 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.625s | Train Loss: 0.019372 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.611s | Train Loss: 0.021450 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.648s | Train Loss: 0.019177 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.678s | Train Loss: 0.021259 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.627s | Train Loss: 0.019146 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.688s | Train Loss: 0.021183 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.609s | Train Loss: 0.018856 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.663s | Train Loss: 0.020808 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.619s | Train Loss: 0.018735 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.610s | Train Loss: 0.020425 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.685s | Train Loss: 0.018758 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.595s | Train Loss: 0.020227 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.591s | Train Loss: 0.018525 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.635s | Train Loss: 0.020248 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.641s | Train Loss: 0.018369 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.615s | Train Loss: 0.019873 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.615s | Train Loss: 0.018482 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.607s | Train Loss: 0.019648 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.654s | Train Loss: 0.018261 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.621s | Train Loss: 0.019374 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.614s | Train Loss: 0.018063 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.631s | Train Loss: 0.019275 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.647s | Train Loss: 0.018160 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.653s | Train Loss: 0.019154 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.712s | Train Loss: 0.017967 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.624s | Train Loss: 0.019129 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.609s | Train Loss: 0.017806 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.732s | Train Loss: 0.018799 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.630s | Train Loss: 0.017677 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.619s | Train Loss: 0.018544 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.668s | Train Loss: 0.017665 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.607s | Train Loss: 0.018574 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.616s | Train Loss: 0.017669 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.615s | Train Loss: 0.018351 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.610s | Train Loss: 0.017450 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.656s | Train Loss: 0.018328 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.611s | Train Loss: 0.017204 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.615s | Train Loss: 0.018169 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.616s | Train Loss: 0.017342 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.628s | Train Loss: 0.018133 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.663s | Train Loss: 0.017466 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.657s | Train Loss: 0.017968 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.640s | Train Loss: 0.017169 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.673s | Train Loss: 0.017793 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.651s | Train Loss: 0.017153 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.646s | Train Loss: 0.017659 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.680s | Train Loss: 0.017038 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.609s | Train Loss: 0.017673 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.638s | Train Loss: 0.016949 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.613s | Train Loss: 0.017535 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.641s | Train Loss: 0.016895 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.637s | Train Loss: 0.017359 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.632s | Train Loss: 0.016886 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.617s | Train Loss: 0.017191 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.709s | Train Loss: 0.016914 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.648s | Train Loss: 0.017039 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.678s | Train Loss: 0.016738 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.623s | Train Loss: 0.016819 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.682s | Train Loss: 0.016821 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.608s | Train Loss: 0.017027 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.607s | Train Loss: 0.016675 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.606s | Train Loss: 0.016806 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.607s | Train Loss: 0.016386 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.651s | Train Loss: 0.016798 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.615s | Train Loss: 0.016602 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.658s | Train Loss: 0.016668 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.610s | Train Loss: 0.016486 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.618s | Train Loss: 0.016619 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.618s | Train Loss: 0.016533 |\n",
      "INFO:root:Pretraining Time: 43.856s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.569s | Train Loss: 0.016586 |\n",
      "INFO:root:Test Loss: 0.018386\n",
      "INFO:root:Test AUC: 50.80%\n",
      "INFO:root:Test AUC: 50.80%\n",
      "INFO:root:Test Time: 0.309s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.535s | Train Loss: 0.016287 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.577s | Train Loss: 0.016315 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.988s | Train Loss: 0.827582 || Validation Loss: 0.013788 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.558s | Train Loss: 0.016138 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 1.000s | Train Loss: 0.672314 || Validation Loss: 0.014447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.574s | Train Loss: 0.015873 |\n",
      "INFO:root:Pretraining Time: 44.304s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017383\n",
      "INFO:root:Test AUC: 48.93%\n",
      "INFO:root:Test AUC: 48.93%\n",
      "INFO:root:Test Time: 0.289s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.952s | Train Loss: 0.607357 || Validation Loss: 0.016116 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.966s | Train Loss: 0.716175 || Validation Loss: 0.016891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.018s | Train Loss: 0.567340 || Validation Loss: 0.016196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.922s | Train Loss: 0.603929 || Validation Loss: 0.016384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.946s | Train Loss: 0.537191 || Validation Loss: 0.017683 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.897s | Train Loss: 0.554335 || Validation Loss: 0.015573 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.037s | Train Loss: 0.507885 || Validation Loss: 0.018318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.001s | Train Loss: 0.530741 || Validation Loss: 0.016933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.005s | Train Loss: 0.496192 || Validation Loss: 0.018067 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.945s | Train Loss: 0.517826 || Validation Loss: 0.016806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.095s | Train Loss: 0.487426 || Validation Loss: 0.019856 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.021s | Train Loss: 0.494850 || Validation Loss: 0.017777 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.113s | Train Loss: 0.469517 || Validation Loss: 0.018755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.009s | Train Loss: 0.490165 || Validation Loss: 0.018233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.139s | Train Loss: 0.463116 || Validation Loss: 0.018625 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.102s | Train Loss: 0.471185 || Validation Loss: 0.018281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.125s | Train Loss: 0.460039 || Validation Loss: 0.018993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.251s | Train Loss: 0.449087 || Validation Loss: 0.020604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.223s | Train Loss: 0.461443 || Validation Loss: 0.018907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.172s | Train Loss: 0.449829 || Validation Loss: 0.020524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.161s | Train Loss: 0.444069 || Validation Loss: 0.020313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.286s | Train Loss: 0.434389 || Validation Loss: 0.020727 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.106s | Train Loss: 0.442939 || Validation Loss: 0.019766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.239s | Train Loss: 0.434434 || Validation Loss: 0.023099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.184s | Train Loss: 0.438137 || Validation Loss: 0.022947 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.242s | Train Loss: 0.427334 || Validation Loss: 0.023429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.158s | Train Loss: 0.434182 || Validation Loss: 0.020448 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.226s | Train Loss: 0.416465 || Validation Loss: 0.020901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.171s | Train Loss: 0.427647 || Validation Loss: 0.019097 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.337s | Train Loss: 0.413778 || Validation Loss: 0.024277 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.187s | Train Loss: 0.417346 || Validation Loss: 0.020617 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.313s | Train Loss: 0.406597 || Validation Loss: 0.022570 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.232s | Train Loss: 0.416007 || Validation Loss: 0.019785 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.281s | Train Loss: 0.395555 || Validation Loss: 0.023488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.245s | Train Loss: 0.419498 || Validation Loss: 0.018964 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.345s | Train Loss: 0.396138 || Validation Loss: 0.022656 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.231s | Train Loss: 0.411754 || Validation Loss: 0.020840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.397s | Train Loss: 0.392104 || Validation Loss: 0.022611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.290s | Train Loss: 0.394824 || Validation Loss: 0.021885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.344s | Train Loss: 0.385142 || Validation Loss: 0.023433 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.436s | Train Loss: 0.401134 || Validation Loss: 0.021604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.425s | Train Loss: 0.386006 || Validation Loss: 0.023730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.293s | Train Loss: 0.388337 || Validation Loss: 0.021169 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.484s | Train Loss: 0.388402 || Validation Loss: 0.022702 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.447s | Train Loss: 0.384777 || Validation Loss: 0.022715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.482s | Train Loss: 0.382009 || Validation Loss: 0.023305 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.455s | Train Loss: 0.389033 || Validation Loss: 0.021383 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.471s | Train Loss: 0.384543 || Validation Loss: 0.023643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.484s | Train Loss: 0.391169 || Validation Loss: 0.023735 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.409s | Train Loss: 0.376136 || Validation Loss: 0.025001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.444s | Train Loss: 0.390459 || Validation Loss: 0.022988 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.407s | Train Loss: 0.375841 || Validation Loss: 0.021977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.626s | Train Loss: 0.378704 || Validation Loss: 0.024712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.657s | Train Loss: 0.378118 || Validation Loss: 0.020122 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.647s | Train Loss: 0.381022 || Validation Loss: 0.023208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.535s | Train Loss: 0.374553 || Validation Loss: 0.021381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.589s | Train Loss: 0.376889 || Validation Loss: 0.024121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.934s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.901%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020365\n",
      "INFO:root:Test AUC: 85.89%\n",
      "INFO:root:Test Time: 0.230s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6151884545330897\n",
      "validation AUC 0.7041879485777376\n",
      "validation AUC 0.7548786909783499\n",
      "validation AUC 0.7857916663385159\n",
      "validation AUC 0.8060120857769927\n",
      "validation AUC 0.8180688905997666\n",
      "validation AUC 0.8285574812746499\n",
      "validation AUC 0.8341866842214938\n",
      "validation AUC 0.8406370455740827\n",
      "validation AUC 0.8442422355436082\n",
      "validation AUC 0.847571427613583\n",
      "validation AUC 0.8514741749915868\n",
      "validation AUC 0.8526480281257537\n",
      "validation AUC 0.855340412486646\n",
      "validation AUC 0.8569508064315292\n",
      "validation AUC 0.8559419927616668\n",
      "validation AUC 0.8555401073383109\n",
      "validation AUC 0.8581011817048076\n",
      "validation AUC 0.8559251293570537\n",
      "validation AUC 0.8578345254609206\n",
      "validation AUC 0.8562120600003023\n",
      "validation AUC 0.8567998339947079\n",
      "validation AUC 0.8570465297997486\n",
      "validation AUC 0.8540548878543682\n",
      "validation AUC 0.8558945421529773\n",
      "validation AUC 0.8552403016532628\n",
      "validation AUC 0.8560603824826637\n",
      "validation AUC 0.8545138422530402\n",
      "validation AUC 0.8533910130239336\n",
      "validation AUC 0.855250215355628\n",
      "validation AUC 0.855250215355628\n",
      "train auc 0.9008475428060982\n",
      "Test AUC 0.8589286428524824\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.99450497e-01\n",
      " 9.99450497e-01 1.00000000e+00] true positive rate [0.         0.         0.00979843 ... 0.99972004 1.         1.        ] tresholds [4.84721603e+01 4.74721603e+01 3.44686623e+01 ... 2.74798274e-02\n",
      " 2.74381712e-02 2.21619066e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 030/030 | Train Time: 1.433s | Train Loss: 0.371064 || Validation Loss: 0.022031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 37.885s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.906%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.019802\n",
      "INFO:root:Test AUC: 85.22%\n",
      "INFO:root:Test Time: 0.204s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7406390064969606\n",
      "validation AUC 0.7906724087029431\n",
      "validation AUC 0.8132729335384968\n",
      "validation AUC 0.8227560443784783\n",
      "validation AUC 0.8320394204565603\n",
      "validation AUC 0.8358243474995867\n",
      "validation AUC 0.8403110694176279\n",
      "validation AUC 0.8436406339831286\n",
      "validation AUC 0.8452554419999987\n",
      "validation AUC 0.845788264922224\n",
      "validation AUC 0.8472246795101704\n",
      "validation AUC 0.8479572797652128\n",
      "validation AUC 0.8486340722083641\n",
      "validation AUC 0.8502104094194113\n",
      "validation AUC 0.8502170584645565\n",
      "validation AUC 0.8500200056704462\n",
      "validation AUC 0.8511165047872061\n",
      "validation AUC 0.8491978282233688\n",
      "validation AUC 0.8506045948281018\n",
      "validation AUC 0.8512592557156246\n",
      "validation AUC 0.8497352913755791\n",
      "validation AUC 0.8506719686260853\n",
      "validation AUC 0.852637337504148\n",
      "validation AUC 0.8505631334153392\n",
      "validation AUC 0.8469023538098736\n",
      "validation AUC 0.8501153698465511\n",
      "validation AUC 0.8490648765879751\n",
      "validation AUC 0.8494245901963859\n",
      "validation AUC 0.8494117949750566\n",
      "validation AUC 0.8494127554813781\n",
      "validation AUC 0.8494127554813781\n",
      "train auc 0.9063427380558386\n",
      "Test AUC 0.8522157625788875\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.94864993e-01\n",
      " 9.94864993e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.57575531e+01 4.47575531e+01 4.15930748e+01 ... 3.11357211e-02\n",
      " 3.11298240e-02 1.24905026e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.477s | Train Loss: 0.182759 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.451s | Train Loss: 0.137020 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.493s | Train Loss: 0.104657 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.651s | Train Loss: 0.203369 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.606s | Train Loss: 0.082441 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.666s | Train Loss: 0.154702 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.607s | Train Loss: 0.068119 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.673s | Train Loss: 0.117665 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.712s | Train Loss: 0.058282 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.657s | Train Loss: 0.092983 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.678s | Train Loss: 0.051302 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.617s | Train Loss: 0.046258 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.706s | Train Loss: 0.076347 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.623s | Train Loss: 0.064802 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.703s | Train Loss: 0.042474 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.627s | Train Loss: 0.057016 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.715s | Train Loss: 0.039521 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.624s | Train Loss: 0.050787 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.647s | Train Loss: 0.037114 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.625s | Train Loss: 0.046271 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.675s | Train Loss: 0.034873 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.597s | Train Loss: 0.042398 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.684s | Train Loss: 0.033110 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.625s | Train Loss: 0.039622 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.601s | Train Loss: 0.031743 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.692s | Train Loss: 0.037060 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.625s | Train Loss: 0.030431 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.646s | Train Loss: 0.035083 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.600s | Train Loss: 0.029445 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.618s | Train Loss: 0.033603 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.669s | Train Loss: 0.028649 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.634s | Train Loss: 0.031847 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.631s | Train Loss: 0.027587 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.656s | Train Loss: 0.030361 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.614s | Train Loss: 0.026803 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.651s | Train Loss: 0.029285 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.628s | Train Loss: 0.026408 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.695s | Train Loss: 0.028438 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.645s | Train Loss: 0.025730 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.627s | Train Loss: 0.027367 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.594s | Train Loss: 0.024782 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.709s | Train Loss: 0.026437 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.629s | Train Loss: 0.024346 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.605s | Train Loss: 0.024239 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.669s | Train Loss: 0.025768 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.637s | Train Loss: 0.023532 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.655s | Train Loss: 0.025023 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.636s | Train Loss: 0.023240 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.656s | Train Loss: 0.024317 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.617s | Train Loss: 0.022913 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.707s | Train Loss: 0.023784 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.607s | Train Loss: 0.022621 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.622s | Train Loss: 0.023368 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.607s | Train Loss: 0.022219 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.626s | Train Loss: 0.022665 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.606s | Train Loss: 0.021980 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.667s | Train Loss: 0.022362 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.601s | Train Loss: 0.021701 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.669s | Train Loss: 0.021812 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.613s | Train Loss: 0.021596 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.617s | Train Loss: 0.021588 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.678s | Train Loss: 0.021313 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.616s | Train Loss: 0.021240 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.643s | Train Loss: 0.021230 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.631s | Train Loss: 0.020923 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.610s | Train Loss: 0.021042 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.628s | Train Loss: 0.020589 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.602s | Train Loss: 0.020743 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.639s | Train Loss: 0.020318 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.697s | Train Loss: 0.020593 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.612s | Train Loss: 0.020023 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.641s | Train Loss: 0.020512 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.617s | Train Loss: 0.019784 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.654s | Train Loss: 0.020185 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.620s | Train Loss: 0.019521 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.600s | Train Loss: 0.020063 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.619s | Train Loss: 0.019404 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.603s | Train Loss: 0.019722 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.693s | Train Loss: 0.019056 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.692s | Train Loss: 0.019742 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.669s | Train Loss: 0.018944 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.659s | Train Loss: 0.019760 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.648s | Train Loss: 0.018868 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.698s | Train Loss: 0.019528 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.627s | Train Loss: 0.018748 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.599s | Train Loss: 0.019430 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.669s | Train Loss: 0.018354 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.632s | Train Loss: 0.019258 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.695s | Train Loss: 0.018460 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.665s | Train Loss: 0.019019 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.623s | Train Loss: 0.018379 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.640s | Train Loss: 0.018958 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.626s | Train Loss: 0.018207 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.653s | Train Loss: 0.019116 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.632s | Train Loss: 0.018050 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.619s | Train Loss: 0.018810 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.656s | Train Loss: 0.018012 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.683s | Train Loss: 0.018648 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.618s | Train Loss: 0.017909 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.607s | Train Loss: 0.018466 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.623s | Train Loss: 0.017882 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.624s | Train Loss: 0.018209 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.649s | Train Loss: 0.017717 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.648s | Train Loss: 0.017951 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.626s | Train Loss: 0.017610 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.604s | Train Loss: 0.018216 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.636s | Train Loss: 0.017463 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.627s | Train Loss: 0.017995 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.635s | Train Loss: 0.017346 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.606s | Train Loss: 0.018065 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.636s | Train Loss: 0.017280 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.675s | Train Loss: 0.017859 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.674s | Train Loss: 0.017052 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.672s | Train Loss: 0.017716 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.654s | Train Loss: 0.016895 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.582s | Train Loss: 0.017572 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.692s | Train Loss: 0.017025 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.637s | Train Loss: 0.017258 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.671s | Train Loss: 0.016892 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.598s | Train Loss: 0.017325 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.661s | Train Loss: 0.016671 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.638s | Train Loss: 0.017144 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.638s | Train Loss: 0.016629 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.695s | Train Loss: 0.016785 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.734s | Train Loss: 0.016710 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.668s | Train Loss: 0.016914 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.633s | Train Loss: 0.016520 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.606s | Train Loss: 0.016969 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.650s | Train Loss: 0.016601 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.633s | Train Loss: 0.016579 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.659s | Train Loss: 0.016307 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.678s | Train Loss: 0.016563 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.626s | Train Loss: 0.016287 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.602s | Train Loss: 0.016443 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.694s | Train Loss: 0.016291 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.644s | Train Loss: 0.016244 |\n",
      "INFO:root:Pretraining Time: 44.153s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017703\n",
      "INFO:root:Test AUC: 50.84%\n",
      "INFO:root:Test AUC: 50.84%\n",
      "INFO:root:Test Time: 0.312s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.526s | Train Loss: 0.016182 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.586s | Train Loss: 0.016271 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.963s | Train Loss: 0.761732 || Validation Loss: 0.013930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.566s | Train Loss: 0.016123 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.579s | Train Loss: 0.015870 |\n",
      "INFO:root:Pretraining Time: 45.088s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.952s | Train Loss: 0.608554 || Validation Loss: 0.014932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Test Loss: 0.017607\n",
      "INFO:root:Test AUC: 49.29%\n",
      "INFO:root:Test AUC: 49.29%\n",
      "INFO:root:Test Time: 0.287s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.939s | Train Loss: 0.547060 || Validation Loss: 0.016120 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.949s | Train Loss: 0.900068 || Validation Loss: 0.013880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.009s | Train Loss: 0.514590 || Validation Loss: 0.018877 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.960s | Train Loss: 0.677851 || Validation Loss: 0.015116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.995s | Train Loss: 0.497879 || Validation Loss: 0.018758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.003s | Train Loss: 0.599629 || Validation Loss: 0.015339 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.053s | Train Loss: 0.478191 || Validation Loss: 0.019793 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.071s | Train Loss: 0.550762 || Validation Loss: 0.017230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.027s | Train Loss: 0.463498 || Validation Loss: 0.020847 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.038s | Train Loss: 0.513675 || Validation Loss: 0.017065 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.067s | Train Loss: 0.455083 || Validation Loss: 0.020994 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.078s | Train Loss: 0.496347 || Validation Loss: 0.018380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.038s | Train Loss: 0.441687 || Validation Loss: 0.021387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.041s | Train Loss: 0.473438 || Validation Loss: 0.019651 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.113s | Train Loss: 0.435614 || Validation Loss: 0.022032 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.158s | Train Loss: 0.455123 || Validation Loss: 0.018974 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.113s | Train Loss: 0.421837 || Validation Loss: 0.022642 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.148s | Train Loss: 0.445047 || Validation Loss: 0.021636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.128s | Train Loss: 0.420670 || Validation Loss: 0.023682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.167s | Train Loss: 0.439423 || Validation Loss: 0.021430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.198s | Train Loss: 0.418640 || Validation Loss: 0.023000 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.137s | Train Loss: 0.425093 || Validation Loss: 0.021148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.169s | Train Loss: 0.411125 || Validation Loss: 0.021322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.139s | Train Loss: 0.413037 || Validation Loss: 0.023213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.230s | Train Loss: 0.395955 || Validation Loss: 0.020945 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.251s | Train Loss: 0.401408 || Validation Loss: 0.023978 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.273s | Train Loss: 0.396524 || Validation Loss: 0.024287 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.248s | Train Loss: 0.403991 || Validation Loss: 0.023995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.258s | Train Loss: 0.385615 || Validation Loss: 0.023131 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.221s | Train Loss: 0.403128 || Validation Loss: 0.023046 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.291s | Train Loss: 0.392795 || Validation Loss: 0.022187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.271s | Train Loss: 0.401151 || Validation Loss: 0.022982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.284s | Train Loss: 0.386706 || Validation Loss: 0.024825 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.305s | Train Loss: 0.397409 || Validation Loss: 0.023448 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.322s | Train Loss: 0.383680 || Validation Loss: 0.024142 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.320s | Train Loss: 0.380859 || Validation Loss: 0.024107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.383s | Train Loss: 0.384825 || Validation Loss: 0.023980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.302s | Train Loss: 0.384211 || Validation Loss: 0.021422 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.294s | Train Loss: 0.376129 || Validation Loss: 0.023703 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.353s | Train Loss: 0.371659 || Validation Loss: 0.024643 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.390s | Train Loss: 0.368929 || Validation Loss: 0.024119 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.492s | Train Loss: 0.379150 || Validation Loss: 0.021369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.384s | Train Loss: 0.364201 || Validation Loss: 0.024937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.332s | Train Loss: 0.370619 || Validation Loss: 0.026202 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.429s | Train Loss: 0.365375 || Validation Loss: 0.025122 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.445s | Train Loss: 0.365937 || Validation Loss: 0.024186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.437s | Train Loss: 0.364990 || Validation Loss: 0.024841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.497s | Train Loss: 0.364054 || Validation Loss: 0.025271 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.374s | Train Loss: 0.359753 || Validation Loss: 0.027127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.475s | Train Loss: 0.376587 || Validation Loss: 0.023657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.571s | Train Loss: 0.360422 || Validation Loss: 0.023217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.470s | Train Loss: 0.362226 || Validation Loss: 0.024614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.465s | Train Loss: 0.363849 || Validation Loss: 0.025465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.421s | Train Loss: 0.366614 || Validation Loss: 0.027033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.483s | Train Loss: 0.358802 || Validation Loss: 0.023338 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.602s | Train Loss: 0.355561 || Validation Loss: 0.024751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 37.717s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.914%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021885\n",
      "INFO:root:Test AUC: 87.22%\n",
      "INFO:root:Test Time: 0.226s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.733360265646781\n",
      "validation AUC 0.7904772582966992\n",
      "validation AUC 0.814241209052365\n",
      "validation AUC 0.8258716767811618\n",
      "validation AUC 0.8369810924463653\n",
      "validation AUC 0.8423794971650961\n",
      "validation AUC 0.8484602338675913\n",
      "validation AUC 0.852272768150483\n",
      "validation AUC 0.8550433526257635\n",
      "validation AUC 0.857022083450492\n",
      "validation AUC 0.8598261047206252\n",
      "validation AUC 0.8617478676759663\n",
      "validation AUC 0.8622601048989973\n",
      "validation AUC 0.8635678568714676\n",
      "validation AUC 0.8661679873939\n",
      "validation AUC 0.8663721575132455\n",
      "validation AUC 0.8671384845775145\n",
      "validation AUC 0.8674807999841637\n",
      "validation AUC 0.8675062547320234\n",
      "validation AUC 0.8687574884904204\n",
      "validation AUC 0.86976055774713\n",
      "validation AUC 0.8695084580962755\n",
      "validation AUC 0.8692631006144473\n",
      "validation AUC 0.8696354125536951\n",
      "validation AUC 0.8704221816403127\n",
      "validation AUC 0.8701750122338172\n",
      "validation AUC 0.870031481725476\n",
      "validation AUC 0.8693864631507204\n",
      "validation AUC 0.869728741307817\n",
      "validation AUC 0.8692189519125304\n",
      "validation AUC 0.8692189519125304\n",
      "train auc 0.9139315830686618\n",
      "Test AUC 0.8722410049636019\n",
      "test false positive rates [0.         0.         0.         ... 0.99454287 0.99454287 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.85110397e+01 6.75110397e+01 5.10373650e+01 ... 5.82327954e-02\n",
      " 5.82304783e-02 1.27979321e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 029/030 | Train Time: 1.484s | Train Loss: 0.351632 || Validation Loss: 0.026883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.450s | Train Loss: 0.353714 || Validation Loss: 0.024398 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.796s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.908%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022142\n",
      "INFO:root:Test AUC: 86.61%\n",
      "INFO:root:Test Time: 0.243s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6485767744143678\n",
      "validation AUC 0.7318669816346933\n",
      "validation AUC 0.7772045722016593\n",
      "validation AUC 0.8069079774280483\n",
      "validation AUC 0.8252951042593692\n",
      "validation AUC 0.8354546084401526\n",
      "validation AUC 0.8419859476594828\n",
      "validation AUC 0.8473240613165952\n",
      "validation AUC 0.8508891627854406\n",
      "validation AUC 0.8527415484490032\n",
      "validation AUC 0.8561057311523116\n",
      "validation AUC 0.8568522787041328\n",
      "validation AUC 0.8574642755504791\n",
      "validation AUC 0.8579720055776415\n",
      "validation AUC 0.8581408099074445\n",
      "validation AUC 0.8579217745559161\n",
      "validation AUC 0.8596561030837735\n",
      "validation AUC 0.8605597027315416\n",
      "validation AUC 0.8595361675063427\n",
      "validation AUC 0.8614326726043163\n",
      "validation AUC 0.8604897267862385\n",
      "validation AUC 0.8614994450880928\n",
      "validation AUC 0.8611721678579782\n",
      "validation AUC 0.8620803066298032\n",
      "validation AUC 0.8599597880436598\n",
      "validation AUC 0.8609946285080832\n",
      "validation AUC 0.8607680767289699\n",
      "validation AUC 0.8599369806747194\n",
      "validation AUC 0.8596529501752113\n",
      "validation AUC 0.8617093729240025\n",
      "validation AUC 0.8617093729240025\n",
      "train auc 0.9083767250087801\n",
      "Test AUC 0.8661194866329678\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.96342965e-01\n",
      " 9.96342965e-01 1.00000000e+00] true positive rate [0.         0.         0.00167973 ... 0.99972004 1.         1.        ] tresholds [6.13989105e+01 6.03989105e+01 3.72964439e+01 ... 1.84372254e-02\n",
      " 1.83746554e-02 3.02916416e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.492s | Train Loss: 0.186205 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.458s | Train Loss: 0.140142 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.454s | Train Loss: 0.107594 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.527s | Train Loss: 0.085211 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.454s | Train Loss: 0.070262 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.470s | Train Loss: 0.059943 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.486s | Train Loss: 0.052434 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.653s | Train Loss: 0.193708 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.651s | Train Loss: 0.046723 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.728s | Train Loss: 0.146571 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.636s | Train Loss: 0.042650 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.600s | Train Loss: 0.039341 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.606s | Train Loss: 0.111614 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.615s | Train Loss: 0.036799 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.616s | Train Loss: 0.088022 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.615s | Train Loss: 0.034809 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.618s | Train Loss: 0.072304 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.610s | Train Loss: 0.061913 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.648s | Train Loss: 0.032852 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.622s | Train Loss: 0.054374 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.644s | Train Loss: 0.031328 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.611s | Train Loss: 0.049268 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.627s | Train Loss: 0.029942 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.627s | Train Loss: 0.044844 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.621s | Train Loss: 0.028914 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.596s | Train Loss: 0.041707 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.593s | Train Loss: 0.028014 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.621s | Train Loss: 0.039274 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.851s | Train Loss: 0.027227 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.634s | Train Loss: 0.036715 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.749s | Train Loss: 0.026183 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.685s | Train Loss: 0.035011 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.595s | Train Loss: 0.025786 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.757s | Train Loss: 0.033294 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.623s | Train Loss: 0.024976 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.626s | Train Loss: 0.031696 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.678s | Train Loss: 0.024422 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.635s | Train Loss: 0.030579 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.693s | Train Loss: 0.024126 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.730s | Train Loss: 0.029353 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.657s | Train Loss: 0.023523 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.608s | Train Loss: 0.028238 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.608s | Train Loss: 0.023020 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.607s | Train Loss: 0.027323 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.608s | Train Loss: 0.022680 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.633s | Train Loss: 0.026438 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.575s | Train Loss: 0.022424 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.651s | Train Loss: 0.025693 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.597s | Train Loss: 0.021949 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.605s | Train Loss: 0.025052 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.601s | Train Loss: 0.021665 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.604s | Train Loss: 0.021443 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.717s | Train Loss: 0.024347 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.603s | Train Loss: 0.021104 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.602s | Train Loss: 0.023739 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.618s | Train Loss: 0.020939 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.619s | Train Loss: 0.023111 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.598s | Train Loss: 0.020762 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.634s | Train Loss: 0.022760 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.614s | Train Loss: 0.020522 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.611s | Train Loss: 0.022456 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.666s | Train Loss: 0.020375 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.631s | Train Loss: 0.021942 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.626s | Train Loss: 0.021514 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.680s | Train Loss: 0.020170 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.607s | Train Loss: 0.021207 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.607s | Train Loss: 0.019890 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.634s | Train Loss: 0.021056 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.625s | Train Loss: 0.019805 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.630s | Train Loss: 0.020658 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.691s | Train Loss: 0.019550 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.688s | Train Loss: 0.020432 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.609s | Train Loss: 0.019580 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.621s | Train Loss: 0.020248 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.624s | Train Loss: 0.018992 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.665s | Train Loss: 0.020005 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.702s | Train Loss: 0.018877 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.663s | Train Loss: 0.019831 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.684s | Train Loss: 0.018746 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.645s | Train Loss: 0.019526 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.651s | Train Loss: 0.018415 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.624s | Train Loss: 0.019312 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.659s | Train Loss: 0.018430 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.607s | Train Loss: 0.019169 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.602s | Train Loss: 0.018231 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.650s | Train Loss: 0.019081 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.608s | Train Loss: 0.018117 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.626s | Train Loss: 0.018777 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.661s | Train Loss: 0.017940 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.633s | Train Loss: 0.018793 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.603s | Train Loss: 0.017894 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.618s | Train Loss: 0.018465 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.614s | Train Loss: 0.017749 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.607s | Train Loss: 0.018391 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.603s | Train Loss: 0.017425 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.613s | Train Loss: 0.018120 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.615s | Train Loss: 0.017642 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.602s | Train Loss: 0.017960 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.627s | Train Loss: 0.017557 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.619s | Train Loss: 0.017812 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.684s | Train Loss: 0.017419 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.613s | Train Loss: 0.017678 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.691s | Train Loss: 0.017253 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.669s | Train Loss: 0.017740 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.607s | Train Loss: 0.017128 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.676s | Train Loss: 0.017473 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.647s | Train Loss: 0.016964 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.602s | Train Loss: 0.017453 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.672s | Train Loss: 0.017116 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.646s | Train Loss: 0.017472 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.620s | Train Loss: 0.016848 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.651s | Train Loss: 0.017231 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.640s | Train Loss: 0.016912 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.686s | Train Loss: 0.017297 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.658s | Train Loss: 0.016837 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.654s | Train Loss: 0.016960 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.602s | Train Loss: 0.016647 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.636s | Train Loss: 0.017093 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.619s | Train Loss: 0.016789 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.613s | Train Loss: 0.017053 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.611s | Train Loss: 0.016766 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.593s | Train Loss: 0.016972 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.591s | Train Loss: 0.016637 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.640s | Train Loss: 0.016617 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.632s | Train Loss: 0.016455 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.619s | Train Loss: 0.016548 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.697s | Train Loss: 0.016713 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.618s | Train Loss: 0.016491 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.651s | Train Loss: 0.016591 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.609s | Train Loss: 0.016286 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.615s | Train Loss: 0.016689 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.606s | Train Loss: 0.016372 |\n",
      "INFO:root:Pretraining Time: 43.330s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.634s | Train Loss: 0.016521 |\n",
      "INFO:root:Test Loss: 0.017342\n",
      "INFO:root:Test AUC: 51.41%\n",
      "INFO:root:Test AUC: 51.41%\n",
      "INFO:root:Test Time: 0.309s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.529s | Train Loss: 0.016459 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.576s | Train Loss: 0.016540 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.968s | Train Loss: 0.891845 || Validation Loss: 0.014675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.604s | Train Loss: 0.016230 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.565s | Train Loss: 0.016222 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 1.010s | Train Loss: 0.683879 || Validation Loss: 0.016533 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.557s | Train Loss: 0.016277 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.032s | Train Loss: 0.609195 || Validation Loss: 0.015375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.548s | Train Loss: 0.016245 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.590s | Train Loss: 0.015959 |\n",
      "INFO:root:Pretraining Time: 44.131s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017227\n",
      "INFO:root:Test AUC: 51.80%\n",
      "INFO:root:Test AUC: 51.80%\n",
      "INFO:root:Test Time: 0.287s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.068s | Train Loss: 0.556420 || Validation Loss: 0.018300 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.894s | Train Loss: 0.815727 || Validation Loss: 0.015518 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.980s | Train Loss: 0.527337 || Validation Loss: 0.016963 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.921s | Train Loss: 0.654557 || Validation Loss: 0.015907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.043s | Train Loss: 0.508707 || Validation Loss: 0.019577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.945s | Train Loss: 0.594964 || Validation Loss: 0.016248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.989s | Train Loss: 0.492539 || Validation Loss: 0.019668 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.999s | Train Loss: 0.551515 || Validation Loss: 0.017624 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.109s | Train Loss: 0.478671 || Validation Loss: 0.019929 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.995s | Train Loss: 0.523921 || Validation Loss: 0.018188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.149s | Train Loss: 0.451216 || Validation Loss: 0.020695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.991s | Train Loss: 0.503958 || Validation Loss: 0.018309 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.098s | Train Loss: 0.452467 || Validation Loss: 0.019480 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.971s | Train Loss: 0.481890 || Validation Loss: 0.019501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.156s | Train Loss: 0.443335 || Validation Loss: 0.023028 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.086s | Train Loss: 0.470579 || Validation Loss: 0.020775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.095s | Train Loss: 0.437753 || Validation Loss: 0.022434 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.078s | Train Loss: 0.463606 || Validation Loss: 0.021340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.191s | Train Loss: 0.431538 || Validation Loss: 0.021210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.105s | Train Loss: 0.451042 || Validation Loss: 0.022139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.186s | Train Loss: 0.417466 || Validation Loss: 0.023935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.162s | Train Loss: 0.449983 || Validation Loss: 0.020326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.144s | Train Loss: 0.435478 || Validation Loss: 0.019733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.235s | Train Loss: 0.416755 || Validation Loss: 0.021728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.234s | Train Loss: 0.434073 || Validation Loss: 0.023664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.244s | Train Loss: 0.408960 || Validation Loss: 0.023064 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.252s | Train Loss: 0.424324 || Validation Loss: 0.023126 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.325s | Train Loss: 0.398413 || Validation Loss: 0.025047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.227s | Train Loss: 0.418747 || Validation Loss: 0.022834 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.336s | Train Loss: 0.407831 || Validation Loss: 0.023856 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.231s | Train Loss: 0.406813 || Validation Loss: 0.021811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.319s | Train Loss: 0.397709 || Validation Loss: 0.022507 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.353s | Train Loss: 0.408742 || Validation Loss: 0.022235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.398s | Train Loss: 0.392204 || Validation Loss: 0.023910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.415s | Train Loss: 0.404620 || Validation Loss: 0.022175 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.557s | Train Loss: 0.384841 || Validation Loss: 0.025042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.237s | Train Loss: 0.402803 || Validation Loss: 0.022444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.297s | Train Loss: 0.383679 || Validation Loss: 0.024324 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.304s | Train Loss: 0.401950 || Validation Loss: 0.021148 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.354s | Train Loss: 0.381459 || Validation Loss: 0.025766 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.398s | Train Loss: 0.392671 || Validation Loss: 0.022849 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.410s | Train Loss: 0.372474 || Validation Loss: 0.027006 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.279s | Train Loss: 0.388754 || Validation Loss: 0.022946 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.403s | Train Loss: 0.387044 || Validation Loss: 0.024805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.358s | Train Loss: 0.394659 || Validation Loss: 0.023692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.384s | Train Loss: 0.368430 || Validation Loss: 0.026631 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.413s | Train Loss: 0.383763 || Validation Loss: 0.021934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.344s | Train Loss: 0.378257 || Validation Loss: 0.024364 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.421s | Train Loss: 0.384750 || Validation Loss: 0.021898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.555s | Train Loss: 0.366792 || Validation Loss: 0.026276 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.385s | Train Loss: 0.385615 || Validation Loss: 0.025071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.452s | Train Loss: 0.365147 || Validation Loss: 0.025686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.358s | Train Loss: 0.375380 || Validation Loss: 0.023212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.517s | Train Loss: 0.359192 || Validation Loss: 0.026136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.639s | Train Loss: 0.374134 || Validation Loss: 0.022975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.325s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.901%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022797\n",
      "INFO:root:Test AUC: 85.48%\n",
      "INFO:root:Test Time: 0.226s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6547073244114623\n",
      "validation AUC 0.7451995916171904\n",
      "validation AUC 0.7866003301800346\n",
      "validation AUC 0.8082833958735585\n",
      "validation AUC 0.8230774548040535\n",
      "validation AUC 0.8309814267344936\n",
      "validation AUC 0.8348274829540726\n",
      "validation AUC 0.839385803833639\n",
      "validation AUC 0.8437782976869305\n",
      "validation AUC 0.8461016720153476\n",
      "validation AUC 0.8475424181941289\n",
      "validation AUC 0.848369802596528\n",
      "validation AUC 0.8497735440054507\n",
      "validation AUC 0.8507756926660739\n",
      "validation AUC 0.8510307563170451\n",
      "validation AUC 0.8538245871738509\n",
      "validation AUC 0.8539772943755517\n",
      "validation AUC 0.8525366147138288\n",
      "validation AUC 0.854471885953364\n",
      "validation AUC 0.8543700137482777\n",
      "validation AUC 0.8537821120411201\n",
      "validation AUC 0.8535426453099727\n",
      "validation AUC 0.8531370508954476\n",
      "validation AUC 0.8538067499595043\n",
      "validation AUC 0.8541723676220077\n",
      "validation AUC 0.8550735513702193\n",
      "validation AUC 0.8541774042936601\n",
      "validation AUC 0.8518086866169595\n",
      "validation AUC 0.8547453056517362\n",
      "validation AUC 0.8515312200207619\n",
      "validation AUC 0.8515312200207619\n",
      "train auc 0.9014311748082358\n",
      "Test AUC 0.8547818577355428\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.97366177e-01\n",
      " 9.97366177e-01 1.00000000e+00] true positive rate [0.         0.         0.00419933 ... 0.99972004 1.         1.        ] tresholds [8.31726074e+01 8.21726074e+01 5.98067360e+01 ... 1.91647559e-02\n",
      " 1.91627443e-02 5.57197817e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 029/030 | Train Time: 1.482s | Train Loss: 0.366960 || Validation Loss: 0.023729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.334s | Train Loss: 0.372734 || Validation Loss: 0.025043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 37.586s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.901%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021487\n",
      "INFO:root:Test AUC: 86.48%\n",
      "INFO:root:Test Time: 0.229s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.649912995687779\n",
      "validation AUC 0.7354256548950275\n",
      "validation AUC 0.7777201672057918\n",
      "validation AUC 0.8014943509457022\n",
      "validation AUC 0.8204649121985473\n",
      "validation AUC 0.8295073049565532\n",
      "validation AUC 0.8374680372231588\n",
      "validation AUC 0.8428993971532401\n",
      "validation AUC 0.8470927894870268\n",
      "validation AUC 0.8504040246119502\n",
      "validation AUC 0.851144774536972\n",
      "validation AUC 0.8537677470172155\n",
      "validation AUC 0.8537189394605371\n",
      "validation AUC 0.8560183782907053\n",
      "validation AUC 0.8567346978305647\n",
      "validation AUC 0.8577793934623417\n",
      "validation AUC 0.8586192644469198\n",
      "validation AUC 0.8587707291099719\n",
      "validation AUC 0.8581996322724164\n",
      "validation AUC 0.8596056671895085\n",
      "validation AUC 0.8591628790966791\n",
      "validation AUC 0.8603914810911693\n",
      "validation AUC 0.8598529297198856\n",
      "validation AUC 0.8609811548127595\n",
      "validation AUC 0.8603596912586793\n",
      "validation AUC 0.8612126847285135\n",
      "validation AUC 0.8625319015811477\n",
      "validation AUC 0.861516819343714\n",
      "validation AUC 0.861645769312882\n",
      "validation AUC 0.861904392956812\n",
      "validation AUC 0.861904392956812\n",
      "train auc 0.9014909578658254\n",
      "Test AUC 0.8647941301442931\n",
      "test false positive rates [0.         0.         0.         ... 0.99715775 0.99715775 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 7.83874580e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.87457047e+01 5.77457047e+01 3.89976578e+01 ... 1.80223901e-02\n",
      " 1.79658178e-02 8.23205430e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.495s | Train Loss: 0.188866 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.456s | Train Loss: 0.142511 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.451s | Train Loss: 0.109724 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.531s | Train Loss: 0.087429 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.452s | Train Loss: 0.072442 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.451s | Train Loss: 0.062335 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.441s | Train Loss: 0.054877 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.583s | Train Loss: 0.049366 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.662s | Train Loss: 0.191717 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.654s | Train Loss: 0.044920 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.646s | Train Loss: 0.144872 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.679s | Train Loss: 0.041459 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.613s | Train Loss: 0.110829 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.633s | Train Loss: 0.038711 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.647s | Train Loss: 0.087978 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.609s | Train Loss: 0.036507 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.626s | Train Loss: 0.072824 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.616s | Train Loss: 0.034641 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.624s | Train Loss: 0.062418 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.624s | Train Loss: 0.032914 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.680s | Train Loss: 0.055002 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.659s | Train Loss: 0.031278 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.615s | Train Loss: 0.049583 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.603s | Train Loss: 0.029936 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.673s | Train Loss: 0.045534 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.610s | Train Loss: 0.028715 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.619s | Train Loss: 0.042084 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.646s | Train Loss: 0.027638 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.616s | Train Loss: 0.039254 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.658s | Train Loss: 0.026571 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.695s | Train Loss: 0.037125 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.679s | Train Loss: 0.025685 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.674s | Train Loss: 0.035236 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.603s | Train Loss: 0.024852 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.727s | Train Loss: 0.033772 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.669s | Train Loss: 0.024024 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.599s | Train Loss: 0.032056 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.583s | Train Loss: 0.023586 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.663s | Train Loss: 0.031245 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.605s | Train Loss: 0.022991 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.641s | Train Loss: 0.030237 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.664s | Train Loss: 0.022568 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.650s | Train Loss: 0.029374 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.602s | Train Loss: 0.022142 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.633s | Train Loss: 0.028369 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.599s | Train Loss: 0.021645 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.632s | Train Loss: 0.027559 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.634s | Train Loss: 0.021324 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.680s | Train Loss: 0.026777 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.600s | Train Loss: 0.021224 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.619s | Train Loss: 0.026213 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.645s | Train Loss: 0.020979 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.638s | Train Loss: 0.025304 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.638s | Train Loss: 0.020593 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.611s | Train Loss: 0.025095 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.659s | Train Loss: 0.020393 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.605s | Train Loss: 0.024409 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.601s | Train Loss: 0.020364 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.641s | Train Loss: 0.024033 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.629s | Train Loss: 0.019910 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.647s | Train Loss: 0.023668 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.606s | Train Loss: 0.019756 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.655s | Train Loss: 0.023480 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.632s | Train Loss: 0.019642 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.651s | Train Loss: 0.023059 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.637s | Train Loss: 0.019525 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.626s | Train Loss: 0.022564 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.625s | Train Loss: 0.019216 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.614s | Train Loss: 0.022339 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.664s | Train Loss: 0.019175 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.604s | Train Loss: 0.022151 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.669s | Train Loss: 0.019101 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.715s | Train Loss: 0.022045 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.634s | Train Loss: 0.018960 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.593s | Train Loss: 0.019005 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.714s | Train Loss: 0.021816 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.616s | Train Loss: 0.018737 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.661s | Train Loss: 0.021607 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.672s | Train Loss: 0.018750 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.682s | Train Loss: 0.021360 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.655s | Train Loss: 0.018676 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.619s | Train Loss: 0.021042 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.634s | Train Loss: 0.018465 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.692s | Train Loss: 0.021055 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.676s | Train Loss: 0.018501 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.631s | Train Loss: 0.020645 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.615s | Train Loss: 0.018369 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.687s | Train Loss: 0.020507 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.613s | Train Loss: 0.018153 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.619s | Train Loss: 0.020258 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.666s | Train Loss: 0.018139 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.608s | Train Loss: 0.020076 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.649s | Train Loss: 0.018001 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.625s | Train Loss: 0.020083 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.681s | Train Loss: 0.017749 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.706s | Train Loss: 0.019888 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.643s | Train Loss: 0.017802 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.636s | Train Loss: 0.019706 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.614s | Train Loss: 0.017758 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.624s | Train Loss: 0.019426 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.646s | Train Loss: 0.017477 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.706s | Train Loss: 0.019526 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.653s | Train Loss: 0.017260 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.615s | Train Loss: 0.019252 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.614s | Train Loss: 0.017102 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.617s | Train Loss: 0.019051 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.601s | Train Loss: 0.016974 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.613s | Train Loss: 0.018944 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.605s | Train Loss: 0.016961 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.690s | Train Loss: 0.019100 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.683s | Train Loss: 0.016871 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.622s | Train Loss: 0.018734 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.672s | Train Loss: 0.016809 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.652s | Train Loss: 0.018761 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.655s | Train Loss: 0.016722 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.661s | Train Loss: 0.018578 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.678s | Train Loss: 0.016532 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.710s | Train Loss: 0.018404 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.665s | Train Loss: 0.016669 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.643s | Train Loss: 0.018380 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.668s | Train Loss: 0.016486 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.626s | Train Loss: 0.018067 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.699s | Train Loss: 0.016459 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.636s | Train Loss: 0.018013 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.671s | Train Loss: 0.016444 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.645s | Train Loss: 0.018094 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.596s | Train Loss: 0.016352 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.644s | Train Loss: 0.017798 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.594s | Train Loss: 0.016142 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.659s | Train Loss: 0.017785 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.582s | Train Loss: 0.016210 |\n",
      "INFO:root:Pretraining Time: 43.350s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.608s | Train Loss: 0.017693 |\n",
      "INFO:root:Test Loss: 0.017543\n",
      "INFO:root:Test AUC: 52.26%\n",
      "INFO:root:Test AUC: 52.26%\n",
      "INFO:root:Test Time: 0.321s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.587s | Train Loss: 0.017651 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.570s | Train Loss: 0.017507 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.967s | Train Loss: 0.724942 || Validation Loss: 0.016291 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.565s | Train Loss: 0.017431 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.550s | Train Loss: 0.017324 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 1.029s | Train Loss: 0.593243 || Validation Loss: 0.016023 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.575s | Train Loss: 0.017222 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.035s | Train Loss: 0.545210 || Validation Loss: 0.018499 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.529s | Train Loss: 0.017271 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.571s | Train Loss: 0.017057 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.134s | Train Loss: 0.515035 || Validation Loss: 0.019563 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.568s | Train Loss: 0.016870 |\n",
      "INFO:root:Pretraining Time: 44.630s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018196\n",
      "INFO:root:Test AUC: 51.47%\n",
      "INFO:root:Test AUC: 51.47%\n",
      "INFO:root:Test Time: 0.310s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.007s | Train Loss: 0.491948 || Validation Loss: 0.019297 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.908s | Train Loss: 0.803381 || Validation Loss: 0.014660 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.063s | Train Loss: 0.476067 || Validation Loss: 0.020473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.929s | Train Loss: 0.627707 || Validation Loss: 0.017227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.059s | Train Loss: 0.455781 || Validation Loss: 0.022584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.035s | Train Loss: 0.571575 || Validation Loss: 0.016368 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.118s | Train Loss: 0.444206 || Validation Loss: 0.022565 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.017s | Train Loss: 0.541357 || Validation Loss: 0.018759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.105s | Train Loss: 0.431603 || Validation Loss: 0.024343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.004s | Train Loss: 0.516601 || Validation Loss: 0.018034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.086s | Train Loss: 0.492281 || Validation Loss: 0.018042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.169s | Train Loss: 0.421614 || Validation Loss: 0.023009 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.065s | Train Loss: 0.477457 || Validation Loss: 0.021662 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.179s | Train Loss: 0.417214 || Validation Loss: 0.023116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.109s | Train Loss: 0.467239 || Validation Loss: 0.019684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.148s | Train Loss: 0.402153 || Validation Loss: 0.023359 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.075s | Train Loss: 0.457146 || Validation Loss: 0.020752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.225s | Train Loss: 0.395213 || Validation Loss: 0.024577 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.107s | Train Loss: 0.442940 || Validation Loss: 0.019997 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.166s | Train Loss: 0.397962 || Validation Loss: 0.022411 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.124s | Train Loss: 0.445302 || Validation Loss: 0.019636 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.197s | Train Loss: 0.387846 || Validation Loss: 0.025051 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.088s | Train Loss: 0.434587 || Validation Loss: 0.021363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.194s | Train Loss: 0.378864 || Validation Loss: 0.026532 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.185s | Train Loss: 0.426678 || Validation Loss: 0.023340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.264s | Train Loss: 0.376179 || Validation Loss: 0.025689 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.179s | Train Loss: 0.430154 || Validation Loss: 0.021698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.306s | Train Loss: 0.370681 || Validation Loss: 0.023670 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.197s | Train Loss: 0.418639 || Validation Loss: 0.021719 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.277s | Train Loss: 0.368487 || Validation Loss: 0.025428 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.204s | Train Loss: 0.407622 || Validation Loss: 0.021198 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.291s | Train Loss: 0.359887 || Validation Loss: 0.027183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.267s | Train Loss: 0.407220 || Validation Loss: 0.023711 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.408s | Train Loss: 0.366128 || Validation Loss: 0.027749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.297s | Train Loss: 0.403678 || Validation Loss: 0.022801 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.317s | Train Loss: 0.360370 || Validation Loss: 0.027089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.283s | Train Loss: 0.402654 || Validation Loss: 0.020948 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.434s | Train Loss: 0.351758 || Validation Loss: 0.027646 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.348s | Train Loss: 0.395003 || Validation Loss: 0.023929 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.449s | Train Loss: 0.358283 || Validation Loss: 0.025635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.472s | Train Loss: 0.396431 || Validation Loss: 0.024898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.331s | Train Loss: 0.393436 || Validation Loss: 0.023652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.484s | Train Loss: 0.352501 || Validation Loss: 0.025846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.490s | Train Loss: 0.384876 || Validation Loss: 0.023386 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.479s | Train Loss: 0.351864 || Validation Loss: 0.026375 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.447s | Train Loss: 0.338332 || Validation Loss: 0.029152 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.517s | Train Loss: 0.378671 || Validation Loss: 0.024121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.520s | Train Loss: 0.378459 || Validation Loss: 0.026582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.676s | Train Loss: 0.341842 || Validation Loss: 0.028369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.470s | Train Loss: 0.380292 || Validation Loss: 0.022927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.529s | Train Loss: 0.346983 || Validation Loss: 0.027740 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.454s | Train Loss: 0.373220 || Validation Loss: 0.023935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.562s | Train Loss: 0.334078 || Validation Loss: 0.027719 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.740s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.919%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.501s | Train Loss: 0.373689 || Validation Loss: 0.024253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Test Loss: 0.024255\n",
      "INFO:root:Test AUC: 86.27%\n",
      "INFO:root:Test Time: 0.213s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7394703922463035\n",
      "validation AUC 0.7985002638332599\n",
      "validation AUC 0.8226987519058467\n",
      "validation AUC 0.8344835205850606\n",
      "validation AUC 0.8414243175296607\n",
      "validation AUC 0.8475056049933983\n",
      "validation AUC 0.8512688182079218\n",
      "validation AUC 0.8548163432092895\n",
      "validation AUC 0.8566355368607739\n",
      "validation AUC 0.8598677257743279\n",
      "validation AUC 0.8605631882253952\n",
      "validation AUC 0.8620585422483362\n",
      "validation AUC 0.8637421794564248\n",
      "validation AUC 0.8644562081487971\n",
      "validation AUC 0.8638263128924107\n",
      "validation AUC 0.864675858117944\n",
      "validation AUC 0.8654410889810927\n",
      "validation AUC 0.8647102713832121\n",
      "validation AUC 0.8657884729875822\n",
      "validation AUC 0.8654228287182558\n",
      "validation AUC 0.8651440184774809\n",
      "validation AUC 0.8655865883943591\n",
      "validation AUC 0.8650432770623853\n",
      "validation AUC 0.8652316241039089\n",
      "validation AUC 0.8638765013432188\n",
      "validation AUC 0.865404427439255\n",
      "validation AUC 0.8653465469558281\n",
      "validation AUC 0.863705565806869\n",
      "validation AUC 0.8628167648742252\n",
      "validation AUC 0.8628605038310632\n",
      "validation AUC 0.8628605038310632\n",
      "train auc 0.9185605339161582\n",
      "Test AUC 0.862681859486092\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.95603979e-01\n",
      " 9.95603979e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.86107788e+01 5.76107788e+01 5.60642738e+01 ... 1.89187936e-02\n",
      " 1.89085770e-02 4.54168953e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 029/030 | Train Time: 1.554s | Train Loss: 0.370895 || Validation Loss: 0.023274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.363s | Train Loss: 0.375607 || Validation Loss: 0.024564 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.147s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.904%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020951\n",
      "INFO:root:Test AUC: 86.01%\n",
      "INFO:root:Test Time: 0.212s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7471577793136418\n",
      "validation AUC 0.7883619626299717\n",
      "validation AUC 0.8103954774360302\n",
      "validation AUC 0.8223945135240354\n",
      "validation AUC 0.8318758895991374\n",
      "validation AUC 0.8389215333916695\n",
      "validation AUC 0.842347736600112\n",
      "validation AUC 0.8479536426124665\n",
      "validation AUC 0.8487363781046705\n",
      "validation AUC 0.8512669291234669\n",
      "validation AUC 0.8525635115515121\n",
      "validation AUC 0.8540905303548733\n",
      "validation AUC 0.8554314211257752\n",
      "validation AUC 0.8565923353617666\n",
      "validation AUC 0.8566877500908359\n",
      "validation AUC 0.8581024481895971\n",
      "validation AUC 0.8585955178571162\n",
      "validation AUC 0.859857918499256\n",
      "validation AUC 0.8596997249705888\n",
      "validation AUC 0.8578125029932676\n",
      "validation AUC 0.8583693013239768\n",
      "validation AUC 0.8587769338211677\n",
      "validation AUC 0.8595227017930658\n",
      "validation AUC 0.8597917606330976\n",
      "validation AUC 0.8582077686389844\n",
      "validation AUC 0.8578951730539716\n",
      "validation AUC 0.8588357561861395\n",
      "validation AUC 0.8585810171384127\n",
      "validation AUC 0.8578417013211672\n",
      "validation AUC 0.8560601430212538\n",
      "validation AUC 0.8560601430212538\n",
      "train auc 0.9041570099096096\n",
      "Test AUC 0.8600626563889995\n",
      "test false positive rates [0.         0.         0.         ... 0.99605874 0.99605874 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.23964166e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.14974937e+01 5.04974937e+01 4.01670494e+01 ... 3.00804544e-02\n",
      " 3.00738811e-02 8.21317919e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.515s | Train Loss: 0.186340 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.474s | Train Loss: 0.140546 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.469s | Train Loss: 0.108054 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.460s | Train Loss: 0.086059 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.460s | Train Loss: 0.071266 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.540s | Train Loss: 0.061606 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.471s | Train Loss: 0.054106 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.467s | Train Loss: 0.049231 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.507s | Train Loss: 0.045062 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.636s | Train Loss: 0.183008 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.622s | Train Loss: 0.041639 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.608s | Train Loss: 0.138485 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.620s | Train Loss: 0.038616 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.640s | Train Loss: 0.107022 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.651s | Train Loss: 0.036233 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.600s | Train Loss: 0.085781 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.619s | Train Loss: 0.034236 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.611s | Train Loss: 0.071126 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.620s | Train Loss: 0.032435 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.602s | Train Loss: 0.061156 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.615s | Train Loss: 0.030785 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.605s | Train Loss: 0.053902 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.619s | Train Loss: 0.029713 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.597s | Train Loss: 0.048203 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.704s | Train Loss: 0.028275 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.601s | Train Loss: 0.043865 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.611s | Train Loss: 0.027274 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.681s | Train Loss: 0.040715 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.636s | Train Loss: 0.026638 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.652s | Train Loss: 0.038050 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.641s | Train Loss: 0.025630 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.632s | Train Loss: 0.035669 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.632s | Train Loss: 0.025085 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.605s | Train Loss: 0.033787 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.624s | Train Loss: 0.024276 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.611s | Train Loss: 0.032358 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.620s | Train Loss: 0.023813 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.607s | Train Loss: 0.030592 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.673s | Train Loss: 0.023233 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.597s | Train Loss: 0.029586 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.623s | Train Loss: 0.022856 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.631s | Train Loss: 0.028521 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.666s | Train Loss: 0.022409 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.620s | Train Loss: 0.027491 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.591s | Train Loss: 0.021719 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.591s | Train Loss: 0.027001 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.640s | Train Loss: 0.021859 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.658s | Train Loss: 0.025831 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.617s | Train Loss: 0.021373 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.598s | Train Loss: 0.025404 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.702s | Train Loss: 0.021264 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.625s | Train Loss: 0.024954 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.688s | Train Loss: 0.020921 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.624s | Train Loss: 0.024435 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.601s | Train Loss: 0.024074 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.654s | Train Loss: 0.020450 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.606s | Train Loss: 0.023568 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.615s | Train Loss: 0.020446 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.599s | Train Loss: 0.023221 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.614s | Train Loss: 0.020469 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.605s | Train Loss: 0.022863 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.618s | Train Loss: 0.019962 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.603s | Train Loss: 0.022651 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.715s | Train Loss: 0.019849 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.598s | Train Loss: 0.022204 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.631s | Train Loss: 0.019796 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.609s | Train Loss: 0.022050 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.618s | Train Loss: 0.019549 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.618s | Train Loss: 0.021781 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.643s | Train Loss: 0.019573 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.619s | Train Loss: 0.021645 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.674s | Train Loss: 0.019369 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.633s | Train Loss: 0.021473 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.682s | Train Loss: 0.019184 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.593s | Train Loss: 0.021253 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.679s | Train Loss: 0.019076 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.682s | Train Loss: 0.021159 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.678s | Train Loss: 0.018905 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.615s | Train Loss: 0.020932 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.672s | Train Loss: 0.018858 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.666s | Train Loss: 0.020882 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.672s | Train Loss: 0.018769 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.691s | Train Loss: 0.020682 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.612s | Train Loss: 0.018610 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.654s | Train Loss: 0.020556 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.638s | Train Loss: 0.018364 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.629s | Train Loss: 0.020403 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.630s | Train Loss: 0.018274 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.606s | Train Loss: 0.020417 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.639s | Train Loss: 0.018290 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.620s | Train Loss: 0.020193 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.614s | Train Loss: 0.018202 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.599s | Train Loss: 0.020163 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.642s | Train Loss: 0.018055 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.635s | Train Loss: 0.019736 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.672s | Train Loss: 0.018023 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.651s | Train Loss: 0.020008 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.730s | Train Loss: 0.017984 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.611s | Train Loss: 0.019894 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.618s | Train Loss: 0.017672 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.594s | Train Loss: 0.019845 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.647s | Train Loss: 0.017531 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.623s | Train Loss: 0.019611 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.631s | Train Loss: 0.019529 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.639s | Train Loss: 0.017500 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.603s | Train Loss: 0.019228 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.612s | Train Loss: 0.017201 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.599s | Train Loss: 0.019176 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.659s | Train Loss: 0.017161 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.600s | Train Loss: 0.018880 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.615s | Train Loss: 0.016951 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.598s | Train Loss: 0.018918 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.609s | Train Loss: 0.016908 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.631s | Train Loss: 0.018881 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.649s | Train Loss: 0.016860 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.627s | Train Loss: 0.018646 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.644s | Train Loss: 0.016803 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.676s | Train Loss: 0.018563 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.614s | Train Loss: 0.016782 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.645s | Train Loss: 0.018516 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.636s | Train Loss: 0.016770 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.606s | Train Loss: 0.018599 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.617s | Train Loss: 0.016511 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.607s | Train Loss: 0.018374 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.627s | Train Loss: 0.016473 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.665s | Train Loss: 0.018302 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.672s | Train Loss: 0.016531 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.671s | Train Loss: 0.018247 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.640s | Train Loss: 0.016340 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.669s | Train Loss: 0.018229 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.666s | Train Loss: 0.016306 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.610s | Train Loss: 0.018013 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.727s | Train Loss: 0.016060 |\n",
      "INFO:root:Pretraining Time: 43.639s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017221\n",
      "INFO:root:Test AUC: 51.48%\n",
      "INFO:root:Test AUC: 51.48%\n",
      "INFO:root:Test Time: 0.330s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.537s | Train Loss: 0.018044 |\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.551s | Train Loss: 0.017942 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.554s | Train Loss: 0.017929 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 1.030s | Train Loss: 0.778226 || Validation Loss: 0.014686 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.549s | Train Loss: 0.017840 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.535s | Train Loss: 0.017650 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 1.092s | Train Loss: 0.626536 || Validation Loss: 0.014295 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.573s | Train Loss: 0.017786 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.562s | Train Loss: 0.017568 |\n",
      "INFO:root:Pretraining Time: 43.101s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.103s | Train Loss: 0.556289 || Validation Loss: 0.017599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Test Loss: 0.019233\n",
      "INFO:root:Test AUC: 50.27%\n",
      "INFO:root:Test AUC: 50.27%\n",
      "INFO:root:Test Time: 0.310s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.010s | Train Loss: 0.525836 || Validation Loss: 0.014877 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.928s | Train Loss: 0.998359 || Validation Loss: 0.014554 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.075s | Train Loss: 0.501201 || Validation Loss: 0.017748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.917s | Train Loss: 0.712509 || Validation Loss: 0.014388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.081s | Train Loss: 0.479653 || Validation Loss: 0.018486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.931s | Train Loss: 0.608502 || Validation Loss: 0.014831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.056s | Train Loss: 0.461392 || Validation Loss: 0.017869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.036s | Train Loss: 0.556404 || Validation Loss: 0.015343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.009s | Train Loss: 0.522858 || Validation Loss: 0.017397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.170s | Train Loss: 0.454082 || Validation Loss: 0.018487 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.037s | Train Loss: 0.503357 || Validation Loss: 0.015806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.118s | Train Loss: 0.440643 || Validation Loss: 0.019249 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.041s | Train Loss: 0.495071 || Validation Loss: 0.018552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.149s | Train Loss: 0.437329 || Validation Loss: 0.019385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.060s | Train Loss: 0.472203 || Validation Loss: 0.018603 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.199s | Train Loss: 0.428876 || Validation Loss: 0.020940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.046s | Train Loss: 0.459646 || Validation Loss: 0.018914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.221s | Train Loss: 0.422689 || Validation Loss: 0.021621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.116s | Train Loss: 0.444933 || Validation Loss: 0.019040 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.237s | Train Loss: 0.415628 || Validation Loss: 0.022064 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.114s | Train Loss: 0.443802 || Validation Loss: 0.020772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.215s | Train Loss: 0.412247 || Validation Loss: 0.020509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.093s | Train Loss: 0.428960 || Validation Loss: 0.021041 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.220s | Train Loss: 0.414056 || Validation Loss: 0.023217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.220s | Train Loss: 0.432862 || Validation Loss: 0.020554 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.220s | Train Loss: 0.405076 || Validation Loss: 0.022248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.185s | Train Loss: 0.430324 || Validation Loss: 0.019972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.315s | Train Loss: 0.394474 || Validation Loss: 0.021197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.223s | Train Loss: 0.414252 || Validation Loss: 0.022293 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.258s | Train Loss: 0.411624 || Validation Loss: 0.020624 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.354s | Train Loss: 0.393279 || Validation Loss: 0.020402 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.311s | Train Loss: 0.404243 || Validation Loss: 0.022582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.334s | Train Loss: 0.392937 || Validation Loss: 0.022390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.336s | Train Loss: 0.394826 || Validation Loss: 0.022109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.396s | Train Loss: 0.392740 || Validation Loss: 0.022787 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.322s | Train Loss: 0.398983 || Validation Loss: 0.023494 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.558s | Train Loss: 0.387395 || Validation Loss: 0.023076 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.307s | Train Loss: 0.394009 || Validation Loss: 0.024417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.351s | Train Loss: 0.385350 || Validation Loss: 0.022093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.405s | Train Loss: 0.384217 || Validation Loss: 0.022099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.485s | Train Loss: 0.382369 || Validation Loss: 0.023509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.301s | Train Loss: 0.381174 || Validation Loss: 0.022899 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.473s | Train Loss: 0.383638 || Validation Loss: 0.021053 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.382s | Train Loss: 0.392386 || Validation Loss: 0.023559 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.459s | Train Loss: 0.372263 || Validation Loss: 0.023807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.386s | Train Loss: 0.372584 || Validation Loss: 0.022498 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.450s | Train Loss: 0.378190 || Validation Loss: 0.023117 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.426s | Train Loss: 0.384559 || Validation Loss: 0.021552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.468s | Train Loss: 0.365694 || Validation Loss: 0.022106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.467s | Train Loss: 0.379374 || Validation Loss: 0.024582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.631s | Train Loss: 0.369833 || Validation Loss: 0.023303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.383s | Train Loss: 0.373728 || Validation Loss: 0.022541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.479s | Train Loss: 0.366276 || Validation Loss: 0.023465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.618s | Train Loss: 0.368213 || Validation Loss: 0.023106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.525s | Train Loss: 0.375403 || Validation Loss: 0.023649 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.456s | Train Loss: 0.367762 || Validation Loss: 0.021954 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 39.547s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.907%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020706\n",
      "INFO:root:Test AUC: 86.64%\n",
      "INFO:root:Test Time: 0.247s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6407684407103129\n",
      "validation AUC 0.7351286376050622\n",
      "validation AUC 0.7803039132464306\n",
      "validation AUC 0.8071648902532395\n",
      "validation AUC 0.819908611415434\n",
      "validation AUC 0.8306192679590203\n",
      "validation AUC 0.8365489204760365\n",
      "validation AUC 0.8417729999491278\n",
      "validation AUC 0.8451000581518732\n",
      "validation AUC 0.8477817864800302\n",
      "validation AUC 0.8504264355392235\n",
      "validation AUC 0.8522447325407623\n",
      "validation AUC 0.8530311770241354\n",
      "validation AUC 0.855187503073088\n",
      "validation AUC 0.8566486487033005\n",
      "validation AUC 0.857877961099973\n",
      "validation AUC 0.8590783944506255\n",
      "validation AUC 0.8605696137732242\n",
      "validation AUC 0.8610333307932686\n",
      "validation AUC 0.861053240679151\n",
      "validation AUC 0.8620388372349934\n",
      "validation AUC 0.861877868614655\n",
      "validation AUC 0.862044879644567\n",
      "validation AUC 0.8629783348087704\n",
      "validation AUC 0.8630195966703582\n",
      "validation AUC 0.8633471665755293\n",
      "validation AUC 0.8635643900023905\n",
      "validation AUC 0.8632390364456008\n",
      "validation AUC 0.862770931960393\n",
      "validation AUC 0.8604685796830723\n",
      "validation AUC 0.8604685796830723\n",
      "train auc 0.906674335485216\n",
      "Test AUC 0.8663833606613468\n",
      "test false positive rates [0.         0.         0.         ... 0.99969683 0.99969683 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 6.71892497e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.16579895e+01 5.06579895e+01 3.31590195e+01 ... 6.12571742e-03\n",
      " 6.11565635e-03 4.96575655e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 030/030 | Train Time: 1.477s | Train Loss: 0.366813 || Validation Loss: 0.022732 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:Training Time: 37.778s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.899%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020597\n",
      "INFO:root:Test AUC: 86.41%\n",
      "INFO:root:Test Time: 0.213s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5463831615632297\n",
      "validation AUC 0.6895780407927302\n",
      "validation AUC 0.7607395914639351\n",
      "validation AUC 0.7973847115703282\n",
      "validation AUC 0.8156417629638553\n",
      "validation AUC 0.8283355244939222\n",
      "validation AUC 0.8368519509080695\n",
      "validation AUC 0.8418572717405949\n",
      "validation AUC 0.8443149493310299\n",
      "validation AUC 0.8465601847407527\n",
      "validation AUC 0.8501016327436764\n",
      "validation AUC 0.8513442379092742\n",
      "validation AUC 0.8533753628904631\n",
      "validation AUC 0.853615484149464\n",
      "validation AUC 0.856550873949004\n",
      "validation AUC 0.8575819575299757\n",
      "validation AUC 0.8571782894494581\n",
      "validation AUC 0.8577066450860497\n",
      "validation AUC 0.8579372517450352\n",
      "validation AUC 0.8594277260765812\n",
      "validation AUC 0.8597028432902807\n",
      "validation AUC 0.8598413743765222\n",
      "validation AUC 0.8591774862426759\n",
      "validation AUC 0.8592015654177707\n",
      "validation AUC 0.8595826629300753\n",
      "validation AUC 0.8600733379834965\n",
      "validation AUC 0.8593228260150024\n",
      "validation AUC 0.8598164677292226\n",
      "validation AUC 0.8587084797861579\n",
      "validation AUC 0.858554551331267\n",
      "validation AUC 0.858554551331267\n",
      "train auc 0.8990530862229744\n",
      "Test AUC 0.8640748455140592\n",
      "test false positive rates [0.         0.         0.         ... 0.99662719 0.99662719 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 4.75923852e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.83598480e+01 5.73598480e+01 3.50313530e+01 ... 2.97751725e-02\n",
      " 2.97445711e-02 8.83054268e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.426s | Train Loss: 0.198230 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.402s | Train Loss: 0.150486 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.405s | Train Loss: 0.114402 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.404s | Train Loss: 0.090316 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.417s | Train Loss: 0.074263 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.411s | Train Loss: 0.063385 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.413s | Train Loss: 0.055565 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.411s | Train Loss: 0.049804 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.416s | Train Loss: 0.045797 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.415s | Train Loss: 0.042548 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.415s | Train Loss: 0.039577 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.412s | Train Loss: 0.037525 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.414s | Train Loss: 0.035272 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.416s | Train Loss: 0.033896 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.416s | Train Loss: 0.032375 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.414s | Train Loss: 0.030983 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.413s | Train Loss: 0.030015 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.420s | Train Loss: 0.028875 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.418s | Train Loss: 0.028010 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.413s | Train Loss: 0.027166 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.414s | Train Loss: 0.026599 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.413s | Train Loss: 0.025937 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.412s | Train Loss: 0.025156 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.416s | Train Loss: 0.024756 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.475s | Train Loss: 0.024224 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.421s | Train Loss: 0.023590 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.417s | Train Loss: 0.023190 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.417s | Train Loss: 0.023046 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.413s | Train Loss: 0.022536 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.417s | Train Loss: 0.022141 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.398s | Train Loss: 0.021810 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.405s | Train Loss: 0.021435 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.415s | Train Loss: 0.021244 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.418s | Train Loss: 0.021104 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.417s | Train Loss: 0.020877 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.418s | Train Loss: 0.020581 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.413s | Train Loss: 0.020406 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.420s | Train Loss: 0.020319 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.421s | Train Loss: 0.020208 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.421s | Train Loss: 0.019999 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 611.397s | Train Loss: 0.019788 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.562s | Train Loss: 0.019876 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.402s | Train Loss: 0.019574 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.457s | Train Loss: 0.019604 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.419s | Train Loss: 0.019338 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.453s | Train Loss: 0.019247 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.400s | Train Loss: 0.019187 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.484s | Train Loss: 0.019019 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.452s | Train Loss: 0.018985 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.456s | Train Loss: 0.018877 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.456s | Train Loss: 0.018774 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.455s | Train Loss: 0.018778 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.452s | Train Loss: 0.018596 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.433s | Train Loss: 0.018510 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.411s | Train Loss: 0.018564 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.412s | Train Loss: 0.018334 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.419s | Train Loss: 0.018427 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.420s | Train Loss: 0.018409 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.423s | Train Loss: 0.018022 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.414s | Train Loss: 0.017963 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.420s | Train Loss: 0.018070 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.406s | Train Loss: 0.017997 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.405s | Train Loss: 0.017928 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.413s | Train Loss: 0.017779 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.411s | Train Loss: 0.017697 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.426s | Train Loss: 0.017875 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.420s | Train Loss: 0.017703 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.424s | Train Loss: 0.017651 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.420s | Train Loss: 0.017596 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.418s | Train Loss: 0.017398 |\n",
      "INFO:root:Pretraining Time: 640.586s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018991\n",
      "INFO:root:Test AUC: 52.82%\n",
      "INFO:root:Test AUC: 52.82%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.704s | Train Loss: 0.869077 || Validation Loss: 0.016163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.701s | Train Loss: 0.669511 || Validation Loss: 0.017313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.751s | Train Loss: 0.598582 || Validation Loss: 0.016569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.790s | Train Loss: 0.554201 || Validation Loss: 0.018217 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.755s | Train Loss: 0.528985 || Validation Loss: 0.018534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.776s | Train Loss: 0.500242 || Validation Loss: 0.020683 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.771s | Train Loss: 0.490730 || Validation Loss: 0.020016 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.869s | Train Loss: 0.478901 || Validation Loss: 0.019815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.841s | Train Loss: 0.472230 || Validation Loss: 0.019078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.904s | Train Loss: 0.456853 || Validation Loss: 0.021109 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.914s | Train Loss: 0.453620 || Validation Loss: 0.020374 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.907s | Train Loss: 0.444354 || Validation Loss: 0.021612 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.985s | Train Loss: 0.435357 || Validation Loss: 0.021608 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.975s | Train Loss: 0.422740 || Validation Loss: 0.022491 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.981s | Train Loss: 0.424572 || Validation Loss: 0.022379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.001s | Train Loss: 0.412854 || Validation Loss: 0.021659 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.043s | Train Loss: 0.414027 || Validation Loss: 0.022296 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.080s | Train Loss: 0.409589 || Validation Loss: 0.023476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.040s | Train Loss: 0.412867 || Validation Loss: 0.023227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.049s | Train Loss: 0.396528 || Validation Loss: 0.024197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.200s | Train Loss: 0.399428 || Validation Loss: 0.023350 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.100s | Train Loss: 0.398463 || Validation Loss: 0.022632 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.140s | Train Loss: 0.386120 || Validation Loss: 0.023412 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.219s | Train Loss: 0.383093 || Validation Loss: 0.023187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.226s | Train Loss: 0.383852 || Validation Loss: 0.022916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.164s | Train Loss: 0.385917 || Validation Loss: 0.023824 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.124s | Train Loss: 0.382892 || Validation Loss: 0.024210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.304s | Train Loss: 0.380587 || Validation Loss: 0.025186 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.206s | Train Loss: 0.369078 || Validation Loss: 0.022741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.236s | Train Loss: 0.380514 || Validation Loss: 0.024225 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.616s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.898%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021433\n",
      "INFO:root:Test AUC: 85.98%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6634171531635619\n",
      "validation AUC 0.7455358167219841\n",
      "validation AUC 0.7870144440993835\n",
      "validation AUC 0.8090935470365003\n",
      "validation AUC 0.821733496266424\n",
      "validation AUC 0.8311106880034942\n",
      "validation AUC 0.8360799193408831\n",
      "validation AUC 0.8388144834987673\n",
      "validation AUC 0.8421279589182133\n",
      "validation AUC 0.8451494962902638\n",
      "validation AUC 0.8472446771985698\n",
      "validation AUC 0.8479816489546818\n",
      "validation AUC 0.8507635732580565\n",
      "validation AUC 0.8512515743257352\n",
      "validation AUC 0.8524140450608902\n",
      "validation AUC 0.8537129449432457\n",
      "validation AUC 0.8533080662522672\n",
      "validation AUC 0.8543702505490052\n",
      "validation AUC 0.8532388831902986\n",
      "validation AUC 0.8550832016650337\n",
      "validation AUC 0.8554086962379868\n",
      "validation AUC 0.8549424648731397\n",
      "validation AUC 0.8544678683230442\n",
      "validation AUC 0.8541815150478614\n",
      "validation AUC 0.8546457748471014\n",
      "validation AUC 0.8553642176114608\n",
      "validation AUC 0.8553788965958804\n",
      "validation AUC 0.8550465055343256\n",
      "validation AUC 0.8570168152994767\n",
      "validation AUC 0.8544790272247402\n",
      "validation AUC 0.8544790272247402\n",
      "train auc 0.8984884028563247\n",
      "Test AUC 0.859803660026428\n",
      "test false positive rates [0.        0.        0.        ... 0.9994505 0.9994505 1.       ] true positive rate [0.00000000e+00 2.79955207e-04 2.23964166e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.14555511e+01 5.04555511e+01 4.31736412e+01 ... 1.86936688e-02\n",
      " 1.85100567e-02 8.85020010e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.420s | Train Loss: 0.200256 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.406s | Train Loss: 0.154028 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.423s | Train Loss: 0.118250 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.418s | Train Loss: 0.093475 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.420s | Train Loss: 0.076179 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.418s | Train Loss: 0.064376 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.414s | Train Loss: 0.055868 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.419s | Train Loss: 0.049662 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.422s | Train Loss: 0.044796 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.420s | Train Loss: 0.041150 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.419s | Train Loss: 0.038122 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.417s | Train Loss: 0.035691 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.419s | Train Loss: 0.033531 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.415s | Train Loss: 0.031687 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.420s | Train Loss: 0.030448 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.418s | Train Loss: 0.029167 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.418s | Train Loss: 0.027988 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.419s | Train Loss: 0.026926 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.423s | Train Loss: 0.026237 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.418s | Train Loss: 0.025496 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.420s | Train Loss: 0.024503 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.401s | Train Loss: 0.023970 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.403s | Train Loss: 0.023420 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.419s | Train Loss: 0.023008 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.420s | Train Loss: 0.022628 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.416s | Train Loss: 0.021947 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.419s | Train Loss: 0.021732 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.417s | Train Loss: 0.021438 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.418s | Train Loss: 0.021290 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.407s | Train Loss: 0.020915 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.455s | Train Loss: 0.020474 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.417s | Train Loss: 0.020311 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.418s | Train Loss: 0.020223 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.421s | Train Loss: 0.020003 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.420s | Train Loss: 0.019704 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.417s | Train Loss: 0.019604 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.424s | Train Loss: 0.019559 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.414s | Train Loss: 0.019414 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.420s | Train Loss: 0.019150 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.415s | Train Loss: 0.019378 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.419s | Train Loss: 0.019124 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.417s | Train Loss: 0.018999 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.415s | Train Loss: 0.018883 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.422s | Train Loss: 0.018867 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.419s | Train Loss: 0.018607 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.421s | Train Loss: 0.018418 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.418s | Train Loss: 0.018352 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.420s | Train Loss: 0.018286 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.419s | Train Loss: 0.018174 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.420s | Train Loss: 0.018200 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.421s | Train Loss: 0.017746 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.418s | Train Loss: 0.018043 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.421s | Train Loss: 0.017882 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.416s | Train Loss: 0.017657 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.419s | Train Loss: 0.017617 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.421s | Train Loss: 0.017499 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.424s | Train Loss: 0.017414 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.424s | Train Loss: 0.017407 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.415s | Train Loss: 0.017238 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.402s | Train Loss: 0.017272 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.405s | Train Loss: 0.017036 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.423s | Train Loss: 0.017065 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.417s | Train Loss: 0.017050 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.416s | Train Loss: 0.016878 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.422s | Train Loss: 0.016892 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.417s | Train Loss: 0.016982 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.420s | Train Loss: 0.016664 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.419s | Train Loss: 0.016478 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.417s | Train Loss: 0.016598 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.417s | Train Loss: 0.016639 |\n",
      "INFO:root:Pretraining Time: 29.275s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018040\n",
      "INFO:root:Test AUC: 50.97%\n",
      "INFO:root:Test AUC: 50.97%\n",
      "INFO:root:Test Time: 0.218s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.670s | Train Loss: 0.832289 || Validation Loss: 0.016315 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.680s | Train Loss: 0.655132 || Validation Loss: 0.015969 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.722s | Train Loss: 0.581687 || Validation Loss: 0.015622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.772s | Train Loss: 0.536933 || Validation Loss: 0.017965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.749s | Train Loss: 0.516671 || Validation Loss: 0.017805 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.793s | Train Loss: 0.490058 || Validation Loss: 0.018206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.777s | Train Loss: 0.472917 || Validation Loss: 0.021002 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.851s | Train Loss: 0.462600 || Validation Loss: 0.021444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.830s | Train Loss: 0.450582 || Validation Loss: 0.022492 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.879s | Train Loss: 0.450147 || Validation Loss: 0.020907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.894s | Train Loss: 0.438745 || Validation Loss: 0.019693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.874s | Train Loss: 0.439813 || Validation Loss: 0.021779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.960s | Train Loss: 0.427479 || Validation Loss: 0.020859 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.939s | Train Loss: 0.423582 || Validation Loss: 0.022241 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.953s | Train Loss: 0.422837 || Validation Loss: 0.022414 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.956s | Train Loss: 0.414729 || Validation Loss: 0.021756 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.016s | Train Loss: 0.410537 || Validation Loss: 0.022173 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.035s | Train Loss: 0.400963 || Validation Loss: 0.021390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.010s | Train Loss: 0.404037 || Validation Loss: 0.024694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.060s | Train Loss: 0.399622 || Validation Loss: 0.022330 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.157s | Train Loss: 0.396075 || Validation Loss: 0.022776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.061s | Train Loss: 0.394003 || Validation Loss: 0.023776 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.146s | Train Loss: 0.382529 || Validation Loss: 0.022976 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.162s | Train Loss: 0.387131 || Validation Loss: 0.023724 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.189s | Train Loss: 0.389061 || Validation Loss: 0.025197 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.166s | Train Loss: 0.384265 || Validation Loss: 0.025013 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.138s | Train Loss: 0.370200 || Validation Loss: 0.020930 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.307s | Train Loss: 0.379097 || Validation Loss: 0.024303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.213s | Train Loss: 0.376189 || Validation Loss: 0.023207 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.248s | Train Loss: 0.365984 || Validation Loss: 0.023960 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 30.079s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.903%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021189\n",
      "INFO:root:Test AUC: 85.72%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 4432})\n",
      "class weights 0.1482670948748829 0.8517329051251171\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6616392878907238\n",
      "validation AUC 0.7531494310503329\n",
      "validation AUC 0.7947277355698957\n",
      "validation AUC 0.8159733611420754\n",
      "validation AUC 0.829354909054685\n",
      "validation AUC 0.8386645327639616\n",
      "validation AUC 0.8442632655767519\n",
      "validation AUC 0.8486096976975306\n",
      "validation AUC 0.8501839409515835\n",
      "validation AUC 0.8534538237517194\n",
      "validation AUC 0.8533362854490688\n",
      "validation AUC 0.8547366211846081\n",
      "validation AUC 0.8558200590118056\n",
      "validation AUC 0.8568779010749795\n",
      "validation AUC 0.8577520735761679\n",
      "validation AUC 0.8581071602580052\n",
      "validation AUC 0.8588280774569327\n",
      "validation AUC 0.8598384556080052\n",
      "validation AUC 0.8587717295265285\n",
      "validation AUC 0.8590796795601913\n",
      "validation AUC 0.8584175900476008\n",
      "validation AUC 0.8590733205294204\n",
      "validation AUC 0.8598296540708547\n",
      "validation AUC 0.8591875728893925\n",
      "validation AUC 0.8591966298520468\n",
      "validation AUC 0.8586108540300718\n",
      "validation AUC 0.8594873838958051\n",
      "validation AUC 0.8587627577057085\n",
      "validation AUC 0.8577969619477728\n",
      "validation AUC 0.8586731193179798\n",
      "validation AUC 0.8586731193179798\n",
      "train auc 0.9033589405723301\n",
      "Test AUC 0.8571979706363988\n",
      "test false positive rates [0.         0.         0.         ... 0.98618664 0.98618664 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 3.63941769e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.15469666e+01 5.05469666e+01 3.47806664e+01 ... 2.91911829e-02\n",
      " 2.91815959e-02 6.72255969e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m6\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be95c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below multiplier = (0.50 share of minority)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b046d82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.472s | Train Loss: 0.185572 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.447s | Train Loss: 0.137374 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.444s | Train Loss: 0.104406 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.445s | Train Loss: 0.082304 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.445s | Train Loss: 0.068119 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.446s | Train Loss: 0.058808 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.442s | Train Loss: 0.052177 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.445s | Train Loss: 0.047310 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.446s | Train Loss: 0.043627 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.445s | Train Loss: 0.040800 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.445s | Train Loss: 0.038020 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.444s | Train Loss: 0.036138 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.448s | Train Loss: 0.033965 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.458s | Train Loss: 0.032410 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.464s | Train Loss: 0.031241 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.468s | Train Loss: 0.030033 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.470s | Train Loss: 0.029189 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.468s | Train Loss: 0.028153 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.470s | Train Loss: 0.027397 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.468s | Train Loss: 0.026880 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.467s | Train Loss: 0.025988 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.465s | Train Loss: 0.025236 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.466s | Train Loss: 0.024709 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.461s | Train Loss: 0.023951 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.463s | Train Loss: 0.023489 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.451s | Train Loss: 0.023018 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.442s | Train Loss: 0.022440 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.441s | Train Loss: 0.022107 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.436s | Train Loss: 0.021685 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.441s | Train Loss: 0.021396 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.439s | Train Loss: 0.020957 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.441s | Train Loss: 0.020941 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.441s | Train Loss: 0.020545 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.443s | Train Loss: 0.020450 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.442s | Train Loss: 0.020409 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.444s | Train Loss: 0.020112 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.446s | Train Loss: 0.019686 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.447s | Train Loss: 0.019546 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.447s | Train Loss: 0.019387 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.446s | Train Loss: 0.019174 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.442s | Train Loss: 0.019237 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.444s | Train Loss: 0.018919 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.445s | Train Loss: 0.018892 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.444s | Train Loss: 0.018818 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.446s | Train Loss: 0.018602 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.445s | Train Loss: 0.018503 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.445s | Train Loss: 0.018410 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.447s | Train Loss: 0.018255 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.445s | Train Loss: 0.018212 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.445s | Train Loss: 0.018060 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.448s | Train Loss: 0.017935 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.447s | Train Loss: 0.017888 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.443s | Train Loss: 0.017749 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.447s | Train Loss: 0.017753 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.435s | Train Loss: 0.017569 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.442s | Train Loss: 0.017504 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.448s | Train Loss: 0.017388 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.443s | Train Loss: 0.017387 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.448s | Train Loss: 0.017165 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.444s | Train Loss: 0.017196 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.446s | Train Loss: 0.017046 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.442s | Train Loss: 0.016802 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.447s | Train Loss: 0.016756 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.442s | Train Loss: 0.016960 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.443s | Train Loss: 0.016863 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.443s | Train Loss: 0.016715 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.445s | Train Loss: 0.016766 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.445s | Train Loss: 0.016416 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.443s | Train Loss: 0.016597 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.442s | Train Loss: 0.016317 |\n",
      "INFO:root:Pretraining Time: 31.397s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017854\n",
      "INFO:root:Test AUC: 49.64%\n",
      "INFO:root:Test AUC: 49.64%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.697s | Train Loss: 0.862044 || Validation Loss: 0.013673 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.739s | Train Loss: 0.671951 || Validation Loss: 0.015043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.716s | Train Loss: 0.612960 || Validation Loss: 0.016226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.794s | Train Loss: 0.574422 || Validation Loss: 0.015977 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.776s | Train Loss: 0.546790 || Validation Loss: 0.017682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.822s | Train Loss: 0.519989 || Validation Loss: 0.018055 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.838s | Train Loss: 0.501857 || Validation Loss: 0.018318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.839s | Train Loss: 0.491619 || Validation Loss: 0.019869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.864s | Train Loss: 0.476413 || Validation Loss: 0.019077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.910s | Train Loss: 0.472872 || Validation Loss: 0.019440 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.930s | Train Loss: 0.462354 || Validation Loss: 0.020998 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.896s | Train Loss: 0.457001 || Validation Loss: 0.021180 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.989s | Train Loss: 0.454403 || Validation Loss: 0.021230 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.976s | Train Loss: 0.439877 || Validation Loss: 0.019085 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.983s | Train Loss: 0.438395 || Validation Loss: 0.021547 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.029s | Train Loss: 0.428734 || Validation Loss: 0.021297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.021s | Train Loss: 0.425239 || Validation Loss: 0.021096 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.073s | Train Loss: 0.422143 || Validation Loss: 0.022611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.091s | Train Loss: 0.418609 || Validation Loss: 0.020916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.135s | Train Loss: 0.411574 || Validation Loss: 0.023712 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.136s | Train Loss: 0.412587 || Validation Loss: 0.020551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.178s | Train Loss: 0.403100 || Validation Loss: 0.021775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.287s | Train Loss: 0.391645 || Validation Loss: 0.021741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.220s | Train Loss: 0.389802 || Validation Loss: 0.023945 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.275s | Train Loss: 0.389522 || Validation Loss: 0.022189 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.257s | Train Loss: 0.386313 || Validation Loss: 0.021995 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.251s | Train Loss: 0.380728 || Validation Loss: 0.025621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.315s | Train Loss: 0.387853 || Validation Loss: 0.022779 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.325s | Train Loss: 0.381055 || Validation Loss: 0.023108 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.373s | Train Loss: 0.379828 || Validation Loss: 0.025413 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.570s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.892%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020099\n",
      "INFO:root:Test AUC: 86.07%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6968325694336983\n",
      "validation AUC 0.7610687817246458\n",
      "validation AUC 0.7928566426488647\n",
      "validation AUC 0.8101596691430879\n",
      "validation AUC 0.8232393945734425\n",
      "validation AUC 0.8303069810131581\n",
      "validation AUC 0.8355486475964566\n",
      "validation AUC 0.8389977007447569\n",
      "validation AUC 0.8430379122753878\n",
      "validation AUC 0.8445906066632426\n",
      "validation AUC 0.8484266666993043\n",
      "validation AUC 0.8502171702132142\n",
      "validation AUC 0.8501643556689455\n",
      "validation AUC 0.8526555472140208\n",
      "validation AUC 0.8549366725677053\n",
      "validation AUC 0.854443054799626\n",
      "validation AUC 0.8566227682462677\n",
      "validation AUC 0.856053866471635\n",
      "validation AUC 0.8569939015032431\n",
      "validation AUC 0.8576402690439402\n",
      "validation AUC 0.8583976908044477\n",
      "validation AUC 0.8606861117490837\n",
      "validation AUC 0.860244558212856\n",
      "validation AUC 0.8603599626482771\n",
      "validation AUC 0.8600062728246635\n",
      "validation AUC 0.8610247846816216\n",
      "validation AUC 0.8617326805012214\n",
      "validation AUC 0.8592706314097167\n",
      "validation AUC 0.8598286829218038\n",
      "validation AUC 0.8587514498058022\n",
      "validation AUC 0.8587514498058022\n",
      "train auc 0.8917315790724851\n",
      "Test AUC 0.8606624819706725\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.89976315e-01\n",
      " 9.89976315e-01 1.00000000e+00] true positive rate [0.         0.         0.00923852 ... 0.99972004 1.         1.        ] tresholds [4.25398979e+01 4.15398979e+01 2.96754494e+01 ... 4.96927053e-02\n",
      " 4.96920124e-02 1.49215683e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.470s | Train Loss: 0.189955 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.446s | Train Loss: 0.142264 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.446s | Train Loss: 0.106903 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.443s | Train Loss: 0.083210 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.445s | Train Loss: 0.068716 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.442s | Train Loss: 0.059240 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.442s | Train Loss: 0.052429 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.428s | Train Loss: 0.047420 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.442s | Train Loss: 0.043530 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.443s | Train Loss: 0.040398 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.443s | Train Loss: 0.038268 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.442s | Train Loss: 0.035758 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.446s | Train Loss: 0.033965 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.441s | Train Loss: 0.032439 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.450s | Train Loss: 0.030913 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.442s | Train Loss: 0.029680 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.442s | Train Loss: 0.028639 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.442s | Train Loss: 0.027635 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.442s | Train Loss: 0.026835 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.440s | Train Loss: 0.026086 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.444s | Train Loss: 0.025399 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.442s | Train Loss: 0.024842 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.441s | Train Loss: 0.024625 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.444s | Train Loss: 0.024127 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.444s | Train Loss: 0.023500 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.442s | Train Loss: 0.023122 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.440s | Train Loss: 0.022567 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.437s | Train Loss: 0.022292 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.442s | Train Loss: 0.021957 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.444s | Train Loss: 0.021612 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.443s | Train Loss: 0.021118 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.443s | Train Loss: 0.020978 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.443s | Train Loss: 0.020606 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.444s | Train Loss: 0.020481 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.445s | Train Loss: 0.020349 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.445s | Train Loss: 0.020160 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.446s | Train Loss: 0.019917 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.446s | Train Loss: 0.019521 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.445s | Train Loss: 0.019556 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.436s | Train Loss: 0.019374 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.441s | Train Loss: 0.019264 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.442s | Train Loss: 0.018928 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.444s | Train Loss: 0.018858 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.446s | Train Loss: 0.018736 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.445s | Train Loss: 0.018803 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.446s | Train Loss: 0.018463 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.445s | Train Loss: 0.018549 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.443s | Train Loss: 0.018357 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.448s | Train Loss: 0.018277 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.442s | Train Loss: 0.018138 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.446s | Train Loss: 0.018189 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.444s | Train Loss: 0.017910 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.445s | Train Loss: 0.017862 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.444s | Train Loss: 0.017807 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.445s | Train Loss: 0.017669 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.443s | Train Loss: 0.017834 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.447s | Train Loss: 0.017604 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.445s | Train Loss: 0.017656 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.445s | Train Loss: 0.017535 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.449s | Train Loss: 0.017358 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.447s | Train Loss: 0.017447 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.447s | Train Loss: 0.017395 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.442s | Train Loss: 0.017424 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.445s | Train Loss: 0.017159 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.443s | Train Loss: 0.017140 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.446s | Train Loss: 0.017189 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.446s | Train Loss: 0.017098 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.447s | Train Loss: 0.016939 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.444s | Train Loss: 0.016869 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.444s | Train Loss: 0.016820 |\n",
      "INFO:root:Pretraining Time: 31.089s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018156\n",
      "INFO:root:Test AUC: 54.35%\n",
      "INFO:root:Test AUC: 54.35%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.691s | Train Loss: 0.828800 || Validation Loss: 0.014820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.732s | Train Loss: 0.647040 || Validation Loss: 0.015657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.700s | Train Loss: 0.581598 || Validation Loss: 0.015425 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.785s | Train Loss: 0.541533 || Validation Loss: 0.016088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.770s | Train Loss: 0.518941 || Validation Loss: 0.018382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.814s | Train Loss: 0.497848 || Validation Loss: 0.019263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.829s | Train Loss: 0.485792 || Validation Loss: 0.019874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.843s | Train Loss: 0.468393 || Validation Loss: 0.019758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.857s | Train Loss: 0.458637 || Validation Loss: 0.019780 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.933s | Train Loss: 0.453734 || Validation Loss: 0.019848 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.925s | Train Loss: 0.442839 || Validation Loss: 0.019871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.894s | Train Loss: 0.437368 || Validation Loss: 0.020208 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.986s | Train Loss: 0.440177 || Validation Loss: 0.018826 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.973s | Train Loss: 0.434146 || Validation Loss: 0.019017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.992s | Train Loss: 0.415572 || Validation Loss: 0.022261 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.036s | Train Loss: 0.408982 || Validation Loss: 0.023582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.998s | Train Loss: 0.408210 || Validation Loss: 0.022012 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.060s | Train Loss: 0.401790 || Validation Loss: 0.023218 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.078s | Train Loss: 0.394254 || Validation Loss: 0.023690 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.138s | Train Loss: 0.393493 || Validation Loss: 0.021808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.119s | Train Loss: 0.390593 || Validation Loss: 0.022637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.141s | Train Loss: 0.381958 || Validation Loss: 0.022667 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.237s | Train Loss: 0.388047 || Validation Loss: 0.023093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.172s | Train Loss: 0.382662 || Validation Loss: 0.023798 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.239s | Train Loss: 0.378432 || Validation Loss: 0.023253 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.205s | Train Loss: 0.379943 || Validation Loss: 0.023957 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.220s | Train Loss: 0.374306 || Validation Loss: 0.022861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.314s | Train Loss: 0.366370 || Validation Loss: 0.024478 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.341s | Train Loss: 0.367269 || Validation Loss: 0.023442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.386s | Train Loss: 0.369818 || Validation Loss: 0.025452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.315s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.905%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020740\n",
      "INFO:root:Test AUC: 85.82%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6646133507079437\n",
      "validation AUC 0.755559405267257\n",
      "validation AUC 0.7984551679284333\n",
      "validation AUC 0.8176662388998995\n",
      "validation AUC 0.8278167978666862\n",
      "validation AUC 0.8356082788088572\n",
      "validation AUC 0.8405040460464069\n",
      "validation AUC 0.8441850601409991\n",
      "validation AUC 0.8463155749106596\n",
      "validation AUC 0.8498044292059481\n",
      "validation AUC 0.8509314915806433\n",
      "validation AUC 0.8520703620635232\n",
      "validation AUC 0.8523536821608486\n",
      "validation AUC 0.85345995396381\n",
      "validation AUC 0.8544879484925957\n",
      "validation AUC 0.8553988570347271\n",
      "validation AUC 0.8556126907522983\n",
      "validation AUC 0.8564174833010256\n",
      "validation AUC 0.857189038606075\n",
      "validation AUC 0.858100540480366\n",
      "validation AUC 0.8577618941546512\n",
      "validation AUC 0.8562721887602988\n",
      "validation AUC 0.8580651959762821\n",
      "validation AUC 0.8566561518274737\n",
      "validation AUC 0.8561105549693777\n",
      "validation AUC 0.855822403072939\n",
      "validation AUC 0.8547307091484688\n",
      "validation AUC 0.8558323034718923\n",
      "validation AUC 0.8549362255730737\n",
      "validation AUC 0.8559274947036458\n",
      "validation AUC 0.8559274947036458\n",
      "train auc 0.9054218987915665\n",
      "Test AUC 0.8582093635269422\n",
      "test false positive rates [0.         0.         0.         ... 0.99846518 0.99846518 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.23964166e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [3.62139778e+01 3.52139778e+01 3.19890308e+01 ... 2.69643720e-02\n",
      " 2.68577300e-02 8.61436408e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.468s | Train Loss: 0.188580 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.453s | Train Loss: 0.140295 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.447s | Train Loss: 0.106959 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.447s | Train Loss: 0.084485 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 55.920s | Train Loss: 0.069416 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.396s | Train Loss: 0.059039 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.432s | Train Loss: 0.051799 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.481s | Train Loss: 0.046137 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.448s | Train Loss: 0.042128 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.423s | Train Loss: 0.038526 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.490s | Train Loss: 0.035768 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.473s | Train Loss: 0.033374 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.490s | Train Loss: 0.031542 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.482s | Train Loss: 0.029903 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.493s | Train Loss: 0.028446 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.482s | Train Loss: 0.027030 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.467s | Train Loss: 0.026048 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.463s | Train Loss: 0.025200 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.465s | Train Loss: 0.024303 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.462s | Train Loss: 0.023731 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.464s | Train Loss: 0.023172 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.464s | Train Loss: 0.022488 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.464s | Train Loss: 0.021942 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.463s | Train Loss: 0.021683 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.463s | Train Loss: 0.021279 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.463s | Train Loss: 0.020800 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.442s | Train Loss: 0.020805 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.439s | Train Loss: 0.020340 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.443s | Train Loss: 0.020112 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.441s | Train Loss: 0.019915 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.442s | Train Loss: 0.019596 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.442s | Train Loss: 0.019415 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.443s | Train Loss: 0.019245 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.443s | Train Loss: 0.019042 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.438s | Train Loss: 0.018868 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.438s | Train Loss: 0.018645 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.447s | Train Loss: 0.018440 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.445s | Train Loss: 0.018314 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.443s | Train Loss: 0.018314 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.444s | Train Loss: 0.018119 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.442s | Train Loss: 0.018048 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.437s | Train Loss: 0.017901 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.444s | Train Loss: 0.017696 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.444s | Train Loss: 0.017693 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.445s | Train Loss: 0.017628 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.444s | Train Loss: 0.017443 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.442s | Train Loss: 0.017270 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.444s | Train Loss: 0.017552 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.446s | Train Loss: 0.017261 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.445s | Train Loss: 0.017156 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.444s | Train Loss: 0.016979 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.443s | Train Loss: 0.016935 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.444s | Train Loss: 0.017024 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.446s | Train Loss: 0.016652 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.446s | Train Loss: 0.016608 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.445s | Train Loss: 0.016576 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.443s | Train Loss: 0.016490 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.438s | Train Loss: 0.016475 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.436s | Train Loss: 0.016112 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.443s | Train Loss: 0.016214 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.444s | Train Loss: 0.016096 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.445s | Train Loss: 0.015971 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.444s | Train Loss: 0.016003 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.446s | Train Loss: 0.015870 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.448s | Train Loss: 0.015628 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.446s | Train Loss: 0.015737 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.478s | Train Loss: 0.015702 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.448s | Train Loss: 0.015629 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.445s | Train Loss: 0.015687 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.444s | Train Loss: 0.015500 |\n",
      "INFO:root:Pretraining Time: 87.010s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017369\n",
      "INFO:root:Test AUC: 49.15%\n",
      "INFO:root:Test AUC: 49.15%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.695s | Train Loss: 0.812584 || Validation Loss: 0.013530 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.732s | Train Loss: 0.655115 || Validation Loss: 0.014509 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.717s | Train Loss: 0.595347 || Validation Loss: 0.015652 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.786s | Train Loss: 0.546096 || Validation Loss: 0.017595 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.771s | Train Loss: 0.521632 || Validation Loss: 0.016194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.812s | Train Loss: 0.500653 || Validation Loss: 0.019272 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.825s | Train Loss: 0.480561 || Validation Loss: 0.018331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.843s | Train Loss: 0.465778 || Validation Loss: 0.018819 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.863s | Train Loss: 0.449082 || Validation Loss: 0.020752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.907s | Train Loss: 0.446231 || Validation Loss: 0.020884 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.926s | Train Loss: 0.431440 || Validation Loss: 0.020602 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.904s | Train Loss: 0.432528 || Validation Loss: 0.020174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.989s | Train Loss: 0.422857 || Validation Loss: 0.020911 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.976s | Train Loss: 0.406659 || Validation Loss: 0.021111 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.992s | Train Loss: 0.405240 || Validation Loss: 0.022501 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.040s | Train Loss: 0.403631 || Validation Loss: 0.022689 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.017s | Train Loss: 0.398175 || Validation Loss: 0.024274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.070s | Train Loss: 0.391464 || Validation Loss: 0.025733 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.081s | Train Loss: 0.386999 || Validation Loss: 0.023619 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.142s | Train Loss: 0.382872 || Validation Loss: 0.024473 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.125s | Train Loss: 0.384105 || Validation Loss: 0.024695 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.138s | Train Loss: 0.383869 || Validation Loss: 0.023269 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.230s | Train Loss: 0.376625 || Validation Loss: 0.024760 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.174s | Train Loss: 0.379293 || Validation Loss: 0.023811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.230s | Train Loss: 0.373157 || Validation Loss: 0.024769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.202s | Train Loss: 0.365731 || Validation Loss: 0.024604 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.221s | Train Loss: 0.357214 || Validation Loss: 0.025679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.311s | Train Loss: 0.362340 || Validation Loss: 0.023039 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.333s | Train Loss: 0.359681 || Validation Loss: 0.024439 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.378s | Train Loss: 0.365908 || Validation Loss: 0.026814 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.254s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.908%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021288\n",
      "INFO:root:Test AUC: 87.10%\n",
      "INFO:root:Test Time: 0.191s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6870392203732492\n",
      "validation AUC 0.7594942032246194\n",
      "validation AUC 0.7942291077369875\n",
      "validation AUC 0.8153452245967097\n",
      "validation AUC 0.8294200159513228\n",
      "validation AUC 0.8378802806828289\n",
      "validation AUC 0.845086509957444\n",
      "validation AUC 0.8494001491684942\n",
      "validation AUC 0.8532458461959586\n",
      "validation AUC 0.8548721962527801\n",
      "validation AUC 0.8576621478347475\n",
      "validation AUC 0.8593750844766642\n",
      "validation AUC 0.8609137623899994\n",
      "validation AUC 0.8629104767666025\n",
      "validation AUC 0.8618790100473752\n",
      "validation AUC 0.8643362486252255\n",
      "validation AUC 0.8665282943344708\n",
      "validation AUC 0.8658234742636456\n",
      "validation AUC 0.8663836942318323\n",
      "validation AUC 0.8667480587129576\n",
      "validation AUC 0.865887197605471\n",
      "validation AUC 0.8658400609572965\n",
      "validation AUC 0.8648992064355308\n",
      "validation AUC 0.8674363027328614\n",
      "validation AUC 0.8659499471375635\n",
      "validation AUC 0.8669860753722347\n",
      "validation AUC 0.8661771587658946\n",
      "validation AUC 0.8655000071306285\n",
      "validation AUC 0.8653201450050585\n",
      "validation AUC 0.8653388522625272\n",
      "validation AUC 0.8653388522625272\n",
      "train auc 0.9084407937180267\n",
      "Test AUC 0.8709592610137376\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.97612506e-01\n",
      " 9.97612506e-01 1.00000000e+00] true positive rate [0.         0.         0.00391937 ... 0.99972004 1.         1.        ] tresholds [4.66179314e+01 4.56179314e+01 3.48700104e+01 ... 1.55234309e-02\n",
      " 1.54822655e-02 4.63920645e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.478s | Train Loss: 0.191991 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.456s | Train Loss: 0.141349 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.454s | Train Loss: 0.106185 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.452s | Train Loss: 0.083789 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.455s | Train Loss: 0.069795 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.453s | Train Loss: 0.060123 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.451s | Train Loss: 0.053089 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.452s | Train Loss: 0.047603 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.454s | Train Loss: 0.043252 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.450s | Train Loss: 0.040167 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.455s | Train Loss: 0.036889 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.455s | Train Loss: 0.034492 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.453s | Train Loss: 0.032571 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.452s | Train Loss: 0.030713 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.453s | Train Loss: 0.029108 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.453s | Train Loss: 0.027695 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.454s | Train Loss: 0.026511 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.452s | Train Loss: 0.025685 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.454s | Train Loss: 0.024719 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.454s | Train Loss: 0.024103 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.455s | Train Loss: 0.023513 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.454s | Train Loss: 0.023020 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.454s | Train Loss: 0.022760 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.456s | Train Loss: 0.022183 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.450s | Train Loss: 0.021846 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.455s | Train Loss: 0.021453 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.454s | Train Loss: 0.021117 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.454s | Train Loss: 0.020790 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.456s | Train Loss: 0.020488 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.455s | Train Loss: 0.020090 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.453s | Train Loss: 0.019991 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.453s | Train Loss: 0.019640 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.455s | Train Loss: 0.019481 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.454s | Train Loss: 0.019308 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.455s | Train Loss: 0.019255 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.457s | Train Loss: 0.018877 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.455s | Train Loss: 0.018762 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.454s | Train Loss: 0.018774 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.454s | Train Loss: 0.018542 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.452s | Train Loss: 0.018457 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.455s | Train Loss: 0.018459 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.439s | Train Loss: 0.018309 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.449s | Train Loss: 0.018130 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.454s | Train Loss: 0.017986 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.454s | Train Loss: 0.017876 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.459s | Train Loss: 0.017653 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.458s | Train Loss: 0.017701 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.456s | Train Loss: 0.017642 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.456s | Train Loss: 0.017602 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.454s | Train Loss: 0.017310 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.454s | Train Loss: 0.017219 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.453s | Train Loss: 0.017260 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.453s | Train Loss: 0.017186 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.456s | Train Loss: 0.017002 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.455s | Train Loss: 0.017119 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.455s | Train Loss: 0.016884 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.455s | Train Loss: 0.016783 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.454s | Train Loss: 0.016763 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.454s | Train Loss: 0.016506 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.455s | Train Loss: 0.016472 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.455s | Train Loss: 0.016480 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.452s | Train Loss: 0.016337 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.454s | Train Loss: 0.016291 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.453s | Train Loss: 0.015993 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.452s | Train Loss: 0.016162 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.451s | Train Loss: 0.016091 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.451s | Train Loss: 0.015947 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.453s | Train Loss: 0.015897 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.438s | Train Loss: 0.015822 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.450s | Train Loss: 0.015768 |\n",
      "INFO:root:Pretraining Time: 31.775s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017907\n",
      "INFO:root:Test AUC: 49.41%\n",
      "INFO:root:Test AUC: 49.41%\n",
      "INFO:root:Test Time: 0.235s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.713s | Train Loss: 0.822167 || Validation Loss: 0.016178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.757s | Train Loss: 0.649832 || Validation Loss: 0.014435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.742s | Train Loss: 0.576657 || Validation Loss: 0.017187 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.823s | Train Loss: 0.540880 || Validation Loss: 0.018127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.797s | Train Loss: 0.506101 || Validation Loss: 0.017622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.844s | Train Loss: 0.491931 || Validation Loss: 0.018729 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.854s | Train Loss: 0.475981 || Validation Loss: 0.018446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.878s | Train Loss: 0.460550 || Validation Loss: 0.020507 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.878s | Train Loss: 0.453455 || Validation Loss: 0.020463 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.935s | Train Loss: 0.441691 || Validation Loss: 0.021795 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.950s | Train Loss: 0.440631 || Validation Loss: 0.021541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.929s | Train Loss: 0.437881 || Validation Loss: 0.022671 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.013s | Train Loss: 0.427334 || Validation Loss: 0.021809 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.000s | Train Loss: 0.409248 || Validation Loss: 0.021222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.013s | Train Loss: 0.419672 || Validation Loss: 0.020391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.058s | Train Loss: 0.411355 || Validation Loss: 0.023980 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.040s | Train Loss: 0.403329 || Validation Loss: 0.024089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.088s | Train Loss: 0.394081 || Validation Loss: 0.021846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.091s | Train Loss: 0.400208 || Validation Loss: 0.021468 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.163s | Train Loss: 0.391465 || Validation Loss: 0.023749 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.142s | Train Loss: 0.389881 || Validation Loss: 0.025159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.152s | Train Loss: 0.388764 || Validation Loss: 0.024746 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.256s | Train Loss: 0.383085 || Validation Loss: 0.025622 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.203s | Train Loss: 0.376291 || Validation Loss: 0.024959 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.255s | Train Loss: 0.380673 || Validation Loss: 0.022924 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.241s | Train Loss: 0.377729 || Validation Loss: 0.025796 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.244s | Train Loss: 0.370254 || Validation Loss: 0.023850 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.335s | Train Loss: 0.370986 || Validation Loss: 0.025637 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.352s | Train Loss: 0.366699 || Validation Loss: 0.026219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.407s | Train Loss: 0.370020 || Validation Loss: 0.023630 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.986s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.905%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022603\n",
      "INFO:root:Test AUC: 85.74%\n",
      "INFO:root:Test Time: 0.205s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6383042311447554\n",
      "validation AUC 0.7397577752055484\n",
      "validation AUC 0.7861936741532591\n",
      "validation AUC 0.8082254302482971\n",
      "validation AUC 0.8222910076599981\n",
      "validation AUC 0.8296395036188473\n",
      "validation AUC 0.8341615407734668\n",
      "validation AUC 0.8392416241188086\n",
      "validation AUC 0.8431059778507775\n",
      "validation AUC 0.8458367824645283\n",
      "validation AUC 0.8471521306850532\n",
      "validation AUC 0.8488899899383636\n",
      "validation AUC 0.8491374573412802\n",
      "validation AUC 0.8515866127662094\n",
      "validation AUC 0.8517850837040019\n",
      "validation AUC 0.8532346473840278\n",
      "validation AUC 0.8540361566507586\n",
      "validation AUC 0.8545873090135616\n",
      "validation AUC 0.8550861071301391\n",
      "validation AUC 0.8555004046365688\n",
      "validation AUC 0.8554639958595527\n",
      "validation AUC 0.8534888942055234\n",
      "validation AUC 0.8532237651932943\n",
      "validation AUC 0.8544026816059367\n",
      "validation AUC 0.8547571297240149\n",
      "validation AUC 0.8543169038682702\n",
      "validation AUC 0.8537121094889937\n",
      "validation AUC 0.8548600848268099\n",
      "validation AUC 0.8549872468174515\n",
      "validation AUC 0.8542083054582514\n",
      "validation AUC 0.8542083054582514\n",
      "train auc 0.904878143893036\n",
      "Test AUC 0.8573552733694301\n",
      "test false positive rates [0.         0.         0.         ... 0.99939365 0.99939365 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 9.79843225e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.90033722e+01 7.80033722e+01 4.56908379e+01 ... 1.20854899e-02\n",
      " 1.20832231e-02 5.59338834e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.472s | Train Loss: 0.195768 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.454s | Train Loss: 0.146903 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.450s | Train Loss: 0.111342 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.452s | Train Loss: 0.087393 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.452s | Train Loss: 0.071508 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.449s | Train Loss: 0.060970 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.449s | Train Loss: 0.053464 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.451s | Train Loss: 0.047996 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.450s | Train Loss: 0.043938 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.455s | Train Loss: 0.040345 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.452s | Train Loss: 0.037673 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.452s | Train Loss: 0.035478 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.453s | Train Loss: 0.033223 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.451s | Train Loss: 0.031679 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.454s | Train Loss: 0.030216 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.452s | Train Loss: 0.028926 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.452s | Train Loss: 0.027746 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.449s | Train Loss: 0.026827 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.452s | Train Loss: 0.025909 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.454s | Train Loss: 0.025177 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.451s | Train Loss: 0.024563 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.450s | Train Loss: 0.023870 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.448s | Train Loss: 0.023362 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.434s | Train Loss: 0.022804 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.447s | Train Loss: 0.022337 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.450s | Train Loss: 0.021840 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.453s | Train Loss: 0.021524 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.452s | Train Loss: 0.021086 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.452s | Train Loss: 0.021012 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.452s | Train Loss: 0.020817 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.450s | Train Loss: 0.020403 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.451s | Train Loss: 0.020141 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.450s | Train Loss: 0.019884 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.453s | Train Loss: 0.019911 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.449s | Train Loss: 0.019511 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.452s | Train Loss: 0.019532 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.449s | Train Loss: 0.019207 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.454s | Train Loss: 0.019136 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.452s | Train Loss: 0.018955 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.450s | Train Loss: 0.018711 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.451s | Train Loss: 0.018689 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.451s | Train Loss: 0.018535 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.455s | Train Loss: 0.018246 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.451s | Train Loss: 0.018219 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.455s | Train Loss: 0.018181 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.452s | Train Loss: 0.018086 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.449s | Train Loss: 0.017806 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.454s | Train Loss: 0.017781 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.450s | Train Loss: 0.017740 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.453s | Train Loss: 0.017692 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.452s | Train Loss: 0.017584 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.454s | Train Loss: 0.017365 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.450s | Train Loss: 0.017160 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.454s | Train Loss: 0.017314 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.452s | Train Loss: 0.017206 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.452s | Train Loss: 0.017010 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.453s | Train Loss: 0.016718 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.448s | Train Loss: 0.016898 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.453s | Train Loss: 0.016707 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.449s | Train Loss: 0.016697 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.452s | Train Loss: 0.016685 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.452s | Train Loss: 0.016649 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.453s | Train Loss: 0.016502 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.454s | Train Loss: 0.016421 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.450s | Train Loss: 0.016298 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.453s | Train Loss: 0.016235 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.453s | Train Loss: 0.016224 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.454s | Train Loss: 0.016111 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.448s | Train Loss: 0.016110 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.453s | Train Loss: 0.015926 |\n",
      "INFO:root:Pretraining Time: 31.622s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017605\n",
      "INFO:root:Test AUC: 50.04%\n",
      "INFO:root:Test AUC: 50.04%\n",
      "INFO:root:Test Time: 0.235s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.723s | Train Loss: 0.724078 || Validation Loss: 0.014696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.773s | Train Loss: 0.591834 || Validation Loss: 0.015965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.744s | Train Loss: 0.536954 || Validation Loss: 0.016912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.818s | Train Loss: 0.510908 || Validation Loss: 0.016725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.796s | Train Loss: 0.495336 || Validation Loss: 0.020132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.834s | Train Loss: 0.471382 || Validation Loss: 0.019552 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.852s | Train Loss: 0.463635 || Validation Loss: 0.019624 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.867s | Train Loss: 0.451005 || Validation Loss: 0.022340 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.883s | Train Loss: 0.436142 || Validation Loss: 0.020188 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.929s | Train Loss: 0.427131 || Validation Loss: 0.021395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.934s | Train Loss: 0.418434 || Validation Loss: 0.021446 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.927s | Train Loss: 0.411319 || Validation Loss: 0.024588 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.011s | Train Loss: 0.407906 || Validation Loss: 0.022373 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.997s | Train Loss: 0.404219 || Validation Loss: 0.022178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.008s | Train Loss: 0.388473 || Validation Loss: 0.023576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.062s | Train Loss: 0.387796 || Validation Loss: 0.023815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.046s | Train Loss: 0.383029 || Validation Loss: 0.023961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.095s | Train Loss: 0.381835 || Validation Loss: 0.021927 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.114s | Train Loss: 0.377168 || Validation Loss: 0.023841 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.180s | Train Loss: 0.376124 || Validation Loss: 0.022549 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.141s | Train Loss: 0.374652 || Validation Loss: 0.021903 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.150s | Train Loss: 0.366077 || Validation Loss: 0.023037 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.267s | Train Loss: 0.358093 || Validation Loss: 0.024949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.196s | Train Loss: 0.358593 || Validation Loss: 0.026029 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.253s | Train Loss: 0.358322 || Validation Loss: 0.025094 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.233s | Train Loss: 0.355087 || Validation Loss: 0.025143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.243s | Train Loss: 0.361042 || Validation Loss: 0.024288 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.333s | Train Loss: 0.350137 || Validation Loss: 0.025648 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.354s | Train Loss: 0.350970 || Validation Loss: 0.026502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.402s | Train Loss: 0.343342 || Validation Loss: 0.024502 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.994s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.920%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022098\n",
      "INFO:root:Test AUC: 86.38%\n",
      "INFO:root:Test Time: 0.202s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7326394575358293\n",
      "validation AUC 0.7860188620027531\n",
      "validation AUC 0.8076138005761121\n",
      "validation AUC 0.8205705678939081\n",
      "validation AUC 0.8271607135141164\n",
      "validation AUC 0.8349620336595471\n",
      "validation AUC 0.8422601176702724\n",
      "validation AUC 0.8468915088686927\n",
      "validation AUC 0.8508585995275054\n",
      "validation AUC 0.8549129951556425\n",
      "validation AUC 0.8553118819900116\n",
      "validation AUC 0.8577390043045583\n",
      "validation AUC 0.8595525706129127\n",
      "validation AUC 0.8594814479135249\n",
      "validation AUC 0.86021412000699\n",
      "validation AUC 0.8613375984425857\n",
      "validation AUC 0.8640227111586674\n",
      "validation AUC 0.8633537836924864\n",
      "validation AUC 0.8623969996656055\n",
      "validation AUC 0.8624630883540232\n",
      "validation AUC 0.8643030326670063\n",
      "validation AUC 0.8626263744286717\n",
      "validation AUC 0.8632751685116548\n",
      "validation AUC 0.8639034567159134\n",
      "validation AUC 0.8645245371317376\n",
      "validation AUC 0.863297305388648\n",
      "validation AUC 0.8628877066472146\n",
      "validation AUC 0.8634079338992854\n",
      "validation AUC 0.8618538878848065\n",
      "validation AUC 0.8627479569684654\n",
      "validation AUC 0.8627479569684654\n",
      "train auc 0.9203028631345331\n",
      "Test AUC 0.8637550573623047\n",
      "test false positive rates [0.         0.         0.         ... 0.99477025 0.99477025 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.67973124e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.44575272e+01 4.34575272e+01 3.97547035e+01 ... 4.41883244e-02\n",
      " 4.41354923e-02 9.31799505e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.464s | Train Loss: 0.179837 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.446s | Train Loss: 0.132818 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.447s | Train Loss: 0.101435 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.446s | Train Loss: 0.080799 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.446s | Train Loss: 0.066815 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.425s | Train Loss: 0.057057 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.443s | Train Loss: 0.050315 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.443s | Train Loss: 0.045149 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.444s | Train Loss: 0.041514 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.445s | Train Loss: 0.038516 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.444s | Train Loss: 0.035799 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.443s | Train Loss: 0.033581 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.443s | Train Loss: 0.031925 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.446s | Train Loss: 0.030533 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.446s | Train Loss: 0.029144 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.442s | Train Loss: 0.028304 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.448s | Train Loss: 0.027015 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.437s | Train Loss: 0.026414 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.446s | Train Loss: 0.025376 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.446s | Train Loss: 0.024556 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.446s | Train Loss: 0.023965 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.446s | Train Loss: 0.023449 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.441s | Train Loss: 0.022803 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.445s | Train Loss: 0.022501 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.447s | Train Loss: 0.021920 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.444s | Train Loss: 0.021788 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.446s | Train Loss: 0.021555 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.446s | Train Loss: 0.020934 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.443s | Train Loss: 0.020795 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.443s | Train Loss: 0.020444 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.458s | Train Loss: 0.020561 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.443s | Train Loss: 0.020087 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.437s | Train Loss: 0.019826 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.428s | Train Loss: 0.019681 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.445s | Train Loss: 0.019339 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.445s | Train Loss: 0.019285 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.443s | Train Loss: 0.019067 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.446s | Train Loss: 0.018705 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.445s | Train Loss: 0.018601 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.444s | Train Loss: 0.018450 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.444s | Train Loss: 0.018274 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.444s | Train Loss: 0.018183 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.444s | Train Loss: 0.018039 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.444s | Train Loss: 0.018026 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.445s | Train Loss: 0.017958 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.443s | Train Loss: 0.017685 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.433s | Train Loss: 0.017502 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.446s | Train Loss: 0.017501 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.444s | Train Loss: 0.017542 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.446s | Train Loss: 0.017242 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.444s | Train Loss: 0.017370 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.441s | Train Loss: 0.017135 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.443s | Train Loss: 0.017145 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.444s | Train Loss: 0.017027 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.445s | Train Loss: 0.016813 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.445s | Train Loss: 0.016994 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.446s | Train Loss: 0.016787 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.444s | Train Loss: 0.016709 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.445s | Train Loss: 0.016740 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.446s | Train Loss: 0.016588 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.442s | Train Loss: 0.016616 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.446s | Train Loss: 0.016479 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.444s | Train Loss: 0.016352 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.444s | Train Loss: 0.016299 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.446s | Train Loss: 0.016433 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.444s | Train Loss: 0.016241 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.445s | Train Loss: 0.016169 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.443s | Train Loss: 0.016151 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.447s | Train Loss: 0.016142 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.447s | Train Loss: 0.016044 |\n",
      "INFO:root:Pretraining Time: 31.100s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017774\n",
      "INFO:root:Test AUC: 48.50%\n",
      "INFO:root:Test AUC: 48.50%\n",
      "INFO:root:Test Time: 0.219s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.688s | Train Loss: 0.794681 || Validation Loss: 0.014213 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.717s | Train Loss: 0.628940 || Validation Loss: 0.016820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.715s | Train Loss: 0.570304 || Validation Loss: 0.015889 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.786s | Train Loss: 0.540049 || Validation Loss: 0.017561 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.769s | Train Loss: 0.508175 || Validation Loss: 0.016808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.814s | Train Loss: 0.491600 || Validation Loss: 0.017435 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.825s | Train Loss: 0.474621 || Validation Loss: 0.019382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.842s | Train Loss: 0.465673 || Validation Loss: 0.018807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.855s | Train Loss: 0.461028 || Validation Loss: 0.019133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.904s | Train Loss: 0.447117 || Validation Loss: 0.021640 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.921s | Train Loss: 0.445361 || Validation Loss: 0.020268 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.902s | Train Loss: 0.430762 || Validation Loss: 0.019696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.986s | Train Loss: 0.428550 || Validation Loss: 0.022436 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.990s | Train Loss: 0.419761 || Validation Loss: 0.021751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.973s | Train Loss: 0.412256 || Validation Loss: 0.023661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.056s | Train Loss: 0.409202 || Validation Loss: 0.022104 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.008s | Train Loss: 0.397487 || Validation Loss: 0.023238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.075s | Train Loss: 0.399957 || Validation Loss: 0.023904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.130s | Train Loss: 0.393026 || Validation Loss: 0.025274 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.210s | Train Loss: 0.386197 || Validation Loss: 0.024713 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.152s | Train Loss: 0.388824 || Validation Loss: 0.024052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.136s | Train Loss: 0.389659 || Validation Loss: 0.023405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.224s | Train Loss: 0.380556 || Validation Loss: 0.023567 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.175s | Train Loss: 0.378474 || Validation Loss: 0.025441 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.252s | Train Loss: 0.374936 || Validation Loss: 0.026141 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.224s | Train Loss: 0.374558 || Validation Loss: 0.024645 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.302s | Train Loss: 0.374124 || Validation Loss: 0.026428 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.384s | Train Loss: 0.361197 || Validation Loss: 0.024498 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.336s | Train Loss: 0.367639 || Validation Loss: 0.026430 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.389s | Train Loss: 0.360697 || Validation Loss: 0.025155 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.598s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.909%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.023096\n",
      "INFO:root:Test AUC: 85.88%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7076761962374547\n",
      "validation AUC 0.7816362312988621\n",
      "validation AUC 0.809799322292282\n",
      "validation AUC 0.8235562126613145\n",
      "validation AUC 0.831568032689356\n",
      "validation AUC 0.8372872837690077\n",
      "validation AUC 0.8411562245173149\n",
      "validation AUC 0.8454321725024655\n",
      "validation AUC 0.8478570784686305\n",
      "validation AUC 0.8492090243746174\n",
      "validation AUC 0.8517608954409315\n",
      "validation AUC 0.8538389548584379\n",
      "validation AUC 0.8550644917468827\n",
      "validation AUC 0.8558420868008232\n",
      "validation AUC 0.855818124695751\n",
      "validation AUC 0.8563355608920566\n",
      "validation AUC 0.8578109491547864\n",
      "validation AUC 0.856979855761218\n",
      "validation AUC 0.8573860142403975\n",
      "validation AUC 0.8581875474532692\n",
      "validation AUC 0.8580418538101928\n",
      "validation AUC 0.8579438342731218\n",
      "validation AUC 0.8586555348684548\n",
      "validation AUC 0.8579169028465683\n",
      "validation AUC 0.856810237262622\n",
      "validation AUC 0.8592568623786543\n",
      "validation AUC 0.8571577409998162\n",
      "validation AUC 0.8583539997398917\n",
      "validation AUC 0.8592449372004471\n",
      "validation AUC 0.8580325839929519\n",
      "validation AUC 0.8580325839929519\n",
      "train auc 0.9086272988145927\n",
      "Test AUC 0.8588194324720457\n",
      "test false positive rates [0.         0.         0.         ... 0.99516817 0.99516817 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.11982083e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.29308319e+01 7.19308319e+01 5.99960556e+01 ... 2.00640410e-02\n",
      " 2.00498365e-02 9.23306588e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.467s | Train Loss: 0.190434 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.436s | Train Loss: 0.141563 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.434s | Train Loss: 0.107211 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.442s | Train Loss: 0.084535 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.440s | Train Loss: 0.069640 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.446s | Train Loss: 0.059019 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.443s | Train Loss: 0.051444 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.447s | Train Loss: 0.046052 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.444s | Train Loss: 0.042115 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.437s | Train Loss: 0.038760 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.444s | Train Loss: 0.036298 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.439s | Train Loss: 0.034136 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.447s | Train Loss: 0.032508 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.443s | Train Loss: 0.031176 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.437s | Train Loss: 0.029818 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.443s | Train Loss: 0.028648 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.441s | Train Loss: 0.027502 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.439s | Train Loss: 0.026636 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.440s | Train Loss: 0.025863 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.433s | Train Loss: 0.025264 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.437s | Train Loss: 0.024565 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.442s | Train Loss: 0.023845 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.433s | Train Loss: 0.023461 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.436s | Train Loss: 0.022954 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.442s | Train Loss: 0.022528 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.440s | Train Loss: 0.022323 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.434s | Train Loss: 0.022021 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.442s | Train Loss: 0.021383 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.438s | Train Loss: 0.021340 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.446s | Train Loss: 0.021151 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.437s | Train Loss: 0.020799 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.443s | Train Loss: 0.020568 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.438s | Train Loss: 0.020180 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.443s | Train Loss: 0.020068 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.433s | Train Loss: 0.019835 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.441s | Train Loss: 0.019649 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.442s | Train Loss: 0.019290 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.445s | Train Loss: 0.019171 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.444s | Train Loss: 0.019163 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.442s | Train Loss: 0.019150 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.445s | Train Loss: 0.018795 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.444s | Train Loss: 0.018716 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.444s | Train Loss: 0.018500 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.443s | Train Loss: 0.018517 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.442s | Train Loss: 0.018390 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.442s | Train Loss: 0.018348 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.439s | Train Loss: 0.018023 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.439s | Train Loss: 0.018092 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.445s | Train Loss: 0.017889 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.443s | Train Loss: 0.017767 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.446s | Train Loss: 0.017624 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.443s | Train Loss: 0.017535 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.445s | Train Loss: 0.017478 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.442s | Train Loss: 0.017448 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.441s | Train Loss: 0.017373 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.443s | Train Loss: 0.017053 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.442s | Train Loss: 0.017094 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.429s | Train Loss: 0.016735 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.444s | Train Loss: 0.016784 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.444s | Train Loss: 0.016742 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.441s | Train Loss: 0.016520 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.444s | Train Loss: 0.016278 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.444s | Train Loss: 0.016360 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.444s | Train Loss: 0.016449 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.444s | Train Loss: 0.016311 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.444s | Train Loss: 0.016088 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.442s | Train Loss: 0.015959 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.444s | Train Loss: 0.015897 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.442s | Train Loss: 0.015969 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.440s | Train Loss: 0.015756 |\n",
      "INFO:root:Pretraining Time: 30.923s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017474\n",
      "INFO:root:Test AUC: 49.55%\n",
      "INFO:root:Test AUC: 49.55%\n",
      "INFO:root:Test Time: 0.219s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.725s | Train Loss: 0.726595 || Validation Loss: 0.017095 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.730s | Train Loss: 0.599717 || Validation Loss: 0.017185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.731s | Train Loss: 0.548604 || Validation Loss: 0.017397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.802s | Train Loss: 0.518256 || Validation Loss: 0.018049 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.788s | Train Loss: 0.488507 || Validation Loss: 0.019352 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.814s | Train Loss: 0.473894 || Validation Loss: 0.021860 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.827s | Train Loss: 0.458534 || Validation Loss: 0.019700 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.850s | Train Loss: 0.450833 || Validation Loss: 0.020031 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.881s | Train Loss: 0.435890 || Validation Loss: 0.021804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.940s | Train Loss: 0.431766 || Validation Loss: 0.021709 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.933s | Train Loss: 0.416790 || Validation Loss: 0.024142 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.908s | Train Loss: 0.416175 || Validation Loss: 0.024133 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.987s | Train Loss: 0.406029 || Validation Loss: 0.023846 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.973s | Train Loss: 0.400558 || Validation Loss: 0.022405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.008s | Train Loss: 0.398146 || Validation Loss: 0.024297 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.050s | Train Loss: 0.384532 || Validation Loss: 0.024115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.014s | Train Loss: 0.384340 || Validation Loss: 0.023557 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.069s | Train Loss: 0.378696 || Validation Loss: 0.025396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.073s | Train Loss: 0.376899 || Validation Loss: 0.024416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.136s | Train Loss: 0.374840 || Validation Loss: 0.025511 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.123s | Train Loss: 0.370824 || Validation Loss: 0.026585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.141s | Train Loss: 0.359836 || Validation Loss: 0.026466 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.219s | Train Loss: 0.365759 || Validation Loss: 0.025657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.172s | Train Loss: 0.356161 || Validation Loss: 0.027101 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.230s | Train Loss: 0.360393 || Validation Loss: 0.026226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.201s | Train Loss: 0.356869 || Validation Loss: 0.024743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.211s | Train Loss: 0.353741 || Validation Loss: 0.026460 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.309s | Train Loss: 0.353584 || Validation Loss: 0.023864 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.327s | Train Loss: 0.351722 || Validation Loss: 0.027444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.370s | Train Loss: 0.347370 || Validation Loss: 0.026657 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.384s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.916%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.022909\n",
      "INFO:root:Test AUC: 87.01%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7503528171198092\n",
      "validation AUC 0.8034217838150907\n",
      "validation AUC 0.82577382752776\n",
      "validation AUC 0.8382896559269466\n",
      "validation AUC 0.8467860447424598\n",
      "validation AUC 0.8521088381907105\n",
      "validation AUC 0.8559667105005213\n",
      "validation AUC 0.8581475254696477\n",
      "validation AUC 0.8608438183728843\n",
      "validation AUC 0.8614677377367556\n",
      "validation AUC 0.863321065281863\n",
      "validation AUC 0.8643738946195256\n",
      "validation AUC 0.8644548245939853\n",
      "validation AUC 0.8650979221560978\n",
      "validation AUC 0.8655898291054382\n",
      "validation AUC 0.866417309292401\n",
      "validation AUC 0.8660499861325236\n",
      "validation AUC 0.8665069663049061\n",
      "validation AUC 0.8668924619251037\n",
      "validation AUC 0.8667047827148453\n",
      "validation AUC 0.8670796382664017\n",
      "validation AUC 0.8668398628961038\n",
      "validation AUC 0.8657801876228038\n",
      "validation AUC 0.8663235654718358\n",
      "validation AUC 0.8645712587134685\n",
      "validation AUC 0.8652554345500882\n",
      "validation AUC 0.8662515301584084\n",
      "validation AUC 0.8662911849678686\n",
      "validation AUC 0.8663784553483227\n",
      "validation AUC 0.8665678613414137\n",
      "validation AUC 0.8665678613414137\n",
      "train auc 0.916166393241132\n",
      "Test AUC 0.8701237850262291\n",
      "test false positive rates [0.         0.         0.         ... 0.99626717 0.99626717 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 1.39977604e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [7.49083633e+01 7.39083633e+01 5.65376930e+01 ... 1.66776832e-02\n",
      " 1.66690648e-02 3.73181608e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.466s | Train Loss: 0.190740 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.438s | Train Loss: 0.140824 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.431s | Train Loss: 0.105214 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.439s | Train Loss: 0.082338 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.445s | Train Loss: 0.067632 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.444s | Train Loss: 0.057789 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.442s | Train Loss: 0.050636 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.444s | Train Loss: 0.045608 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.442s | Train Loss: 0.041901 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.444s | Train Loss: 0.038979 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.443s | Train Loss: 0.036525 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.445s | Train Loss: 0.034507 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.444s | Train Loss: 0.032723 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.445s | Train Loss: 0.031623 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.441s | Train Loss: 0.030309 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.443s | Train Loss: 0.028925 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.445s | Train Loss: 0.028254 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.443s | Train Loss: 0.027184 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.444s | Train Loss: 0.026416 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.442s | Train Loss: 0.025882 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.441s | Train Loss: 0.025250 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.444s | Train Loss: 0.024809 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.443s | Train Loss: 0.024421 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.443s | Train Loss: 0.024013 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.443s | Train Loss: 0.023243 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.445s | Train Loss: 0.022947 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.443s | Train Loss: 0.022618 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.445s | Train Loss: 0.022335 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.445s | Train Loss: 0.021680 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.443s | Train Loss: 0.021482 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.444s | Train Loss: 0.021271 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.443s | Train Loss: 0.020814 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.443s | Train Loss: 0.020725 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.444s | Train Loss: 0.020403 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.444s | Train Loss: 0.020218 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.442s | Train Loss: 0.019933 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.444s | Train Loss: 0.019593 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.437s | Train Loss: 0.019551 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.449s | Train Loss: 0.019263 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.448s | Train Loss: 0.019066 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.445s | Train Loss: 0.018835 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.448s | Train Loss: 0.018866 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.434s | Train Loss: 0.018681 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.445s | Train Loss: 0.018487 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.445s | Train Loss: 0.018498 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.442s | Train Loss: 0.017998 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.444s | Train Loss: 0.017875 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.436s | Train Loss: 0.017652 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.446s | Train Loss: 0.017887 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.446s | Train Loss: 0.017480 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.444s | Train Loss: 0.017189 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.445s | Train Loss: 0.017267 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.443s | Train Loss: 0.017287 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.454s | Train Loss: 0.017077 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.443s | Train Loss: 0.017122 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.444s | Train Loss: 0.016845 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.444s | Train Loss: 0.016623 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.443s | Train Loss: 0.016442 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.444s | Train Loss: 0.016269 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.443s | Train Loss: 0.016229 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.442s | Train Loss: 0.016170 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.445s | Train Loss: 0.016134 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.444s | Train Loss: 0.016136 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.444s | Train Loss: 0.015986 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.444s | Train Loss: 0.015944 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.446s | Train Loss: 0.015707 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.445s | Train Loss: 0.015582 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.445s | Train Loss: 0.015831 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.443s | Train Loss: 0.015658 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.442s | Train Loss: 0.015482 |\n",
      "INFO:root:Pretraining Time: 31.070s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016558\n",
      "INFO:root:Test AUC: 53.88%\n",
      "INFO:root:Test AUC: 53.88%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.696s | Train Loss: 0.850352 || Validation Loss: 0.014212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.748s | Train Loss: 0.659777 || Validation Loss: 0.014664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.724s | Train Loss: 0.595848 || Validation Loss: 0.014040 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.788s | Train Loss: 0.551899 || Validation Loss: 0.018115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.775s | Train Loss: 0.522633 || Validation Loss: 0.017967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.824s | Train Loss: 0.495143 || Validation Loss: 0.019212 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.831s | Train Loss: 0.484664 || Validation Loss: 0.019381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.850s | Train Loss: 0.459814 || Validation Loss: 0.020569 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.869s | Train Loss: 0.453924 || Validation Loss: 0.022238 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.914s | Train Loss: 0.452110 || Validation Loss: 0.021692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.952s | Train Loss: 0.435860 || Validation Loss: 0.021757 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.940s | Train Loss: 0.432303 || Validation Loss: 0.023725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.021s | Train Loss: 0.424377 || Validation Loss: 0.020853 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.044s | Train Loss: 0.422188 || Validation Loss: 0.022072 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.028s | Train Loss: 0.415601 || Validation Loss: 0.022449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.086s | Train Loss: 0.407651 || Validation Loss: 0.021361 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.028s | Train Loss: 0.404380 || Validation Loss: 0.023298 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.092s | Train Loss: 0.397084 || Validation Loss: 0.024426 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.103s | Train Loss: 0.397529 || Validation Loss: 0.021722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.155s | Train Loss: 0.397056 || Validation Loss: 0.024541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.162s | Train Loss: 0.391814 || Validation Loss: 0.021278 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.178s | Train Loss: 0.391641 || Validation Loss: 0.023812 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.299s | Train Loss: 0.386337 || Validation Loss: 0.023030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.193s | Train Loss: 0.387514 || Validation Loss: 0.023163 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.237s | Train Loss: 0.381598 || Validation Loss: 0.021972 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.223s | Train Loss: 0.385087 || Validation Loss: 0.022173 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.232s | Train Loss: 0.377036 || Validation Loss: 0.024309 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.320s | Train Loss: 0.375110 || Validation Loss: 0.023890 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.327s | Train Loss: 0.376887 || Validation Loss: 0.021799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.384s | Train Loss: 0.371938 || Validation Loss: 0.021050 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.848s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.903%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021384\n",
      "INFO:root:Test AUC: 86.90%\n",
      "INFO:root:Test Time: 0.195s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6730958667257834\n",
      "validation AUC 0.7555046138360164\n",
      "validation AUC 0.7938365905761187\n",
      "validation AUC 0.8152404149983302\n",
      "validation AUC 0.8278948383401344\n",
      "validation AUC 0.8356829987507564\n",
      "validation AUC 0.8419977557676674\n",
      "validation AUC 0.8472904861662611\n",
      "validation AUC 0.8511946570093122\n",
      "validation AUC 0.8545756499035876\n",
      "validation AUC 0.8567649843775377\n",
      "validation AUC 0.858165916105919\n",
      "validation AUC 0.8596663546927945\n",
      "validation AUC 0.8599560684097612\n",
      "validation AUC 0.8618972170965657\n",
      "validation AUC 0.8628901837424647\n",
      "validation AUC 0.8623781247851499\n",
      "validation AUC 0.8634778193813891\n",
      "validation AUC 0.8648425498659761\n",
      "validation AUC 0.86427263969274\n",
      "validation AUC 0.8633873482000909\n",
      "validation AUC 0.8645022485958515\n",
      "validation AUC 0.8650971505582218\n",
      "validation AUC 0.8638835361873018\n",
      "validation AUC 0.863954823848994\n",
      "validation AUC 0.8648534719669444\n",
      "validation AUC 0.8637891910525297\n",
      "validation AUC 0.8627581260963342\n",
      "validation AUC 0.8634803896005208\n",
      "validation AUC 0.8633118167500808\n",
      "validation AUC 0.8633118167500808\n",
      "train auc 0.9034231721027018\n",
      "Test AUC 0.868962797122522\n",
      "test false positive rates [0.         0.         0.         ... 0.99774514 0.99774514 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 4.75923852e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.45731583e+01 5.35731583e+01 3.76866837e+01 ... 3.86134125e-02\n",
      " 3.85767370e-02 1.54382894e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.460s | Train Loss: 0.195776 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.447s | Train Loss: 0.147538 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.443s | Train Loss: 0.110334 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.447s | Train Loss: 0.085803 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.445s | Train Loss: 0.070340 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.445s | Train Loss: 0.060259 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.443s | Train Loss: 0.053527 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.445s | Train Loss: 0.047947 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.448s | Train Loss: 0.043668 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.441s | Train Loss: 0.039801 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.446s | Train Loss: 0.037119 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.446s | Train Loss: 0.034543 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.443s | Train Loss: 0.032404 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.446s | Train Loss: 0.030762 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.443s | Train Loss: 0.029384 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.442s | Train Loss: 0.028128 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.445s | Train Loss: 0.027056 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.441s | Train Loss: 0.026434 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.445s | Train Loss: 0.025651 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.445s | Train Loss: 0.024710 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.447s | Train Loss: 0.024259 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.443s | Train Loss: 0.023574 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.444s | Train Loss: 0.023378 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.444s | Train Loss: 0.023084 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.443s | Train Loss: 0.022809 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.434s | Train Loss: 0.022271 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.441s | Train Loss: 0.022060 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.441s | Train Loss: 0.021733 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.443s | Train Loss: 0.021599 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.444s | Train Loss: 0.021487 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.446s | Train Loss: 0.021364 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.445s | Train Loss: 0.021077 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.446s | Train Loss: 0.021012 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.444s | Train Loss: 0.020639 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.446s | Train Loss: 0.020741 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.445s | Train Loss: 0.020357 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.443s | Train Loss: 0.020339 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.442s | Train Loss: 0.020025 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.444s | Train Loss: 0.020006 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.445s | Train Loss: 0.019707 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.443s | Train Loss: 0.019620 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.442s | Train Loss: 0.019614 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.446s | Train Loss: 0.019359 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.448s | Train Loss: 0.019243 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.445s | Train Loss: 0.019091 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.445s | Train Loss: 0.018870 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.444s | Train Loss: 0.018924 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.447s | Train Loss: 0.018462 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.443s | Train Loss: 0.018265 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.445s | Train Loss: 0.018193 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.445s | Train Loss: 0.017879 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.443s | Train Loss: 0.017702 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.445s | Train Loss: 0.017626 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.442s | Train Loss: 0.017589 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.442s | Train Loss: 0.017454 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.444s | Train Loss: 0.017081 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.444s | Train Loss: 0.017197 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.444s | Train Loss: 0.016973 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.447s | Train Loss: 0.017043 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.443s | Train Loss: 0.016999 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.446s | Train Loss: 0.016906 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.446s | Train Loss: 0.016695 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.446s | Train Loss: 0.016394 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.445s | Train Loss: 0.016691 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.443s | Train Loss: 0.016550 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.487s | Train Loss: 0.016284 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.443s | Train Loss: 0.016281 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.446s | Train Loss: 0.016216 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.438s | Train Loss: 0.016074 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.443s | Train Loss: 0.016135 |\n",
      "INFO:root:Pretraining Time: 31.159s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017170\n",
      "INFO:root:Test AUC: 52.00%\n",
      "INFO:root:Test AUC: 52.00%\n",
      "INFO:root:Test Time: 0.219s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.695s | Train Loss: 0.692333 || Validation Loss: 0.016932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.743s | Train Loss: 0.576328 || Validation Loss: 0.017768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.731s | Train Loss: 0.525087 || Validation Loss: 0.018539 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.802s | Train Loss: 0.491607 || Validation Loss: 0.018322 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.783s | Train Loss: 0.478058 || Validation Loss: 0.019505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.820s | Train Loss: 0.456847 || Validation Loss: 0.020544 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.841s | Train Loss: 0.448507 || Validation Loss: 0.021098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.842s | Train Loss: 0.445038 || Validation Loss: 0.019730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.868s | Train Loss: 0.442793 || Validation Loss: 0.020755 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.918s | Train Loss: 0.427502 || Validation Loss: 0.020392 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.934s | Train Loss: 0.421379 || Validation Loss: 0.021584 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.911s | Train Loss: 0.420217 || Validation Loss: 0.020139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.968s | Train Loss: 0.409988 || Validation Loss: 0.021313 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.986s | Train Loss: 0.409021 || Validation Loss: 0.022358 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.995s | Train Loss: 0.404873 || Validation Loss: 0.022325 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.045s | Train Loss: 0.393155 || Validation Loss: 0.022661 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.024s | Train Loss: 0.402556 || Validation Loss: 0.021932 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.085s | Train Loss: 0.393700 || Validation Loss: 0.023075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.096s | Train Loss: 0.379073 || Validation Loss: 0.024205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.141s | Train Loss: 0.386538 || Validation Loss: 0.021565 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.133s | Train Loss: 0.384254 || Validation Loss: 0.024127 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.145s | Train Loss: 0.369325 || Validation Loss: 0.021380 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.239s | Train Loss: 0.376535 || Validation Loss: 0.023199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.168s | Train Loss: 0.372394 || Validation Loss: 0.023916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.238s | Train Loss: 0.367862 || Validation Loss: 0.026331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.217s | Train Loss: 0.366648 || Validation Loss: 0.022694 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.232s | Train Loss: 0.371128 || Validation Loss: 0.022396 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.340s | Train Loss: 0.356264 || Validation Loss: 0.024321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.349s | Train Loss: 0.359977 || Validation Loss: 0.022832 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.392s | Train Loss: 0.360103 || Validation Loss: 0.025377 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.517s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.915%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.020873\n",
      "INFO:root:Test AUC: 86.21%\n",
      "INFO:root:Test Time: 0.199s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7455613033980318\n",
      "validation AUC 0.804078017165871\n",
      "validation AUC 0.8262876185892722\n",
      "validation AUC 0.8379180969607983\n",
      "validation AUC 0.8428390315925163\n",
      "validation AUC 0.8481087630530415\n",
      "validation AUC 0.8506167328608956\n",
      "validation AUC 0.853002117051717\n",
      "validation AUC 0.8521023674112816\n",
      "validation AUC 0.8546215546558429\n",
      "validation AUC 0.8568020663071836\n",
      "validation AUC 0.8576344128821299\n",
      "validation AUC 0.8575965194443728\n",
      "validation AUC 0.8571935484626257\n",
      "validation AUC 0.858323529605838\n",
      "validation AUC 0.858532334633804\n",
      "validation AUC 0.8576878234192408\n",
      "validation AUC 0.8570662960087849\n",
      "validation AUC 0.8587771865859891\n",
      "validation AUC 0.8592636444579158\n",
      "validation AUC 0.8576329042752481\n",
      "validation AUC 0.8580266772781775\n",
      "validation AUC 0.8563863586391185\n",
      "validation AUC 0.856723602785245\n",
      "validation AUC 0.856111949166919\n",
      "validation AUC 0.8565185413373186\n",
      "validation AUC 0.854713584996987\n",
      "validation AUC 0.8566978766477872\n",
      "validation AUC 0.8541632946952593\n",
      "validation AUC 0.8554992206329316\n",
      "validation AUC 0.8554992206329316\n",
      "train auc 0.9154766957496849\n",
      "Test AUC 0.8620930809289368\n",
      "test false positive rates [0.         0.         0.         ... 0.99986736 0.99986736 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [4.85284958e+01 4.75284958e+01 4.70074158e+01 ... 1.30399931e-02\n",
      " 1.29873268e-02 9.66914557e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.467s | Train Loss: 0.195963 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.449s | Train Loss: 0.145648 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.445s | Train Loss: 0.109255 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.447s | Train Loss: 0.085759 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.446s | Train Loss: 0.070832 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.442s | Train Loss: 0.060951 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.441s | Train Loss: 0.053996 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.442s | Train Loss: 0.048597 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.442s | Train Loss: 0.044566 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.447s | Train Loss: 0.041204 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.428s | Train Loss: 0.038018 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.443s | Train Loss: 0.035797 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.444s | Train Loss: 0.033730 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.443s | Train Loss: 0.032022 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.447s | Train Loss: 0.030627 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.445s | Train Loss: 0.029462 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.445s | Train Loss: 0.028304 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.445s | Train Loss: 0.027042 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.445s | Train Loss: 0.026296 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.445s | Train Loss: 0.025453 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.445s | Train Loss: 0.024685 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.447s | Train Loss: 0.024306 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.444s | Train Loss: 0.023535 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.445s | Train Loss: 0.022995 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.444s | Train Loss: 0.022682 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.444s | Train Loss: 0.022084 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.446s | Train Loss: 0.021781 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.441s | Train Loss: 0.021557 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.444s | Train Loss: 0.021229 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.442s | Train Loss: 0.021225 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.445s | Train Loss: 0.020939 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.445s | Train Loss: 0.020765 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.445s | Train Loss: 0.020208 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.446s | Train Loss: 0.020012 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.443s | Train Loss: 0.019858 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.445s | Train Loss: 0.019770 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.442s | Train Loss: 0.019670 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.444s | Train Loss: 0.019150 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.442s | Train Loss: 0.018943 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.442s | Train Loss: 0.018854 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.443s | Train Loss: 0.018477 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.442s | Train Loss: 0.018448 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.446s | Train Loss: 0.018411 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.445s | Train Loss: 0.018324 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.444s | Train Loss: 0.018208 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.445s | Train Loss: 0.018082 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.447s | Train Loss: 0.017816 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.445s | Train Loss: 0.017802 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.444s | Train Loss: 0.017468 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.444s | Train Loss: 0.017527 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.445s | Train Loss: 0.017446 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.442s | Train Loss: 0.017257 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.446s | Train Loss: 0.017378 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.446s | Train Loss: 0.017161 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.445s | Train Loss: 0.016992 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.447s | Train Loss: 0.017097 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.444s | Train Loss: 0.016773 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.447s | Train Loss: 0.016685 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.447s | Train Loss: 0.016659 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.443s | Train Loss: 0.016717 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.442s | Train Loss: 0.016529 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.444s | Train Loss: 0.016452 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.446s | Train Loss: 0.016484 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.447s | Train Loss: 0.016380 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.443s | Train Loss: 0.016407 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.447s | Train Loss: 0.016355 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.444s | Train Loss: 0.016305 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.444s | Train Loss: 0.015983 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.445s | Train Loss: 0.015963 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.441s | Train Loss: 0.016029 |\n",
      "INFO:root:Pretraining Time: 31.141s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017286\n",
      "INFO:root:Test AUC: 51.27%\n",
      "INFO:root:Test AUC: 51.27%\n",
      "INFO:root:Test Time: 0.220s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.711s | Train Loss: 0.857002 || Validation Loss: 0.016275 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.759s | Train Loss: 0.677092 || Validation Loss: 0.014743 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.731s | Train Loss: 0.598173 || Validation Loss: 0.016756 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.802s | Train Loss: 0.546881 || Validation Loss: 0.017645 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.785s | Train Loss: 0.516535 || Validation Loss: 0.016967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.828s | Train Loss: 0.492542 || Validation Loss: 0.015707 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.837s | Train Loss: 0.470713 || Validation Loss: 0.017940 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.843s | Train Loss: 0.457229 || Validation Loss: 0.018895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.874s | Train Loss: 0.442978 || Validation Loss: 0.022033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.922s | Train Loss: 0.436575 || Validation Loss: 0.019384 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.932s | Train Loss: 0.425417 || Validation Loss: 0.020378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.926s | Train Loss: 0.418905 || Validation Loss: 0.020675 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.995s | Train Loss: 0.418842 || Validation Loss: 0.020924 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.982s | Train Loss: 0.404989 || Validation Loss: 0.020782 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.975s | Train Loss: 0.403561 || Validation Loss: 0.022327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.037s | Train Loss: 0.397434 || Validation Loss: 0.022530 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.009s | Train Loss: 0.389083 || Validation Loss: 0.023452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.070s | Train Loss: 0.390455 || Validation Loss: 0.022205 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.078s | Train Loss: 0.380931 || Validation Loss: 0.026839 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.143s | Train Loss: 0.377332 || Validation Loss: 0.023565 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.116s | Train Loss: 0.376041 || Validation Loss: 0.025365 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.130s | Train Loss: 0.381846 || Validation Loss: 0.025914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.220s | Train Loss: 0.371795 || Validation Loss: 0.024872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.203s | Train Loss: 0.366828 || Validation Loss: 0.022294 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.230s | Train Loss: 0.369789 || Validation Loss: 0.022909 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.200s | Train Loss: 0.364386 || Validation Loss: 0.025939 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.219s | Train Loss: 0.358188 || Validation Loss: 0.025452 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.312s | Train Loss: 0.357523 || Validation Loss: 0.024938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.330s | Train Loss: 0.353202 || Validation Loss: 0.025945 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.375s | Train Loss: 0.356919 || Validation Loss: 0.024490 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.401s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.909%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.021917\n",
      "INFO:root:Test AUC: 86.04%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6228878785272931\n",
      "validation AUC 0.7308602246722731\n",
      "validation AUC 0.7879019625831437\n",
      "validation AUC 0.8166682967780414\n",
      "validation AUC 0.831716488120692\n",
      "validation AUC 0.8418823726177049\n",
      "validation AUC 0.8470262325185188\n",
      "validation AUC 0.8514984111469394\n",
      "validation AUC 0.8549229993212067\n",
      "validation AUC 0.8575329770289459\n",
      "validation AUC 0.8587621750162779\n",
      "validation AUC 0.859669824222554\n",
      "validation AUC 0.862448678098519\n",
      "validation AUC 0.8613049758165261\n",
      "validation AUC 0.8625748955469331\n",
      "validation AUC 0.8630377531665845\n",
      "validation AUC 0.8640264733634835\n",
      "validation AUC 0.8622627575992813\n",
      "validation AUC 0.863297055284509\n",
      "validation AUC 0.863070514148125\n",
      "validation AUC 0.8626275132007093\n",
      "validation AUC 0.8621912757077788\n",
      "validation AUC 0.8615029518674053\n",
      "validation AUC 0.8625952258206235\n",
      "validation AUC 0.862460225459835\n",
      "validation AUC 0.8609384242545246\n",
      "validation AUC 0.8625163259467825\n",
      "validation AUC 0.8619015992403645\n",
      "validation AUC 0.8625386996245032\n",
      "validation AUC 0.8619871348559389\n",
      "validation AUC 0.8619871348559389\n",
      "train auc 0.9085510862763827\n",
      "Test AUC 0.8603754927397311\n",
      "test false positive rates [0.         0.         0.         ... 0.98645192 0.98645192 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 5.03919373e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.50054092e+01 6.40054092e+01 4.08728561e+01 ... 3.73621061e-02\n",
      " 3.73308361e-02 3.40593094e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m8\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 1 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef871b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation without balanced batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "133a9c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.468s | Train Loss: 0.192906 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.444s | Train Loss: 0.144105 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.445s | Train Loss: 0.108742 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.443s | Train Loss: 0.085283 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.444s | Train Loss: 0.070388 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.426s | Train Loss: 0.060231 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.443s | Train Loss: 0.053200 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.441s | Train Loss: 0.047985 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.442s | Train Loss: 0.044357 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.438s | Train Loss: 0.040933 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.444s | Train Loss: 0.038375 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.442s | Train Loss: 0.035914 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.444s | Train Loss: 0.034134 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.442s | Train Loss: 0.032369 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.445s | Train Loss: 0.030889 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.442s | Train Loss: 0.029425 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.441s | Train Loss: 0.028141 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.444s | Train Loss: 0.026946 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.443s | Train Loss: 0.026205 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.442s | Train Loss: 0.025427 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.441s | Train Loss: 0.024551 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.443s | Train Loss: 0.023981 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.445s | Train Loss: 0.023505 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.442s | Train Loss: 0.023188 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.445s | Train Loss: 0.022805 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.446s | Train Loss: 0.022357 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.444s | Train Loss: 0.021889 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.440s | Train Loss: 0.021421 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.439s | Train Loss: 0.021209 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.442s | Train Loss: 0.020943 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.443s | Train Loss: 0.020914 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.443s | Train Loss: 0.020460 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.442s | Train Loss: 0.020242 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.440s | Train Loss: 0.020006 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.443s | Train Loss: 0.019977 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.444s | Train Loss: 0.019680 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.445s | Train Loss: 0.019586 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.444s | Train Loss: 0.019318 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.444s | Train Loss: 0.019318 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.446s | Train Loss: 0.019167 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.446s | Train Loss: 0.019031 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.444s | Train Loss: 0.018888 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.446s | Train Loss: 0.018770 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.443s | Train Loss: 0.018622 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.447s | Train Loss: 0.018523 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.442s | Train Loss: 0.018338 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.442s | Train Loss: 0.018101 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.446s | Train Loss: 0.018428 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.443s | Train Loss: 0.018223 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.443s | Train Loss: 0.018156 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.445s | Train Loss: 0.017956 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.447s | Train Loss: 0.017858 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.443s | Train Loss: 0.017786 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.446s | Train Loss: 0.017726 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.437s | Train Loss: 0.017834 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.444s | Train Loss: 0.017624 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.444s | Train Loss: 0.017551 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.442s | Train Loss: 0.017378 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.451s | Train Loss: 0.017396 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.444s | Train Loss: 0.017138 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.430s | Train Loss: 0.016858 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.431s | Train Loss: 0.017132 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.440s | Train Loss: 0.016703 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.446s | Train Loss: 0.016761 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.443s | Train Loss: 0.016648 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.438s | Train Loss: 0.016613 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.441s | Train Loss: 0.016336 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.443s | Train Loss: 0.016453 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.441s | Train Loss: 0.016362 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.443s | Train Loss: 0.016247 |\n",
      "INFO:root:Pretraining Time: 31.015s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016551\n",
      "INFO:root:Test AUC: 55.04%\n",
      "INFO:root:Test AUC: 55.04%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.709s | Train Loss: 0.680924 || Validation Loss: 0.007947 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.732s | Train Loss: 0.526388 || Validation Loss: 0.006894 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.730s | Train Loss: 0.472170 || Validation Loss: 0.006078 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.791s | Train Loss: 0.446383 || Validation Loss: 0.006165 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.772s | Train Loss: 0.425870 || Validation Loss: 0.006075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.825s | Train Loss: 0.398889 || Validation Loss: 0.006442 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.840s | Train Loss: 0.395217 || Validation Loss: 0.006673 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.855s | Train Loss: 0.388106 || Validation Loss: 0.005607 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.853s | Train Loss: 0.373742 || Validation Loss: 0.005886 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.927s | Train Loss: 0.372331 || Validation Loss: 0.005922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.937s | Train Loss: 0.366100 || Validation Loss: 0.005783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.917s | Train Loss: 0.351733 || Validation Loss: 0.005934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.004s | Train Loss: 0.351866 || Validation Loss: 0.005838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.987s | Train Loss: 0.338357 || Validation Loss: 0.005982 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.975s | Train Loss: 0.338603 || Validation Loss: 0.006707 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.046s | Train Loss: 0.335236 || Validation Loss: 0.005685 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.032s | Train Loss: 0.333507 || Validation Loss: 0.006417 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.069s | Train Loss: 0.332455 || Validation Loss: 0.006500 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.077s | Train Loss: 0.326149 || Validation Loss: 0.006159 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.138s | Train Loss: 0.320692 || Validation Loss: 0.005725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.116s | Train Loss: 0.325155 || Validation Loss: 0.005852 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.138s | Train Loss: 0.316333 || Validation Loss: 0.006641 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.213s | Train Loss: 0.317855 || Validation Loss: 0.006641 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.161s | Train Loss: 0.317380 || Validation Loss: 0.006173 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.221s | Train Loss: 0.314864 || Validation Loss: 0.006106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.202s | Train Loss: 0.311956 || Validation Loss: 0.005215 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.208s | Train Loss: 0.304024 || Validation Loss: 0.006284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.297s | Train Loss: 0.305633 || Validation Loss: 0.005800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.322s | Train Loss: 0.303744 || Validation Loss: 0.006171 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.382s | Train Loss: 0.304280 || Validation Loss: 0.005906 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.293s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.895%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.006198\n",
      "INFO:root:Test AUC: 86.72%\n",
      "INFO:root:Test Time: 0.195s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6783006908408459\n",
      "validation AUC 0.7564129308735574\n",
      "validation AUC 0.7908452599305712\n",
      "validation AUC 0.8115037021798226\n",
      "validation AUC 0.8239879163515533\n",
      "validation AUC 0.8321396696454247\n",
      "validation AUC 0.8397047345032813\n",
      "validation AUC 0.8430880395305024\n",
      "validation AUC 0.8464117505948754\n",
      "validation AUC 0.8497451013113332\n",
      "validation AUC 0.8524108309566344\n",
      "validation AUC 0.8534304523181248\n",
      "validation AUC 0.8555343948533464\n",
      "validation AUC 0.8582852211016373\n",
      "validation AUC 0.8578146794314143\n",
      "validation AUC 0.8589955620882992\n",
      "validation AUC 0.8582330238356695\n",
      "validation AUC 0.859226059659309\n",
      "validation AUC 0.8602071197517775\n",
      "validation AUC 0.8605093706038875\n",
      "validation AUC 0.8608776436273573\n",
      "validation AUC 0.8625306750065932\n",
      "validation AUC 0.8612119051485906\n",
      "validation AUC 0.8610081873452413\n",
      "validation AUC 0.8629303786704379\n",
      "validation AUC 0.8623899621608402\n",
      "validation AUC 0.8631280487428488\n",
      "validation AUC 0.8617772176627586\n",
      "validation AUC 0.8631341017951518\n",
      "validation AUC 0.8623002678987824\n",
      "validation AUC 0.8623002678987824\n",
      "train auc 0.8948190243067294\n",
      "Test AUC 0.8672182504802074\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.98086215e-01\n",
      " 9.98086215e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.98536186e+01 1.88536186e+01 1.82605000e+01 ... 1.49432411e-02\n",
      " 1.49132339e-02 7.47143663e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.472s | Train Loss: 0.177492 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.458s | Train Loss: 0.129688 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.450s | Train Loss: 0.098047 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.452s | Train Loss: 0.077804 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.452s | Train Loss: 0.064925 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.447s | Train Loss: 0.055978 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.455s | Train Loss: 0.049695 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.454s | Train Loss: 0.045080 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.454s | Train Loss: 0.041400 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.451s | Train Loss: 0.038674 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.452s | Train Loss: 0.036093 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.452s | Train Loss: 0.033797 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.451s | Train Loss: 0.031989 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.452s | Train Loss: 0.030610 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.457s | Train Loss: 0.029330 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.452s | Train Loss: 0.028338 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.447s | Train Loss: 0.027256 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.453s | Train Loss: 0.026404 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.453s | Train Loss: 0.025877 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.450s | Train Loss: 0.025111 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.453s | Train Loss: 0.024713 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.450s | Train Loss: 0.024387 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.451s | Train Loss: 0.023953 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.452s | Train Loss: 0.023582 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.452s | Train Loss: 0.023362 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.454s | Train Loss: 0.022919 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.450s | Train Loss: 0.022720 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.449s | Train Loss: 0.022471 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.448s | Train Loss: 0.022145 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.452s | Train Loss: 0.021716 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.451s | Train Loss: 0.021605 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.452s | Train Loss: 0.021435 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.450s | Train Loss: 0.021446 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.453s | Train Loss: 0.021034 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.453s | Train Loss: 0.021049 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.451s | Train Loss: 0.020859 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.450s | Train Loss: 0.020989 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.451s | Train Loss: 0.020508 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.451s | Train Loss: 0.020539 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.450s | Train Loss: 0.020291 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.451s | Train Loss: 0.020322 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.451s | Train Loss: 0.020212 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.451s | Train Loss: 0.019952 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.451s | Train Loss: 0.019782 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.450s | Train Loss: 0.019562 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.453s | Train Loss: 0.019616 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.451s | Train Loss: 0.019405 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.453s | Train Loss: 0.019157 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.452s | Train Loss: 0.018864 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.453s | Train Loss: 0.018959 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.444s | Train Loss: 0.018531 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.450s | Train Loss: 0.018538 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.453s | Train Loss: 0.018359 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.453s | Train Loss: 0.018254 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.451s | Train Loss: 0.018105 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.454s | Train Loss: 0.017999 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.438s | Train Loss: 0.018005 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.448s | Train Loss: 0.017898 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.454s | Train Loss: 0.017679 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.461s | Train Loss: 0.017724 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.453s | Train Loss: 0.017486 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.449s | Train Loss: 0.017507 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.453s | Train Loss: 0.017382 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.453s | Train Loss: 0.017154 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.454s | Train Loss: 0.017243 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.453s | Train Loss: 0.017273 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.451s | Train Loss: 0.017055 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.454s | Train Loss: 0.016966 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.449s | Train Loss: 0.017047 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.451s | Train Loss: 0.017005 |\n",
      "INFO:root:Pretraining Time: 31.638s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016965\n",
      "INFO:root:Test AUC: 56.80%\n",
      "INFO:root:Test AUC: 56.80%\n",
      "INFO:root:Test Time: 0.237s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.708s | Train Loss: 0.648249 || Validation Loss: 0.008817 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.761s | Train Loss: 0.499903 || Validation Loss: 0.006350 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.738s | Train Loss: 0.447953 || Validation Loss: 0.006582 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.833s | Train Loss: 0.412753 || Validation Loss: 0.006318 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.813s | Train Loss: 0.394050 || Validation Loss: 0.006548 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.835s | Train Loss: 0.379441 || Validation Loss: 0.006083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.842s | Train Loss: 0.368057 || Validation Loss: 0.006018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.865s | Train Loss: 0.361957 || Validation Loss: 0.005543 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.887s | Train Loss: 0.346282 || Validation Loss: 0.005458 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.937s | Train Loss: 0.346402 || Validation Loss: 0.005747 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.946s | Train Loss: 0.338855 || Validation Loss: 0.006429 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.932s | Train Loss: 0.337121 || Validation Loss: 0.005929 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.012s | Train Loss: 0.327420 || Validation Loss: 0.005304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.001s | Train Loss: 0.324694 || Validation Loss: 0.005967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.008s | Train Loss: 0.320102 || Validation Loss: 0.006113 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.062s | Train Loss: 0.316704 || Validation Loss: 0.006005 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.037s | Train Loss: 0.317075 || Validation Loss: 0.005379 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.084s | Train Loss: 0.311206 || Validation Loss: 0.005765 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.098s | Train Loss: 0.307294 || Validation Loss: 0.006191 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.165s | Train Loss: 0.301348 || Validation Loss: 0.005462 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.140s | Train Loss: 0.302106 || Validation Loss: 0.006168 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.158s | Train Loss: 0.299696 || Validation Loss: 0.005807 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.258s | Train Loss: 0.289996 || Validation Loss: 0.005725 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.197s | Train Loss: 0.294308 || Validation Loss: 0.006120 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.236s | Train Loss: 0.290630 || Validation Loss: 0.005768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.236s | Train Loss: 0.292996 || Validation Loss: 0.005541 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.247s | Train Loss: 0.288368 || Validation Loss: 0.006243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.323s | Train Loss: 0.284227 || Validation Loss: 0.006045 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.415s | Train Loss: 0.288386 || Validation Loss: 0.005646 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.406s | Train Loss: 0.286816 || Validation Loss: 0.005799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 32.004s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.906%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.005952\n",
      "INFO:root:Test AUC: 86.57%\n",
      "INFO:root:Test Time: 0.208s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.7318867478437299\n",
      "validation AUC 0.788311736929611\n",
      "validation AUC 0.8124006314969873\n",
      "validation AUC 0.8268454519254081\n",
      "validation AUC 0.8356597284230901\n",
      "validation AUC 0.8420393475538646\n",
      "validation AUC 0.8472416945736767\n",
      "validation AUC 0.8503074205578792\n",
      "validation AUC 0.8545830678859261\n",
      "validation AUC 0.8563372424432897\n",
      "validation AUC 0.8587218417328236\n",
      "validation AUC 0.8607151424539963\n",
      "validation AUC 0.8617791439967664\n",
      "validation AUC 0.8622090384230199\n",
      "validation AUC 0.8640729953940396\n",
      "validation AUC 0.8651401258992307\n",
      "validation AUC 0.8654212110233985\n",
      "validation AUC 0.8649348782035414\n",
      "validation AUC 0.8652834355709387\n",
      "validation AUC 0.865173525444531\n",
      "validation AUC 0.8665399082128452\n",
      "validation AUC 0.8670408481786991\n",
      "validation AUC 0.8673179662255114\n",
      "validation AUC 0.8656949872532032\n",
      "validation AUC 0.8679697562367988\n",
      "validation AUC 0.8674165338631427\n",
      "validation AUC 0.8661043146050387\n",
      "validation AUC 0.8675519173621868\n",
      "validation AUC 0.8669392207563873\n",
      "validation AUC 0.8665561596605224\n",
      "validation AUC 0.8665561596605224\n",
      "train auc 0.9062264868305292\n",
      "Test AUC 0.8657173431123593\n",
      "test false positive rates [0.         0.         0.         ... 0.99920417 0.99920417 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 9.23852184e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.68934984e+01 1.58934984e+01 1.07987261e+01 ... 9.24865156e-03\n",
      " 9.12963971e-03 3.82126030e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 13.391s | Train Loss: 0.190686 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.533s | Train Loss: 0.141089 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.402s | Train Loss: 0.106584 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.435s | Train Loss: 0.083941 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.437s | Train Loss: 0.069153 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.423s | Train Loss: 0.059382 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.411s | Train Loss: 0.052475 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.464s | Train Loss: 0.047609 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.462s | Train Loss: 0.043765 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.456s | Train Loss: 0.040707 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.455s | Train Loss: 0.038070 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.460s | Train Loss: 0.036017 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.453s | Train Loss: 0.034137 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.452s | Train Loss: 0.032770 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.442s | Train Loss: 0.031456 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.439s | Train Loss: 0.030132 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.436s | Train Loss: 0.029102 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.429s | Train Loss: 0.028157 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.433s | Train Loss: 0.027589 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.433s | Train Loss: 0.026424 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.441s | Train Loss: 0.026071 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.430s | Train Loss: 0.025451 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.439s | Train Loss: 0.025102 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.439s | Train Loss: 0.024720 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.438s | Train Loss: 0.024111 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.438s | Train Loss: 0.023833 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.441s | Train Loss: 0.023445 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.447s | Train Loss: 0.023380 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.449s | Train Loss: 0.022799 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.447s | Train Loss: 0.022622 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.446s | Train Loss: 0.022368 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.444s | Train Loss: 0.022140 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.430s | Train Loss: 0.021968 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.443s | Train Loss: 0.022086 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.442s | Train Loss: 0.021578 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.448s | Train Loss: 0.021445 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.450s | Train Loss: 0.021154 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.444s | Train Loss: 0.021187 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.445s | Train Loss: 0.020704 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.426s | Train Loss: 0.020835 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.433s | Train Loss: 0.020607 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.442s | Train Loss: 0.020493 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.429s | Train Loss: 0.020044 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.429s | Train Loss: 0.019909 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.434s | Train Loss: 0.019790 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.437s | Train Loss: 0.019399 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.434s | Train Loss: 0.019533 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.443s | Train Loss: 0.019180 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.435s | Train Loss: 0.019135 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.442s | Train Loss: 0.018862 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.437s | Train Loss: 0.018690 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.443s | Train Loss: 0.018589 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.454s | Train Loss: 0.018596 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.455s | Train Loss: 0.018398 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.463s | Train Loss: 0.018368 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.435s | Train Loss: 0.017967 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.457s | Train Loss: 0.018035 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.447s | Train Loss: 0.018091 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.425s | Train Loss: 0.017674 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.440s | Train Loss: 0.017624 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.437s | Train Loss: 0.017547 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.444s | Train Loss: 0.017541 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.431s | Train Loss: 0.017520 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.448s | Train Loss: 0.017253 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.439s | Train Loss: 0.017201 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.435s | Train Loss: 0.017192 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.441s | Train Loss: 0.017216 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.442s | Train Loss: 0.016819 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.433s | Train Loss: 0.016873 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.463s | Train Loss: 0.016598 |\n",
      "INFO:root:Pretraining Time: 43.912s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016565\n",
      "INFO:root:Test AUC: 59.16%\n",
      "INFO:root:Test AUC: 59.16%\n",
      "INFO:root:Test Time: 0.219s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.685s | Train Loss: 0.724014 || Validation Loss: 0.008248 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.741s | Train Loss: 0.526948 || Validation Loss: 0.007382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.721s | Train Loss: 0.466874 || Validation Loss: 0.005699 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.789s | Train Loss: 0.433573 || Validation Loss: 0.006075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.760s | Train Loss: 0.411395 || Validation Loss: 0.006321 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.804s | Train Loss: 0.392576 || Validation Loss: 0.005770 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.814s | Train Loss: 0.380295 || Validation Loss: 0.005798 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.836s | Train Loss: 0.371238 || Validation Loss: 0.005876 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.848s | Train Loss: 0.369157 || Validation Loss: 0.005833 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.900s | Train Loss: 0.355296 || Validation Loss: 0.006459 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.906s | Train Loss: 0.341504 || Validation Loss: 0.006315 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.881s | Train Loss: 0.338035 || Validation Loss: 0.006337 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.984s | Train Loss: 0.337097 || Validation Loss: 0.005804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.991s | Train Loss: 0.334305 || Validation Loss: 0.005693 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.009s | Train Loss: 0.325683 || Validation Loss: 0.005479 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.068s | Train Loss: 0.322530 || Validation Loss: 0.006143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.068s | Train Loss: 0.320742 || Validation Loss: 0.005918 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.103s | Train Loss: 0.318288 || Validation Loss: 0.006123 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.071s | Train Loss: 0.307352 || Validation Loss: 0.005769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.132s | Train Loss: 0.310201 || Validation Loss: 0.006287 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.118s | Train Loss: 0.306485 || Validation Loss: 0.005399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.126s | Train Loss: 0.305098 || Validation Loss: 0.005672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.245s | Train Loss: 0.300417 || Validation Loss: 0.005928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.190s | Train Loss: 0.299693 || Validation Loss: 0.006325 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.214s | Train Loss: 0.292565 || Validation Loss: 0.005759 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.212s | Train Loss: 0.299950 || Validation Loss: 0.005748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.223s | Train Loss: 0.295186 || Validation Loss: 0.006231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.320s | Train Loss: 0.294971 || Validation Loss: 0.005987 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.311s | Train Loss: 0.290235 || Validation Loss: 0.006183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.382s | Train Loss: 0.291090 || Validation Loss: 0.006058 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.267s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.900%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.006314\n",
      "INFO:root:Test AUC: 87.46%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6992206941273207\n",
      "validation AUC 0.7759473519080178\n",
      "validation AUC 0.805258040422363\n",
      "validation AUC 0.8204723754124856\n",
      "validation AUC 0.8315011005646393\n",
      "validation AUC 0.8384147931383344\n",
      "validation AUC 0.8437308949705229\n",
      "validation AUC 0.8475537101299414\n",
      "validation AUC 0.8506249836368037\n",
      "validation AUC 0.852981765492568\n",
      "validation AUC 0.8564285251326987\n",
      "validation AUC 0.8577968182709268\n",
      "validation AUC 0.8595181174374096\n",
      "validation AUC 0.8598461290158477\n",
      "validation AUC 0.8621070757547342\n",
      "validation AUC 0.8634195743844831\n",
      "validation AUC 0.8646413916390293\n",
      "validation AUC 0.8649095751145743\n",
      "validation AUC 0.8661260976112819\n",
      "validation AUC 0.8674500238716418\n",
      "validation AUC 0.8677727380315592\n",
      "validation AUC 0.8678581592377932\n",
      "validation AUC 0.8687627406773417\n",
      "validation AUC 0.8688950803557993\n",
      "validation AUC 0.8691771180042414\n",
      "validation AUC 0.8683153229972563\n",
      "validation AUC 0.8688979858209046\n",
      "validation AUC 0.8686527879800164\n",
      "validation AUC 0.8691384556292908\n",
      "validation AUC 0.8688454692730568\n",
      "validation AUC 0.8688454692730568\n",
      "train auc 0.8995632823811212\n",
      "Test AUC 0.8745568034552652\n",
      "test false positive rates [0.         0.         0.         ... 0.99821885 0.99821885 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.37650928e+01 2.27650928e+01 1.40485058e+01 ... 6.53833570e-03\n",
      " 6.51962869e-03 1.32616365e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.475s | Train Loss: 0.189513 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.452s | Train Loss: 0.139934 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.454s | Train Loss: 0.105311 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.453s | Train Loss: 0.083402 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.455s | Train Loss: 0.069073 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.452s | Train Loss: 0.059780 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.447s | Train Loss: 0.053172 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.451s | Train Loss: 0.048235 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.450s | Train Loss: 0.044589 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.447s | Train Loss: 0.041697 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.453s | Train Loss: 0.038998 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.452s | Train Loss: 0.036831 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.453s | Train Loss: 0.035279 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.449s | Train Loss: 0.033891 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.448s | Train Loss: 0.032401 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.449s | Train Loss: 0.031340 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.452s | Train Loss: 0.030104 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.447s | Train Loss: 0.029134 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.451s | Train Loss: 0.028269 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.450s | Train Loss: 0.027496 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.452s | Train Loss: 0.026631 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.449s | Train Loss: 0.025869 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.453s | Train Loss: 0.025264 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.453s | Train Loss: 0.024527 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.437s | Train Loss: 0.024066 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.451s | Train Loss: 0.023508 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.454s | Train Loss: 0.023244 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.452s | Train Loss: 0.022575 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.449s | Train Loss: 0.022423 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.451s | Train Loss: 0.022020 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.453s | Train Loss: 0.021786 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.455s | Train Loss: 0.021393 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.450s | Train Loss: 0.021272 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.449s | Train Loss: 0.020918 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.451s | Train Loss: 0.020712 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.451s | Train Loss: 0.020491 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.456s | Train Loss: 0.020270 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.454s | Train Loss: 0.019959 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.454s | Train Loss: 0.019725 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.448s | Train Loss: 0.019875 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.451s | Train Loss: 0.019576 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.450s | Train Loss: 0.019400 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.453s | Train Loss: 0.019272 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.454s | Train Loss: 0.018909 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.457s | Train Loss: 0.018804 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.451s | Train Loss: 0.018878 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.450s | Train Loss: 0.018614 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.452s | Train Loss: 0.018693 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.452s | Train Loss: 0.018404 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.447s | Train Loss: 0.018237 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.453s | Train Loss: 0.018111 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.453s | Train Loss: 0.018068 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.451s | Train Loss: 0.018023 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.451s | Train Loss: 0.017740 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.452s | Train Loss: 0.017712 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.451s | Train Loss: 0.017647 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.454s | Train Loss: 0.017510 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.454s | Train Loss: 0.017510 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.457s | Train Loss: 0.017434 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.451s | Train Loss: 0.017238 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.451s | Train Loss: 0.017174 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.454s | Train Loss: 0.017027 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.449s | Train Loss: 0.016912 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.453s | Train Loss: 0.016827 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.453s | Train Loss: 0.017030 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.451s | Train Loss: 0.016620 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.454s | Train Loss: 0.016658 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.450s | Train Loss: 0.016515 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.451s | Train Loss: 0.016445 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.454s | Train Loss: 0.016551 |\n",
      "INFO:root:Pretraining Time: 31.634s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016411\n",
      "INFO:root:Test AUC: 56.16%\n",
      "INFO:root:Test AUC: 56.16%\n",
      "INFO:root:Test Time: 0.238s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.720s | Train Loss: 0.710168 || Validation Loss: 0.007281 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.754s | Train Loss: 0.521454 || Validation Loss: 0.006286 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.740s | Train Loss: 0.461139 || Validation Loss: 0.006385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.811s | Train Loss: 0.424757 || Validation Loss: 0.005967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.801s | Train Loss: 0.401991 || Validation Loss: 0.006243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.834s | Train Loss: 0.389346 || Validation Loss: 0.005858 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.855s | Train Loss: 0.376865 || Validation Loss: 0.005789 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.858s | Train Loss: 0.358655 || Validation Loss: 0.005768 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.877s | Train Loss: 0.357391 || Validation Loss: 0.006750 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.933s | Train Loss: 0.350491 || Validation Loss: 0.005961 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.946s | Train Loss: 0.343645 || Validation Loss: 0.005471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.925s | Train Loss: 0.341474 || Validation Loss: 0.005425 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.014s | Train Loss: 0.326095 || Validation Loss: 0.005672 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.987s | Train Loss: 0.331592 || Validation Loss: 0.005488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.009s | Train Loss: 0.324194 || Validation Loss: 0.005849 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.056s | Train Loss: 0.320634 || Validation Loss: 0.006381 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.044s | Train Loss: 0.316183 || Validation Loss: 0.005730 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.096s | Train Loss: 0.308763 || Validation Loss: 0.006183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.101s | Train Loss: 0.309564 || Validation Loss: 0.005923 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.152s | Train Loss: 0.304060 || Validation Loss: 0.005924 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.145s | Train Loss: 0.301290 || Validation Loss: 0.006093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.158s | Train Loss: 0.301602 || Validation Loss: 0.005879 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.245s | Train Loss: 0.296995 || Validation Loss: 0.005576 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.185s | Train Loss: 0.288415 || Validation Loss: 0.006068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.247s | Train Loss: 0.291304 || Validation Loss: 0.005842 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.230s | Train Loss: 0.291193 || Validation Loss: 0.005611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.228s | Train Loss: 0.286733 || Validation Loss: 0.006190 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.321s | Train Loss: 0.290473 || Validation Loss: 0.006222 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.354s | Train Loss: 0.281861 || Validation Loss: 0.005603 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.395s | Train Loss: 0.279038 || Validation Loss: 0.006320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.842s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.901%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.006436\n",
      "INFO:root:Test AUC: 86.83%\n",
      "INFO:root:Test Time: 0.203s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6848090151580136\n",
      "validation AUC 0.7647519323471498\n",
      "validation AUC 0.7994563800288759\n",
      "validation AUC 0.8179205469170887\n",
      "validation AUC 0.8289751046339933\n",
      "validation AUC 0.8361138111124141\n",
      "validation AUC 0.8408341356177456\n",
      "validation AUC 0.8463007123391592\n",
      "validation AUC 0.8486124062721434\n",
      "validation AUC 0.8510866120212183\n",
      "validation AUC 0.8540969106711028\n",
      "validation AUC 0.8562671228211406\n",
      "validation AUC 0.857901744939329\n",
      "validation AUC 0.8589666537747738\n",
      "validation AUC 0.859699469545085\n",
      "validation AUC 0.8603010897303409\n",
      "validation AUC 0.8605318560302663\n",
      "validation AUC 0.862667045618782\n",
      "validation AUC 0.8639823938393072\n",
      "validation AUC 0.8636990178676526\n",
      "validation AUC 0.863702960998867\n",
      "validation AUC 0.8646850055437977\n",
      "validation AUC 0.8653065569003945\n",
      "validation AUC 0.8657048903128386\n",
      "validation AUC 0.86439361293628\n",
      "validation AUC 0.8655319433006465\n",
      "validation AUC 0.8663288362835335\n",
      "validation AUC 0.8648932438464272\n",
      "validation AUC 0.8667818759853838\n",
      "validation AUC 0.8655629854814016\n",
      "validation AUC 0.8655629854814016\n",
      "train auc 0.901358116845281\n",
      "Test AUC 0.8682522997173128\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.98616769e-01\n",
      " 9.98616769e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.35344086e+01 2.25344086e+01 1.93823624e+01 ... 7.62193464e-03\n",
      " 7.61335157e-03 3.21088079e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.447s | Train Loss: 0.196558 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.442s | Train Loss: 0.147528 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.443s | Train Loss: 0.111621 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.439s | Train Loss: 0.088194 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.439s | Train Loss: 0.072954 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.442s | Train Loss: 0.062723 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.445s | Train Loss: 0.055315 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.424s | Train Loss: 0.049893 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.440s | Train Loss: 0.045599 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.442s | Train Loss: 0.042068 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.440s | Train Loss: 0.039575 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.442s | Train Loss: 0.036989 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.439s | Train Loss: 0.034435 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.445s | Train Loss: 0.032938 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.440s | Train Loss: 0.031171 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.441s | Train Loss: 0.029722 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.442s | Train Loss: 0.028706 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.442s | Train Loss: 0.027929 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.443s | Train Loss: 0.026951 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.442s | Train Loss: 0.026161 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.440s | Train Loss: 0.025841 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.439s | Train Loss: 0.024987 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.437s | Train Loss: 0.024678 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.439s | Train Loss: 0.024226 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.446s | Train Loss: 0.023805 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.441s | Train Loss: 0.023231 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.439s | Train Loss: 0.022922 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.443s | Train Loss: 0.022847 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.441s | Train Loss: 0.022577 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.444s | Train Loss: 0.022254 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.437s | Train Loss: 0.021964 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.435s | Train Loss: 0.021706 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.432s | Train Loss: 0.021239 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.440s | Train Loss: 0.021356 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.435s | Train Loss: 0.021136 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.439s | Train Loss: 0.020946 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.439s | Train Loss: 0.020595 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.438s | Train Loss: 0.020477 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.438s | Train Loss: 0.020374 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.435s | Train Loss: 0.019984 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.439s | Train Loss: 0.020062 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.437s | Train Loss: 0.020015 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.439s | Train Loss: 0.019800 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.435s | Train Loss: 0.019622 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.440s | Train Loss: 0.019487 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.437s | Train Loss: 0.019370 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.438s | Train Loss: 0.019302 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.444s | Train Loss: 0.019200 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.439s | Train Loss: 0.019133 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.439s | Train Loss: 0.018957 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.438s | Train Loss: 0.018887 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.438s | Train Loss: 0.018743 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.442s | Train Loss: 0.018643 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.441s | Train Loss: 0.018372 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.436s | Train Loss: 0.018397 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.438s | Train Loss: 0.018391 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.439s | Train Loss: 0.018166 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.435s | Train Loss: 0.017824 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.440s | Train Loss: 0.017709 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.438s | Train Loss: 0.017922 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.439s | Train Loss: 0.017710 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.438s | Train Loss: 0.017537 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.436s | Train Loss: 0.017334 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.439s | Train Loss: 0.017105 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.442s | Train Loss: 0.017018 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.441s | Train Loss: 0.017102 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.440s | Train Loss: 0.017100 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.438s | Train Loss: 0.016910 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.435s | Train Loss: 0.016675 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.435s | Train Loss: 0.016869 |\n",
      "INFO:root:Pretraining Time: 30.762s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016511\n",
      "INFO:root:Test AUC: 56.95%\n",
      "INFO:root:Test AUC: 56.95%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.680s | Train Loss: 0.829330 || Validation Loss: 0.007510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.724s | Train Loss: 0.586298 || Validation Loss: 0.006842 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.718s | Train Loss: 0.516085 || Validation Loss: 0.006282 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.775s | Train Loss: 0.472989 || Validation Loss: 0.006036 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.775s | Train Loss: 0.437926 || Validation Loss: 0.005904 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.820s | Train Loss: 0.414369 || Validation Loss: 0.005838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.839s | Train Loss: 0.402414 || Validation Loss: 0.005843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.847s | Train Loss: 0.392695 || Validation Loss: 0.006017 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.864s | Train Loss: 0.382302 || Validation Loss: 0.005326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.895s | Train Loss: 0.379212 || Validation Loss: 0.006199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.959s | Train Loss: 0.366134 || Validation Loss: 0.005378 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.945s | Train Loss: 0.360809 || Validation Loss: 0.005519 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.031s | Train Loss: 0.355026 || Validation Loss: 0.005680 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.025s | Train Loss: 0.356557 || Validation Loss: 0.006026 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.017s | Train Loss: 0.354807 || Validation Loss: 0.005898 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.052s | Train Loss: 0.352531 || Validation Loss: 0.006282 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.009s | Train Loss: 0.338712 || Validation Loss: 0.005616 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.072s | Train Loss: 0.342663 || Validation Loss: 0.006173 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.070s | Train Loss: 0.327167 || Validation Loss: 0.006033 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.147s | Train Loss: 0.332331 || Validation Loss: 0.006098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.118s | Train Loss: 0.331639 || Validation Loss: 0.005800 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.132s | Train Loss: 0.323733 || Validation Loss: 0.005517 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.256s | Train Loss: 0.324714 || Validation Loss: 0.005614 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.214s | Train Loss: 0.323996 || Validation Loss: 0.006447 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.231s | Train Loss: 0.321467 || Validation Loss: 0.006394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.206s | Train Loss: 0.326147 || Validation Loss: 0.005992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.213s | Train Loss: 0.314683 || Validation Loss: 0.006012 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.337s | Train Loss: 0.317996 || Validation Loss: 0.005579 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.357s | Train Loss: 0.308958 || Validation Loss: 0.005912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.395s | Train Loss: 0.314795 || Validation Loss: 0.006020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.582s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.878%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.006206\n",
      "INFO:root:Test AUC: 86.51%\n",
      "INFO:root:Test Time: 0.201s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5673672553656915\n",
      "validation AUC 0.6730107328732411\n",
      "validation AUC 0.7437325658790266\n",
      "validation AUC 0.7825755213394172\n",
      "validation AUC 0.8063683032488208\n",
      "validation AUC 0.8184237404808767\n",
      "validation AUC 0.828763048252219\n",
      "validation AUC 0.8349040627129211\n",
      "validation AUC 0.8394353936309225\n",
      "validation AUC 0.8428068426576768\n",
      "validation AUC 0.8452924866800922\n",
      "validation AUC 0.847985171698088\n",
      "validation AUC 0.8498722340344693\n",
      "validation AUC 0.8512547937513558\n",
      "validation AUC 0.8516573682914353\n",
      "validation AUC 0.8526732726797094\n",
      "validation AUC 0.8539773795173863\n",
      "validation AUC 0.8550961591879852\n",
      "validation AUC 0.8568306021251827\n",
      "validation AUC 0.8575576921071177\n",
      "validation AUC 0.8575660732564602\n",
      "validation AUC 0.8577622373826719\n",
      "validation AUC 0.8584552892555475\n",
      "validation AUC 0.8590708115059824\n",
      "validation AUC 0.8596799135299528\n",
      "validation AUC 0.860583835120283\n",
      "validation AUC 0.8611433127580994\n",
      "validation AUC 0.861911701851175\n",
      "validation AUC 0.8630571176125889\n",
      "validation AUC 0.8634911760066905\n",
      "validation AUC 0.8634911760066905\n",
      "train auc 0.8783381694309447\n",
      "Test AUC 0.8650763584126872\n",
      "test false positive rates [0.         0.         0.         ... 0.99984841 0.99984841 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [1.76320896e+01 1.66320896e+01 1.34444370e+01 ... 1.21497875e-02\n",
      " 1.21125514e-02 6.52586948e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.481s | Train Loss: 0.191404 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.458s | Train Loss: 0.142810 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.454s | Train Loss: 0.107167 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.459s | Train Loss: 0.084004 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.447s | Train Loss: 0.068533 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.444s | Train Loss: 0.058957 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.442s | Train Loss: 0.052599 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.441s | Train Loss: 0.047835 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.440s | Train Loss: 0.043804 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.448s | Train Loss: 0.041128 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.447s | Train Loss: 0.038787 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.440s | Train Loss: 0.036783 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.445s | Train Loss: 0.034753 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.451s | Train Loss: 0.033362 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.467s | Train Loss: 0.031749 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.468s | Train Loss: 0.030529 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.467s | Train Loss: 0.029358 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.472s | Train Loss: 0.028357 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.457s | Train Loss: 0.027583 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.466s | Train Loss: 0.026793 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.466s | Train Loss: 0.026000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.451s | Train Loss: 0.025276 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.445s | Train Loss: 0.025019 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.449s | Train Loss: 0.024577 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.444s | Train Loss: 0.024063 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.446s | Train Loss: 0.023796 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.456s | Train Loss: 0.023621 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.453s | Train Loss: 0.023158 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.442s | Train Loss: 0.022811 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.441s | Train Loss: 0.022657 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.442s | Train Loss: 0.022370 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.446s | Train Loss: 0.022293 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.444s | Train Loss: 0.022043 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.445s | Train Loss: 0.022028 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.446s | Train Loss: 0.021709 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.450s | Train Loss: 0.021659 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.440s | Train Loss: 0.021569 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.450s | Train Loss: 0.021406 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.444s | Train Loss: 0.021166 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.449s | Train Loss: 0.021124 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.447s | Train Loss: 0.020896 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.437s | Train Loss: 0.020698 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.443s | Train Loss: 0.020499 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.443s | Train Loss: 0.020652 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.453s | Train Loss: 0.020549 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.453s | Train Loss: 0.020150 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.450s | Train Loss: 0.020188 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.506s | Train Loss: 0.020024 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.452s | Train Loss: 0.020011 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.450s | Train Loss: 0.019783 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.450s | Train Loss: 0.019902 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.449s | Train Loss: 0.019900 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.446s | Train Loss: 0.019632 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.451s | Train Loss: 0.019547 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.449s | Train Loss: 0.019461 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.452s | Train Loss: 0.019305 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.449s | Train Loss: 0.019401 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.451s | Train Loss: 0.019373 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.434s | Train Loss: 0.019106 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.451s | Train Loss: 0.019078 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.453s | Train Loss: 0.019017 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.451s | Train Loss: 0.018979 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.448s | Train Loss: 0.018721 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.451s | Train Loss: 0.018844 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.451s | Train Loss: 0.018614 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.450s | Train Loss: 0.018677 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.451s | Train Loss: 0.018767 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.459s | Train Loss: 0.018552 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.454s | Train Loss: 0.018516 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.459s | Train Loss: 0.018315 |\n",
      "INFO:root:Pretraining Time: 31.595s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018408\n",
      "INFO:root:Test AUC: 60.14%\n",
      "INFO:root:Test AUC: 60.14%\n",
      "INFO:root:Test Time: 0.239s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.725s | Train Loss: 0.710596 || Validation Loss: 0.007136 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.786s | Train Loss: 0.525567 || Validation Loss: 0.006711 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.752s | Train Loss: 0.473962 || Validation Loss: 0.005985 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.836s | Train Loss: 0.450470 || Validation Loss: 0.005951 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.816s | Train Loss: 0.421268 || Validation Loss: 0.005794 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.873s | Train Loss: 0.403270 || Validation Loss: 0.006290 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.842s | Train Loss: 0.392544 || Validation Loss: 0.006092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.852s | Train Loss: 0.381566 || Validation Loss: 0.005312 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.879s | Train Loss: 0.373920 || Validation Loss: 0.006451 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.920s | Train Loss: 0.365755 || Validation Loss: 0.005938 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.947s | Train Loss: 0.357893 || Validation Loss: 0.005246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.921s | Train Loss: 0.355302 || Validation Loss: 0.006107 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.011s | Train Loss: 0.349751 || Validation Loss: 0.005806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.988s | Train Loss: 0.352705 || Validation Loss: 0.005665 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.001s | Train Loss: 0.342174 || Validation Loss: 0.005485 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.059s | Train Loss: 0.329078 || Validation Loss: 0.005635 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.041s | Train Loss: 0.332798 || Validation Loss: 0.005612 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.085s | Train Loss: 0.330413 || Validation Loss: 0.005912 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.095s | Train Loss: 0.327836 || Validation Loss: 0.005875 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.149s | Train Loss: 0.326836 || Validation Loss: 0.005054 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.130s | Train Loss: 0.325768 || Validation Loss: 0.005510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.153s | Train Loss: 0.317243 || Validation Loss: 0.005290 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.242s | Train Loss: 0.319988 || Validation Loss: 0.005600 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.189s | Train Loss: 0.315337 || Validation Loss: 0.005232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.248s | Train Loss: 0.310232 || Validation Loss: 0.005890 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.222s | Train Loss: 0.312485 || Validation Loss: 0.005490 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.227s | Train Loss: 0.313571 || Validation Loss: 0.005613 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.329s | Train Loss: 0.303453 || Validation Loss: 0.005910 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.345s | Train Loss: 0.305604 || Validation Loss: 0.005585 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.385s | Train Loss: 0.302068 || Validation Loss: 0.006256 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.872s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.891%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.005910\n",
      "INFO:root:Test AUC: 86.73%\n",
      "INFO:root:Test Time: 0.203s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6660415650729464\n",
      "validation AUC 0.7525801062272099\n",
      "validation AUC 0.7892318274864661\n",
      "validation AUC 0.8069404164670267\n",
      "validation AUC 0.820371530230779\n",
      "validation AUC 0.8289676600448315\n",
      "validation AUC 0.8351351855442788\n",
      "validation AUC 0.838896562887994\n",
      "validation AUC 0.842269640252335\n",
      "validation AUC 0.846316607255404\n",
      "validation AUC 0.8485246383440934\n",
      "validation AUC 0.850211965918575\n",
      "validation AUC 0.8520419832257815\n",
      "validation AUC 0.8532101291963483\n",
      "validation AUC 0.8546416694142646\n",
      "validation AUC 0.8550048445703882\n",
      "validation AUC 0.8566162495745568\n",
      "validation AUC 0.8574398744328223\n",
      "validation AUC 0.859465930814171\n",
      "validation AUC 0.8595513307349465\n",
      "validation AUC 0.8600693043890828\n",
      "validation AUC 0.8595435136502582\n",
      "validation AUC 0.8603065787179897\n",
      "validation AUC 0.861212503802115\n",
      "validation AUC 0.8616023363345121\n",
      "validation AUC 0.861671314523941\n",
      "validation AUC 0.8627121415236088\n",
      "validation AUC 0.8627903123704912\n",
      "validation AUC 0.8628461760567112\n",
      "validation AUC 0.8637156497929032\n",
      "validation AUC 0.8637156497929032\n",
      "train auc 0.8906601460818241\n",
      "Test AUC 0.8673430725740443\n",
      "test false positive rates [0.         0.         0.         ... 0.99753671 0.99753671 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.16034946e+01 2.06034946e+01 1.54967785e+01 ... 7.03674648e-03\n",
      " 7.00785452e-03 1.06670219e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.469s | Train Loss: 0.196491 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.458s | Train Loss: 0.145866 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.450s | Train Loss: 0.109413 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.455s | Train Loss: 0.086149 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.448s | Train Loss: 0.070916 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.454s | Train Loss: 0.061252 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.451s | Train Loss: 0.054268 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.442s | Train Loss: 0.049397 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.449s | Train Loss: 0.045379 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.445s | Train Loss: 0.042472 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.435s | Train Loss: 0.040154 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.435s | Train Loss: 0.037864 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.442s | Train Loss: 0.035753 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.434s | Train Loss: 0.034334 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.431s | Train Loss: 0.032648 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.436s | Train Loss: 0.031007 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.433s | Train Loss: 0.029663 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.441s | Train Loss: 0.028924 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.435s | Train Loss: 0.028054 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.442s | Train Loss: 0.027035 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.450s | Train Loss: 0.026554 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.443s | Train Loss: 0.026150 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.434s | Train Loss: 0.025300 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.436s | Train Loss: 0.024916 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.449s | Train Loss: 0.024615 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.439s | Train Loss: 0.024265 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.434s | Train Loss: 0.023963 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.437s | Train Loss: 0.023810 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.436s | Train Loss: 0.023569 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.444s | Train Loss: 0.023066 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.445s | Train Loss: 0.022874 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.449s | Train Loss: 0.022767 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.443s | Train Loss: 0.022216 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.440s | Train Loss: 0.022252 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.472s | Train Loss: 0.022136 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.441s | Train Loss: 0.021989 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.441s | Train Loss: 0.021725 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.438s | Train Loss: 0.021484 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.442s | Train Loss: 0.021338 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.442s | Train Loss: 0.021035 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.432s | Train Loss: 0.021015 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.433s | Train Loss: 0.020837 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.436s | Train Loss: 0.020679 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.429s | Train Loss: 0.020275 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.436s | Train Loss: 0.020342 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.427s | Train Loss: 0.019970 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.440s | Train Loss: 0.019627 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.432s | Train Loss: 0.019775 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.433s | Train Loss: 0.019269 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.447s | Train Loss: 0.019221 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.531s | Train Loss: 0.018985 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.439s | Train Loss: 0.018718 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.501s | Train Loss: 0.018929 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.450s | Train Loss: 0.018464 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.444s | Train Loss: 0.018468 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.443s | Train Loss: 0.018366 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.451s | Train Loss: 0.018340 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.467s | Train Loss: 0.018067 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.460s | Train Loss: 0.017986 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.452s | Train Loss: 0.018114 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.432s | Train Loss: 0.017699 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.449s | Train Loss: 0.017644 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.446s | Train Loss: 0.017749 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.456s | Train Loss: 0.017550 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.443s | Train Loss: 0.017473 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.435s | Train Loss: 0.017484 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.438s | Train Loss: 0.017515 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.443s | Train Loss: 0.017326 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.442s | Train Loss: 0.017175 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.447s | Train Loss: 0.017133 |\n",
      "INFO:root:Pretraining Time: 31.175s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017209\n",
      "INFO:root:Test AUC: 57.57%\n",
      "INFO:root:Test AUC: 57.57%\n",
      "INFO:root:Test Time: 0.223s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.691s | Train Loss: 0.699179 || Validation Loss: 0.007979 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.736s | Train Loss: 0.527289 || Validation Loss: 0.006537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.713s | Train Loss: 0.468276 || Validation Loss: 0.005664 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.777s | Train Loss: 0.430485 || Validation Loss: 0.006252 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.782s | Train Loss: 0.418946 || Validation Loss: 0.005609 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.805s | Train Loss: 0.393952 || Validation Loss: 0.005526 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.821s | Train Loss: 0.379641 || Validation Loss: 0.005524 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.852s | Train Loss: 0.371464 || Validation Loss: 0.005848 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.867s | Train Loss: 0.357031 || Validation Loss: 0.006199 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.933s | Train Loss: 0.355183 || Validation Loss: 0.005349 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.969s | Train Loss: 0.345574 || Validation Loss: 0.006143 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.902s | Train Loss: 0.344404 || Validation Loss: 0.005851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.997s | Train Loss: 0.340016 || Validation Loss: 0.006210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.977s | Train Loss: 0.334951 || Validation Loss: 0.005454 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.968s | Train Loss: 0.331286 || Validation Loss: 0.005840 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.038s | Train Loss: 0.328070 || Validation Loss: 0.006393 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.004s | Train Loss: 0.325702 || Validation Loss: 0.006009 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.061s | Train Loss: 0.316058 || Validation Loss: 0.005742 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.077s | Train Loss: 0.311863 || Validation Loss: 0.005510 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.146s | Train Loss: 0.313261 || Validation Loss: 0.005973 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.133s | Train Loss: 0.304814 || Validation Loss: 0.006008 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.121s | Train Loss: 0.307256 || Validation Loss: 0.005085 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.241s | Train Loss: 0.304163 || Validation Loss: 0.005618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.174s | Train Loss: 0.309940 || Validation Loss: 0.006267 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.239s | Train Loss: 0.298022 || Validation Loss: 0.005813 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.214s | Train Loss: 0.297445 || Validation Loss: 0.005310 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.220s | Train Loss: 0.294726 || Validation Loss: 0.005774 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.310s | Train Loss: 0.293333 || Validation Loss: 0.005482 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.343s | Train Loss: 0.290934 || Validation Loss: 0.005751 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.402s | Train Loss: 0.292724 || Validation Loss: 0.005869 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 31.377s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.899%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.006316\n",
      "INFO:root:Test AUC: 86.96%\n",
      "INFO:root:Test Time: 0.201s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 6156})\n",
      "class weights 0.19471153846153846 0.8052884615384616\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6550991072240079\n",
      "validation AUC 0.7402992267205728\n",
      "validation AUC 0.7841703210251588\n",
      "validation AUC 0.8086001181768665\n",
      "validation AUC 0.8250123961189797\n",
      "validation AUC 0.8340397985927331\n",
      "validation AUC 0.8397713659708947\n",
      "validation AUC 0.8448011730629115\n",
      "validation AUC 0.8479858847609527\n",
      "validation AUC 0.850444121094677\n",
      "validation AUC 0.8536138371871012\n",
      "validation AUC 0.8552040392137746\n",
      "validation AUC 0.8564862645999621\n",
      "validation AUC 0.8577902969385338\n",
      "validation AUC 0.8590888535928684\n",
      "validation AUC 0.8605844284524429\n",
      "validation AUC 0.8609114928279711\n",
      "validation AUC 0.862451163175816\n",
      "validation AUC 0.8636921639499682\n",
      "validation AUC 0.8643670273984295\n",
      "validation AUC 0.864826019046654\n",
      "validation AUC 0.8654745736682273\n",
      "validation AUC 0.8651690049452507\n",
      "validation AUC 0.8655782444945694\n",
      "validation AUC 0.8661993142676643\n",
      "validation AUC 0.8666876851701868\n",
      "validation AUC 0.8671050717288027\n",
      "validation AUC 0.8666787346348257\n",
      "validation AUC 0.8669960343061995\n",
      "validation AUC 0.8673754369638591\n",
      "validation AUC 0.8673754369638591\n",
      "train auc 0.8994007136510123\n",
      "Test AUC 0.8695617713008646\n",
      "test false positive rates [0.         0.         0.         ... 0.99437234 0.99437234 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 3.35946249e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [2.24303608e+01 2.14303608e+01 1.51379299e+01 ... 1.72639694e-02\n",
      " 1.72591414e-02 2.89439037e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {4..10}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m8_exc_balanced_batch\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 0 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c97b71e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.392s | Train Loss: 0.199474 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.371s | Train Loss: 0.155454 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.371s | Train Loss: 0.121901 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.371s | Train Loss: 0.097568 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.370s | Train Loss: 0.080552 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.369s | Train Loss: 0.068872 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.370s | Train Loss: 0.060071 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.368s | Train Loss: 0.054324 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.368s | Train Loss: 0.049496 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.371s | Train Loss: 0.045542 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.370s | Train Loss: 0.042639 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.372s | Train Loss: 0.040107 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.368s | Train Loss: 0.038153 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.370s | Train Loss: 0.036265 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.373s | Train Loss: 0.034675 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.373s | Train Loss: 0.033374 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.373s | Train Loss: 0.032179 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.372s | Train Loss: 0.031158 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.374s | Train Loss: 0.029873 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.372s | Train Loss: 0.028954 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.373s | Train Loss: 0.028526 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.372s | Train Loss: 0.027760 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.372s | Train Loss: 0.027056 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.372s | Train Loss: 0.026385 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.371s | Train Loss: 0.025850 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.373s | Train Loss: 0.025583 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.370s | Train Loss: 0.025141 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.372s | Train Loss: 0.024645 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.371s | Train Loss: 0.024460 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.371s | Train Loss: 0.024080 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.371s | Train Loss: 0.023626 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.373s | Train Loss: 0.023584 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.373s | Train Loss: 0.023205 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.372s | Train Loss: 0.023095 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.374s | Train Loss: 0.022662 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.371s | Train Loss: 0.022581 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.371s | Train Loss: 0.022430 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.371s | Train Loss: 0.022344 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.372s | Train Loss: 0.022412 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.373s | Train Loss: 0.022162 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.372s | Train Loss: 0.021888 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.371s | Train Loss: 0.021638 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.371s | Train Loss: 0.021504 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.372s | Train Loss: 0.021361 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.373s | Train Loss: 0.021177 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.384s | Train Loss: 0.020936 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.384s | Train Loss: 0.020668 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.385s | Train Loss: 0.021079 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.384s | Train Loss: 0.020592 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.382s | Train Loss: 0.020687 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.381s | Train Loss: 0.020665 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.388s | Train Loss: 0.020517 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.386s | Train Loss: 0.020152 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.388s | Train Loss: 0.020126 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.384s | Train Loss: 0.020240 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.384s | Train Loss: 0.020097 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.384s | Train Loss: 0.019806 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.385s | Train Loss: 0.019700 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.386s | Train Loss: 0.019665 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.385s | Train Loss: 0.019424 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.384s | Train Loss: 0.019355 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.385s | Train Loss: 0.019256 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.387s | Train Loss: 0.018906 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.384s | Train Loss: 0.018986 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.384s | Train Loss: 0.019046 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.383s | Train Loss: 0.018801 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.387s | Train Loss: 0.018604 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.384s | Train Loss: 0.018700 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.383s | Train Loss: 0.018384 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.384s | Train Loss: 0.018363 |\n",
      "INFO:root:Pretraining Time: 26.362s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017985\n",
      "INFO:root:Test AUC: 59.45%\n",
      "INFO:root:Test AUC: 59.45%\n",
      "INFO:root:Test Time: 0.236s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.679s | Train Loss: 0.607771 || Validation Loss: 0.007301 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.665s | Train Loss: 0.404637 || Validation Loss: 0.004903 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.718s | Train Loss: 0.322348 || Validation Loss: 0.003410 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.737s | Train Loss: 0.279308 || Validation Loss: 0.003067 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.738s | Train Loss: 0.259126 || Validation Loss: 0.002650 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.790s | Train Loss: 0.252794 || Validation Loss: 0.002611 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.778s | Train Loss: 0.246774 || Validation Loss: 0.002483 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.796s | Train Loss: 0.236806 || Validation Loss: 0.002534 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.796s | Train Loss: 0.230641 || Validation Loss: 0.002390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.812s | Train Loss: 0.223798 || Validation Loss: 0.002232 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.851s | Train Loss: 0.222979 || Validation Loss: 0.002215 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.840s | Train Loss: 0.216907 || Validation Loss: 0.002303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.878s | Train Loss: 0.217792 || Validation Loss: 0.002427 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.888s | Train Loss: 0.212384 || Validation Loss: 0.002184 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.898s | Train Loss: 0.220320 || Validation Loss: 0.002162 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.928s | Train Loss: 0.215509 || Validation Loss: 0.002160 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.932s | Train Loss: 0.212264 || Validation Loss: 0.001878 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.952s | Train Loss: 0.213027 || Validation Loss: 0.002178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.988s | Train Loss: 0.210536 || Validation Loss: 0.002047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.964s | Train Loss: 0.209470 || Validation Loss: 0.002088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.072s | Train Loss: 0.197224 || Validation Loss: 0.002093 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.031s | Train Loss: 0.197619 || Validation Loss: 0.002150 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.020s | Train Loss: 0.199832 || Validation Loss: 0.002105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.096s | Train Loss: 0.199467 || Validation Loss: 0.002092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.081s | Train Loss: 0.207456 || Validation Loss: 0.002416 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.162s | Train Loss: 0.198697 || Validation Loss: 0.002054 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.072s | Train Loss: 0.203487 || Validation Loss: 0.002272 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.176s | Train Loss: 0.193422 || Validation Loss: 0.002115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.164s | Train Loss: 0.193234 || Validation Loss: 0.002106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.239s | Train Loss: 0.196163 || Validation Loss: 0.001952 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.429s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.848%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.002321\n",
      "INFO:root:Test AUC: 86.01%\n",
      "INFO:root:Test Time: 0.195s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1477})\n",
      "class weights 0.05483164420685303 0.945168355793147\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6515390637122733\n",
      "validation AUC 0.7130483586463555\n",
      "validation AUC 0.7587969607770724\n",
      "validation AUC 0.7850030800058663\n",
      "validation AUC 0.8016505542839859\n",
      "validation AUC 0.8126661782363421\n",
      "validation AUC 0.8206945157802941\n",
      "validation AUC 0.8241837798206616\n",
      "validation AUC 0.8278290529695024\n",
      "validation AUC 0.8314245101630616\n",
      "validation AUC 0.8330179981324138\n",
      "validation AUC 0.8362113703514421\n",
      "validation AUC 0.8382825572264877\n",
      "validation AUC 0.8386424570826618\n",
      "validation AUC 0.8398134739294638\n",
      "validation AUC 0.840477657399049\n",
      "validation AUC 0.8428771990805534\n",
      "validation AUC 0.8448547565443739\n",
      "validation AUC 0.8450232548957087\n",
      "validation AUC 0.8466195419028733\n",
      "validation AUC 0.8462687415802708\n",
      "validation AUC 0.8496653473584639\n",
      "validation AUC 0.8496217307930132\n",
      "validation AUC 0.8505410258058516\n",
      "validation AUC 0.8493857442343546\n",
      "validation AUC 0.8508188010411996\n",
      "validation AUC 0.8515642603739473\n",
      "validation AUC 0.8530973003014234\n",
      "validation AUC 0.8527476893038228\n",
      "validation AUC 0.854329001990829\n",
      "validation AUC 0.854329001990829\n",
      "train auc 0.8477048710200392\n",
      "Test AUC 0.8601000438698164\n",
      "test false positive rates [0.00000000e+00 0.00000000e+00 1.89483657e-05 ... 9.97688299e-01\n",
      " 9.97688299e-01 1.00000000e+00] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [8.27310801e+00 7.27310801e+00 6.33839607e+00 ... 8.66178237e-03\n",
      " 8.64219759e-03 5.58855338e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.465s | Train Loss: 0.199250 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.383s | Train Loss: 0.154089 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.368s | Train Loss: 0.118767 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.372s | Train Loss: 0.094907 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.375s | Train Loss: 0.078849 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.373s | Train Loss: 0.067637 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.369s | Train Loss: 0.059732 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.362s | Train Loss: 0.053726 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.371s | Train Loss: 0.049236 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.371s | Train Loss: 0.045264 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.372s | Train Loss: 0.042536 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.363s | Train Loss: 0.040233 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.357s | Train Loss: 0.038092 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.367s | Train Loss: 0.036222 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.359s | Train Loss: 0.034507 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.365s | Train Loss: 0.033098 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.397s | Train Loss: 0.031908 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.381s | Train Loss: 0.030836 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.412s | Train Loss: 0.029562 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.392s | Train Loss: 0.028890 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.371s | Train Loss: 0.028005 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.375s | Train Loss: 0.027291 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.364s | Train Loss: 0.026664 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.371s | Train Loss: 0.026080 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.373s | Train Loss: 0.025633 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.371s | Train Loss: 0.024985 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.373s | Train Loss: 0.024868 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.373s | Train Loss: 0.024358 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.361s | Train Loss: 0.024134 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.373s | Train Loss: 0.023613 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.375s | Train Loss: 0.023311 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.372s | Train Loss: 0.023005 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.374s | Train Loss: 0.022795 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.362s | Train Loss: 0.022835 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.369s | Train Loss: 0.022229 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.365s | Train Loss: 0.022107 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.371s | Train Loss: 0.021755 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.370s | Train Loss: 0.021875 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.364s | Train Loss: 0.021376 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.369s | Train Loss: 0.021459 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.372s | Train Loss: 0.021360 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.374s | Train Loss: 0.021149 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.376s | Train Loss: 0.020971 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.367s | Train Loss: 0.020999 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.367s | Train Loss: 0.020892 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.370s | Train Loss: 0.020498 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.372s | Train Loss: 0.020381 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.367s | Train Loss: 0.020417 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.375s | Train Loss: 0.020238 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.370s | Train Loss: 0.020175 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.370s | Train Loss: 0.020139 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.372s | Train Loss: 0.019865 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.375s | Train Loss: 0.019906 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.376s | Train Loss: 0.019708 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.374s | Train Loss: 0.019681 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.380s | Train Loss: 0.019731 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.380s | Train Loss: 0.019512 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.378s | Train Loss: 0.019643 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.376s | Train Loss: 0.019137 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.375s | Train Loss: 0.019179 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.380s | Train Loss: 0.019061 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.376s | Train Loss: 0.019014 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.374s | Train Loss: 0.019059 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.376s | Train Loss: 0.018594 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.375s | Train Loss: 0.018734 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.375s | Train Loss: 0.018688 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.376s | Train Loss: 0.018584 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.375s | Train Loss: 0.018482 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.378s | Train Loss: 0.018546 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.379s | Train Loss: 0.018220 |\n",
      "INFO:root:Pretraining Time: 26.209s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018116\n",
      "INFO:root:Test AUC: 59.25%\n",
      "INFO:root:Test AUC: 59.25%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.650s | Train Loss: 0.522078 || Validation Loss: 0.004808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.658s | Train Loss: 0.359978 || Validation Loss: 0.003935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.702s | Train Loss: 0.314508 || Validation Loss: 0.003356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.705s | Train Loss: 0.286797 || Validation Loss: 0.002356 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.716s | Train Loss: 0.269947 || Validation Loss: 0.002806 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.755s | Train Loss: 0.253812 || Validation Loss: 0.002363 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.768s | Train Loss: 0.245261 || Validation Loss: 0.002623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.787s | Train Loss: 0.239240 || Validation Loss: 0.002474 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.790s | Train Loss: 0.235686 || Validation Loss: 0.002170 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.802s | Train Loss: 0.234877 || Validation Loss: 0.002243 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.855s | Train Loss: 0.229138 || Validation Loss: 0.002387 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.819s | Train Loss: 0.230065 || Validation Loss: 0.002084 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.877s | Train Loss: 0.224610 || Validation Loss: 0.002098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.895s | Train Loss: 0.219497 || Validation Loss: 0.002012 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.901s | Train Loss: 0.216837 || Validation Loss: 0.002193 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.915s | Train Loss: 0.213352 || Validation Loss: 0.002129 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.111s | Train Loss: 0.216619 || Validation Loss: 0.002464 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.941s | Train Loss: 0.208331 || Validation Loss: 0.002132 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.962s | Train Loss: 0.215654 || Validation Loss: 0.002390 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.967s | Train Loss: 0.208029 || Validation Loss: 0.002178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.048s | Train Loss: 0.208024 || Validation Loss: 0.001974 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.040s | Train Loss: 0.207533 || Validation Loss: 0.002099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 0.995s | Train Loss: 0.200585 || Validation Loss: 0.002092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.100s | Train Loss: 0.204796 || Validation Loss: 0.002057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.056s | Train Loss: 0.204542 || Validation Loss: 0.002125 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.116s | Train Loss: 0.199655 || Validation Loss: 0.002154 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.049s | Train Loss: 0.195421 || Validation Loss: 0.002083 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.184s | Train Loss: 0.203672 || Validation Loss: 0.002116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.142s | Train Loss: 0.200338 || Validation Loss: 0.001993 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.247s | Train Loss: 0.194478 || Validation Loss: 0.001992 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.256s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.848%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.002194\n",
      "INFO:root:Test AUC: 85.51%\n",
      "INFO:root:Test Time: 0.190s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1477})\n",
      "class weights 0.05483164420685303 0.945168355793147\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5926701979952078\n",
      "validation AUC 0.6665566252799303\n",
      "validation AUC 0.7244607435478987\n",
      "validation AUC 0.7599326570659315\n",
      "validation AUC 0.7817111348278422\n",
      "validation AUC 0.7937271567118478\n",
      "validation AUC 0.8050143591704036\n",
      "validation AUC 0.8127969640763185\n",
      "validation AUC 0.8185637721919532\n",
      "validation AUC 0.823237266027578\n",
      "validation AUC 0.827230242464788\n",
      "validation AUC 0.8295261399267738\n",
      "validation AUC 0.8328323623261855\n",
      "validation AUC 0.8354481642675472\n",
      "validation AUC 0.8363529665437289\n",
      "validation AUC 0.8389691941942634\n",
      "validation AUC 0.8410821617639513\n",
      "validation AUC 0.8424640829170785\n",
      "validation AUC 0.8431238363505826\n",
      "validation AUC 0.8438524109932581\n",
      "validation AUC 0.8468572472623176\n",
      "validation AUC 0.8450575085200369\n",
      "validation AUC 0.8449975021514277\n",
      "validation AUC 0.8485589451820683\n",
      "validation AUC 0.8486504061371938\n",
      "validation AUC 0.8498615939658278\n",
      "validation AUC 0.8503563052743451\n",
      "validation AUC 0.8513914463958715\n",
      "validation AUC 0.8528029250690127\n",
      "validation AUC 0.8525565778133577\n",
      "validation AUC 0.8525565778133577\n",
      "train auc 0.8478407240350898\n",
      "Test AUC 0.8551100273032581\n",
      "test false positive rates [0.         0.         0.         ... 0.98908574 0.98908574 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 2.79955207e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [6.83665276e+00 5.83665276e+00 3.85738301e+00 ... 1.54570602e-02\n",
      " 1.54375052e-02 4.55119321e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.393s | Train Loss: 0.207744 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.380s | Train Loss: 0.161495 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.384s | Train Loss: 0.125319 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.382s | Train Loss: 0.100140 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.383s | Train Loss: 0.082608 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.380s | Train Loss: 0.070283 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.385s | Train Loss: 0.061625 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.379s | Train Loss: 0.055218 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.380s | Train Loss: 0.050357 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.381s | Train Loss: 0.046244 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.380s | Train Loss: 0.042940 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.380s | Train Loss: 0.040144 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.378s | Train Loss: 0.037916 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.381s | Train Loss: 0.035713 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.380s | Train Loss: 0.034257 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.366s | Train Loss: 0.032996 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.380s | Train Loss: 0.031837 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.381s | Train Loss: 0.030839 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.384s | Train Loss: 0.029449 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.383s | Train Loss: 0.028615 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.383s | Train Loss: 0.027986 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.383s | Train Loss: 0.027139 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.381s | Train Loss: 0.026442 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.382s | Train Loss: 0.026071 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.382s | Train Loss: 0.025666 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.382s | Train Loss: 0.025058 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.382s | Train Loss: 0.024549 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.381s | Train Loss: 0.024149 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.380s | Train Loss: 0.023962 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.381s | Train Loss: 0.023670 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.382s | Train Loss: 0.023230 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.382s | Train Loss: 0.023279 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.382s | Train Loss: 0.022995 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.384s | Train Loss: 0.022699 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.384s | Train Loss: 0.022708 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.384s | Train Loss: 0.022492 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.381s | Train Loss: 0.022423 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.382s | Train Loss: 0.022068 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.381s | Train Loss: 0.021924 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.382s | Train Loss: 0.021939 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.383s | Train Loss: 0.021527 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.380s | Train Loss: 0.021707 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.382s | Train Loss: 0.021398 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.382s | Train Loss: 0.021534 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.384s | Train Loss: 0.021190 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.383s | Train Loss: 0.021138 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.381s | Train Loss: 0.021220 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.381s | Train Loss: 0.021190 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.380s | Train Loss: 0.020828 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.380s | Train Loss: 0.020836 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.382s | Train Loss: 0.020640 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.383s | Train Loss: 0.020563 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.385s | Train Loss: 0.020532 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.380s | Train Loss: 0.020344 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.381s | Train Loss: 0.020147 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.383s | Train Loss: 0.020011 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.383s | Train Loss: 0.019893 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.380s | Train Loss: 0.020144 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.379s | Train Loss: 0.019857 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.378s | Train Loss: 0.019775 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.375s | Train Loss: 0.019718 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.378s | Train Loss: 0.019662 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.380s | Train Loss: 0.019564 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.380s | Train Loss: 0.019231 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.366s | Train Loss: 0.019471 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.368s | Train Loss: 0.019176 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.375s | Train Loss: 0.019231 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.381s | Train Loss: 0.019105 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.375s | Train Loss: 0.018936 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.367s | Train Loss: 0.018865 |\n",
      "INFO:root:Pretraining Time: 26.645s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018437\n",
      "INFO:root:Test AUC: 62.71%\n",
      "INFO:root:Test AUC: 62.71%\n",
      "INFO:root:Test Time: 0.232s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.664s | Train Loss: 0.489098 || Validation Loss: 0.004850 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.664s | Train Loss: 0.348230 || Validation Loss: 0.003088 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.711s | Train Loss: 0.309321 || Validation Loss: 0.002874 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.722s | Train Loss: 0.292346 || Validation Loss: 0.002438 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.739s | Train Loss: 0.276534 || Validation Loss: 0.002528 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.767s | Train Loss: 0.270933 || Validation Loss: 0.002621 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.792s | Train Loss: 0.255476 || Validation Loss: 0.002399 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.785s | Train Loss: 0.246423 || Validation Loss: 0.002099 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.790s | Train Loss: 0.243326 || Validation Loss: 0.002329 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.804s | Train Loss: 0.242872 || Validation Loss: 0.002228 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.868s | Train Loss: 0.243462 || Validation Loss: 0.002457 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.847s | Train Loss: 0.227047 || Validation Loss: 0.002388 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.896s | Train Loss: 0.233470 || Validation Loss: 0.002391 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.907s | Train Loss: 0.222327 || Validation Loss: 0.002394 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.927s | Train Loss: 0.230060 || Validation Loss: 0.002139 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.949s | Train Loss: 0.221975 || Validation Loss: 0.002315 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.947s | Train Loss: 0.226801 || Validation Loss: 0.002263 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.955s | Train Loss: 0.216305 || Validation Loss: 0.002224 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.002s | Train Loss: 0.214251 || Validation Loss: 0.002042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.985s | Train Loss: 0.215335 || Validation Loss: 0.002069 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.049s | Train Loss: 0.213709 || Validation Loss: 0.002121 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.125s | Train Loss: 0.206460 || Validation Loss: 0.002106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.115s | Train Loss: 0.213350 || Validation Loss: 0.002254 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.230s | Train Loss: 0.204655 || Validation Loss: 0.002327 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.148s | Train Loss: 0.208322 || Validation Loss: 0.001999 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.150s | Train Loss: 0.204279 || Validation Loss: 0.002106 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.073s | Train Loss: 0.201213 || Validation Loss: 0.002059 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.199s | Train Loss: 0.207027 || Validation Loss: 0.002178 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.180s | Train Loss: 0.204445 || Validation Loss: 0.001950 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.257s | Train Loss: 0.199410 || Validation Loss: 0.002034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.939s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.841%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.002278\n",
      "INFO:root:Test AUC: 86.64%\n",
      "INFO:root:Test Time: 0.203s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1477})\n",
      "class weights 0.05483164420685303 0.945168355793147\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6031705648500876\n",
      "validation AUC 0.6775076691507506\n",
      "validation AUC 0.7231351330756232\n",
      "validation AUC 0.7502068840153231\n",
      "validation AUC 0.7696061583514667\n",
      "validation AUC 0.788786322836977\n",
      "validation AUC 0.7994284348823543\n",
      "validation AUC 0.8105851415153075\n",
      "validation AUC 0.8188082303631534\n",
      "validation AUC 0.8241690875328302\n",
      "validation AUC 0.8285246883649213\n",
      "validation AUC 0.8319296965779155\n",
      "validation AUC 0.8358948289745326\n",
      "validation AUC 0.8377409620346173\n",
      "validation AUC 0.8390024234558942\n",
      "validation AUC 0.8419627598129691\n",
      "validation AUC 0.8442284345843579\n",
      "validation AUC 0.8462550044773963\n",
      "validation AUC 0.8477204071993382\n",
      "validation AUC 0.8479115931889085\n",
      "validation AUC 0.8486862827777438\n",
      "validation AUC 0.8488455751682137\n",
      "validation AUC 0.8508785892338575\n",
      "validation AUC 0.8521142606613008\n",
      "validation AUC 0.8531083022228617\n",
      "validation AUC 0.8532812572171008\n",
      "validation AUC 0.853525409409833\n",
      "validation AUC 0.8531369896997537\n",
      "validation AUC 0.8546032252152651\n",
      "validation AUC 0.855445924505161\n",
      "validation AUC 0.855445924505161\n",
      "train auc 0.840738713322542\n",
      "Test AUC 0.8663890287265075\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.97764093e-01\n",
      " 9.97764093e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.65980291 4.65980291 4.00920868 ... 0.01315711 0.0131309  0.00755163]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m2_exc_balanced_batch\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 0 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3246cc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.420s | Train Loss: 0.191417 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.402s | Train Loss: 0.146953 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.398s | Train Loss: 0.113962 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.394s | Train Loss: 0.090857 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.396s | Train Loss: 0.075227 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.396s | Train Loss: 0.064544 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.393s | Train Loss: 0.057048 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.392s | Train Loss: 0.052055 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.395s | Train Loss: 0.048207 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.387s | Train Loss: 0.044745 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.389s | Train Loss: 0.042479 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.386s | Train Loss: 0.040030 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.391s | Train Loss: 0.038045 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.395s | Train Loss: 0.036385 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.392s | Train Loss: 0.034848 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.384s | Train Loss: 0.033341 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.386s | Train Loss: 0.032466 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.388s | Train Loss: 0.031008 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.388s | Train Loss: 0.029972 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.395s | Train Loss: 0.029395 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.392s | Train Loss: 0.028561 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.391s | Train Loss: 0.027809 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.383s | Train Loss: 0.027235 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.391s | Train Loss: 0.026517 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.447s | Train Loss: 0.026045 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.386s | Train Loss: 0.025417 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.390s | Train Loss: 0.025031 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.386s | Train Loss: 0.024788 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.386s | Train Loss: 0.023916 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.389s | Train Loss: 0.024214 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.392s | Train Loss: 0.023572 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.383s | Train Loss: 0.023463 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.391s | Train Loss: 0.023032 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.399s | Train Loss: 0.022791 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.388s | Train Loss: 0.022698 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.391s | Train Loss: 0.022445 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.388s | Train Loss: 0.021952 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.381s | Train Loss: 0.021951 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.389s | Train Loss: 0.022037 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.385s | Train Loss: 0.021552 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.389s | Train Loss: 0.021418 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.391s | Train Loss: 0.021423 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.387s | Train Loss: 0.021150 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.394s | Train Loss: 0.021112 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.386s | Train Loss: 0.020911 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.393s | Train Loss: 0.021076 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.396s | Train Loss: 0.020773 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.390s | Train Loss: 0.020397 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.391s | Train Loss: 0.020579 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.389s | Train Loss: 0.020376 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.392s | Train Loss: 0.020142 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.383s | Train Loss: 0.020104 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.383s | Train Loss: 0.020090 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.388s | Train Loss: 0.019847 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.378s | Train Loss: 0.019908 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.398s | Train Loss: 0.019824 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.387s | Train Loss: 0.019575 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.385s | Train Loss: 0.019493 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.384s | Train Loss: 0.019434 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.386s | Train Loss: 0.019157 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.388s | Train Loss: 0.019291 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.392s | Train Loss: 0.019184 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.394s | Train Loss: 0.019155 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.387s | Train Loss: 0.019021 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.386s | Train Loss: 0.018707 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.386s | Train Loss: 0.018754 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.392s | Train Loss: 0.018844 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.398s | Train Loss: 0.018798 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.387s | Train Loss: 0.018338 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.399s | Train Loss: 0.018316 |\n",
      "INFO:root:Pretraining Time: 27.403s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018008\n",
      "INFO:root:Test AUC: 61.25%\n",
      "INFO:root:Test AUC: 61.25%\n",
      "INFO:root:Test Time: 0.218s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.643s | Train Loss: 0.602469 || Validation Loss: 0.006231 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.692s | Train Loss: 0.456799 || Validation Loss: 0.005042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.712s | Train Loss: 0.399153 || Validation Loss: 0.003752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.727s | Train Loss: 0.365861 || Validation Loss: 0.003601 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.732s | Train Loss: 0.349862 || Validation Loss: 0.003504 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.787s | Train Loss: 0.334788 || Validation Loss: 0.003410 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.809s | Train Loss: 0.321381 || Validation Loss: 0.003470 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.815s | Train Loss: 0.315978 || Validation Loss: 0.003488 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.836s | Train Loss: 0.307600 || Validation Loss: 0.003449 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.824s | Train Loss: 0.299077 || Validation Loss: 0.003486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.885s | Train Loss: 0.299616 || Validation Loss: 0.002937 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.833s | Train Loss: 0.279691 || Validation Loss: 0.003089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.946s | Train Loss: 0.280413 || Validation Loss: 0.003395 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.877s | Train Loss: 0.279279 || Validation Loss: 0.003185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.961s | Train Loss: 0.278587 || Validation Loss: 0.002975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.958s | Train Loss: 0.273528 || Validation Loss: 0.003290 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.976s | Train Loss: 0.267303 || Validation Loss: 0.003287 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.994s | Train Loss: 0.260913 || Validation Loss: 0.003405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.029s | Train Loss: 0.263694 || Validation Loss: 0.002966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.022s | Train Loss: 0.256089 || Validation Loss: 0.003177 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.247s | Train Loss: 0.254539 || Validation Loss: 0.003210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.034s | Train Loss: 0.257737 || Validation Loss: 0.002894 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.100s | Train Loss: 0.254545 || Validation Loss: 0.003279 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.073s | Train Loss: 0.253860 || Validation Loss: 0.003284 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.132s | Train Loss: 0.251417 || Validation Loss: 0.003167 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.140s | Train Loss: 0.248689 || Validation Loss: 0.002845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.140s | Train Loss: 0.248525 || Validation Loss: 0.003052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.164s | Train Loss: 0.244826 || Validation Loss: 0.003034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.241s | Train Loss: 0.246632 || Validation Loss: 0.003195 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.242s | Train Loss: 0.246251 || Validation Loss: 0.002845 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.320s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.867%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.003372\n",
      "INFO:root:Test AUC: 86.59%\n",
      "INFO:root:Test Time: 0.188s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6118062084574365\n",
      "validation AUC 0.6854808587320295\n",
      "validation AUC 0.7363304837780327\n",
      "validation AUC 0.7693525660578245\n",
      "validation AUC 0.7934359822802813\n",
      "validation AUC 0.8088709809595187\n",
      "validation AUC 0.8165714931727955\n",
      "validation AUC 0.8245631691749097\n",
      "validation AUC 0.8300605379729389\n",
      "validation AUC 0.8361080454138031\n",
      "validation AUC 0.8408983165969319\n",
      "validation AUC 0.8430361003507205\n",
      "validation AUC 0.8467976985310692\n",
      "validation AUC 0.8472003848198069\n",
      "validation AUC 0.8499406774267498\n",
      "validation AUC 0.8519830624155633\n",
      "validation AUC 0.8514553426820486\n",
      "validation AUC 0.8525272597547532\n",
      "validation AUC 0.8526331256440182\n",
      "validation AUC 0.8537133600096891\n",
      "validation AUC 0.8567771915880718\n",
      "validation AUC 0.8583592093558957\n",
      "validation AUC 0.8587322476614199\n",
      "validation AUC 0.8585673758201021\n",
      "validation AUC 0.8599912027199411\n",
      "validation AUC 0.8602202635224924\n",
      "validation AUC 0.8597691208871439\n",
      "validation AUC 0.8608066246945804\n",
      "validation AUC 0.8614451698292246\n",
      "validation AUC 0.8594407820447791\n",
      "validation AUC 0.8594407820447791\n",
      "train auc 0.8671283374964406\n",
      "Test AUC 0.8658805446647249\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.93728091e-01\n",
      " 9.93728091e-01 1.00000000e+00] true positive rate [0.00000000e+00 0.00000000e+00 5.59910414e-04 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [8.65618515e+00 7.65618515e+00 7.09057808e+00 ... 1.24089336e-02\n",
      " 1.23994751e-02 1.57897908e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.420s | Train Loss: 0.193101 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.404s | Train Loss: 0.147387 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.397s | Train Loss: 0.113112 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.406s | Train Loss: 0.090431 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.406s | Train Loss: 0.074702 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.402s | Train Loss: 0.063913 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.401s | Train Loss: 0.056060 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.398s | Train Loss: 0.050907 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.401s | Train Loss: 0.046598 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.400s | Train Loss: 0.043283 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.396s | Train Loss: 0.040388 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.400s | Train Loss: 0.038144 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.398s | Train Loss: 0.036406 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.400s | Train Loss: 0.034517 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.409s | Train Loss: 0.033249 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.396s | Train Loss: 0.031696 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.401s | Train Loss: 0.030425 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.401s | Train Loss: 0.029458 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.398s | Train Loss: 0.028658 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.399s | Train Loss: 0.027703 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.398s | Train Loss: 0.027159 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.396s | Train Loss: 0.026343 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.395s | Train Loss: 0.025754 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.394s | Train Loss: 0.025368 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.404s | Train Loss: 0.024729 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.393s | Train Loss: 0.024385 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.406s | Train Loss: 0.023807 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.400s | Train Loss: 0.023500 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.401s | Train Loss: 0.023238 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.393s | Train Loss: 0.023212 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.403s | Train Loss: 0.022827 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.400s | Train Loss: 0.022203 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.402s | Train Loss: 0.022374 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.391s | Train Loss: 0.021921 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.399s | Train Loss: 0.021700 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.403s | Train Loss: 0.021375 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.403s | Train Loss: 0.021332 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.399s | Train Loss: 0.021086 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.402s | Train Loss: 0.020917 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.402s | Train Loss: 0.020824 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.401s | Train Loss: 0.020455 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.401s | Train Loss: 0.020211 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.403s | Train Loss: 0.020232 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.400s | Train Loss: 0.019997 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.405s | Train Loss: 0.019961 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.404s | Train Loss: 0.019761 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.402s | Train Loss: 0.019550 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.403s | Train Loss: 0.019264 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.402s | Train Loss: 0.019325 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.400s | Train Loss: 0.019123 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.396s | Train Loss: 0.018939 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.401s | Train Loss: 0.018611 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.400s | Train Loss: 0.018535 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.394s | Train Loss: 0.018446 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.396s | Train Loss: 0.018158 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.398s | Train Loss: 0.018236 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.406s | Train Loss: 0.017998 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.401s | Train Loss: 0.017852 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.400s | Train Loss: 0.017847 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.405s | Train Loss: 0.017416 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.402s | Train Loss: 0.017352 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.403s | Train Loss: 0.017327 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.401s | Train Loss: 0.017093 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.402s | Train Loss: 0.017150 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.403s | Train Loss: 0.016785 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.401s | Train Loss: 0.016879 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.402s | Train Loss: 0.016673 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.402s | Train Loss: 0.016707 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.400s | Train Loss: 0.016499 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.402s | Train Loss: 0.016348 |\n",
      "INFO:root:Pretraining Time: 28.064s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.016038\n",
      "INFO:root:Test AUC: 60.87%\n",
      "INFO:root:Test AUC: 60.87%\n",
      "INFO:root:Test Time: 0.236s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.687s | Train Loss: 0.630628 || Validation Loss: 0.005969 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.699s | Train Loss: 0.439581 || Validation Loss: 0.005075 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.735s | Train Loss: 0.364081 || Validation Loss: 0.004348 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.741s | Train Loss: 0.345517 || Validation Loss: 0.003550 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.757s | Train Loss: 0.322210 || Validation Loss: 0.003299 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.798s | Train Loss: 0.314513 || Validation Loss: 0.003539 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.818s | Train Loss: 0.308901 || Validation Loss: 0.003934 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.831s | Train Loss: 0.289277 || Validation Loss: 0.003242 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.852s | Train Loss: 0.282535 || Validation Loss: 0.003542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.865s | Train Loss: 0.285433 || Validation Loss: 0.003484 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.915s | Train Loss: 0.276115 || Validation Loss: 0.003553 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.859s | Train Loss: 0.283898 || Validation Loss: 0.003405 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.938s | Train Loss: 0.275002 || Validation Loss: 0.003573 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.888s | Train Loss: 0.275113 || Validation Loss: 0.003465 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.017s | Train Loss: 0.272055 || Validation Loss: 0.003444 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.937s | Train Loss: 0.262674 || Validation Loss: 0.003935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.009s | Train Loss: 0.270020 || Validation Loss: 0.003181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.023s | Train Loss: 0.263498 || Validation Loss: 0.003493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.039s | Train Loss: 0.254550 || Validation Loss: 0.003206 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.004s | Train Loss: 0.255499 || Validation Loss: 0.003331 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.096s | Train Loss: 0.253620 || Validation Loss: 0.003086 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.042s | Train Loss: 0.248427 || Validation Loss: 0.003385 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.154s | Train Loss: 0.249060 || Validation Loss: 0.003195 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.106s | Train Loss: 0.244961 || Validation Loss: 0.002945 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.180s | Train Loss: 0.241955 || Validation Loss: 0.003471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.170s | Train Loss: 0.246291 || Validation Loss: 0.003233 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.174s | Train Loss: 0.241994 || Validation Loss: 0.002916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.200s | Train Loss: 0.240076 || Validation Loss: 0.003219 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.248s | Train Loss: 0.243192 || Validation Loss: 0.003336 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.318s | Train Loss: 0.239706 || Validation Loss: 0.002913 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.842s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.880%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.003498\n",
      "INFO:root:Test AUC: 86.39%\n",
      "INFO:root:Test Time: 0.198s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6665573489855244\n",
      "validation AUC 0.7431271063291672\n",
      "validation AUC 0.7859784701842873\n",
      "validation AUC 0.8071514963783858\n",
      "validation AUC 0.8187289899219738\n",
      "validation AUC 0.8266777571001371\n",
      "validation AUC 0.8333727495682777\n",
      "validation AUC 0.8368808618822774\n",
      "validation AUC 0.8414108810838896\n",
      "validation AUC 0.844751157556455\n",
      "validation AUC 0.846700208469782\n",
      "validation AUC 0.8509897738271021\n",
      "validation AUC 0.8515411017949388\n",
      "validation AUC 0.8532238583171758\n",
      "validation AUC 0.853156410020087\n",
      "validation AUC 0.8542651748823926\n",
      "validation AUC 0.8546070432944101\n",
      "validation AUC 0.8560524935595524\n",
      "validation AUC 0.8572522484362106\n",
      "validation AUC 0.8572631359483087\n",
      "validation AUC 0.8576140453588866\n",
      "validation AUC 0.8595137964893041\n",
      "validation AUC 0.8593153494976526\n",
      "validation AUC 0.8596158788882946\n",
      "validation AUC 0.860761757608434\n",
      "validation AUC 0.8597015901089028\n",
      "validation AUC 0.8597941419437839\n",
      "validation AUC 0.860070552249096\n",
      "validation AUC 0.8602266810882745\n",
      "validation AUC 0.8611657210246905\n",
      "validation AUC 0.8611657210246905\n",
      "train auc 0.8800640794872207\n",
      "Test AUC 0.8639284227076959\n",
      "test false positive rates [0.         0.         0.         ... 0.99005211 1.         1.        ] true positive rate [0.00000000e+00 2.79955207e-04 7.55879059e-03 ... 9.99720045e-01\n",
      " 9.99720045e-01 1.00000000e+00] tresholds [1.28942432e+01 1.18942432e+01 7.30836582e+00 ... 1.35662640e-02\n",
      " 1.94461155e-03 1.54626870e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.414s | Train Loss: 0.195407 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.396s | Train Loss: 0.150333 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.400s | Train Loss: 0.115899 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.399s | Train Loss: 0.091965 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.399s | Train Loss: 0.076024 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.398s | Train Loss: 0.065679 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.400s | Train Loss: 0.058192 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.400s | Train Loss: 0.052136 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.398s | Train Loss: 0.047873 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.393s | Train Loss: 0.044487 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.395s | Train Loss: 0.041785 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.391s | Train Loss: 0.039025 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.392s | Train Loss: 0.036879 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.393s | Train Loss: 0.035289 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.395s | Train Loss: 0.033826 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.395s | Train Loss: 0.032476 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.397s | Train Loss: 0.030916 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.394s | Train Loss: 0.030110 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.398s | Train Loss: 0.029296 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.395s | Train Loss: 0.028450 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.393s | Train Loss: 0.027433 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.381s | Train Loss: 0.026899 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.396s | Train Loss: 0.026098 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.409s | Train Loss: 0.025769 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.399s | Train Loss: 0.025323 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.393s | Train Loss: 0.024753 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.393s | Train Loss: 0.024216 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.394s | Train Loss: 0.023970 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.380s | Train Loss: 0.023653 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.393s | Train Loss: 0.023405 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.391s | Train Loss: 0.023118 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.395s | Train Loss: 0.022917 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.401s | Train Loss: 0.022772 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.393s | Train Loss: 0.022269 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.401s | Train Loss: 0.022418 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.397s | Train Loss: 0.021871 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.392s | Train Loss: 0.021528 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.429s | Train Loss: 0.021771 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.478s | Train Loss: 0.021253 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.408s | Train Loss: 0.021311 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.457s | Train Loss: 0.021160 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.384s | Train Loss: 0.021305 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.393s | Train Loss: 0.020944 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.395s | Train Loss: 0.020942 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.394s | Train Loss: 0.020720 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.388s | Train Loss: 0.020301 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.393s | Train Loss: 0.020459 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.384s | Train Loss: 0.020409 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.395s | Train Loss: 0.020334 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.401s | Train Loss: 0.020013 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.401s | Train Loss: 0.020164 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.397s | Train Loss: 0.020055 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.386s | Train Loss: 0.019777 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.393s | Train Loss: 0.019591 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.397s | Train Loss: 0.019385 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.402s | Train Loss: 0.019469 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.397s | Train Loss: 0.019471 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.385s | Train Loss: 0.019073 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.401s | Train Loss: 0.019122 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.411s | Train Loss: 0.019047 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.399s | Train Loss: 0.018779 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.383s | Train Loss: 0.018625 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.397s | Train Loss: 0.018783 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.393s | Train Loss: 0.018774 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.409s | Train Loss: 0.018710 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.451s | Train Loss: 0.018590 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.577s | Train Loss: 0.018514 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.405s | Train Loss: 0.018571 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.392s | Train Loss: 0.018382 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.381s | Train Loss: 0.018251 |\n",
      "INFO:root:Pretraining Time: 28.105s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017963\n",
      "INFO:root:Test AUC: 61.99%\n",
      "INFO:root:Test AUC: 61.99%\n",
      "INFO:root:Test Time: 0.230s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.746s | Train Loss: 0.677276 || Validation Loss: 0.006572 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.694s | Train Loss: 0.459116 || Validation Loss: 0.004692 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.708s | Train Loss: 0.397778 || Validation Loss: 0.004204 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.726s | Train Loss: 0.369066 || Validation Loss: 0.003895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.744s | Train Loss: 0.353986 || Validation Loss: 0.003599 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.784s | Train Loss: 0.341993 || Validation Loss: 0.003764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.792s | Train Loss: 0.331725 || Validation Loss: 0.003568 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.821s | Train Loss: 0.324585 || Validation Loss: 0.003810 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.817s | Train Loss: 0.310188 || Validation Loss: 0.003551 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.856s | Train Loss: 0.304043 || Validation Loss: 0.003246 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.914s | Train Loss: 0.301579 || Validation Loss: 0.003586 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.903s | Train Loss: 0.294259 || Validation Loss: 0.003369 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.989s | Train Loss: 0.295993 || Validation Loss: 0.003210 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.881s | Train Loss: 0.292228 || Validation Loss: 0.003306 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.954s | Train Loss: 0.284192 || Validation Loss: 0.003091 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.917s | Train Loss: 0.285192 || Validation Loss: 0.003183 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.978s | Train Loss: 0.279710 || Validation Loss: 0.003177 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.030s | Train Loss: 0.269437 || Validation Loss: 0.002953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.016s | Train Loss: 0.277423 || Validation Loss: 0.003326 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.002s | Train Loss: 0.272806 || Validation Loss: 0.003471 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.085s | Train Loss: 0.268596 || Validation Loss: 0.003382 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.037s | Train Loss: 0.265565 || Validation Loss: 0.003020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.109s | Train Loss: 0.263408 || Validation Loss: 0.003071 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.095s | Train Loss: 0.261794 || Validation Loss: 0.002966 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.129s | Train Loss: 0.264630 || Validation Loss: 0.002811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.159s | Train Loss: 0.264290 || Validation Loss: 0.003235 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.178s | Train Loss: 0.263112 || Validation Loss: 0.003618 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.182s | Train Loss: 0.255336 || Validation Loss: 0.003194 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.254s | Train Loss: 0.259310 || Validation Loss: 0.002895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.245s | Train Loss: 0.255591 || Validation Loss: 0.003226 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.496s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.860%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.003439\n",
      "INFO:root:Test AUC: 86.09%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 2955})\n",
      "class weights 0.1039943691712124 0.8960056308287876\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5969208854623084\n",
      "validation AUC 0.684790717645624\n",
      "validation AUC 0.74188040371278\n",
      "validation AUC 0.7740752825165712\n",
      "validation AUC 0.7950479992413861\n",
      "validation AUC 0.8093157459393199\n",
      "validation AUC 0.819212933449098\n",
      "validation AUC 0.8273917565250042\n",
      "validation AUC 0.8324700518918198\n",
      "validation AUC 0.8354257453582269\n",
      "validation AUC 0.8385147656162364\n",
      "validation AUC 0.8416272796992279\n",
      "validation AUC 0.8420142466767545\n",
      "validation AUC 0.8449256903353162\n",
      "validation AUC 0.8462088682457781\n",
      "validation AUC 0.8476191868614229\n",
      "validation AUC 0.8478572327882057\n",
      "validation AUC 0.8496249182904456\n",
      "validation AUC 0.8501002092786294\n",
      "validation AUC 0.8501120306902258\n",
      "validation AUC 0.8524234213054245\n",
      "validation AUC 0.8538109086059876\n",
      "validation AUC 0.8533207071540214\n",
      "validation AUC 0.85440770763486\n",
      "validation AUC 0.8550020082830234\n",
      "validation AUC 0.8555151474773646\n",
      "validation AUC 0.8560141105562465\n",
      "validation AUC 0.8565513635145527\n",
      "validation AUC 0.8571296947473661\n",
      "validation AUC 0.8580188974430417\n",
      "validation AUC 0.8580188974430417\n",
      "train auc 0.86002388301125\n",
      "Test AUC 0.8609376125589683\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.98844150e-01\n",
      " 9.98844150e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [9.95644474e+00 8.95644474e+00 8.60789394e+00 ... 4.47927509e-03\n",
      " 4.47002519e-03 2.41970923e-03]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m4_exc_balanced_batch\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 0 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "13da90c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 056/070 | Train Time: 0.634s | Train Loss: 0.019269 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.409s | Train Loss: 0.019084 |\n",
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.406s | Train Loss: 0.018901 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.401s | Train Loss: 0.019013 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.404s | Train Loss: 0.018604 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.398s | Train Loss: 0.018827 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.400s | Train Loss: 0.018316 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.411s | Train Loss: 0.018551 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.417s | Train Loss: 0.018397 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.399s | Train Loss: 0.018153 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.408s | Train Loss: 0.018381 |\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.438s | Train Loss: 0.018072 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.564s | Train Loss: 0.189536 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.512s | Train Loss: 0.018137 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.611s | Train Loss: 0.145202 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.573s | Train Loss: 0.017875 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.515s | Train Loss: 0.017801 |\n",
      "INFO:root:Pretraining Time: 26.792s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.603s | Train Loss: 0.113796 |\n",
      "INFO:root:Test Loss: 0.017527\n",
      "INFO:root:Test AUC: 61.87%\n",
      "INFO:root:Test AUC: 61.87%\n",
      "INFO:root:Test Time: 0.309s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.468s | Train Loss: 0.092140 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.516s | Train Loss: 0.077069 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.931s | Train Loss: 0.538667 || Validation Loss: 0.005237 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.486s | Train Loss: 0.066701 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.577s | Train Loss: 0.059100 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.962s | Train Loss: 0.340119 || Validation Loss: 0.002856 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.504s | Train Loss: 0.052882 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.511s | Train Loss: 0.048306 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 1.036s | Train Loss: 0.258139 || Validation Loss: 0.002227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.481s | Train Loss: 0.044853 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.511s | Train Loss: 0.041627 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 1.013s | Train Loss: 0.241851 || Validation Loss: 0.002098 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.494s | Train Loss: 0.039436 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.517s | Train Loss: 0.037089 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 1.010s | Train Loss: 0.216530 || Validation Loss: 0.001928 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.491s | Train Loss: 0.035272 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.526s | Train Loss: 0.033803 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 1.004s | Train Loss: 0.210473 || Validation Loss: 0.001951 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.481s | Train Loss: 0.032265 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.514s | Train Loss: 0.031292 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 1.046s | Train Loss: 0.201893 || Validation Loss: 0.001973 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.466s | Train Loss: 0.030486 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.509s | Train Loss: 0.029198 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 1.016s | Train Loss: 0.199417 || Validation Loss: 0.001811 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.465s | Train Loss: 0.028647 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.523s | Train Loss: 0.028167 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 1.088s | Train Loss: 0.194589 || Validation Loss: 0.001769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.491s | Train Loss: 0.027400 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.545s | Train Loss: 0.026907 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.507s | Train Loss: 0.026125 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 1.186s | Train Loss: 0.195738 || Validation Loss: 0.001907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.474s | Train Loss: 0.025839 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.496s | Train Loss: 0.025607 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 1.097s | Train Loss: 0.189621 || Validation Loss: 0.001738 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.456s | Train Loss: 0.025289 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.510s | Train Loss: 0.024728 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.445s | Train Loss: 0.024523 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 1.187s | Train Loss: 0.182580 || Validation Loss: 0.001522 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.502s | Train Loss: 0.024005 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.505s | Train Loss: 0.023939 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 1.259s | Train Loss: 0.184319 || Validation Loss: 0.001708 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.446s | Train Loss: 0.023399 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.511s | Train Loss: 0.023184 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.437s | Train Loss: 0.023036 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 1.179s | Train Loss: 0.187244 || Validation Loss: 0.001546 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.479s | Train Loss: 0.023292 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.501s | Train Loss: 0.022920 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 1.268s | Train Loss: 0.182403 || Validation Loss: 0.001456 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.428s | Train Loss: 0.022912 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.511s | Train Loss: 0.022279 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.475s | Train Loss: 0.022397 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 1.252s | Train Loss: 0.181221 || Validation Loss: 0.001619 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.469s | Train Loss: 0.022187 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.528s | Train Loss: 0.021970 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.439s | Train Loss: 0.021701 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 1.277s | Train Loss: 0.177841 || Validation Loss: 0.001589 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.495s | Train Loss: 0.021754 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.478s | Train Loss: 0.021574 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 1.297s | Train Loss: 0.179122 || Validation Loss: 0.001527 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.432s | Train Loss: 0.021387 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.508s | Train Loss: 0.021436 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.479s | Train Loss: 0.021068 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.301s | Train Loss: 0.179955 || Validation Loss: 0.001682 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.441s | Train Loss: 0.021133 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.512s | Train Loss: 0.021003 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.458s | Train Loss: 0.020790 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.396s | Train Loss: 0.169693 || Validation Loss: 0.001507 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.465s | Train Loss: 0.020671 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.523s | Train Loss: 0.020752 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.462s | Train Loss: 0.020478 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.425s | Train Loss: 0.171429 || Validation Loss: 0.001505 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.479s | Train Loss: 0.020532 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.504s | Train Loss: 0.020246 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.459s | Train Loss: 0.020254 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.430s | Train Loss: 0.167233 || Validation Loss: 0.001706 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.460s | Train Loss: 0.020167 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.506s | Train Loss: 0.020105 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.455s | Train Loss: 0.019899 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.348s | Train Loss: 0.171259 || Validation Loss: 0.001495 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.502s | Train Loss: 0.019903 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.535s | Train Loss: 0.019536 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.446s | Train Loss: 0.019781 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.463s | Train Loss: 0.167063 || Validation Loss: 0.001593 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.504s | Train Loss: 0.019851 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.511s | Train Loss: 0.019572 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.445s | Train Loss: 0.019506 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.437s | Train Loss: 0.162235 || Validation Loss: 0.001493 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.502s | Train Loss: 0.019436 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.507s | Train Loss: 0.019240 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.441s | Train Loss: 0.019127 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.438s | Train Loss: 0.167152 || Validation Loss: 0.001496 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.514s | Train Loss: 0.019124 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.507s | Train Loss: 0.019035 |\n",
      "INFO:root:Pretraining Time: 34.521s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018731\n",
      "INFO:root:Test AUC: 63.03%\n",
      "INFO:root:Test AUC: 63.03%\n",
      "INFO:root:Test Time: 0.248s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.546s | Train Loss: 0.165513 || Validation Loss: 0.001561 || Validation number of baches in epoch: 441.000000 |\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.845s | Train Loss: 0.414867 || Validation Loss: 0.003752 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.338s | Train Loss: 0.170532 || Validation Loss: 0.001537 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.798s | Train Loss: 0.300446 || Validation Loss: 0.003057 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.914s | Train Loss: 0.285583 || Validation Loss: 0.002615 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.542s | Train Loss: 0.162171 || Validation Loss: 0.001367 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.934s | Train Loss: 0.259697 || Validation Loss: 0.002181 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.879s | Train Loss: 0.255825 || Validation Loss: 0.002174 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.406s | Train Loss: 0.164690 || Validation Loss: 0.001644 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.823s | Train Loss: 0.256353 || Validation Loss: 0.002154 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 38.076s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.819%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001661\n",
      "INFO:root:Test AUC: 85.52%\n",
      "INFO:root:Test Time: 0.243s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 837})\n",
      "class weights 0.03182872571015705 0.968171274289843\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5718249519001848\n",
      "validation AUC 0.6414933063618191\n",
      "validation AUC 0.6983917878146283\n",
      "validation AUC 0.7351066550476444\n",
      "validation AUC 0.7583372746907595\n",
      "validation AUC 0.7787977462104967\n",
      "validation AUC 0.7904648036427082\n",
      "validation AUC 0.7994433799350069\n",
      "validation AUC 0.8053377651077268\n",
      "validation AUC 0.8137015162483614\n",
      "validation AUC 0.8169459576040491\n",
      "validation AUC 0.8237693359767035\n",
      "validation AUC 0.8274325048749022\n",
      "validation AUC 0.8303607507423835\n",
      "validation AUC 0.8308379893500337\n",
      "validation AUC 0.8333387939403705\n",
      "validation AUC 0.8369011389423214\n",
      "validation AUC 0.8389525622690128\n",
      "validation AUC 0.8405976302260324\n",
      "validation AUC 0.8405702863937176\n",
      "validation AUC 0.8412684175091629\n",
      "validation AUC 0.8428704941610794\n",
      "validation AUC 0.8429146056134437\n",
      "validation AUC 0.8451448454175493\n",
      "validation AUC 0.8466352186431669\n",
      "validation AUC 0.8464841105115469\n",
      "validation AUC 0.8461436256543415\n",
      "validation AUC 0.8478577196930722\n",
      "validation AUC 0.8485264476080785\n",
      "validation AUC 0.8498809610725148\n",
      "validation AUC 0.8498809610725148\n",
      "train auc 0.8185601026833418\n",
      "Test AUC 0.8552069228373957\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.78967314e-05 ... 9.99658929e-01\n",
      " 9.99658929e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [4.45529532 3.45529532 3.22065187 ... 0.01041798 0.01035631 0.00661945]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:| Epoch: 007/030 | Train Time: 0.879s | Train Loss: 0.247813 || Validation Loss: 0.002001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.786s | Train Loss: 0.259017 || Validation Loss: 0.002196 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.758s | Train Loss: 0.253340 || Validation Loss: 0.002052 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.821s | Train Loss: 0.247659 || Validation Loss: 0.001941 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.826s | Train Loss: 0.239513 || Validation Loss: 0.002227 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.813s | Train Loss: 0.251509 || Validation Loss: 0.001815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.873s | Train Loss: 0.249575 || Validation Loss: 0.002146 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.884s | Train Loss: 0.244167 || Validation Loss: 0.002040 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.888s | Train Loss: 0.249575 || Validation Loss: 0.001907 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.907s | Train Loss: 0.238587 || Validation Loss: 0.001828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.955s | Train Loss: 0.242283 || Validation Loss: 0.002120 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.931s | Train Loss: 0.237179 || Validation Loss: 0.002009 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.980s | Train Loss: 0.239770 || Validation Loss: 0.002019 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.959s | Train Loss: 0.239867 || Validation Loss: 0.001941 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.044s | Train Loss: 0.236783 || Validation Loss: 0.002043 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.986s | Train Loss: 0.235791 || Validation Loss: 0.001831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.030s | Train Loss: 0.233735 || Validation Loss: 0.001808 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.043s | Train Loss: 0.239427 || Validation Loss: 0.002047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.059s | Train Loss: 0.235738 || Validation Loss: 0.001679 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.094s | Train Loss: 0.235530 || Validation Loss: 0.001715 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.046s | Train Loss: 0.232448 || Validation Loss: 0.001965 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.170s | Train Loss: 0.238565 || Validation Loss: 0.002056 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.110s | Train Loss: 0.233640 || Validation Loss: 0.001911 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.126s | Train Loss: 0.229056 || Validation Loss: 0.002066 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.907s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.557%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001781\n",
      "INFO:root:Test AUC: 49.46%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5043162147515465\n",
      "validation AUC 0.5228530395528521\n",
      "validation AUC 0.5160415491088524\n",
      "validation AUC 0.5184721116856529\n",
      "validation AUC 0.5207602426159148\n",
      "validation AUC 0.5197999092388044\n",
      "validation AUC 0.5044869001837573\n",
      "validation AUC 0.49480273488343973\n",
      "validation AUC 0.49900907409744866\n",
      "validation AUC 0.5007150557157523\n",
      "validation AUC 0.5082596812119429\n",
      "validation AUC 0.5153532518753021\n",
      "validation AUC 0.5003349772447804\n",
      "validation AUC 0.4991131120979523\n",
      "validation AUC 0.5004547638240008\n",
      "validation AUC 0.498823225436634\n",
      "validation AUC 0.4810862336721907\n",
      "validation AUC 0.49046655703236436\n",
      "validation AUC 0.47578132799125084\n",
      "validation AUC 0.4802470225368307\n",
      "validation AUC 0.48937986519067617\n",
      "validation AUC 0.49958101703198543\n",
      "validation AUC 0.487984619340436\n",
      "validation AUC 0.4916089875082029\n",
      "validation AUC 0.507322990658025\n",
      "validation AUC 0.505284942581411\n",
      "validation AUC 0.48829040091799925\n",
      "validation AUC 0.4981036412395544\n",
      "validation AUC 0.4801929255436785\n",
      "validation AUC 0.49315960656383945\n",
      "validation AUC 0.49315960656383945\n",
      "train auc 0.557459697482366\n",
      "Test AUC 0.4946122242421317\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.51586926e-04 ... 9.99715775e-01\n",
      " 9.99715775e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [3.01263571 2.01263571 1.17998886 ... 0.02597643 0.02565976 0.01286109]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.404s | Train Loss: 0.190868 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.383s | Train Loss: 0.149262 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.380s | Train Loss: 0.118293 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.381s | Train Loss: 0.096817 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.379s | Train Loss: 0.081258 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.379s | Train Loss: 0.070546 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.378s | Train Loss: 0.062502 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.380s | Train Loss: 0.056270 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.377s | Train Loss: 0.051273 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.378s | Train Loss: 0.047518 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.378s | Train Loss: 0.044268 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.375s | Train Loss: 0.041347 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.379s | Train Loss: 0.038838 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.380s | Train Loss: 0.037178 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.379s | Train Loss: 0.034975 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.378s | Train Loss: 0.033929 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.379s | Train Loss: 0.032494 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.379s | Train Loss: 0.031214 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.379s | Train Loss: 0.030251 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.380s | Train Loss: 0.029504 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.378s | Train Loss: 0.028478 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.378s | Train Loss: 0.027742 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.378s | Train Loss: 0.026952 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.388s | Train Loss: 0.026125 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.379s | Train Loss: 0.025651 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.377s | Train Loss: 0.025260 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.378s | Train Loss: 0.024803 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.363s | Train Loss: 0.024374 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.366s | Train Loss: 0.023798 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.374s | Train Loss: 0.023480 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.382s | Train Loss: 0.023283 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.379s | Train Loss: 0.023048 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.380s | Train Loss: 0.022956 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.381s | Train Loss: 0.022570 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.380s | Train Loss: 0.022312 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.378s | Train Loss: 0.022026 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.380s | Train Loss: 0.021985 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.380s | Train Loss: 0.021872 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.378s | Train Loss: 0.021684 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.381s | Train Loss: 0.021306 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.381s | Train Loss: 0.021391 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.379s | Train Loss: 0.021197 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.376s | Train Loss: 0.021147 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.380s | Train Loss: 0.021119 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.380s | Train Loss: 0.020803 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.379s | Train Loss: 0.020749 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.379s | Train Loss: 0.020302 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.379s | Train Loss: 0.020543 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.380s | Train Loss: 0.020208 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.381s | Train Loss: 0.020188 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.378s | Train Loss: 0.020123 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.378s | Train Loss: 0.019977 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.362s | Train Loss: 0.019828 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.379s | Train Loss: 0.020021 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.376s | Train Loss: 0.019743 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.379s | Train Loss: 0.019773 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.380s | Train Loss: 0.019607 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.381s | Train Loss: 0.019459 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.382s | Train Loss: 0.019460 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.381s | Train Loss: 0.019310 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.379s | Train Loss: 0.018991 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.382s | Train Loss: 0.019098 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.382s | Train Loss: 0.019137 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.380s | Train Loss: 0.019101 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.377s | Train Loss: 0.018982 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.379s | Train Loss: 0.018689 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.381s | Train Loss: 0.018720 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.380s | Train Loss: 0.018622 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.378s | Train Loss: 0.018609 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.377s | Train Loss: 0.018402 |\n",
      "INFO:root:Pretraining Time: 26.540s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.018115\n",
      "INFO:root:Test AUC: 61.59%\n",
      "INFO:root:Test AUC: 61.59%\n",
      "INFO:root:Test Time: 0.222s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.651s | Train Loss: 0.601152 || Validation Loss: 0.006476 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.662s | Train Loss: 0.374007 || Validation Loss: 0.003696 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.702s | Train Loss: 0.293287 || Validation Loss: 0.002777 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.738s | Train Loss: 0.273692 || Validation Loss: 0.002138 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.716s | Train Loss: 0.258600 || Validation Loss: 0.002273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.733s | Train Loss: 0.256067 || Validation Loss: 0.002244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.772s | Train Loss: 0.260781 || Validation Loss: 0.002164 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.782s | Train Loss: 0.257361 || Validation Loss: 0.002343 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.762s | Train Loss: 0.257361 || Validation Loss: 0.001828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.834s | Train Loss: 0.250150 || Validation Loss: 0.001963 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.854s | Train Loss: 0.242379 || Validation Loss: 0.002244 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.834s | Train Loss: 0.254834 || Validation Loss: 0.002092 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.877s | Train Loss: 0.242527 || Validation Loss: 0.002070 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.885s | Train Loss: 0.251876 || Validation Loss: 0.002042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.900s | Train Loss: 0.253085 || Validation Loss: 0.002096 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.911s | Train Loss: 0.236158 || Validation Loss: 0.002044 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.961s | Train Loss: 0.239825 || Validation Loss: 0.002010 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.942s | Train Loss: 0.241008 || Validation Loss: 0.001939 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.988s | Train Loss: 0.235240 || Validation Loss: 0.001872 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.971s | Train Loss: 0.242415 || Validation Loss: 0.001772 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.055s | Train Loss: 0.242489 || Validation Loss: 0.001828 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.996s | Train Loss: 0.243070 || Validation Loss: 0.001741 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.044s | Train Loss: 0.240863 || Validation Loss: 0.001678 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.057s | Train Loss: 0.237772 || Validation Loss: 0.001736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.063s | Train Loss: 0.239912 || Validation Loss: 0.001941 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.124s | Train Loss: 0.236843 || Validation Loss: 0.001854 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.040s | Train Loss: 0.227490 || Validation Loss: 0.001728 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.178s | Train Loss: 0.233271 || Validation Loss: 0.001697 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.118s | Train Loss: 0.237032 || Validation Loss: 0.001901 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.146s | Train Loss: 0.237482 || Validation Loss: 0.001933 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.052s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.542%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001793\n",
      "INFO:root:Test AUC: 49.54%\n",
      "INFO:root:Test Time: 0.192s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.5421638222553519\n",
      "validation AUC 0.5678824619102039\n",
      "validation AUC 0.5505051278798427\n",
      "validation AUC 0.5416396252652435\n",
      "validation AUC 0.533116387504329\n",
      "validation AUC 0.5212214160236507\n",
      "validation AUC 0.5084798047825445\n",
      "validation AUC 0.5125847640176985\n",
      "validation AUC 0.5029280383342596\n",
      "validation AUC 0.5099794504860854\n",
      "validation AUC 0.499029595940267\n",
      "validation AUC 0.5045772888839033\n",
      "validation AUC 0.48104879255043\n",
      "validation AUC 0.47534738932785425\n",
      "validation AUC 0.47460684427537164\n",
      "validation AUC 0.48566017275703943\n",
      "validation AUC 0.4691444687713586\n",
      "validation AUC 0.4617758474113583\n",
      "validation AUC 0.47899381189146206\n",
      "validation AUC 0.45745769036171013\n",
      "validation AUC 0.4584909929517461\n",
      "validation AUC 0.4736110200566491\n",
      "validation AUC 0.4714845335600376\n",
      "validation AUC 0.47230323881667324\n",
      "validation AUC 0.4822804516689183\n",
      "validation AUC 0.48196582332260474\n",
      "validation AUC 0.49149143856282285\n",
      "validation AUC 0.4874321313151029\n",
      "validation AUC 0.47716557192005266\n",
      "validation AUC 0.48577620245280856\n",
      "validation AUC 0.48577620245280856\n",
      "train auc 0.5418929859864537\n",
      "Test AUC 0.495448811562959\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 3.60018948e-04 ... 9.99393652e-01\n",
      " 9.99393652e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.30515635 1.30515635 0.8802039  ... 0.04954714 0.04901252 0.02669864]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.407s | Train Loss: 0.187365 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.386s | Train Loss: 0.143832 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.384s | Train Loss: 0.113132 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.383s | Train Loss: 0.092060 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.382s | Train Loss: 0.077292 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.385s | Train Loss: 0.067136 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.383s | Train Loss: 0.059048 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.383s | Train Loss: 0.053439 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.384s | Train Loss: 0.048883 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.381s | Train Loss: 0.045403 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.382s | Train Loss: 0.042587 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.380s | Train Loss: 0.040040 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.386s | Train Loss: 0.037727 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.385s | Train Loss: 0.035873 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.383s | Train Loss: 0.034561 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.384s | Train Loss: 0.032864 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.382s | Train Loss: 0.031818 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.381s | Train Loss: 0.030426 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.382s | Train Loss: 0.029696 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.384s | Train Loss: 0.028919 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.382s | Train Loss: 0.027985 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.384s | Train Loss: 0.027199 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.383s | Train Loss: 0.026656 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.387s | Train Loss: 0.025972 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.385s | Train Loss: 0.025816 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.383s | Train Loss: 0.025179 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.385s | Train Loss: 0.024791 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.382s | Train Loss: 0.024355 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.383s | Train Loss: 0.024078 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.387s | Train Loss: 0.023894 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.385s | Train Loss: 0.023335 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.383s | Train Loss: 0.023044 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.384s | Train Loss: 0.022597 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.386s | Train Loss: 0.022618 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.385s | Train Loss: 0.022223 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.381s | Train Loss: 0.022016 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.383s | Train Loss: 0.021907 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.383s | Train Loss: 0.021561 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.383s | Train Loss: 0.021424 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.383s | Train Loss: 0.021253 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.382s | Train Loss: 0.021107 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.382s | Train Loss: 0.020909 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.385s | Train Loss: 0.020438 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.380s | Train Loss: 0.020529 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.383s | Train Loss: 0.020312 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.385s | Train Loss: 0.020296 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.386s | Train Loss: 0.020308 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.381s | Train Loss: 0.019842 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.382s | Train Loss: 0.019926 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.385s | Train Loss: 0.019781 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.384s | Train Loss: 0.019705 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.384s | Train Loss: 0.019560 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.382s | Train Loss: 0.019442 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.378s | Train Loss: 0.019240 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.382s | Train Loss: 0.019195 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.365s | Train Loss: 0.019029 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.382s | Train Loss: 0.019108 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.381s | Train Loss: 0.018888 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.386s | Train Loss: 0.018992 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.381s | Train Loss: 0.018773 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.382s | Train Loss: 0.018612 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.377s | Train Loss: 0.018541 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.382s | Train Loss: 0.018415 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.385s | Train Loss: 0.018360 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.380s | Train Loss: 0.018204 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.381s | Train Loss: 0.018091 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.377s | Train Loss: 0.017954 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.384s | Train Loss: 0.018041 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.380s | Train Loss: 0.017781 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.380s | Train Loss: 0.017890 |\n",
      "INFO:root:Pretraining Time: 26.826s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017522\n",
      "INFO:root:Test AUC: 63.15%\n",
      "INFO:root:Test AUC: 63.15%\n",
      "INFO:root:Test Time: 0.225s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.655s | Train Loss: 0.517793 || Validation Loss: 0.005089 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.671s | Train Loss: 0.354605 || Validation Loss: 0.003300 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.709s | Train Loss: 0.291413 || Validation Loss: 0.002803 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.751s | Train Loss: 0.270481 || Validation Loss: 0.002320 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.731s | Train Loss: 0.263324 || Validation Loss: 0.002341 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.740s | Train Loss: 0.262840 || Validation Loss: 0.002303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.782s | Train Loss: 0.247099 || Validation Loss: 0.002307 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.786s | Train Loss: 0.247005 || Validation Loss: 0.001804 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.768s | Train Loss: 0.247053 || Validation Loss: 0.002035 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.844s | Train Loss: 0.244787 || Validation Loss: 0.001922 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.862s | Train Loss: 0.252536 || Validation Loss: 0.001895 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.833s | Train Loss: 0.251475 || Validation Loss: 0.001859 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.881s | Train Loss: 0.247715 || Validation Loss: 0.001658 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.886s | Train Loss: 0.236557 || Validation Loss: 0.002030 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.905s | Train Loss: 0.237944 || Validation Loss: 0.001903 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.920s | Train Loss: 0.235335 || Validation Loss: 0.001883 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.969s | Train Loss: 0.238187 || Validation Loss: 0.001764 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.941s | Train Loss: 0.235016 || Validation Loss: 0.001860 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.990s | Train Loss: 0.238351 || Validation Loss: 0.001778 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 0.969s | Train Loss: 0.247138 || Validation Loss: 0.001916 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.062s | Train Loss: 0.240349 || Validation Loss: 0.001914 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 0.998s | Train Loss: 0.237913 || Validation Loss: 0.002001 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.048s | Train Loss: 0.236441 || Validation Loss: 0.001820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.057s | Train Loss: 0.230936 || Validation Loss: 0.001880 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.070s | Train Loss: 0.226923 || Validation Loss: 0.001974 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.125s | Train Loss: 0.245228 || Validation Loss: 0.001996 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.051s | Train Loss: 0.230833 || Validation Loss: 0.001769 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.178s | Train Loss: 0.236137 || Validation Loss: 0.001917 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.122s | Train Loss: 0.236570 || Validation Loss: 0.002042 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.143s | Train Loss: 0.227930 || Validation Loss: 0.001821 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.212s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.555%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001790\n",
      "INFO:root:Test AUC: 43.58%\n",
      "INFO:root:Test Time: 0.194s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 0\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "class weights 0.03171826272153343 0.9682817372784666\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.543994672356128\n",
      "validation AUC 0.5312245598858929\n",
      "validation AUC 0.5178459626699883\n",
      "validation AUC 0.5075702664918138\n",
      "validation AUC 0.500326926020047\n",
      "validation AUC 0.49612895199128654\n",
      "validation AUC 0.49703939225332766\n",
      "validation AUC 0.4868335895393039\n",
      "validation AUC 0.4832712498587177\n",
      "validation AUC 0.4660507630517643\n",
      "validation AUC 0.4641826846838141\n",
      "validation AUC 0.48283380175532664\n",
      "validation AUC 0.4872247843410543\n",
      "validation AUC 0.48831426191714294\n",
      "validation AUC 0.4704838030430969\n",
      "validation AUC 0.45800715851259766\n",
      "validation AUC 0.4618971372760956\n",
      "validation AUC 0.4530959939063989\n",
      "validation AUC 0.45013593426028664\n",
      "validation AUC 0.4591570122921395\n",
      "validation AUC 0.4538265587394581\n",
      "validation AUC 0.44407152616525647\n",
      "validation AUC 0.442721312363467\n",
      "validation AUC 0.4499459828273177\n",
      "validation AUC 0.4471411234422503\n",
      "validation AUC 0.44700587031664035\n",
      "validation AUC 0.4496289465634946\n",
      "validation AUC 0.4423449641691232\n",
      "validation AUC 0.4402178816796696\n",
      "validation AUC 0.43431990990716773\n",
      "validation AUC 0.43431990990716773\n",
      "train auc 0.5551178581841644\n",
      "Test AUC 0.43584882259672186\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 2.27380388e-04 ... 9.99393652e-01\n",
      " 9.99393652e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [2.48287296 1.48287296 0.95799625 ... 0.03134766 0.03131667 0.01435633]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m1_exc_balanced_batch\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 0 --balanced_batches 0 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f46891a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.391s | Train Loss: 0.191817 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.373s | Train Loss: 0.150283 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.372s | Train Loss: 0.118153 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.370s | Train Loss: 0.095338 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.370s | Train Loss: 0.079650 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.367s | Train Loss: 0.068608 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.367s | Train Loss: 0.061017 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.365s | Train Loss: 0.055284 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.371s | Train Loss: 0.050734 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.369s | Train Loss: 0.047294 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.369s | Train Loss: 0.044390 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.371s | Train Loss: 0.041949 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.369s | Train Loss: 0.039849 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.370s | Train Loss: 0.038139 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.370s | Train Loss: 0.036790 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.371s | Train Loss: 0.035114 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.370s | Train Loss: 0.034185 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.367s | Train Loss: 0.032904 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.372s | Train Loss: 0.031576 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.369s | Train Loss: 0.030888 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.367s | Train Loss: 0.029832 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.371s | Train Loss: 0.028936 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.361s | Train Loss: 0.028008 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.364s | Train Loss: 0.027537 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.372s | Train Loss: 0.026833 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.455s | Train Loss: 0.026247 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.368s | Train Loss: 0.025335 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.360s | Train Loss: 0.025121 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.367s | Train Loss: 0.024780 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.368s | Train Loss: 0.024268 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.370s | Train Loss: 0.024014 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.367s | Train Loss: 0.023976 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.371s | Train Loss: 0.023564 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.369s | Train Loss: 0.023138 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.369s | Train Loss: 0.022923 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.367s | Train Loss: 0.022460 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.367s | Train Loss: 0.022325 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.370s | Train Loss: 0.022153 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.370s | Train Loss: 0.021857 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.369s | Train Loss: 0.021806 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.369s | Train Loss: 0.021563 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.370s | Train Loss: 0.021226 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.368s | Train Loss: 0.020859 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.369s | Train Loss: 0.020574 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.369s | Train Loss: 0.020723 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.368s | Train Loss: 0.020727 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.369s | Train Loss: 0.020263 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.367s | Train Loss: 0.019985 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.364s | Train Loss: 0.019934 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.374s | Train Loss: 0.019581 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.369s | Train Loss: 0.019659 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.348s | Train Loss: 0.019262 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.353s | Train Loss: 0.019205 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.361s | Train Loss: 0.019126 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.374s | Train Loss: 0.019025 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.368s | Train Loss: 0.019085 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.372s | Train Loss: 0.018678 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.370s | Train Loss: 0.018543 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.372s | Train Loss: 0.018628 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.372s | Train Loss: 0.018336 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.371s | Train Loss: 0.018347 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.374s | Train Loss: 0.018218 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.373s | Train Loss: 0.018149 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.371s | Train Loss: 0.017994 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.375s | Train Loss: 0.017949 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.372s | Train Loss: 0.017955 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.365s | Train Loss: 0.017871 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.358s | Train Loss: 0.017503 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.378s | Train Loss: 0.017269 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.370s | Train Loss: 0.017581 |\n",
      "INFO:root:Pretraining Time: 25.916s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017044\n",
      "INFO:root:Test AUC: 62.29%\n",
      "INFO:root:Test AUC: 62.29%\n",
      "INFO:root:Test Time: 0.216s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.647s | Train Loss: 0.535588 || Validation Loss: 0.005486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.649s | Train Loss: 0.352582 || Validation Loss: 0.003631 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.698s | Train Loss: 0.289051 || Validation Loss: 0.002799 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.744s | Train Loss: 0.256183 || Validation Loss: 0.002515 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.717s | Train Loss: 0.250499 || Validation Loss: 0.002397 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.746s | Train Loss: 0.245362 || Validation Loss: 0.002424 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.770s | Train Loss: 0.232296 || Validation Loss: 0.002334 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.731s | Train Loss: 0.220380 || Validation Loss: 0.002018 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.811s | Train Loss: 0.217877 || Validation Loss: 0.002116 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.791s | Train Loss: 0.212580 || Validation Loss: 0.001918 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.807s | Train Loss: 0.202163 || Validation Loss: 0.001990 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.916s | Train Loss: 0.203538 || Validation Loss: 0.001820 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.829s | Train Loss: 0.205619 || Validation Loss: 0.001871 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.873s | Train Loss: 0.203982 || Validation Loss: 0.001891 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.922s | Train Loss: 0.198736 || Validation Loss: 0.001835 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.883s | Train Loss: 0.202019 || Validation Loss: 0.001771 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.943s | Train Loss: 0.198929 || Validation Loss: 0.001867 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.907s | Train Loss: 0.198881 || Validation Loss: 0.002105 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 0.961s | Train Loss: 0.197659 || Validation Loss: 0.002111 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.011s | Train Loss: 0.186525 || Validation Loss: 0.001875 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.020s | Train Loss: 0.182345 || Validation Loss: 0.001851 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.028s | Train Loss: 0.182722 || Validation Loss: 0.001748 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.021s | Train Loss: 0.187107 || Validation Loss: 0.001861 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.087s | Train Loss: 0.190092 || Validation Loss: 0.001918 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.048s | Train Loss: 0.185278 || Validation Loss: 0.001645 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.106s | Train Loss: 0.180100 || Validation Loss: 0.001823 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.062s | Train Loss: 0.182060 || Validation Loss: 0.001542 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.144s | Train Loss: 0.186467 || Validation Loss: 0.001698 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.195s | Train Loss: 0.180930 || Validation Loss: 0.001894 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.203s | Train Loss: 0.174789 || Validation Loss: 0.001623 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 27.929s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.838%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001924\n",
      "INFO:root:Test AUC: 86.23%\n",
      "INFO:root:Test Time: 0.186s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1157})\n",
      "class weights 0.0434684600067626 0.9565315399932374\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6395783441105158\n",
      "validation AUC 0.6768339364922799\n",
      "validation AUC 0.7203599163524047\n",
      "validation AUC 0.7500652266273424\n",
      "validation AUC 0.7718251647547713\n",
      "validation AUC 0.791746260091702\n",
      "validation AUC 0.80153435696523\n",
      "validation AUC 0.8114753286634455\n",
      "validation AUC 0.8168368669677991\n",
      "validation AUC 0.8222700973575594\n",
      "validation AUC 0.8271665696759267\n",
      "validation AUC 0.8318708609095319\n",
      "validation AUC 0.8336043912326899\n",
      "validation AUC 0.8369249414064537\n",
      "validation AUC 0.8391154146524415\n",
      "validation AUC 0.8413632841376718\n",
      "validation AUC 0.84310133496011\n",
      "validation AUC 0.8447902323371668\n",
      "validation AUC 0.8460216865831156\n",
      "validation AUC 0.8466307859464035\n",
      "validation AUC 0.8473870769169205\n",
      "validation AUC 0.8494444415472571\n",
      "validation AUC 0.8501188553404044\n",
      "validation AUC 0.8507394887615972\n",
      "validation AUC 0.8514220974563239\n",
      "validation AUC 0.8525491092780547\n",
      "validation AUC 0.8533149654015513\n",
      "validation AUC 0.854034512349078\n",
      "validation AUC 0.8543602437227586\n",
      "validation AUC 0.8544757519247907\n",
      "validation AUC 0.8544757519247907\n",
      "train auc 0.8381095032568938\n",
      "Test AUC 0.8622864131412115\n",
      "test false positive rates [0.         0.         0.         ... 0.99340597 0.99340597 1.        ] true positive rate [0.00000000e+00 2.79955207e-04 3.07950728e-03 ... 9.99720045e-01\n",
      " 1.00000000e+00 1.00000000e+00] tresholds [5.05914736e+00 4.05914736e+00 3.54676700e+00 ... 1.30985454e-02\n",
      " 1.30802607e-02 4.03626123e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.398s | Train Loss: 0.201827 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.373s | Train Loss: 0.159625 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.375s | Train Loss: 0.126936 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.375s | Train Loss: 0.102632 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.385s | Train Loss: 0.085504 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.377s | Train Loss: 0.073131 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.375s | Train Loss: 0.063966 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.378s | Train Loss: 0.057800 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.378s | Train Loss: 0.052746 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.373s | Train Loss: 0.048739 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.377s | Train Loss: 0.045169 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.377s | Train Loss: 0.042762 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.367s | Train Loss: 0.040420 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.378s | Train Loss: 0.038166 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.375s | Train Loss: 0.036441 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.371s | Train Loss: 0.034778 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.381s | Train Loss: 0.033309 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.373s | Train Loss: 0.032359 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.373s | Train Loss: 0.030993 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.377s | Train Loss: 0.030174 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.380s | Train Loss: 0.029563 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.377s | Train Loss: 0.028607 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.371s | Train Loss: 0.027936 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.371s | Train Loss: 0.027272 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.379s | Train Loss: 0.026460 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.375s | Train Loss: 0.026046 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.372s | Train Loss: 0.025472 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.374s | Train Loss: 0.025068 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.378s | Train Loss: 0.024647 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.373s | Train Loss: 0.024170 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.372s | Train Loss: 0.023792 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.374s | Train Loss: 0.023447 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.378s | Train Loss: 0.023253 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.375s | Train Loss: 0.022738 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.381s | Train Loss: 0.022446 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.377s | Train Loss: 0.022175 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.370s | Train Loss: 0.022140 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.379s | Train Loss: 0.021782 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.369s | Train Loss: 0.021670 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.373s | Train Loss: 0.021452 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.374s | Train Loss: 0.021339 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.372s | Train Loss: 0.021075 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.371s | Train Loss: 0.021044 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.374s | Train Loss: 0.020954 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.376s | Train Loss: 0.020669 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.376s | Train Loss: 0.020608 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.378s | Train Loss: 0.020398 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.379s | Train Loss: 0.020302 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.376s | Train Loss: 0.020062 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.374s | Train Loss: 0.019926 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.369s | Train Loss: 0.019670 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.375s | Train Loss: 0.019711 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.383s | Train Loss: 0.019485 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.374s | Train Loss: 0.019378 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.373s | Train Loss: 0.019345 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.382s | Train Loss: 0.019121 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.373s | Train Loss: 0.019179 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.365s | Train Loss: 0.019169 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.394s | Train Loss: 0.019195 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.376s | Train Loss: 0.018989 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.375s | Train Loss: 0.018864 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.379s | Train Loss: 0.018631 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.368s | Train Loss: 0.018740 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.377s | Train Loss: 0.018673 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.378s | Train Loss: 0.018336 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.376s | Train Loss: 0.018420 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.369s | Train Loss: 0.018340 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.388s | Train Loss: 0.018083 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.378s | Train Loss: 0.018014 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.378s | Train Loss: 0.018099 |\n",
      "INFO:root:Pretraining Time: 26.316s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017569\n",
      "INFO:root:Test AUC: 63.85%\n",
      "INFO:root:Test AUC: 63.85%\n",
      "INFO:root:Test Time: 0.234s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.687s | Train Loss: 0.617886 || Validation Loss: 0.005486 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.701s | Train Loss: 0.397455 || Validation Loss: 0.004273 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.738s | Train Loss: 0.305912 || Validation Loss: 0.003176 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.811s | Train Loss: 0.269174 || Validation Loss: 0.002831 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.737s | Train Loss: 0.253776 || Validation Loss: 0.002722 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.756s | Train Loss: 0.247414 || Validation Loss: 0.002272 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.786s | Train Loss: 0.228663 || Validation Loss: 0.002216 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.769s | Train Loss: 0.227662 || Validation Loss: 0.002303 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.863s | Train Loss: 0.218137 || Validation Loss: 0.002304 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.830s | Train Loss: 0.215387 || Validation Loss: 0.002185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.876s | Train Loss: 0.212953 || Validation Loss: 0.002068 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.938s | Train Loss: 0.205821 || Validation Loss: 0.001976 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.861s | Train Loss: 0.206510 || Validation Loss: 0.001953 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.912s | Train Loss: 0.208272 || Validation Loss: 0.002047 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.980s | Train Loss: 0.200867 || Validation Loss: 0.001958 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.914s | Train Loss: 0.199890 || Validation Loss: 0.001986 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.982s | Train Loss: 0.198992 || Validation Loss: 0.002077 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.992s | Train Loss: 0.198513 || Validation Loss: 0.001978 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.025s | Train Loss: 0.198004 || Validation Loss: 0.001885 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.050s | Train Loss: 0.193832 || Validation Loss: 0.001949 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.034s | Train Loss: 0.195193 || Validation Loss: 0.001843 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.064s | Train Loss: 0.193601 || Validation Loss: 0.002022 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.082s | Train Loss: 0.184806 || Validation Loss: 0.001745 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.138s | Train Loss: 0.186805 || Validation Loss: 0.001888 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.092s | Train Loss: 0.186346 || Validation Loss: 0.002048 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.152s | Train Loss: 0.187226 || Validation Loss: 0.001838 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.083s | Train Loss: 0.185961 || Validation Loss: 0.001866 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.194s | Train Loss: 0.186212 || Validation Loss: 0.002003 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.225s | Train Loss: 0.187723 || Validation Loss: 0.001935 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.226s | Train Loss: 0.183183 || Validation Loss: 0.001796 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 29.151s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.826%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.001948\n",
      "INFO:root:Test AUC: 84.85%\n",
      "INFO:root:Test Time: 0.208s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1157})\n",
      "class weights 0.0434684600067626 0.9565315399932374\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6105482032625075\n",
      "validation AUC 0.6784470523151874\n",
      "validation AUC 0.7182784991537966\n",
      "validation AUC 0.7474342348505537\n",
      "validation AUC 0.773962323248212\n",
      "validation AUC 0.7866183669455558\n",
      "validation AUC 0.7967447695774602\n",
      "validation AUC 0.8043027064673525\n",
      "validation AUC 0.8104292494768567\n",
      "validation AUC 0.816919302888458\n",
      "validation AUC 0.8213566904347194\n",
      "validation AUC 0.825244463492565\n",
      "validation AUC 0.8268898959630637\n",
      "validation AUC 0.8263764215227489\n",
      "validation AUC 0.8304429578443621\n",
      "validation AUC 0.8294204762493661\n",
      "validation AUC 0.8329223173351116\n",
      "validation AUC 0.8325392748640232\n",
      "validation AUC 0.83447349779472\n",
      "validation AUC 0.8361213248793168\n",
      "validation AUC 0.8379227717796538\n",
      "validation AUC 0.838795874686545\n",
      "validation AUC 0.8406703041032192\n",
      "validation AUC 0.8391019728853055\n",
      "validation AUC 0.8401939062711856\n",
      "validation AUC 0.8416954756480517\n",
      "validation AUC 0.8422887572548826\n",
      "validation AUC 0.8425359958391185\n",
      "validation AUC 0.8441450913710241\n",
      "validation AUC 0.8441054950965754\n",
      "validation AUC 0.8441054950965754\n",
      "train auc 0.8262365503622819\n",
      "Test AUC 0.8484984030219778\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 5.68450971e-05 ... 9.97915680e-01\n",
      " 9.97915680e-01 1.00000000e+00] true positive rate [0.         0.         0.         ... 0.99972004 1.         1.        ] tresholds [5.48348713e+00 4.48348713e+00 3.74697638e+00 ... 6.10438222e-03\n",
      " 6.07203133e-03 1.47955841e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Load config is None\n",
      "INFO:root:Log file is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test/log.txt\n",
      "INFO:root:Data path is data\n",
      "INFO:root:Export path is /Users/glrz/Desktop/Thesis/src/log/DeepSAD/cancer_test\n",
      "INFO:root:Dataset: cancer\n",
      "INFO:root:Normal class: 0\n",
      "INFO:root:Known outlier selected by farthest point algo: 0\n",
      "INFO:root:Ratio of labeled normal train samples: 0.05\n",
      "INFO:root:Ratio of unlabeled normal train samples: 0.05\n",
      "INFO:root:Ratio of labeled anomalous samples: 0.05\n",
      "INFO:root:Pollution ratio of unlabeled train data: 0.00\n",
      "INFO:root:Known anomaly class: 1\n",
      "INFO:root:Network: cancer_mlp\n",
      "INFO:root:Eta-parameter: 1.00\n",
      "INFO:root:Computation device: cpu\n",
      "INFO:root:Number of threads: 0\n",
      "INFO:root:Number of dataloader workers: 0\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/datasets/cancer.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  np_semi_targets_w= torch.clone(torch.tensor(self.train_set.semi_targets))\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:61: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_train = df_tr.drop(['outcome'], 1)\n",
      "/Users/glrz/Desktop/Thesis/src/base/cancer_dataset.py:62: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  X_test = df_te.drop(['outcome'], 1)\n",
      "INFO:root:Pretraining: True\n",
      "INFO:root:Pretraining optimizer: adam\n",
      "INFO:root:Pretraining learning rate: 0.0001\n",
      "INFO:root:Pretraining epochs: 70\n",
      "INFO:root:Pretraining learning rate scheduler milestones: ()\n",
      "INFO:root:Pretraining batch size: 128\n",
      "INFO:root:Pretraining weight decay: 0.0005\n",
      "INFO:root:Starting pretraining...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/070 | Train Time: 0.390s | Train Loss: 0.198223 |\n",
      "INFO:root:| Epoch: 002/070 | Train Time: 0.367s | Train Loss: 0.154653 |\n",
      "INFO:root:| Epoch: 003/070 | Train Time: 0.374s | Train Loss: 0.121932 |\n",
      "INFO:root:| Epoch: 004/070 | Train Time: 0.371s | Train Loss: 0.098703 |\n",
      "INFO:root:| Epoch: 005/070 | Train Time: 0.371s | Train Loss: 0.082245 |\n",
      "INFO:root:| Epoch: 006/070 | Train Time: 0.372s | Train Loss: 0.070051 |\n",
      "INFO:root:| Epoch: 007/070 | Train Time: 0.369s | Train Loss: 0.061679 |\n",
      "INFO:root:| Epoch: 008/070 | Train Time: 0.371s | Train Loss: 0.055794 |\n",
      "INFO:root:| Epoch: 009/070 | Train Time: 0.372s | Train Loss: 0.050776 |\n",
      "INFO:root:| Epoch: 010/070 | Train Time: 0.373s | Train Loss: 0.046693 |\n",
      "INFO:root:| Epoch: 011/070 | Train Time: 0.370s | Train Loss: 0.043600 |\n",
      "INFO:root:| Epoch: 012/070 | Train Time: 0.370s | Train Loss: 0.041015 |\n",
      "INFO:root:| Epoch: 013/070 | Train Time: 0.363s | Train Loss: 0.038547 |\n",
      "INFO:root:| Epoch: 014/070 | Train Time: 0.388s | Train Loss: 0.036275 |\n",
      "INFO:root:| Epoch: 015/070 | Train Time: 0.367s | Train Loss: 0.034612 |\n",
      "INFO:root:| Epoch: 016/070 | Train Time: 0.385s | Train Loss: 0.033335 |\n",
      "INFO:root:| Epoch: 017/070 | Train Time: 0.373s | Train Loss: 0.032036 |\n",
      "INFO:root:| Epoch: 018/070 | Train Time: 0.370s | Train Loss: 0.030851 |\n",
      "INFO:root:| Epoch: 019/070 | Train Time: 0.361s | Train Loss: 0.029898 |\n",
      "INFO:root:| Epoch: 020/070 | Train Time: 0.354s | Train Loss: 0.028610 |\n",
      "INFO:root:| Epoch: 021/070 | Train Time: 0.366s | Train Loss: 0.027882 |\n",
      "INFO:root:| Epoch: 022/070 | Train Time: 0.365s | Train Loss: 0.027329 |\n",
      "INFO:root:| Epoch: 023/070 | Train Time: 0.364s | Train Loss: 0.026641 |\n",
      "INFO:root:| Epoch: 024/070 | Train Time: 0.366s | Train Loss: 0.026099 |\n",
      "INFO:root:| Epoch: 025/070 | Train Time: 0.368s | Train Loss: 0.025492 |\n",
      "INFO:root:| Epoch: 026/070 | Train Time: 0.362s | Train Loss: 0.025131 |\n",
      "INFO:root:| Epoch: 027/070 | Train Time: 0.370s | Train Loss: 0.024704 |\n",
      "INFO:root:| Epoch: 028/070 | Train Time: 0.378s | Train Loss: 0.024156 |\n",
      "INFO:root:| Epoch: 029/070 | Train Time: 0.368s | Train Loss: 0.024070 |\n",
      "INFO:root:| Epoch: 030/070 | Train Time: 0.365s | Train Loss: 0.023498 |\n",
      "INFO:root:| Epoch: 031/070 | Train Time: 0.369s | Train Loss: 0.023174 |\n",
      "INFO:root:| Epoch: 032/070 | Train Time: 0.367s | Train Loss: 0.023129 |\n",
      "INFO:root:| Epoch: 033/070 | Train Time: 0.360s | Train Loss: 0.022783 |\n",
      "INFO:root:| Epoch: 034/070 | Train Time: 0.361s | Train Loss: 0.022362 |\n",
      "INFO:root:| Epoch: 035/070 | Train Time: 0.362s | Train Loss: 0.022140 |\n",
      "INFO:root:| Epoch: 036/070 | Train Time: 0.373s | Train Loss: 0.022152 |\n",
      "INFO:root:| Epoch: 037/070 | Train Time: 0.365s | Train Loss: 0.021979 |\n",
      "INFO:root:| Epoch: 038/070 | Train Time: 0.371s | Train Loss: 0.022065 |\n",
      "INFO:root:| Epoch: 039/070 | Train Time: 0.367s | Train Loss: 0.021392 |\n",
      "INFO:root:| Epoch: 040/070 | Train Time: 0.367s | Train Loss: 0.021563 |\n",
      "INFO:root:| Epoch: 041/070 | Train Time: 0.372s | Train Loss: 0.021096 |\n",
      "INFO:root:| Epoch: 042/070 | Train Time: 0.365s | Train Loss: 0.021184 |\n",
      "INFO:root:| Epoch: 043/070 | Train Time: 0.368s | Train Loss: 0.020930 |\n",
      "INFO:root:| Epoch: 044/070 | Train Time: 0.360s | Train Loss: 0.020881 |\n",
      "INFO:root:| Epoch: 045/070 | Train Time: 0.372s | Train Loss: 0.020640 |\n",
      "INFO:root:| Epoch: 046/070 | Train Time: 0.367s | Train Loss: 0.020378 |\n",
      "INFO:root:| Epoch: 047/070 | Train Time: 0.370s | Train Loss: 0.020487 |\n",
      "INFO:root:| Epoch: 048/070 | Train Time: 0.371s | Train Loss: 0.020392 |\n",
      "INFO:root:| Epoch: 049/070 | Train Time: 0.370s | Train Loss: 0.020275 |\n",
      "INFO:root:| Epoch: 050/070 | Train Time: 0.365s | Train Loss: 0.020188 |\n",
      "INFO:root:| Epoch: 051/070 | Train Time: 0.363s | Train Loss: 0.020091 |\n",
      "INFO:root:| Epoch: 052/070 | Train Time: 0.371s | Train Loss: 0.020124 |\n",
      "INFO:root:| Epoch: 053/070 | Train Time: 0.359s | Train Loss: 0.019847 |\n",
      "INFO:root:| Epoch: 054/070 | Train Time: 0.365s | Train Loss: 0.019814 |\n",
      "INFO:root:| Epoch: 055/070 | Train Time: 0.367s | Train Loss: 0.019822 |\n",
      "INFO:root:| Epoch: 056/070 | Train Time: 0.368s | Train Loss: 0.019603 |\n",
      "INFO:root:| Epoch: 057/070 | Train Time: 0.367s | Train Loss: 0.019378 |\n",
      "INFO:root:| Epoch: 058/070 | Train Time: 0.361s | Train Loss: 0.019344 |\n",
      "INFO:root:| Epoch: 059/070 | Train Time: 0.372s | Train Loss: 0.019304 |\n",
      "INFO:root:| Epoch: 060/070 | Train Time: 0.362s | Train Loss: 0.019230 |\n",
      "INFO:root:| Epoch: 061/070 | Train Time: 0.368s | Train Loss: 0.019256 |\n",
      "INFO:root:| Epoch: 062/070 | Train Time: 0.372s | Train Loss: 0.018999 |\n",
      "INFO:root:| Epoch: 063/070 | Train Time: 0.371s | Train Loss: 0.018734 |\n",
      "INFO:root:| Epoch: 064/070 | Train Time: 0.370s | Train Loss: 0.018720 |\n",
      "INFO:root:| Epoch: 065/070 | Train Time: 0.361s | Train Loss: 0.018378 |\n",
      "INFO:root:| Epoch: 066/070 | Train Time: 0.366s | Train Loss: 0.018486 |\n",
      "INFO:root:| Epoch: 067/070 | Train Time: 0.361s | Train Loss: 0.018228 |\n",
      "INFO:root:| Epoch: 068/070 | Train Time: 0.359s | Train Loss: 0.018174 |\n",
      "INFO:root:| Epoch: 069/070 | Train Time: 0.370s | Train Loss: 0.018080 |\n",
      "INFO:root:| Epoch: 070/070 | Train Time: 0.357s | Train Loss: 0.017859 |\n",
      "INFO:root:Pretraining Time: 25.760s\n",
      "INFO:root:Finished pretraining.\n",
      "INFO:root:Testing autoencoder...\n",
      "INFO:root:Test Loss: 0.017519\n",
      "INFO:root:Test AUC: 60.90%\n",
      "INFO:root:Test AUC: 60.90%\n",
      "INFO:root:Test Time: 0.221s\n",
      "INFO:root:Finished testing autoencoder.\n",
      "INFO:root:Training optimizer: adam\n",
      "INFO:root:Training learning rate: 0.0001\n",
      "INFO:root:Training epochs: 30\n",
      "INFO:root:Training learning rate scheduler milestones: (50,)\n",
      "INFO:root:Training batch size: 128\n",
      "INFO:root:Training weight decay: 5e-07\n",
      "INFO:root:Initializing center c...\n",
      "INFO:root:Center c initialized.\n",
      "INFO:root:Starting training...\n",
      "/Users/glrz/miniforge3/envs/torchenv/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "INFO:root:| Epoch: 001/030 | Train Time: 0.702s | Train Loss: 0.510132 || Validation Loss: 0.004775 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 002/030 | Train Time: 0.654s | Train Loss: 0.345052 || Validation Loss: 0.003512 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 003/030 | Train Time: 0.695s | Train Loss: 0.280077 || Validation Loss: 0.002684 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 004/030 | Train Time: 0.713s | Train Loss: 0.263137 || Validation Loss: 0.002247 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 005/030 | Train Time: 0.751s | Train Loss: 0.246305 || Validation Loss: 0.002603 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 006/030 | Train Time: 0.780s | Train Loss: 0.242612 || Validation Loss: 0.002314 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 007/030 | Train Time: 0.829s | Train Loss: 0.232423 || Validation Loss: 0.002268 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 008/030 | Train Time: 0.804s | Train Loss: 0.225425 || Validation Loss: 0.002045 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 009/030 | Train Time: 0.888s | Train Loss: 0.220432 || Validation Loss: 0.001975 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 010/030 | Train Time: 0.831s | Train Loss: 0.222048 || Validation Loss: 0.002252 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 011/030 | Train Time: 0.806s | Train Loss: 0.219760 || Validation Loss: 0.002034 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 012/030 | Train Time: 0.868s | Train Loss: 0.214099 || Validation Loss: 0.002192 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 013/030 | Train Time: 0.859s | Train Loss: 0.213541 || Validation Loss: 0.002115 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 014/030 | Train Time: 0.892s | Train Loss: 0.213972 || Validation Loss: 0.002185 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 015/030 | Train Time: 0.934s | Train Loss: 0.210297 || Validation Loss: 0.001848 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 016/030 | Train Time: 0.884s | Train Loss: 0.201361 || Validation Loss: 0.002037 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 017/030 | Train Time: 0.952s | Train Loss: 0.203678 || Validation Loss: 0.002020 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 018/030 | Train Time: 0.944s | Train Loss: 0.196567 || Validation Loss: 0.001952 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 019/030 | Train Time: 1.025s | Train Loss: 0.198947 || Validation Loss: 0.001967 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 020/030 | Train Time: 1.035s | Train Loss: 0.196745 || Validation Loss: 0.001963 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 021/030 | Train Time: 1.008s | Train Loss: 0.197516 || Validation Loss: 0.001981 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 022/030 | Train Time: 1.024s | Train Loss: 0.203731 || Validation Loss: 0.002223 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 023/030 | Train Time: 1.044s | Train Loss: 0.196092 || Validation Loss: 0.001855 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 024/030 | Train Time: 1.087s | Train Loss: 0.198353 || Validation Loss: 0.001815 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 025/030 | Train Time: 1.065s | Train Loss: 0.194973 || Validation Loss: 0.001758 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 026/030 | Train Time: 1.103s | Train Loss: 0.194779 || Validation Loss: 0.001783 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 027/030 | Train Time: 1.046s | Train Loss: 0.186949 || Validation Loss: 0.001739 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 028/030 | Train Time: 1.124s | Train Loss: 0.182671 || Validation Loss: 0.001786 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 029/030 | Train Time: 1.175s | Train Loss: 0.190907 || Validation Loss: 0.001736 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:| Epoch: 030/030 | Train Time: 1.205s | Train Loss: 0.187976 || Validation Loss: 0.001790 || Validation number of baches in epoch: 441.000000 |\n",
      "INFO:root:Training Time: 28.386s\n",
      "INFO:root:Finished training.\n",
      "INFO:root:Training AUC: 0.831%\n",
      "INFO:root:Starting testing...\n",
      "INFO:root:Test Loss: 0.002008\n",
      "INFO:root:Test AUC: 86.08%\n",
      "INFO:root:Test Time: 0.187s\n",
      "INFO:root:Finished testing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method 6\n",
      "experiment_method type 6\n",
      "method num 6\n",
      "root data dataset name cancer balance 1\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting train data from cancer_dataset\n",
      "ten size torch.Size([262941, 40])\n",
      "ten size torch.Size([262941, 40]) torch.Size([262941]) torch.Size([262941])\n",
      "shape (262941, 40)\n",
      "cancer file train y targets                    0\n",
      "count  262941.000000\n",
      "mean        0.063421\n",
      "std         0.243719\n",
      "min         0.000000\n",
      "25%         0.000000\n",
      "50%         0.000000\n",
      "75%         0.000000\n",
      "max         1.000000\n",
      "experiment_method in preprocessing 6\n",
      "5_per_labeled_5_unlabeled\n",
      "incidens loaded sizes 834 834 12313 12313\n",
      "final sizes 12313 834 12313 834\n",
      "len of selected to training 26294\n",
      "apply smote\n",
      "Original dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 834})\n",
      "Resampled dataset shape Counter({0.0: 13147, 1.0: 12313, -1.0: 1157})\n",
      "class weights 0.0434684600067626 0.9565315399932374\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting val data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "test size 56347\n",
      "Original dataset shape Counter({0: 246265, 1: 16676})\n",
      "shape of validation (56347, 40)\n",
      "shape of test (56347, 40)\n",
      "in getting test data from cancer_dataset\n",
      "ten size torch.Size([56347, 40]) torch.Size([56347]) torch.Size([56347])\n",
      "check error cancer_mlp\n",
      "encoder dimentions 40 [32, 16] decoder dim are just diversed\n",
      "in ae trainer\n",
      "with age var\n",
      "validation AUC 0.6053942780004354\n",
      "validation AUC 0.6686636621887028\n",
      "validation AUC 0.7221699093643885\n",
      "validation AUC 0.7522800664021169\n",
      "validation AUC 0.7727288016520922\n",
      "validation AUC 0.7896813151774047\n",
      "validation AUC 0.799669705556122\n",
      "validation AUC 0.8081747016789758\n",
      "validation AUC 0.8174258755188064\n",
      "validation AUC 0.8219621473238963\n",
      "validation AUC 0.8259765129863647\n",
      "validation AUC 0.828189647263765\n",
      "validation AUC 0.8304242346227992\n",
      "validation AUC 0.8346508056652524\n",
      "validation AUC 0.8350405903053676\n",
      "validation AUC 0.838724493900971\n",
      "validation AUC 0.8409467383546724\n",
      "validation AUC 0.8421438538344583\n",
      "validation AUC 0.843024478490299\n",
      "validation AUC 0.844694128491374\n",
      "validation AUC 0.8461538320317631\n",
      "validation AUC 0.8469398933768806\n",
      "validation AUC 0.8475413326357378\n",
      "validation AUC 0.8480887653646423\n",
      "validation AUC 0.849180624251417\n",
      "validation AUC 0.8506211895037997\n",
      "validation AUC 0.8502822611457579\n",
      "validation AUC 0.8507973160313774\n",
      "validation AUC 0.8529265164239666\n",
      "validation AUC 0.8528190355005265\n",
      "validation AUC 0.8528190355005265\n",
      "train auc 0.8311887676715008\n",
      "Test AUC 0.8608214265063873\n",
      "test false positive rates [0.00000000e+00 1.89483657e-05 1.89483657e-05 ... 9.96854571e-01\n",
      " 9.96854571e-01 1.00000000e+00] true positive rate [0.         0.         0.00307951 ... 0.99972004 1.         1.        ] tresholds [4.707479   3.707479   2.98470616 ... 0.01331235 0.01330749 0.00691905]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for i in {1..3}\n",
    "do\n",
    "    python main.py cancer cancer_mlp log/DeepSAD/cancer_test data --experiment_name \"5_per_labeled_5_unlabeled_smote_m1.5_exc_balanced_batch\" --index_baseline_name \"5_per_labeled_5_unlabeled\" --iteration_num $i --experiment_method 6 --balanced_train 1 --balanced_batches 0 --minority_loss 1 --ratio_known_normal 0.05 --ratio_unknown_normal 0.05 --ratio_known_outlier 0.05 --ratio_unknown_outlier 0.05 --lr 0.0001 --n_epochs 30 --lr_milestone 50 --batch_size 128 --weight_decay 0.5e-6 --pretrain True --ae_lr 0.0001 --ae_n_epochs 70 --ae_batch_size 128 --ae_weight_decay 0.5e-3 --normal_class 0 --known_outlier_class 1 --n_known_outlier_classes 1 \n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6472d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb1bd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"click>=7,<8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "561e3e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!brew install libomp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11a3504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "file_path = '/Users/glrz/Desktop/Thesis/src/datasets/log/DeepSAD/cancer_test/80_per_labeled_0_unlabeled/index_split_iteration1.csv'\n",
    "data_to_open = pd.read_csv(file_path)\n",
    "idx_known_outlier = data_to_open['known_outlier']\n",
    "idx_unlabeled_outlier = data_to_open['unlabeled_outlier']\n",
    "idx_known_normal = data_to_open['known_normal']\n",
    "idx_unlabeled_normal = data_to_open['unlabeled_normal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e244569",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_known_outlier.dropna().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2916c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miniforge3-torchenv]",
   "language": "python",
   "name": "conda-env-miniforge3-torchenv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
